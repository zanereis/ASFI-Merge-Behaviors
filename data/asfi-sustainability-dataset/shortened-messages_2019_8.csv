numid,list,messageid,senderaliasid,senderalias,referenceid,recipaliasid,recipalias,datetime,subject,body,from_commit
19615,54,JIRA.12965744.1462571211000.33745.1465056899194@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.12965744.1462571211000@Atlassian.JIRA,,,2016-06-04 09:14:59-07,[jira] [Commented] (AIRFLOW-64) dags_folder not handled correctly,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-64?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15315555#comment-15315555 ] 

ASF subversion and git services commented on AIRFLOW-64:
--------------------------------------------------------

Commit d32fb8d974f85a316d0cb96d5b6fb2d3502972be in incubator-airflow''s branch refs/heads/master from [~giacomo.tag@gmail.com]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d32fb8d ]

[AIRFLOW-64] Add note about relative DAGS_FOLDER

Closes #1474 from itajaja/patch-1.


> dags_folder not handled correctly
> ---------------------------------
>
>                 Key: AIRFLOW-64
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-64
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli, executor, scheduler
>    Affects Versions: Airflow 1.7.0
>         Environment: running on mac os X
> airflow.cfg:
> ```
> airflow_home = .
> dags_folder = ./dags
> ```
>            Reporter: Giacomo Tagliabe
>
> running `airflow list_dags` with the command above from a location with a `dags` subdirectory, the cli erros the following message:
> ```
> airflow.utils.AirflowException: subdir has to be part of your DAGS_FOLDER as defined in your airflow.cfg
> ```
> if I change the airflow.cfg to `dags_folder = dags`, list_dags seem to work correctly, but any other task doesn''t seem to handle the dags_folder correctly. Here is what `airflow scheduler` outputs when it tries to execute a scheduled dag:
> ```
> DAG [process_deleted_runs] could not be found in /Users/giacomo/workspace/qsi/scheduler/dags/Users/giacomo/workspace/qsi/scheduler/process_deleted_runs.py
> ```
> As you see, it seems that the absolute path (`/Users/giacomo/workspace/qsi/scheduler/`) is chained twice.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",t
19616,54,JIRA.12965744.1462571211000.33748.1465056959189@Atlassian.JIRA,1277,Jeremiah Lowin (JIRA),JIRA.12965744.1462571211000@Atlassian.JIRA,,,2016-06-04 09:15:59-07,[jira] [Resolved] (AIRFLOW-64) dags_folder not handled correctly,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-64?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Jeremiah Lowin resolved AIRFLOW-64.
-----------------------------------
    Resolution: Fixed

Issue resolved by pull request #1474
[https://github.com/apache/incubator-airflow/pull/1474]

> dags_folder not handled correctly
> ---------------------------------
>
>                 Key: AIRFLOW-64
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-64
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli, executor, scheduler
>    Affects Versions: Airflow 1.7.0
>         Environment: running on mac os X
> airflow.cfg:
> ```
> airflow_home = .
> dags_folder = ./dags
> ```
>            Reporter: Giacomo Tagliabe
>
> running `airflow list_dags` with the command above from a location with a `dags` subdirectory, the cli erros the following message:
> ```
> airflow.utils.AirflowException: subdir has to be part of your DAGS_FOLDER as defined in your airflow.cfg
> ```
> if I change the airflow.cfg to `dags_folder = dags`, list_dags seem to work correctly, but any other task doesn''t seem to handle the dags_folder correctly. Here is what `airflow scheduler` outputs when it tries to execute a scheduled dag:
> ```
> DAG [process_deleted_runs] could not be found in /Users/giacomo/workspace/qsi/scheduler/dags/Users/giacomo/workspace/qsi/scheduler/process_deleted_runs.py
> ```
> As you see, it seems that the absolute path (`/Users/giacomo/workspace/qsi/scheduler/`) is chained twice.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",t
19617,54,JIRA.12965744.1462571211000.33825.1465060019165@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.12965744.1462571211000@Atlassian.JIRA,,,2016-06-04 10:06:59-07,[jira] [Updated] (AIRFLOW-64) dags_folder not handled correctly,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-64?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-64:
-----------------------------------
    Assignee: Giacomo Tagliabe

> dags_folder not handled correctly
> ---------------------------------
>
>                 Key: AIRFLOW-64
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-64
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli, executor, scheduler
>    Affects Versions: Airflow 1.7.0
>         Environment: running on mac os X
> airflow.cfg:
> ```
> airflow_home = .
> dags_folder = ./dags
> ```
>            Reporter: Giacomo Tagliabe
>            Assignee: Giacomo Tagliabe
>
> running `airflow list_dags` with the command above from a location with a `dags` subdirectory, the cli erros the following message:
> ```
> airflow.utils.AirflowException: subdir has to be part of your DAGS_FOLDER as defined in your airflow.cfg
> ```
> if I change the airflow.cfg to `dags_folder = dags`, list_dags seem to work correctly, but any other task doesn''t seem to handle the dags_folder correctly. Here is what `airflow scheduler` outputs when it tries to execute a scheduled dag:
> ```
> DAG [process_deleted_runs] could not be found in /Users/giacomo/workspace/qsi/scheduler/dags/Users/giacomo/workspace/qsi/scheduler/process_deleted_runs.py
> ```
> As you see, it seems that the absolute path (`/Users/giacomo/workspace/qsi/scheduler/`) is chained twice.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",t
19618,54,JIRA.12975651.1464993953000.33830.1465060199183@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.12975651.1464993953000@Atlassian.JIRA,,,2016-06-04 10:09:59-07,"[jira] [Commented] (AIRFLOW-211) Fix bug when PR tool tries to
 ""close"" an issue instead of ""resolving"" it","
    [ https://issues.apache.org/jira/browse/AIRFLOW-211?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15315573#comment-15315573 ] 

ASF subversion and git services commented on AIRFLOW-211:
---------------------------------------------------------

Commit c78101eaa1154c266e6b4d970916a8d36b563343 in incubator-airflow''s branch refs/heads/master from jlowin
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=c78101e ]

[AIRFLOW-211] Fix JIRA ""resolve"" vs ""close"" behavior

Closes #1571 from jlowin/pr-tool-8.


> Fix bug when PR tool tries to ""close"" an issue instead of ""resolving"" it
> ------------------------------------------------------------------------
>
>                 Key: AIRFLOW-211
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-211
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: PR tool
>            Reporter: Jeremiah Lowin
>            Assignee: Jeremiah Lowin
>            Priority: Minor
>
> Inadvertently introduced an issue when fixing AIRFLOW-207: ""closing"" an issue (as opposed to ""resolving"" it) sometimes causes a crash. 
> I don''t know why closing doesn''t work; it claims that the provided fields are invalid, but they are valid (fix-version, resolution). Let''s just go back to the old behavior: ""resolving"" rather than ""closing"".



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",t
666938,23,CA+tQj+V184OvSNZOJUpoSZ_knEEvKi0zcU6o8TUsFnaXe9PK5g@mail.gmail.com,25072,slack-milagro-dev-apache,NULL,,,2016-05-27 05:15:05-07,Re: [FROM MILAGRO''S SLACK],"NOTE: ALL CHAT IN THIS CHANNEL is sent to the
[url=dev@milagro.incubating.apache.org]dev@milagro.incubating.apache.org[/url]
mail list. If you want to have a private chat, please do so in a one
on one message.

@brian pinned a message to this channel.

@brian set the channel topic: The CHAT version of
[url=dev@milagro.incubating.apache.org]dev@milagro.incubating.apache.org[/url]
using Slack. ALL CHAT IS SENT TO
[url=dev@milagro.incubating.apache.org]dev@milagro.incubating.apache.org[/url]
.

@brian set the channel topic: ALL CHAT IN GENERAL IS SENT TO
[url=dev@milagro.incubating.apache.org]dev@milagro.incubating.apache.org[/url]
.

| By Brian Spector

NOTE: ALL CHAT IN THIS CHANNEL is sent to the
[url=dev@milagro.incubating.apache.org]dev@milagro.incubating.apache.org[/url]
mail list. If you want to have a private chat, please do so in a one
on one message.

",f
26606,54,JIRA.13109826.1508195516000.119.1508351460605@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109826.1508195516000@Atlassian.JIRA,,,2017-10-18 11:31:00-07,"[jira] [Commented] (AIRFLOW-1718) Increase num_retries polling
 value on Dataproc hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1718?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209792#comment-16209792 ] 

ASF subversion and git services commented on AIRFLOW-1718:
----------------------------------------------------------

Commit 6078e753aac35aa4f5971a719d9f736c35396770 in incubator-airflow''s branch refs/heads/master from [~cjqian]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6078e75 ]

[AIRFLOW-1718] Set num_retries on Dataproc job request execution

Closes #2696 from cjqian/1718


> Increase num_retries polling value on Dataproc hook
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1718
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1718
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Crystal Qian
>            Assignee: Crystal Qian
>            Priority: Minor
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> Currently, `num_retries = 0` when execute() is called (https://google.github.io/google-api-python-client/docs/epy/googleapiclient.http.HttpRequest-class.html#execute), which causes intermittent 500 errors (https://stackoverflow.com/questions/46522261/deadline-exceeded-when-airflow-runs-spark-jobs). We should increase this to allow retries for internal Dataproc queries to other services in the short-term; also seeing if the `num_retries` count can be increased at the _google-api-python-client_ level in the long-term.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26607,54,JIRA.13109826.1508195516000.134.1508351580585@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13109826.1508195516000@Atlassian.JIRA,,,2017-10-18 11:33:00-07,"[jira] [Resolved] (AIRFLOW-1718) Increase num_retries polling value
 on Dataproc hook","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1718?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1718.
--------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> Increase num_retries polling value on Dataproc hook
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1718
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1718
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Crystal Qian
>            Assignee: Crystal Qian
>            Priority: Minor
>             Fix For: 1.10.0
>
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> Currently, `num_retries = 0` when execute() is called (https://google.github.io/google-api-python-client/docs/epy/googleapiclient.http.HttpRequest-class.html#execute), which causes intermittent 500 errors (https://stackoverflow.com/questions/46522261/deadline-exceeded-when-airflow-runs-spark-jobs). We should increase this to allow retries for internal Dataproc queries to other services in the short-term; also seeing if the `num_retries` count can be increased at the _google-api-python-client_ level in the long-term.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26608,54,JIRA.13094153.1502471732000.284.1508352420413@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13094153.1502471732000@Atlassian.JIRA,,,2017-10-18 11:47:00-07,"[jira] [Resolved] (AIRFLOW-1506) Improve
 DataprocClusterCreateOperator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1506?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1506.
--------------------------------------
    Resolution: Duplicate

> Improve DataprocClusterCreateOperator
> -------------------------------------
>
>                 Key: AIRFLOW-1506
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1506
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>            Reporter: Yu Ishikawa
>            Assignee: Jessica Chen Fan 
>
> h2. Goals
> {{DataprocClusterCreateOperator}} should support {{$. gceClusterConfig.serviceAccountScopes}} to specify scopes for a Dataproc cluster.
> For example, I guess some users would like to store the result to Google Datastore and like this. In such a case, we have to put additional scopes to access to the produces from the Dataproc cluster.
> https://cloud.google.com/dataproc/docs/reference/rest/v1/projects.regions.clusters#gceclusterconfig



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26609,54,JIRA.13110016.1508253770000.587.1508354880513@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-18 12:28:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209874#comment-16209874 ] 

ASF subversion and git services commented on AIRFLOW-1723:
----------------------------------------------------------

Commit 7cb818bbacb2a2695282471591a9e323d8efbf5c in incubator-airflow''s branch refs/heads/master from [~fenglu]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=7cb818b ]

[AIRFLOW-1723] Support sendgrid in email backend

Closes #2695 from fenglu-g/master


> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26610,54,JIRA.13110016.1508253770000.591.1508354880588@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-18 12:28:00-07,[jira] [Resolved] (AIRFLOW-1723) Support sendgrid in email backend,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1723.
--------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
666946,23,DM2PR02MB133870E39116A5743F71CD3EAA470@DM2PR02MB1338.namprd02.prod.outlook.com,25068,Go Yamamoto,1464792782.7490.219.camel@mimir.webthing.com,,,2016-06-01 10:46:15-07,Re: Multiple calls to /rps/accessnumber,"Hi Nick,

The unsuccessful calls are intentional.
The Javascript client attempts polling for WebOTT to be authenticated by th=
e user using the Mobile apps.
The calls will turn successful right after you authenticate the Access Numb=
er using the Mobile apps.

About the www-autentivate header, I think we need more information to repro=
duce it.
Accessing our sample Milagro MFA server (https://public.milagro.io/rps/acce=
ssnumber), I got the response header as shown below.

$ curl -v -X POST -d  ""{\""webOTT\"":\""428e2b2975e80e9f2119086c27ad34ea\""}"" h=
ttps://public.milagro.io/rps/accessnumber
(snip)
< HTTP/1.1 401 Unauthorized
< Date: Wed, 01 Jun 2016 17:27:39 GMT
< Server: TornadoServer/4.1
< Strict-Transport-Security: max-age=3D31536000; includeSubDomains
< Access-Control-Allow-Credentials: true
< Access-Control-Allow-Headers: Content-Type, Depth, User-Agent, X-File-Siz=
e, X-Requested-With, X-Requested-By, If-Modified-Since, X-File-Name, Cache-=
Control
< Access-Control-Allow-Methods: GET,PUT,POST,DELETE,OPTIONS
< Content-Length: 0
< Content-Type: text/html; charset=3DUTF-8
< Set-Cookie: mpindemo_session=3D05301611-977A-EA38-B168-CDE28221FEA9; Max-=
Age=3D14400; HttpOnly; Secure
<

Regards,
Go Yamamoto
________________________________________
From: Nick Kew <niq@apache.org>
Sent: Wednesday, June 1, 2016 7:53:02 AM
To: dev@milagro.incubator.apache.org
Subject: Multiple calls to /rps/accessnumber

I''ve been running some M-Pin sessions capturing
and looking at the traffic between the Miracl
Javascript client and Python RP server.

Among other things, I see repeated unsuccessful calls:

POST /rps/accessnumber HTTP/1.1^M
....
Content-Type: text/plain;charset=3DUTF-8^M
Cookie: mpindemo_session=3D""...""^M
^M
{""webOTT"":""13468f969413889e287a69ddc526fef6""}

(aside: that''s JSON being sent as text/plain)

Now the response to this is a 401, with other HTTP headers
whose legitimacy might be in question, and no body:

HTTP/1.1 401 Unauthorized^M
Server: TornadoServer/4.1^M
Www-Authenticate: Authenticate^M
[more]

I guess the ""Authenticate"" value to WWW-Authenticate is handled
by the Javascript, but shouldn''t the browser itself be presented
with something it knows?  Conventional values are Basic or Digest
(with appropriate parameters).

What I''m seeing is what looks like ""undefined"" browser behaviour
in response to an unknown WWW-Authenticate.  That is to say, the
browser sends off an identical request and receives an identical
reply, repeated many times while slower human interaction takes
place as I (successfully or otherwise) log in.

Also, at no point in the interaction is there a successful
call to /rps/accessnumber .  Always 401 as above.

How much of this is intentional, and why?

--
Nick Kew

________________________________
This email message is intended for the use of the person to whom it has bee=
n sent, and may contain information that is confidential or legally protect=
ed. If you are not the intended recipient or have received this message in =
error, you are not authorized to copy, distribute, or otherwise use this me=
ssage or its attachments. Please notify the sender immediately by return e-=
mail and permanently delete this message and any attachments. NTTI3 makes n=
o warranty that this email is error or virus free. Thank you.
________________________________

",f
26611,54,JIRA.13109971.1508245727000.668.1508355721741@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109971.1508245727000@Atlassian.JIRA,,,2017-10-18 12:42:01-07,"[jira] [Commented] (AIRFLOW-1722) Typo in
 airflow_scheduler_autorestart.sh log file name","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1722?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209887#comment-16209887 ] 

ASF subversion and git services commented on AIRFLOW-1722:
----------------------------------------------------------

Commit 97a2393f644ff438986843c04a47754a37ae0bc5 in incubator-airflow''s branch refs/heads/master from [~mschmoyer]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=97a2393 ]

[AIRFLOW-1722] Fix typo in scheduler autorestart output filename

Extra r in /tmp/airflow_scheduler_errors.txt has
been removed.

Closes #2693 from mschmo/AIRFLOW-1722


> Typo in airflow_scheduler_autorestart.sh log file name
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1722
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1722
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Matthew Schmoyer
>            Assignee: Matthew Schmoyer
>            Priority: Trivial
>   Original Estimate: 1m
>  Remaining Estimate: 1m
>
> Rename `airflow_scheduler_errrors.txt` to `airflow_scheduler_errors.txt`



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26612,54,JIRA.13109971.1508245727000.672.1508355721778@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109971.1508245727000@Atlassian.JIRA,,,2017-10-18 12:42:01-07,"[jira] [Commented] (AIRFLOW-1722) Typo in
 airflow_scheduler_autorestart.sh log file name","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1722?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209889#comment-16209889 ] 

ASF subversion and git services commented on AIRFLOW-1722:
----------------------------------------------------------

Commit fcc3c2f6e5860d5414752ace22160dbe19f68617 in incubator-airflow''s branch refs/heads/v1-9-test from [~mschmoyer]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=fcc3c2f ]

[AIRFLOW-1722] Fix typo in scheduler autorestart output filename

Extra r in /tmp/airflow_scheduler_errors.txt has
been removed.

Closes #2693 from mschmo/AIRFLOW-1722

(cherry picked from commit 97a2393f644ff438986843c04a47754a37ae0bc5)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Typo in airflow_scheduler_autorestart.sh log file name
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1722
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1722
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Matthew Schmoyer
>            Assignee: Matthew Schmoyer
>            Priority: Trivial
>   Original Estimate: 1m
>  Remaining Estimate: 1m
>
> Rename `airflow_scheduler_errrors.txt` to `airflow_scheduler_errors.txt`



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26613,54,JIRA.13109971.1508245727000.669.1508355721751@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109971.1508245727000@Atlassian.JIRA,,,2017-10-18 12:42:01-07,"[jira] [Commented] (AIRFLOW-1722) Typo in
 airflow_scheduler_autorestart.sh log file name","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1722?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209888#comment-16209888 ] 

ASF subversion and git services commented on AIRFLOW-1722:
----------------------------------------------------------

Commit 97a2393f644ff438986843c04a47754a37ae0bc5 in incubator-airflow''s branch refs/heads/master from [~mschmoyer]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=97a2393 ]

[AIRFLOW-1722] Fix typo in scheduler autorestart output filename

Extra r in /tmp/airflow_scheduler_errors.txt has
been removed.

Closes #2693 from mschmo/AIRFLOW-1722


> Typo in airflow_scheduler_autorestart.sh log file name
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1722
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1722
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Matthew Schmoyer
>            Assignee: Matthew Schmoyer
>            Priority: Trivial
>   Original Estimate: 1m
>  Remaining Estimate: 1m
>
> Rename `airflow_scheduler_errrors.txt` to `airflow_scheduler_errors.txt`



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26614,54,JIRA.13109971.1508245727000.674.1508355721795@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109971.1508245727000@Atlassian.JIRA,,,2017-10-18 12:42:01-07,"[jira] [Commented] (AIRFLOW-1722) Typo in
 airflow_scheduler_autorestart.sh log file name","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1722?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209890#comment-16209890 ] 

ASF subversion and git services commented on AIRFLOW-1722:
----------------------------------------------------------

Commit fcc3c2f6e5860d5414752ace22160dbe19f68617 in incubator-airflow''s branch refs/heads/v1-9-test from [~mschmoyer]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=fcc3c2f ]

[AIRFLOW-1722] Fix typo in scheduler autorestart output filename

Extra r in /tmp/airflow_scheduler_errors.txt has
been removed.

Closes #2693 from mschmo/AIRFLOW-1722

(cherry picked from commit 97a2393f644ff438986843c04a47754a37ae0bc5)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Typo in airflow_scheduler_autorestart.sh log file name
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1722
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1722
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Matthew Schmoyer
>            Assignee: Matthew Schmoyer
>            Priority: Trivial
>   Original Estimate: 1m
>  Remaining Estimate: 1m
>
> Rename `airflow_scheduler_errrors.txt` to `airflow_scheduler_errors.txt`



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26615,54,JIRA.13107696.1507373278000.746.1508356260494@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13107696.1507373278000@Atlassian.JIRA,,,2017-10-18 12:51:00-07,"[jira] [Commented] (AIRFLOW-1692) Master cannot be checked out on
 windows","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1692?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209905#comment-16209905 ] 

ASF subversion and git services commented on AIRFLOW-1692:
----------------------------------------------------------

Commit 2da48568116f7382258e2678e20ee0f9573d653c in incubator-airflow''s branch refs/heads/master from root
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2da4856 ]

[AIRFLOW-1692] Change test_views filename to support Windows

Closes #2673 from NielsZeilemaker/AIRFLOW-1692


> Master cannot be checked out on windows
> ---------------------------------------
>
>                 Key: AIRFLOW-1692
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1692
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>
> This file:
> /tests/www/test_logs/dag_for_testing_log_view/task_for_testing_log_view/2017-09-01T00:00:00/1.log
> Cannot be created on windows due to the colons in the directory name



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26616,54,JIRA.13107696.1507373278000.751.1508356320530@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13107696.1507373278000@Atlassian.JIRA,,,2017-10-18 12:52:00-07,"[jira] [Commented] (AIRFLOW-1692) Master cannot be checked out on
 windows","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1692?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209906#comment-16209906 ] 

ASF subversion and git services commented on AIRFLOW-1692:
----------------------------------------------------------

Commit 31805e836a04700b5881705f58b33f32f2c8742a in incubator-airflow''s branch refs/heads/v1-9-test from root
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=31805e8 ]

[AIRFLOW-1692] Change test_views filename to support Windows

Closes #2673 from NielsZeilemaker/AIRFLOW-1692

(cherry picked from commit 2da48568116f7382258e2678e20ee0f9573d653c)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Master cannot be checked out on windows
> ---------------------------------------
>
>                 Key: AIRFLOW-1692
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1692
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>             Fix For: 1.9.0
>
>
> This file:
> /tests/www/test_logs/dag_for_testing_log_view/task_for_testing_log_view/2017-09-01T00:00:00/1.log
> Cannot be created on windows due to the colons in the directory name



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26617,54,JIRA.13107696.1507373278000.754.1508356320574@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13107696.1507373278000@Atlassian.JIRA,,,2017-10-18 12:52:00-07,"[jira] [Commented] (AIRFLOW-1692) Master cannot be checked out on
 windows","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1692?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209907#comment-16209907 ] 

ASF subversion and git services commented on AIRFLOW-1692:
----------------------------------------------------------

Commit 31805e836a04700b5881705f58b33f32f2c8742a in incubator-airflow''s branch refs/heads/v1-9-test from root
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=31805e8 ]

[AIRFLOW-1692] Change test_views filename to support Windows

Closes #2673 from NielsZeilemaker/AIRFLOW-1692

(cherry picked from commit 2da48568116f7382258e2678e20ee0f9573d653c)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Master cannot be checked out on windows
> ---------------------------------------
>
>                 Key: AIRFLOW-1692
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1692
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>             Fix For: 1.9.0
>
>
> This file:
> /tests/www/test_logs/dag_for_testing_log_view/task_for_testing_log_view/2017-09-01T00:00:00/1.log
> Cannot be created on windows due to the colons in the directory name



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26618,54,JIRA.13107696.1507373278000.758.1508356320631@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13107696.1507373278000@Atlassian.JIRA,,,2017-10-18 12:52:00-07,"[jira] [Resolved] (AIRFLOW-1692) Master cannot be checked out on
 windows","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1692?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1692.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2673
[https://github.com/apache/incubator-airflow/pull/2673]

> Master cannot be checked out on windows
> ---------------------------------------
>
>                 Key: AIRFLOW-1692
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1692
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>             Fix For: 1.9.0
>
>
> This file:
> /tests/www/test_logs/dag_for_testing_log_view/task_for_testing_log_view/2017-09-01T00:00:00/1.log
> Cannot be created on windows due to the colons in the directory name



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26619,54,JIRA.13107919.1507538960000.790.1508356500480@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13107919.1507538960000@Atlassian.JIRA,,,2017-10-18 12:55:00-07,"[jira] [Commented] (AIRFLOW-1694) Hive Hooks: Python 3 does not
 have an `itertools.izip` function","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1694?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209916#comment-16209916 ] 

ASF subversion and git services commented on AIRFLOW-1694:
----------------------------------------------------------

Commit c6e5ae7c57224ceb52cbf8d15b3362afd3616053 in incubator-airflow''s branch refs/heads/v1-9-test from Yati Sagade
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=c6e5ae7 ]

[AIRFLOW-1694] Stop using itertools.izip

Itertools.zip does not exist in Python 3.

Closes #2674 from yati-sagade/fix-py3-zip

(cherry picked from commit 6110139a8fd7069c87e3c99b3976865576180f67)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Hive Hooks: Python 3 does not have an `itertools.izip` function
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1694
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1694
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hive_hooks
>            Reporter: Yati
>             Fix For: 1.9.0
>
>
> In `hooks/hive_hooks.py`, we have a use of `itertools.izip()`, which would not work in Python 3, but the builtin `zip()` returns an iterator. `six.moves` can be used from either Python 2/3 to import such objects that have been moved around.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26620,54,JIRA.13107919.1507538960000.796.1508356500543@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13107919.1507538960000@Atlassian.JIRA,,,2017-10-18 12:55:00-07,"[jira] [Resolved] (AIRFLOW-1694) Hive Hooks: Python 3 does not have
 an `itertools.izip` function","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1694?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1694.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2674
[https://github.com/apache/incubator-airflow/pull/2674]

> Hive Hooks: Python 3 does not have an `itertools.izip` function
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1694
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1694
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hive_hooks
>            Reporter: Yati
>             Fix For: 1.9.0
>
>
> In `hooks/hive_hooks.py`, we have a use of `itertools.izip()`, which would not work in Python 3, but the builtin `zip()` returns an iterator. `six.moves` can be used from either Python 2/3 to import such objects that have been moved around.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
875218,186,CAMPvFO-RQSXP+zj++2JO-x4Pn6b4odjPjEXxo=XDDE9c07Cy+w@mail.gmail.com,30911,Dascalita Dragos,978B4ADE-A89D-492D-89E7-C7E7F450EDEA@adobe.com,30917,Tyson Norris,2017-05-01 17:06:23-07,Re: concurrent requests on actions,"Why not considering giving developers options to control the level of
concurrency, instead of deciding on their behalf ? I think that cases such
as the ones Tyson is mentioning make sense; unless we build something that
will estimate the resources needed by an action automatically, letting the
developer specify it instead, might be a mean of ""supervised learning"" that
the system can use further in order to make decisions at runtime.

Dragos
On Mon, May 1, 2017 at 4:46 PM Tyson Norris <tnorris@adobe.com> wrote:

> Sure, many of our use cases are mostly stitching together API calls, as
> opposed to being CPU bound - consider a simple javascript action that wra=
ps
> a downstream http API (or many APIs).
>
> What do you mean by =E2=80=9Cmore efficient packing of I/O-bound processe=
s=E2=80=9D? For
> example, in the case of actions that wrap an API call, typically the acti=
on
> developer is NOT the owner of the API call, so its not clear how to handl=
e
> this more efficiently than by creating a nodejs action that proxies
> (multiple concurrent) network requests around, but does little actual
> computing besides possibly some minor request/response parsing etc. In ou=
r
> cases we our much more likely to run into bottlenecks with concurrent use=
rs
> without any concurrent container usage support, unless we greatly over
> provision clusters which will provide drastic reduction in efficiency. It
> is much simpler to provision for anticipated or immediate load changes wh=
en
> each new container can support multiple concurrent requests, instead of
> each new container supporting a single request.
>
> More tests demonstrating these cases (e.g. API wrappers, and
> compute-centric actions) will help this discussion, I=E2=80=99ll work on =
providing
> those.
>
> Thanks
> Tyson
>
> > On May 1, 2017, at 3:24 PM, Nick Mitchell <moosevan@gmail.com> wrote:
> >
> > won''t this only be of benefit for invocations that are mostly sleepy?
> e.g.
> > I/O-bound? because if an action uses CPU flat-out, then there is no
> > throughput win to be had (by increasing the parallelism of CPU-bound
> > processes), given the small CPU sliver that each container gets -- unle=
ss
> > there is a concomitant increase in concurrency, i.e. CPU slice?
> >
> > if so, then my gut tells me that there are more general solutions to th=
is
> > (i.e. more efficient packing of I/O-bound processes)
> >
> > On Mon, May 1, 2017 at 5:36 PM, Tyson Norris <tnorris@adobe.com> wrote:
> >
> >> Thanks Markus.
> >>
> >> Can you direct me to the travis job where I can see the 40+RPS? I agre=
e
> >> that is a big gap and would like to take a look - I didn=E2=80=99t see=
 anything
> in
> >>
> https://na01.safelinks.protection.outlook.com/?url=3Dhttps%%%%3A%%%%2F%%%%2Ftravis=
-ci.org%%%%2Fopenwhisk%%%%2Fopenwhisk%%%%2Fbuilds%%%%2F226918375&data=3D02%%%%7C01%%%%7C%%%%7C8a=
29a490bc6545d4460408d490e0c979%%%%7Cfa7b1b5a7b34438794aed2c178decee1%%%%7C0%%%%7C0%%%%7=
C636292742509382993&sdata=3D2RiV65g7zvR07ditlzosUxsrWvQIo8WfpMvr7g2JHWY%%%%3D&=
reserved=3D0
> ; maybe I=E2=80=99m
> >> looking in the wrong place.
> >>
> >> I will work on putting together a PR to discuss.
> >>
> >> Thanks
> >> Tyson
> >>
> >>
> >> On May 1, 2017, at 2:22 PM, Markus Th=C3=B6mmes <markusthoemmes@me.com
> <mailto:
> >> markusthoemmes@me.com>> wrote:
> >>
> >> Hi Tyson,
> >>
> >> Sounds like you did a lot of investigation here, thanks a lot for that
> :)
> >>
> >> Seeing the numbers, 4 RPS in the ""off"" case seem very odd. The Travis
> >> build that runs the current system as is also reaches 40+ RPS. So we''d
> need
> >> to look at a mismatch here.
> >>
> >> Other than that I''d indeed suspect a great improvement in throughput
> from
> >> your work!
> >>
> >> Implementationwise I don''t have a strong opionion but it might be wort=
h
> to
> >> discuss the details first and land your impl. once all my staging is
> done
> >> (the open PRs). That''d ease git operation. If you want to discuss your
> >> impl. now I suggest you send a PR to my new-containerpool branch and
> share
> >> the diff here for discussion.
> >>
> >> Cheers,
> >> Markus
> >>
> >> Von meinem iPhone gesendet
> >>
> >> Am 01.05.2017 um 23:16 schrieb Tyson Norris <tnorris@adobe.com<mailto:
> tnor
> >> ris@adobe.com>>:
> >>
> >> Hi Michael -
> >> Concurrent requests would only reuse a running/warm container for
> >> same-action requests. So if the action has bad/rogue behavior, it will
> >> limit its own throughput only, not the throughput of other actions.
> >>
> >> This is ignoring the current implementation of the activation feed,
> which
> >> I guess is susceptible to a flood of slow running activations. If thos=
e
> >> activations are for the same action, running concurrently should be
> enough
> >> to not starve the system for other activations (with faster actions) t=
o
> be
> >> processed. In case they are all different actions, OR not allowed to
> >> execute concurrently, then in the name of quality-of-service, it may
> also
> >> be desirable to reserve some resources (i.e. separate activation feeds=
)
> for
> >> known-to-be-faster actions, so that fast-running actions are not
> penalized
> >> for existing alongside the slow-running actions. This would require a
> more
> >> complicated throughput test to demonstrate.
> >>
> >> Thanks
> >> Tyson
> >>
> >>
> >>
> >>
> >>
> >>
> >>
> >> On May 1, 2017, at 1:13 PM, Michael Marth <mmarth@adobe.com<mailto:
> mmart
> >> h@adobe.com><mailto:mmarth@adobe.com>> wrote:
> >>
> >> Hi Tyson,
> >>
> >> 10x more throughput, i.e. Being able to run OW at 1/10 of the cost -
> >> definitely worth looking into :)
> >>
> >> Like Rodric mentioned before I figured some features might become more
> >> complex to implement, like billing, log collection, etc. But given suc=
h
> a
> >> huge advancement on throughput that would be worth it IMHO.
> >> One thing I wonder about, though, is resilience against rogue actions.
> If
> >> an action is blocking (in the Node-sense, not the OW-sense), would tha=
t
> not
> >> block Node=E2=80=99s event loop and thus block other actions in that c=
ontainer?
> One
> >> could argue, though, that this rogue action would only block other
> >> executions of itself, not affect other actions or customers. WDYT?
> >>
> >> Michael
> >>
> >>
> >>
> >>
> >> On 01/05/17 17:54, ""Tyson Norris"" <tnorris@adobe.com<mailto:tnor
> >> ris@adobe.com><mailto:tnorris@adobe.com>> wrote:
> >>
> >> Hi All -
> >> I created this issue some time ago to discuss concurrent requests on
> >> actions: [1] Some people mentioned discussing on the mailing list so I
> >> wanted to start that discussion.
> >>
> >> I=E2=80=99ve been doing some testing against this branch with Markus=
=E2=80=99s work on
> the
> >> new container pool: [2]
> >> I believe there are a few open PRs in upstream related to this work, b=
ut
> >> this seemed like a reasonable place to test against a variety of the
> >> reactive invoker and pool changes - I=E2=80=99d be interested to hear =
if anyone
> >> disagrees.
> >>
> >> Recently I ran some tests
> >> - with =E2=80=9Cthroughput.sh=E2=80=9D in [3] using concurrency of 10 =
(it will also be
> >> interesting to test with the --rps option in loadtest...)
> >> - using a change that checks actions for an annotation =E2=80=9Cmax-co=
ncurrent=E2=80=9D
> >> (in case there is some reason actions want to enforce current behavior
> of
> >> strict serial invocation per container?)
> >> - when scheduling an actions against the pool, if there is a currently
> >> =E2=80=9Cbusy=E2=80=9D container with this action, AND the annotation =
is present for
> this
> >> action, AND concurrent requests < max-concurrent, the this container i=
s
> >> used to invoke the action
> >>
> >> Below is a summary (approx 10x throughput with concurrent requests) an=
d
> I
> >> would like to get some feedback on:
> >> - what are the cases for having actions that require container isolati=
on
> >> per request? node is a good example that should NOT need this, but may=
be
> >> there are cases where it is more important, e.g. if there are cases
> where
> >> stateful actions are used?
> >> - log collection approach: I have not attempted to resolve log
> collection
> >> issues; I would expect that revising the log sentinel marker to includ=
e
> the
> >> activation ID would help, and logs stored with the activation would
> include
> >> interleaved activations in some cases (which should be expected with
> >> concurrent request processing?), and require some different logic to
> >> process logs after an activation completes (e.g. logs emitted at the
> start
> >> of an activation may have already been collected as part of another
> >> activation=E2=80=99s log collection, etc).
> >> - advice on creating a PR to discuss this in more detail - should I wa=
it
> >> for more of the container pooling changes to get to master? Or submit =
a
> PR
> >> to Markus=E2=80=99s new-containerpool branch?
> >>
> >> Thanks
> >> Tyson
> >>
> >> Summary of loadtest report with max-concurrent ENABLED (I used 10000,
> but
> >> this limit wasn=E2=80=99t reached):
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Target URL:
> >> https://na01.safelinks.protection.outlook.com/?url=3D
> >> https%%%%3A%%%%2F%%%%2F192.168.99.100%%%%2Fapi%%%%2Fv1%%%%2Fnamespaces%%%%2F_%%%%2Factions%%%%
> >> 2FnoopThroughputConcurrent%%%%3Fblocking%%%%3Dtrue&data=3D02%%%%7C01%%%%7C%%%%
> >> 7C796dfc317cde44c9e83908d490ce7faa%%%%7Cfa7b1b5a7b34438794aed2c178de
> >> cee1%%%%7C0%%%%7C0%%%%7C636292663971484169&sdata=3Duv9kYh5uBoIDXDlEivgMClJ6TDGD=
mz
> >> TdKOgZPZjkBko%%%%3D&reserved=3D0
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Max requests:
> 10000
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Concurrency level:   10
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Agent:
> >> keepalive
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Completed requests:
> 10000
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Total errors:        0
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Total time:
> >> 241.900480915 s
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Requests per second: 41
> >> [Sat Apr 29 2017 16:32:37 GMT+0000 (UTC)] INFO Mean latency:
> 241.7
> >> ms
> >>
> >> Summary of loadtest report with max-concurrent DISABLED:
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Target URL:
> >> https://na01.safelinks.protection.outlook.com/?url=3D
> >> https%%%%3A%%%%2F%%%%2F192.168.99.100%%%%2Fapi%%%%2Fv1%%%%2Fnamespaces%%%%2F_%%%%
> >> 2Factions%%%%2FnoopThroughput%%%%3Fblocking%%%%3Dtrue&data=3D02%%%%7C01%%%%7C%%%%
> >> 7C796dfc317cde44c9e83908d490ce7faa%%%%7Cfa7b1b5a7b34438794aed2c178de
> >> cee1%%%%7C0%%%%7C0%%%%7C636292663971494178&sdata=3Dh6sMS0s2WQXFMcLg8sSAq%%%%2F56p%%%%
> >> 2F%%%%2BmVmth%%%%2B%%%%2FsqTOVmeAc%%%%3D&reserved=3D0
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Max requests:
> 10000
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Concurrency level:   10
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Agent:
> >> keepalive
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Completed requests:
> 10000
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Total errors:        19
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Total time:
> >> 2770.658048791 s
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Requests per second: 4
> >> [Sat Apr 29 2017 19:21:51 GMT+0000 (UTC)] INFO Mean latency:
> 2767.3
> >> ms
> >>
> >>
> >>
> >>
> >>
> >> [1] https://na01.safelinks.protection.outlook.com/?url=3D
> >> https%%%%3A%%%%2F%%%%2Fgithub.com%%%%2Fopenwhisk%%%%2Fopenwhisk%%%%
> >> 2Fissues%%%%2F2026&data=3D02%%%%7C01%%%%7C%%%%7C796dfc317cde44c9e83908d490ce7faa%%%%
> >>
> 7Cfa7b1b5a7b34438794aed2c178decee1%%%%7C0%%%%7C0%%%%7C636292663971494178&sdata=3De=
g%%%%
> >> 2FsSPRQYapQHPNbfMLCW%%%%2B%%%%2F1yAqn8zSo0nJ5yQjmkns%%%%3D&reserved=3D0
> >> [2] https://na01.safelinks.protection.outlook.com/?url=3D
> >> https%%%%3A%%%%2F%%%%2Fgithub.com%%%%2Fmarkusthoemmes%%%%2Fopenwhisk%%%%
> >>
> 2Ftree%%%%2Fnew-containerpool&data=3D02%%%%7C01%%%%7C%%%%7C796dfc317cde44c9e83908d490=
ce
> >> 7faa%%%%7Cfa7b1b5a7b34438794aed2c178decee1%%%%7C0%%%%7C0%%%%
> >> 7C636292663971494178&sdata=3DIZcN9szW71SdL%%%%2ByssJm9k3EgzaU4b5idI5yFWyR=
7%%%%
> >> 2BL4%%%%3D&reserved=3D0
> >> [3] https://na01.safelinks.protection.outlook.com/?url=3D
> >> https%%%%3A%%%%2F%%%%2Fgithub.com%%%%2Fmarkusthoemmes%%%%2Fopenwhisk-
> >> performance&data=3D02%%%%7C01%%%%7C%%%%7C796dfc317cde44c9e83908d490ce7faa%%%%
> >> 7Cfa7b1b5a7b34438794aed2c178decee1%%%%7C0%%%%7C0%%%%7C636292663971494178&sdata=
=3D
> >> WkOlhTsplKQm6mUkZtwWLXzCrQg%%%%2FUmKtqOErIw6gFAA%%%%3D&reserved=3D0
> >>
> >>
> >>
>
>
",f
26621,54,JIRA.13107919.1507538960000.789.1508356500471@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13107919.1507538960000@Atlassian.JIRA,,,2017-10-18 12:55:00-07,"[jira] [Commented] (AIRFLOW-1694) Hive Hooks: Python 3 does not
 have an `itertools.izip` function","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1694?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209915#comment-16209915 ] 

ASF subversion and git services commented on AIRFLOW-1694:
----------------------------------------------------------

Commit 6110139a8fd7069c87e3c99b3976865576180f67 in incubator-airflow''s branch refs/heads/master from Yati Sagade
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6110139 ]

[AIRFLOW-1694] Stop using itertools.izip

Itertools.zip does not exist in Python 3.

Closes #2674 from yati-sagade/fix-py3-zip


> Hive Hooks: Python 3 does not have an `itertools.izip` function
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1694
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1694
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hive_hooks
>            Reporter: Yati
>             Fix For: 1.9.0
>
>
> In `hooks/hive_hooks.py`, we have a use of `itertools.izip()`, which would not work in Python 3, but the builtin `zip()` returns an iterator. `six.moves` can be used from either Python 2/3 to import such objects that have been moved around.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26622,54,JIRA.13108128.1507590025000.806.1508356682124@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13108128.1507590025000@Atlassian.JIRA,,,2017-10-18 12:58:02-07,"[jira] [Commented] (AIRFLOW-1698) Remove confusing SCHEDULER_RUNS
 env var from systemd scripts","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1698?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209920#comment-16209920 ] 

ASF subversion and git services commented on AIRFLOW-1698:
----------------------------------------------------------

Commit b464d23a6d9799b75ae53b3f858e779a48115fed in incubator-airflow''s branch refs/heads/master from [~maxime.beauchemin@apache.org]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=b464d23 ]

[AIRFLOW-1698] Remove SCHEDULER_RUNS env var in systemd

In the very early days, the Airflow scheduler
needed to be restarted
every so often to take new DAG_FOLDERS mutations
into account properly. This is no longer
required.

Closes #2677 from mistercrunch/scheduler_runs


> Remove confusing SCHEDULER_RUNS env var from systemd scripts
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1698
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1698
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Maxime Beauchemin
>             Fix For: 1.9.0
>
>
> In the very early days, the Airflow scheduler needed to be restarted
> every so often to take new DAG_FOLDERS mutations into account properly.
> This is no longer needed, so I''m removing artifacts in the
> systemd scripts.
> I''m unclear whether daemon scripts belong here. I can see how it''s
> useful to have this somewhere,
> but it sits on the edge of what should be in the repo and can get
> out-of-sync.
> As a result it ends up creating more confusion than help.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26623,54,JIRA.13108128.1507590025000.808.1508356682144@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13108128.1507590025000@Atlassian.JIRA,,,2017-10-18 12:58:02-07,"[jira] [Commented] (AIRFLOW-1698) Remove confusing SCHEDULER_RUNS
 env var from systemd scripts","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1698?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16209921#comment-16209921 ] 

ASF subversion and git services commented on AIRFLOW-1698:
----------------------------------------------------------

Commit 00dd06eb0a387f85c1f16ce3aa17fc36473990bc in incubator-airflow''s branch refs/heads/v1-9-test from [~maxime.beauchemin@apache.org]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=00dd06e ]

[AIRFLOW-1698] Remove SCHEDULER_RUNS env var in systemd

In the very early days, the Airflow scheduler
needed to be restarted
every so often to take new DAG_FOLDERS mutations
into account properly. This is no longer
required.

Closes #2677 from mistercrunch/scheduler_runs

(cherry picked from commit b464d23a6d9799b75ae53b3f858e779a48115fed)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Remove confusing SCHEDULER_RUNS env var from systemd scripts
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1698
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1698
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Maxime Beauchemin
>             Fix For: 1.9.0
>
>
> In the very early days, the Airflow scheduler needed to be restarted
> every so often to take new DAG_FOLDERS mutations into account properly.
> This is no longer needed, so I''m removing artifacts in the
> systemd scripts.
> I''m unclear whether daemon scripts belong here. I can see how it''s
> useful to have this somewhere,
> but it sits on the edge of what should be in the repo and can get
> out-of-sync.
> As a result it ends up creating more confusion than help.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26624,54,JIRA.13108128.1507590025000.813.1508356682202@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13108128.1507590025000@Atlassian.JIRA,,,2017-10-18 12:58:02-07,"[jira] [Resolved] (AIRFLOW-1698) Remove confusing SCHEDULER_RUNS
 env var from systemd scripts","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1698?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1698.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2677
[https://github.com/apache/incubator-airflow/pull/2677]

> Remove confusing SCHEDULER_RUNS env var from systemd scripts
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1698
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1698
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Maxime Beauchemin
>             Fix For: 1.9.0
>
>
> In the very early days, the Airflow scheduler needed to be restarted
> every so often to take new DAG_FOLDERS mutations into account properly.
> This is no longer needed, so I''m removing artifacts in the
> systemd scripts.
> I''m unclear whether daemon scripts belong here. I can see how it''s
> useful to have this somewhere,
> but it sits on the edge of what should be in the repo and can get
> out-of-sync.
> As a result it ends up creating more confusion than help.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26625,54,JIRA.13110477.1508360188000.1429.1508360220031@Atlassian.JIRA,2133,Trevor Edwards (JIRA),JIRA.13110477.1508360188000@Atlassian.JIRA,,,2017-10-18 13:57:00-07,[jira] [Created] (AIRFLOW-1732) Improve Dataflow Hook Logging,"Trevor Edwards created AIRFLOW-1732:
---------------------------------------

             Summary: Improve Dataflow Hook Logging
                 Key: AIRFLOW-1732
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1732
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Trevor Edwards
            Assignee: Trevor Edwards


Logging output for Dataflow hook could be a bit more useful. Namely:

# Log the command that is used for opening a dataflow subprocess
# If the dataflow subprocess experiences an error, log that error at warning (instead of debug)

Currently, errors are extremely opaque, only showing the exit code. The command used is unknown and the error is logged, but it is at the debug level which makes it difficult to find.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26626,54,JIRA.13110524.1508368853000.2903.1508368860208@Atlassian.JIRA,1301,Dan Davydov (JIRA),JIRA.13110524.1508368853000@Atlassian.JIRA,,,2017-10-18 16:21:00-07,[jira] [Created] (AIRFLOW-1733) BashOperator readline can hang,"Dan Davydov created AIRFLOW-1733:
------------------------------------

             Summary: BashOperator readline can hang
                 Key: AIRFLOW-1733
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1733
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Dan Davydov


If the child bash process dies for some reason, the BashOperator will hang since we use a blocking readline call to get output from the child. Instead we should read asynchronously or poll the child bash process like described in the solutions here:
https://stackoverflow.com/questions/18421757/live-output-from-subprocess-command



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26627,54,JIRA.13110554.1508379412000.4088.1508379420026@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13110554.1508379412000@Atlassian.JIRA,,,2017-10-18 19:17:00-07,"[jira] [Created] (AIRFLOW-1734) Sqoop Operator contains logic
 errors & needs options to pass more sqoop options","Ace Haidrey created AIRFLOW-1734:
------------------------------------

             Summary: Sqoop Operator contains logic errors & needs options to pass more sqoop options
                 Key: AIRFLOW-1734
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1734
             Project: Apache Airflow
          Issue Type: Bug
          Components: contrib
            Reporter: Ace Haidrey
            Assignee: Ace Haidrey


After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes passign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.

I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26628,54,JIRA.13110554.1508379412000.4199.1508380500118@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13110554.1508379412000@Atlassian.JIRA,,,2017-10-18 19:35:00-07,"[jira] [Updated] (AIRFLOW-1734) Sqoop Operator contains logic
 errors & needs options to pass more sqoop options","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1734?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ace Haidrey updated AIRFLOW-1734:
---------------------------------
    Description: 
After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.

I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796


  was:
After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes passign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.

I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796



> Sqoop Operator contains logic errors & needs options to pass more sqoop options
> -------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1734
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1734
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Ace Haidrey
>            Assignee: Ace Haidrey
>              Labels: patch
>
> After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.
> I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
> https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26629,54,JIRA.13110554.1508379412000.4326.1508382180066@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13110554.1508379412000@Atlassian.JIRA,,,2017-10-18 20:03:00-07,"[jira] [Updated] (AIRFLOW-1734) Sqoop Operator contains logic
 errors & needs options to pass more sqoop options","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1734?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ace Haidrey updated AIRFLOW-1734:
---------------------------------
    Description: 
After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.

I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796

Here is my PR with change by change explanation:
https://github.com/apache/incubator-airflow/pull/2703/files#diff-8e77f042c2e060bbfd60828431a91e9bL131

  was:
After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.

I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796



> Sqoop Operator contains logic errors & needs options to pass more sqoop options
> -------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1734
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1734
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Ace Haidrey
>            Assignee: Ace Haidrey
>              Labels: patch
>
> After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.
> I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
> https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796
> Here is my PR with change by change explanation:
> https://github.com/apache/incubator-airflow/pull/2703/files#diff-8e77f042c2e060bbfd60828431a91e9bL131



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26630,54,JIRA.13110563.1508383237000.4484.1508383260031@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13110563.1508383237000@Atlassian.JIRA,,,2017-10-18 20:21:00-07,"[jira] [Created] (AIRFLOW-1735) Log files do not show up for
 unscheduled dags","Ace Haidrey created AIRFLOW-1735:
------------------------------------

             Summary: Log files do not show up for unscheduled dags
                 Key: AIRFLOW-1735
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1735
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Ace Haidrey
            Assignee: Ace Haidrey
         Attachments: Screen Shot 2017-06-21 at 10.53.28 PM.png

I created a new dag with a schedule of {{@once}} and left it unscheduled.  I ran different tasks a few times from the UI but the logs never showed up.  Instead I''d see messages like the following:
{code}
*** Log file isn''t local.
*** Fetching here: http://al-tools2.savagebeast.com:18700/log/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21
*** Failed to fetch log file from worker.
{code}

There is a log file for this task, but it seems to be at a slightly different path on the machine:

{code}
(airflow)hadoop@al-tools2:~/.virtualenvs/airflow$ ls -lth ./airflow_home/logs/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21T00\:00\:00
-rw-rw-rw- 1 hadoop hadoop 5.2K Jun 21 21:57 ./airflow_home/logs/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21T00:00:00
{code}

And indeed, when I change the URL to WEB_SERVER/admin/airflow/log?task_id=TASK_ID_request&dag_id=DAG_ID&execution_date=2017-06-21T00:00:00, I see the actual log:

{code}
*** Reading local log.
[2017-06-21 21:57:20,931] {models.py:172} INFO - Filling up the DagBag from ~/analytics_kochava_import_dag.py
[2017-06-21 21:57:21,307] {analytics_kochava_import_dag.py:139} INFO - Kochava app IDs 
...
[2017-06-21 21:57:26,400] {jobs.py:2083} INFO - Task exited with return code 0
{code}

I''ve added a screen shot of it below.

*FIX:*
I have noticed in www/view.py the issue is not have the time component 00:00:00 and we can get that quite easily if it doesnt have it. You can see in this PR I made.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
631121,147,1081875729.2270.1.camel@gideon.rkunet.org,21601,Rolf Kulemann,407C18F1.1030103@wyona.com,29,Michael Wechner,2004-04-13 10:02:10-07,Re: Clone Proposal,"On Tue, 2004-04-13 at 18:44, Michael Wechner wrote:
> On IRC we did some brainstorming on introducing a clone ""build file"" for 
> publications.
> 
> Each publication has some generic and some specific parts, therefore it 
> would probably make sense to do it as follows:
> 
> 
> .../lenya/bin/clone.xml  (for the generic parts)
> 
> and
> 
> .../lenya/pubs/@PUB@/bin/clone.xml (for the specific parts)
> 
> whereas I guess the specific clone.xml imports the generic one (or is it 
> the opposite way around).
> 
> Anyway, for the beginning we could do this for the default and blog 
> publication.

+1

> 
> Also I think we should add two new publication instances:
> 
> my-default and my-blog where we have slight changes re the layout for 
> instance.

Can u please explain what they are for? What is the difference to blog
and default?

-- 
Regards,

    Rolf Kulemann


---------------------------------------------------------------------
To unsubscribe, e-mail: lenya-dev-unsubscribe@cocoon.apache.org
For additional commands, e-mail: lenya-dev-help@cocoon.apache.org


",f
26631,54,JIRA.13110563.1508383237000.4499.1508383560041@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13110563.1508383237000@Atlassian.JIRA,,,2017-10-18 20:26:00-07,"[jira] [Updated] (AIRFLOW-1735) Log files do not show up for
 unscheduled dags","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1735?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ace Haidrey updated AIRFLOW-1735:
---------------------------------
    Description: 
I created a new dag with a schedule of {{@once}} and left it unscheduled.  I ran different tasks a few times from the UI but the logs never showed up.  Instead I''d see messages like the following:
{code}
*** Log file isn''t local.
*** Fetching here: http://al-tools2.savagebeast.com:18700/log/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21
*** Failed to fetch log file from worker.
{code}

There is a log file for this task, but it seems to be at a slightly different path on the machine:

{code}
(airflow)hadoop@al-tools2:~/.virtualenvs/airflow$ ls -lth ./airflow_home/logs/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21T00\:00\:00
-rw-rw-rw- 1 hadoop hadoop 5.2K Jun 21 21:57 ./airflow_home/logs/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21T00:00:00
{code}

And indeed, when I change the URL to WEB_SERVER/admin/airflow/log?task_id=TASK_ID_request&dag_id=DAG_ID&execution_date=2017-06-21T00:00:00, I see the actual log:

{code}
*** Reading local log.
[2017-06-21 21:57:20,931] {models.py:172} INFO - Filling up the DagBag from ~/analytics_kochava_import_dag.py
[2017-06-21 21:57:21,307] {analytics_kochava_import_dag.py:139} INFO - Kochava app IDs 
...
[2017-06-21 21:57:26,400] {jobs.py:2083} INFO - Task exited with return code 0
{code}

I''ve added a screen shot of it below.

*FIX:*
I have noticed in www/view.py the issue is not have the time component 00:00:00 and we can get that quite easily if it doesnt have it. You can see in this PR I made, but basically if we set
{code}
dttm = dateutil.parser.parse(request.args.get(''execution_date''))
execution_date = dttm.isoformat()
{code}
This will fix it.

  was:
I created a new dag with a schedule of {{@once}} and left it unscheduled.  I ran different tasks a few times from the UI but the logs never showed up.  Instead I''d see messages like the following:
{code}
*** Log file isn''t local.
*** Fetching here: http://al-tools2.savagebeast.com:18700/log/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21
*** Failed to fetch log file from worker.
{code}

There is a log file for this task, but it seems to be at a slightly different path on the machine:

{code}
(airflow)hadoop@al-tools2:~/.virtualenvs/airflow$ ls -lth ./airflow_home/logs/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21T00\:00\:00
-rw-rw-rw- 1 hadoop hadoop 5.2K Jun 21 21:57 ./airflow_home/logs/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21T00:00:00
{code}

And indeed, when I change the URL to WEB_SERVER/admin/airflow/log?task_id=TASK_ID_request&dag_id=DAG_ID&execution_date=2017-06-21T00:00:00, I see the actual log:

{code}
*** Reading local log.
[2017-06-21 21:57:20,931] {models.py:172} INFO - Filling up the DagBag from ~/analytics_kochava_import_dag.py
[2017-06-21 21:57:21,307] {analytics_kochava_import_dag.py:139} INFO - Kochava app IDs 
...
[2017-06-21 21:57:26,400] {jobs.py:2083} INFO - Task exited with return code 0
{code}

I''ve added a screen shot of it below.

*FIX:*
I have noticed in www/view.py the issue is not have the time component 00:00:00 and we can get that quite easily if it doesnt have it. You can see in this PR I made.


> Log files do not show up for unscheduled dags
> ---------------------------------------------
>
>                 Key: AIRFLOW-1735
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1735
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ace Haidrey
>            Assignee: Ace Haidrey
>              Labels: www
>         Attachments: Screen Shot 2017-06-21 at 10.53.28 PM.png
>
>
> I created a new dag with a schedule of {{@once}} and left it unscheduled.  I ran different tasks a few times from the UI but the logs never showed up.  Instead I''d see messages like the following:
> {code}
> *** Log file isn''t local.
> *** Fetching here: http://al-tools2.savagebeast.com:18700/log/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21
> *** Failed to fetch log file from worker.
> {code}
> There is a log file for this task, but it seems to be at a slightly different path on the machine:
> {code}
> (airflow)hadoop@al-tools2:~/.virtualenvs/airflow$ ls -lth ./airflow_home/logs/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21T00\:00\:00
> -rw-rw-rw- 1 hadoop hadoop 5.2K Jun 21 21:57 ./airflow_home/logs/analytics_kochava_import_dag/kopandoraradioandroid1134523b395684da7_request/2017-06-21T00:00:00
> {code}
> And indeed, when I change the URL to WEB_SERVER/admin/airflow/log?task_id=TASK_ID_request&dag_id=DAG_ID&execution_date=2017-06-21T00:00:00, I see the actual log:
> {code}
> *** Reading local log.
> [2017-06-21 21:57:20,931] {models.py:172} INFO - Filling up the DagBag from ~/analytics_kochava_import_dag.py
> [2017-06-21 21:57:21,307] {analytics_kochava_import_dag.py:139} INFO - Kochava app IDs 
> ...
> [2017-06-21 21:57:26,400] {jobs.py:2083} INFO - Task exited with return code 0
> {code}
> I''ve added a screen shot of it below.
> *FIX:*
> I have noticed in www/view.py the issue is not have the time component 00:00:00 and we can get that quite easily if it doesnt have it. You can see in this PR I made, but basically if we set
> {code}
> dttm = dateutil.parser.parse(request.args.get(''execution_date''))
> execution_date = dttm.isoformat()
> {code}
> This will fix it.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26632,54,JIRA.13110574.1508391320000.5169.1508391360039@Atlassian.JIRA,2336,Jetsada Machom (JIRA),JIRA.13110574.1508391320000@Atlassian.JIRA,,,2017-10-18 22:36:00-07,"[jira] [Created] (AIRFLOW-1736) Add HotelQuickly to Who Uses
 Airflow","Jetsada Machom created AIRFLOW-1736:
---------------------------------------

             Summary: Add HotelQuickly to Who Uses Airflow
                 Key: AIRFLOW-1736
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1736
             Project: Apache Airflow
          Issue Type: Task
            Reporter: Jetsada Machom
            Assignee: Jetsada Machom
            Priority: Trivial






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26633,54,JIRA.13080275.1497572872000.5379.1508392860348@Atlassian.JIRA,2027,Yu Ishikawa (JIRA),JIRA.13080275.1497572872000@Atlassian.JIRA,,,2017-10-18 23:01:00-07,[jira] [Closed] (AIRFLOW-1310) Kubernetes execute operator,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1310?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Yu Ishikawa closed AIRFLOW-1310.
--------------------------------
    Resolution: Duplicate

> Kubernetes execute operator
> ---------------------------
>
>                 Key: AIRFLOW-1310
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1310
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: operators
>            Reporter: Yu Ishikawa
>            Assignee: Dennis Docter
>
> We should support operators for Kubernetes to execute containers remotely.
> - Create a Kubernetes cluster
> - Execute PODs
> - https://github.com/kubernetes-incubator/client-python



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26634,54,JIRA.13110574.1508391320000.5401.1508393040236@Atlassian.JIRA,2336,Jetsada Machom (JIRA),JIRA.13110574.1508391320000@Atlassian.JIRA,,,2017-10-18 23:04:00-07,"[jira] [Work started] (AIRFLOW-1736) Add HotelQuickly to Who Uses
 Airflow","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1736?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1736 started by Jetsada Machom.
-----------------------------------------------
> Add HotelQuickly to Who Uses Airflow
> ------------------------------------
>
>                 Key: AIRFLOW-1736
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1736
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Jetsada Machom
>            Assignee: Jetsada Machom
>            Priority: Trivial
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26635,54,JIRA.13110574.1508391320000.5402.1508393100068@Atlassian.JIRA,2336,Jetsada Machom (JIRA),JIRA.13110574.1508391320000@Atlassian.JIRA,,,2017-10-18 23:05:00-07,"[jira] [Resolved] (AIRFLOW-1736) Add HotelQuickly to Who Uses
 Airflow","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1736?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Jetsada Machom resolved AIRFLOW-1736.
-------------------------------------
    Resolution: Done

> Add HotelQuickly to Who Uses Airflow
> ------------------------------------
>
>                 Key: AIRFLOW-1736
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1736
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Jetsada Machom
>            Assignee: Jetsada Machom
>            Priority: Trivial
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26636,54,JIRA.13110115.1508271682000.8939.1508427121318@Atlassian.JIRA,2332,Jessica Chen Fan  (JIRA),JIRA.13110115.1508271682000@Atlassian.JIRA,,,2017-10-19 08:32:01-07,"[jira] [Commented] (AIRFLOW-1728) Add networkUri, subnetworkUri and
 tags to DataprocClusterCreateOperator","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1728?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16211215#comment-16211215 ] 

Jessica Chen Fan  commented on AIRFLOW-1728:
--------------------------------------------

PR here: https://github.com/apache/incubator-airflow/pull/2706

> Add networkUri, subnetworkUri and tags to DataprocClusterCreateOperator 
> ------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1728
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1728
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Jessica Chen Fan 
>            Assignee: Jessica Chen Fan 
>            Priority: Minor
>
> Add ability to specify subnetwork and firewall tags for easier networking when creating 
> dataproc clusters.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
631122,147,407C25FA.5040300@wyona.com,29,Michael Wechner,1081875729.2270.1.camel@gideon.rkunet.org,21601,Rolf Kulemann,2004-04-13 10:40:10-07,Re: Clone Proposal,"Rolf Kulemann wrote:

>
>Can u please explain what they are for?
>

very little changes in order to show people the idea of cloning a 
publication, e.g.

michi@neptun:~/src/cocoon-lenya$ ls src/webapp/lenya/pubs/my-default/
publication.xml  xslt
michi@neptun:~/src/cocoon-lenya$ ls src/webapp/lenya/pubs/my-default/xslt/
page2xhtml.xsl

> What is the difference to blog
>and default?
>  
>

difference between blog and default with regard to cloning?


-- 
Michael Wechner
Wyona Inc.  -   Open Source Content Management   -   Apache Lenya
http://www.wyona.com              http://cocoon.apache.org/lenya/
michael.wechner@wyona.com                        michi@apache.org


---------------------------------------------------------------------
To unsubscribe, e-mail: lenya-dev-unsubscribe@cocoon.apache.org
For additional commands, e-mail: lenya-dev-help@cocoon.apache.org


",f
26637,54,JIRA.13110712.1508429923000.9370.1508429940051@Atlassian.JIRA,2337,Andre Boechat (JIRA),JIRA.13110712.1508429923000@Atlassian.JIRA,,,2017-10-19 09:19:00-07,"[jira] [Created] (AIRFLOW-1737) set_task_instance_state fails
 because of strptime","Andre Boechat created AIRFLOW-1737:
--------------------------------------

             Summary: set_task_instance_state fails because of strptime
                 Key: AIRFLOW-1737
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1737
             Project: Apache Airflow
          Issue Type: Bug
          Components: webapp
            Reporter: Andre Boechat
            Priority: Minor
         Attachments: Screenshot_2017-10-18_15-58-29.png

Context:

* DAG run triggered manually
* Using the web application to change the state of a task

When trying to set the state of a task, an exception is thrown: `ValueError: unconverted data remains: ..372649` (look at the attached screenshot).

I think the problem comes from the ""execution date"" created by manually triggered DAGs, since the date-time includes a fractional part. In my database, I see scheduled DAGs with execution dates like ""10-18T15:00:00"", while manually triggered ones with dates like ""09-21T16:36:16.170988"". If we look at the method `set_task_instance_state` in *airflow.www.views"", we see that the format string used with `strptime` doesn''t consider any fractional part.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26638,54,JIRA.13081281.1498007085000.9433.1508430540192@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13081281.1498007085000@Atlassian.JIRA,,,2017-10-19 09:29:00-07,"[jira] [Commented] (AIRFLOW-1330) Connection.parse_from_uri doesn''t
 work for google_cloud_platform and so on","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1330?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16211311#comment-16211311 ] 

ASF subversion and git services commented on AIRFLOW-1330:
----------------------------------------------------------

Commit 2f107d8a30910fd025774004d5c4c95407ed55c5 in incubator-airflow''s branch refs/heads/master from [~mrkm4ntr]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2f107d8 ]

[AIRFLOW-1330] Add conn_type argument to CLI when adding connection

Closes #2525 from mrkm4ntr/airflow-1330


> Connection.parse_from_uri doesn''t work for google_cloud_platform and so on
> --------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1330
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1330
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli
>            Reporter: Yu Ishikawa
>            Assignee: Shintaro Murakami
>
> h2. Overview
> {{Connection.parse_from_uri}} doesn''t work for some types like {{google_cloud_platform}} whose type name includes under scores. Since `urllib.parse.urlparse()` which is used in {{Connection.parse_from_url}} doesn''t support a schema name which include under scores.
> So, airflow''s CLI doesn''t work when a given connection URI includes under scores like {{google_cloud_platform://XXXXX}}.
> h3. Workaround
> https://medium.com/@yuu.ishikawa/apache-airflow-how-to-add-a-connection-to-google-cloud-with-cli-af2cc8df138d



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26639,54,JIRA.13081281.1498007085000.9443.1508430600731@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13081281.1498007085000@Atlassian.JIRA,,,2017-10-19 09:30:00-07,"[jira] [Resolved] (AIRFLOW-1330) Connection.parse_from_uri doesn''t
 work for google_cloud_platform and so on","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1330?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1330.
--------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> Connection.parse_from_uri doesn''t work for google_cloud_platform and so on
> --------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1330
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1330
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli
>            Reporter: Yu Ishikawa
>            Assignee: Shintaro Murakami
>             Fix For: 1.10.0
>
>
> h2. Overview
> {{Connection.parse_from_uri}} doesn''t work for some types like {{google_cloud_platform}} whose type name includes under scores. Since `urllib.parse.urlparse()` which is used in {{Connection.parse_from_url}} doesn''t support a schema name which include under scores.
> So, airflow''s CLI doesn''t work when a given connection URI includes under scores like {{google_cloud_platform://XXXXX}}.
> h3. Workaround
> https://medium.com/@yuu.ishikawa/apache-airflow-how-to-add-a-connection-to-google-cloud-with-cli-af2cc8df138d



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26640,54,JIRA.13110712.1508429923000.9444.1508430600742@Atlassian.JIRA,2337,Andre Boechat (JIRA),JIRA.13110712.1508429923000@Atlassian.JIRA,,,2017-10-19 09:30:00-07,"[jira] [Updated] (AIRFLOW-1737) set_task_instance_state fails
 because of strptime","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1737?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Andre Boechat updated AIRFLOW-1737:
-----------------------------------
    Description: 
Context:

* DAG run triggered manually
* Using the web application to change the state of a task

When trying to set the state of a task, an exception is thrown: `ValueError: unconverted data remains: ..372649` (look at the attached screenshot).

I think the problem comes from the ""execution date"" created by manually triggered DAGs, since the date-time includes a fractional part. In my database, I see scheduled DAGs with execution dates like ""10-18T15:00:00"", while manually triggered ones with dates like ""09-21T16:36:16.170988"". If we look at the method *set_task_instance_state* in *airflow.www.views*, we see that the format string used with *strptime* doesn''t consider any fractional part.

  was:
Context:

* DAG run triggered manually
* Using the web application to change the state of a task

When trying to set the state of a task, an exception is thrown: `ValueError: unconverted data remains: ..372649` (look at the attached screenshot).

I think the problem comes from the ""execution date"" created by manually triggered DAGs, since the date-time includes a fractional part. In my database, I see scheduled DAGs with execution dates like ""10-18T15:00:00"", while manually triggered ones with dates like ""09-21T16:36:16.170988"". If we look at the method `set_task_instance_state` in *airflow.www.views"", we see that the format string used with `strptime` doesn''t consider any fractional part.


> set_task_instance_state fails because of strptime
> -------------------------------------------------
>
>                 Key: AIRFLOW-1737
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1737
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webapp
>            Reporter: Andre Boechat
>            Priority: Minor
>         Attachments: Screenshot_2017-10-18_15-58-29.png
>
>
> Context:
> * DAG run triggered manually
> * Using the web application to change the state of a task
> When trying to set the state of a task, an exception is thrown: `ValueError: unconverted data remains: ..372649` (look at the attached screenshot).
> I think the problem comes from the ""execution date"" created by manually triggered DAGs, since the date-time includes a fractional part. In my database, I see scheduled DAGs with execution dates like ""10-18T15:00:00"", while manually triggered ones with dates like ""09-21T16:36:16.170988"". If we look at the method *set_task_instance_state* in *airflow.www.views*, we see that the format string used with *strptime* doesn''t consider any fractional part.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26641,54,JIRA.13110712.1508429923000.9445.1508430660064@Atlassian.JIRA,2337,Andre Boechat (JIRA),JIRA.13110712.1508429923000@Atlassian.JIRA,,,2017-10-19 09:31:00-07,"[jira] [Updated] (AIRFLOW-1737) set_task_instance_state fails
 because of strptime","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1737?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Andre Boechat updated AIRFLOW-1737:
-----------------------------------
    Description: 
Context:

* DAG run triggered manually
* Using the web application to change the state of a task

When trying to set the state of a task, an exception is thrown: *ValueError: unconverted data remains: ..372649* (look at the attached screenshot).

I think the problem comes from the ""execution date"" created by manually triggered DAGs, since the date-time includes a fractional part. In my database, I see scheduled DAGs with execution dates like ""10-18T15:00:00"", while manually triggered ones with dates like ""09-21T16:36:16.170988"". If we look at the method *set_task_instance_state* in *airflow.www.views*, we see that the format string used with *strptime* doesn''t consider any fractional part.

  was:
Context:

* DAG run triggered manually
* Using the web application to change the state of a task

When trying to set the state of a task, an exception is thrown: `ValueError: unconverted data remains: ..372649` (look at the attached screenshot).

I think the problem comes from the ""execution date"" created by manually triggered DAGs, since the date-time includes a fractional part. In my database, I see scheduled DAGs with execution dates like ""10-18T15:00:00"", while manually triggered ones with dates like ""09-21T16:36:16.170988"". If we look at the method *set_task_instance_state* in *airflow.www.views*, we see that the format string used with *strptime* doesn''t consider any fractional part.


> set_task_instance_state fails because of strptime
> -------------------------------------------------
>
>                 Key: AIRFLOW-1737
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1737
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webapp
>            Reporter: Andre Boechat
>            Priority: Minor
>         Attachments: Screenshot_2017-10-18_15-58-29.png
>
>
> Context:
> * DAG run triggered manually
> * Using the web application to change the state of a task
> When trying to set the state of a task, an exception is thrown: *ValueError: unconverted data remains: ..372649* (look at the attached screenshot).
> I think the problem comes from the ""execution date"" created by manually triggered DAGs, since the date-time includes a fractional part. In my database, I see scheduled DAGs with execution dates like ""10-18T15:00:00"", while manually triggered ones with dates like ""09-21T16:36:16.170988"". If we look at the method *set_task_instance_state* in *airflow.www.views*, we see that the format string used with *strptime* doesn''t consider any fractional part.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26642,54,JIRA.13110728.1508433863000.9786.1508433900229@Atlassian.JIRA,2338,Christopher Groskopf (JIRA),JIRA.13110728.1508433863000@Atlassian.JIRA,,,2017-10-19 10:25:00-07,"[jira] [Created] (AIRFLOW-1738) Deleting dag.py after manual run
 causes Airflow to hang on next manual run","Christopher Groskopf created AIRFLOW-1738:
---------------------------------------------

             Summary: Deleting dag.py after manual run causes Airflow to hang on next manual run
                 Key: AIRFLOW-1738
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1738
             Project: Apache Airflow
          Issue Type: Bug
          Components: DagRun
         Environment: Airflow 1.8.2, running on Alpine Docker images with Celery.
            Reporter: Christopher Groskopf
            Priority: Minor


We''re building integration tests around Airflow that upload a new {{dag.py}} file, trigger it''s execution (using {{airflow.api.common.experimental.trigger_dag}}), wait for it to complete, and then cleanup. However, we''ve run into a peculiar set of circumstances where if we do this multiple times in sequence, Airflow will sometime hang indefinitely when we try to trigger our next dag.

I''ve traced this and determined that the issue only happens if remove the dag files after each test. I speculate that this is because triggering a dag invokes {{create_dagrun}} which in turn invokes {{verify_integrity}}, which I believe may be choking on the sudden disappearance of the DAG files. I''m certain this is correct, but it''s the best explanation I''ve been able to come up with.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26643,54,JIRA.13110728.1508433863000.9790.1508433900280@Atlassian.JIRA,2338,Christopher Groskopf (JIRA),JIRA.13110728.1508433863000@Atlassian.JIRA,,,2017-10-19 10:25:00-07,"[jira] [Updated] (AIRFLOW-1738) Deleting dag.py after manual run
 causes Airflow to hang on next manual run","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1738?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Christopher Groskopf updated AIRFLOW-1738:
------------------------------------------
    Description: 
We''re building integration tests around Airflow that upload a new {{dag.py}} file, trigger it''s execution (using {{airflow.api.common.experimental.trigger_dag}}), wait for it to complete, and then cleanup. However, we''ve run into a peculiar set of circumstances where if we do this multiple times in sequence, Airflow will sometime hang indefinitely when we try to trigger our next dag.

I''ve traced this and determined that the issue only happens if remove the dag files after each test. I speculate that this is because triggering a dag invokes {{create_dagrun}} which in turn invokes {{verify_integrity}}, which I believe may be choking on the sudden disappearance of the DAG files. I''m not certain this is correct, but it''s the best explanation I''ve been able to come up with.

  was:
We''re building integration tests around Airflow that upload a new {{dag.py}} file, trigger it''s execution (using {{airflow.api.common.experimental.trigger_dag}}), wait for it to complete, and then cleanup. However, we''ve run into a peculiar set of circumstances where if we do this multiple times in sequence, Airflow will sometime hang indefinitely when we try to trigger our next dag.

I''ve traced this and determined that the issue only happens if remove the dag files after each test. I speculate that this is because triggering a dag invokes {{create_dagrun}} which in turn invokes {{verify_integrity}}, which I believe may be choking on the sudden disappearance of the DAG files. I''m certain this is correct, but it''s the best explanation I''ve been able to come up with.


> Deleting dag.py after manual run causes Airflow to hang on next manual run
> --------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1738
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1738
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DagRun
>         Environment: Airflow 1.8.2, running on Alpine Docker images with Celery.
>            Reporter: Christopher Groskopf
>            Priority: Minor
>
> We''re building integration tests around Airflow that upload a new {{dag.py}} file, trigger it''s execution (using {{airflow.api.common.experimental.trigger_dag}}), wait for it to complete, and then cleanup. However, we''ve run into a peculiar set of circumstances where if we do this multiple times in sequence, Airflow will sometime hang indefinitely when we try to trigger our next dag.
> I''ve traced this and determined that the issue only happens if remove the dag files after each test. I speculate that this is because triggering a dag invokes {{create_dagrun}} which in turn invokes {{verify_integrity}}, which I believe may be choking on the sudden disappearance of the DAG files. I''m not certain this is correct, but it''s the best explanation I''ve been able to come up with.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26644,54,JIRA.13110079.1508264991000.10775.1508439120164@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110079.1508264991000@Atlassian.JIRA,,,2017-10-19 11:52:00-07,"[jira] [Commented] (AIRFLOW-1726) Copy Expert command for Postgres
 Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1726?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16211541#comment-16211541 ] 

ASF subversion and git services commented on AIRFLOW-1726:
----------------------------------------------------------

Commit 6372770be6aab67654cede81d6b027a838077a8a in incubator-airflow''s branch refs/heads/master from [~andyxhadji]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6372770 ]

[AIRFLOW-1726] Add copy_expert psycopg2 method to PostgresHook

Executes SQL using psycopg2 copy_expert method
Necessary to execute COPY command without access to a superuser

Closes #2698 from andyxhadji/AIRFLOW-1726


> Copy Expert command for Postgres Hook
> -------------------------------------
>
>                 Key: AIRFLOW-1726
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1726
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>
> The Copy command is useful for postgres users, and it''s client-based version ""\COPY"" can be frustrating to use due to python escape character shenanigans. Fortunately, psycopg2 supports a copy_expert method that simplifies the command and it''s options, so I think it would be a useful addition to the PostgresHook.
> I''m working on the PR now, will post here when complete.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26645,54,JIRA.13110079.1508264991000.10774.1508439120155@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110079.1508264991000@Atlassian.JIRA,,,2017-10-19 11:52:00-07,"[jira] [Commented] (AIRFLOW-1726) Copy Expert command for Postgres
 Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1726?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16211540#comment-16211540 ] 

ASF subversion and git services commented on AIRFLOW-1726:
----------------------------------------------------------

Commit 6372770be6aab67654cede81d6b027a838077a8a in incubator-airflow''s branch refs/heads/master from [~andyxhadji]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6372770 ]

[AIRFLOW-1726] Add copy_expert psycopg2 method to PostgresHook

Executes SQL using psycopg2 copy_expert method
Necessary to execute COPY command without access to a superuser

Closes #2698 from andyxhadji/AIRFLOW-1726


> Copy Expert command for Postgres Hook
> -------------------------------------
>
>                 Key: AIRFLOW-1726
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1726
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>
> The Copy command is useful for postgres users, and it''s client-based version ""\COPY"" can be frustrating to use due to python escape character shenanigans. Fortunately, psycopg2 supports a copy_expert method that simplifies the command and it''s options, so I think it would be a useful addition to the PostgresHook.
> I''m working on the PR now, will post here when complete.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
631123,147,1081878898.2266.11.camel@gideon.rkunet.org,21601,Rolf Kulemann,407C25FA.5040300@wyona.com,29,Michael Wechner,2004-04-13 10:54:58-07,Re: Clone Proposal,"On Tue, 2004-04-13 at 19:40, Michael Wechner wrote:
> Rolf Kulemann wrote:
> 
> >
> >Can u please explain what they are for?
> >
> 
> very little changes in order to show people the idea of cloning a 
> publication, e.g.
> 
> michi@neptun:~/src/cocoon-lenya$ ls src/webapp/lenya/pubs/my-default/
> publication.xml  xslt
> michi@neptun:~/src/cocoon-lenya$ ls src/webapp/lenya/pubs/my-default/xslt/
> page2xhtml.xsl

Ah yes ok. 
+1 to add my-* pubs :)

-- 
Regards,

    Rolf Kulemann


---------------------------------------------------------------------
To unsubscribe, e-mail: lenya-dev-unsubscribe@cocoon.apache.org
For additional commands, e-mail: lenya-dev-help@cocoon.apache.org


",f
26646,54,JIRA.13110079.1508264991000.10779.1508439180612@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110079.1508264991000@Atlassian.JIRA,,,2017-10-19 11:53:00-07,"[jira] [Commented] (AIRFLOW-1726) Copy Expert command for Postgres
 Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1726?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16211542#comment-16211542 ] 

ASF subversion and git services commented on AIRFLOW-1726:
----------------------------------------------------------

Commit 8a4ad30104d9e05589167ec0cde53ad941b9a1f7 in incubator-airflow''s branch refs/heads/v1-9-test from [~andyxhadji]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=8a4ad30 ]

[AIRFLOW-1726] Add copy_expert psycopg2 method to PostgresHook

Executes SQL using psycopg2 copy_expert method
Necessary to execute COPY command without access to a superuser

Closes #2698 from andyxhadji/AIRFLOW-1726

(cherry picked from commit 6372770be6aab67654cede81d6b027a838077a8a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Copy Expert command for Postgres Hook
> -------------------------------------
>
>                 Key: AIRFLOW-1726
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1726
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> The Copy command is useful for postgres users, and it''s client-based version ""\COPY"" can be frustrating to use due to python escape character shenanigans. Fortunately, psycopg2 supports a copy_expert method that simplifies the command and it''s options, so I think it would be a useful addition to the PostgresHook.
> I''m working on the PR now, will post here when complete.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26647,54,JIRA.13110079.1508264991000.10792.1508439180790@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13110079.1508264991000@Atlassian.JIRA,,,2017-10-19 11:53:00-07,"[jira] [Resolved] (AIRFLOW-1726) Copy Expert command for Postgres
 Hook","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1726?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1726.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2698
[https://github.com/apache/incubator-airflow/pull/2698]

> Copy Expert command for Postgres Hook
> -------------------------------------
>
>                 Key: AIRFLOW-1726
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1726
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> The Copy command is useful for postgres users, and it''s client-based version ""\COPY"" can be frustrating to use due to python escape character shenanigans. Fortunately, psycopg2 supports a copy_expert method that simplifies the command and it''s options, so I think it would be a useful addition to the PostgresHook.
> I''m working on the PR now, will post here when complete.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26648,54,JIRA.13110079.1508264991000.10781.1508439180626@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110079.1508264991000@Atlassian.JIRA,,,2017-10-19 11:53:00-07,"[jira] [Commented] (AIRFLOW-1726) Copy Expert command for Postgres
 Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1726?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16211543#comment-16211543 ] 

ASF subversion and git services commented on AIRFLOW-1726:
----------------------------------------------------------

Commit 8a4ad30104d9e05589167ec0cde53ad941b9a1f7 in incubator-airflow''s branch refs/heads/v1-9-test from [~andyxhadji]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=8a4ad30 ]

[AIRFLOW-1726] Add copy_expert psycopg2 method to PostgresHook

Executes SQL using psycopg2 copy_expert method
Necessary to execute COPY command without access to a superuser

Closes #2698 from andyxhadji/AIRFLOW-1726

(cherry picked from commit 6372770be6aab67654cede81d6b027a838077a8a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Copy Expert command for Postgres Hook
> -------------------------------------
>
>                 Key: AIRFLOW-1726
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1726
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> The Copy command is useful for postgres users, and it''s client-based version ""\COPY"" can be frustrating to use due to python escape character shenanigans. Fortunately, psycopg2 supports a copy_expert method that simplifies the command and it''s options, so I think it would be a useful addition to the PostgresHook.
> I''m working on the PR now, will post here when complete.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26649,54,JIRA.13110771.1508440455000.10987.1508440500725@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13110771.1508440455000@Atlassian.JIRA,,,2017-10-19 12:15:00-07,"[jira] [Work started] (AIRFLOW-1739) Cleanup naming ambiguity with
 TestDbApiHook test class","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1739?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1739 started by Andy Hadjigeorgiou.
---------------------------------------------------
> Cleanup naming ambiguity with TestDbApiHook test class
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1739
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1739
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Trivial
>
> The TestDbApiHook class creates a class whose name is TestDBApiHook - I''m proposing a simple naming change to make sure the two are distinguishable.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26650,54,JIRA.13110771.1508440455000.10984.1508440500491@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13110771.1508440455000@Atlassian.JIRA,,,2017-10-19 12:15:00-07,"[jira] [Created] (AIRFLOW-1739) Cleanup naming ambiguity with
 TestDbApiHook test class","Andy Hadjigeorgiou created AIRFLOW-1739:
-------------------------------------------

             Summary: Cleanup naming ambiguity with TestDbApiHook test class
                 Key: AIRFLOW-1739
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1739
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Andy Hadjigeorgiou
            Assignee: Andy Hadjigeorgiou
            Priority: Trivial


The TestDbApiHook class creates a class whose name is TestDBApiHook - I''m proposing a simple naming change to make sure the two are distinguishable.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26651,54,JIRA.13109034.1507839702000.11692.1508444520305@Atlassian.JIRA,2240,Steve Jacobs (JIRA),JIRA.13109034.1507839702000@Atlassian.JIRA,,,2017-10-19 13:22:00-07,"[jira] [Commented] (AIRFLOW-1711) Ldap Attributes not always a
 ""list"" part 2","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1711?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16211671#comment-16211671 ] 

Steve Jacobs commented on AIRFLOW-1711:
---------------------------------------

I''ve opened a PR with a fix for this issue. 

> Ldap Attributes not always a ""list"" part 2
> ------------------------------------------
>
>                 Key: AIRFLOW-1711
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1711
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.7.1
>         Environment: Linux + Active Directory
>            Reporter: Steve Jacobs
>
> in the LDAP auth module
> `group_contains_user` checks for `resp[''attributes''].get(user_name_attr)[0] == username`
> Some Ldaps apparently have this as a simple string
> `resp[''attributes''].get(user_name_attr) == username` 
> also should be checked. 
> But really a test should be done to see if the return is a ''list'' and perform the check differently. If its not a list, python will check both arguments and exit with an error. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26652,54,JIRA.13110799.1508447038000.12225.1508447040192@Atlassian.JIRA,1375,Joy Gao (JIRA),JIRA.13110799.1508447038000@Atlassian.JIRA,,,2017-10-19 14:04:00-07,[jira] [Created] (AIRFLOW-1740) Cannot add XCOM via UI,"Joy Gao created AIRFLOW-1740:
--------------------------------

             Summary: Cannot add XCOM via UI
                 Key: AIRFLOW-1740
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1740
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.9.0
            Reporter: Joy Gao
            Priority: Minor


I cannot create/update XCOM via UI. 

When attempting to update an existing dag''s xcom, the following error is received:

{code:java}
Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''\\x80\\x03J+\\x92\\xdbYa.''""}]]
{code}

And for creating a new xcom:


{code:java}
Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
{code}







--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26653,54,JIRA.13110799.1508447038000.12281.1508447340976@Atlassian.JIRA,1375,Joy Gao (JIRA),JIRA.13110799.1508447038000@Atlassian.JIRA,,,2017-10-19 14:09:00-07,[jira] [Updated] (AIRFLOW-1740) Cannot add XCOM via UI in PY3,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1740?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Joy Gao updated AIRFLOW-1740:
-----------------------------
    Summary: Cannot add XCOM via UI in PY3  (was: Cannot add XCOM via UI)

> Cannot add XCOM via UI in PY3
> -----------------------------
>
>                 Key: AIRFLOW-1740
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1740
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>         Environment: PY3
>            Reporter: Joy Gao
>            Priority: Minor
>
> I cannot create/update XCOM via UI in PY3.
> When attempting to update an existing dag''s xcom, the following error is received:
> {code:java}
> Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''\\x80\\x03J+\\x92\\xdbYa.''""}]]
> {code}
> And for creating a new xcom:
> {code:java}
> Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26654,54,JIRA.13110799.1508447038000.12279.1508447340956@Atlassian.JIRA,1375,Joy Gao (JIRA),JIRA.13110799.1508447038000@Atlassian.JIRA,,,2017-10-19 14:09:00-07,[jira] [Updated] (AIRFLOW-1740) Cannot add XCOM via UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1740?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Joy Gao updated AIRFLOW-1740:
-----------------------------
    Environment: PY3
    Description: 
I cannot create/update XCOM via UI in PY3.

When attempting to update an existing dag''s xcom, the following error is received:

{code:java}
Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''\\x80\\x03J+\\x92\\xdbYa.''""}]]
{code}

And for creating a new xcom:


{code:java}
Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
{code}





  was:
I cannot create/update XCOM via UI. 

When attempting to update an existing dag''s xcom, the following error is received:

{code:java}
Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''\\x80\\x03J+\\x92\\xdbYa.''""}]]
{code}

And for creating a new xcom:


{code:java}
Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
{code}






> Cannot add XCOM via UI
> ----------------------
>
>                 Key: AIRFLOW-1740
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1740
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>         Environment: PY3
>            Reporter: Joy Gao
>            Priority: Minor
>
> I cannot create/update XCOM via UI in PY3.
> When attempting to update an existing dag''s xcom, the following error is received:
> {code:java}
> Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''\\x80\\x03J+\\x92\\xdbYa.''""}]]
> {code}
> And for creating a new xcom:
> {code:java}
> Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26655,54,JIRA.13110799.1508447038000.12282.1508447340985@Atlassian.JIRA,1375,Joy Gao (JIRA),JIRA.13110799.1508447038000@Atlassian.JIRA,,,2017-10-19 14:09:00-07,"[jira] [Updated] (AIRFLOW-1740) Cannot create/update XCOM via UI in
 PY3","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1740?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Joy Gao updated AIRFLOW-1740:
-----------------------------
    Summary: Cannot create/update XCOM via UI in PY3  (was: Cannot add XCOM via UI in PY3)

> Cannot create/update XCOM via UI in PY3
> ---------------------------------------
>
>                 Key: AIRFLOW-1740
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1740
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>         Environment: PY3
>            Reporter: Joy Gao
>            Priority: Minor
>
> I cannot create/update XCOM via UI in PY3.
> When attempting to update an existing dag''s xcom, the following error is received:
> {code:java}
> Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''\\x80\\x03J+\\x92\\xdbYa.''""}]]
> {code}
> And for creating a new xcom:
> {code:java}
> Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26656,54,JIRA.13110799.1508447038000.12304.1508447460085@Atlassian.JIRA,1375,Joy Gao (JIRA),JIRA.13110799.1508447038000@Atlassian.JIRA,,,2017-10-19 14:11:00-07,"[jira] [Updated] (AIRFLOW-1740) Cannot create/update XCOM via UI in
 PY3","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1740?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Joy Gao updated AIRFLOW-1740:
-----------------------------
    Description: 
I cannot create/update XCOM via UI in PY3.

When attempting to update an existing dag''s xcom, the following error is received:

{code:java}
Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''bar''""}]]
{code}

And for creating a new xcom:


{code:java}
Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
{code}





  was:
I cannot create/update XCOM via UI in PY3.

When attempting to update an existing dag''s xcom, the following error is received:

{code:java}
Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''\\x80\\x03J+\\x92\\xdbYa.''""}]]
{code}

And for creating a new xcom:


{code:java}
Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
{code}






> Cannot create/update XCOM via UI in PY3
> ---------------------------------------
>
>                 Key: AIRFLOW-1740
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1740
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>         Environment: PY3
>            Reporter: Joy Gao
>            Priority: Minor
>
> I cannot create/update XCOM via UI in PY3.
> When attempting to update an existing dag''s xcom, the following error is received:
> {code:java}
> Failed to update record. (builtins.TypeError) string argument without an encoding [SQL: ''UPDATE xcom SET value=%%%%s WHERE xcom.id = %%%%s''] [parameters: [{''xcom_id'': 165, ''value'': ""b''bar''""}]]
> {code}
> And for creating a new xcom:
> {code:java}
> Failed to create record. (builtins.TypeError) string argument without an encoding [SQL: ''INSERT INTO xcom (`key`, value, timestamp, execution_date, task_id, dag_id) VALUES (%%%%s, %%%%s, now(), %%%%s, %%%%s, %%%%s)''] [parameters: [{''execution_date'': datetime.datetime(2017, 10, 7, 1, 1), ''value'': ''bar'', ''task_id'': ''test_task'', ''key'': ''foo'', ''dag_id'': ''test_dag''}]]
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26657,54,JIRA.13086271.1499782721000.15286.1508477220112@Atlassian.JIRA,2330,Manu Zhang (JIRA),JIRA.13086271.1499782721000@Atlassian.JIRA,,,2017-10-19 22:27:00-07,[jira] [Commented] (AIRFLOW-1400) catchup=False caused exception,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1400?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212185#comment-16212185 ] 

Manu Zhang commented on AIRFLOW-1400:
-------------------------------------

I''ve also seen the same issue when {{schedule_interval == ''@once''}} and {{catchup == False}} in Airflow 1.8.2.

> catchup=False caused exception
> ------------------------------
>
>                 Key: AIRFLOW-1400
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1400
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: Airflow 1.8
>            Reporter: Xi Wang
>
> When I set of the task with catchup=False, it threw a error as follow(logs/scheduler/my_dag_name):
> [2017-07-10 15:13:12,534] {jobs.py:354} DagFileProcessor373 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1171, in _process_dags
>     dag_run = self.create_dag_run(dag)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 776, in create_dag_run
>     if next_start <= now:
> TypeError: can''t compare datetime.datetime to NoneType
> It seems next_start was not defined properly, (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L777)
> Any help is appreciated.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26658,54,JIRA.13086271.1499782721000.15290.1508477340755@Atlassian.JIRA,2330,Manu Zhang (JIRA),JIRA.13086271.1499782721000@Atlassian.JIRA,,,2017-10-19 22:29:00-07,"[jira] [Comment Edited] (AIRFLOW-1400) catchup=False caused
 exception","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1400?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212185#comment-16212185 ] 

Manu Zhang edited comment on AIRFLOW-1400 at 10/20/17 5:28 AM:
---------------------------------------------------------------

I''ve also seen the same issue when {{schedule_interval == ''@once''}} and {{catchup == False}} in Airflow 1.8.2. When set to {{@once}}, {{schedule_interval}} is actually set to {{None}} internally. 


was (Author: mauzhang):
I''ve also seen the same issue when {{schedule_interval == ''@once''}} and {{catchup == False}} in Airflow 1.8.2.

> catchup=False caused exception
> ------------------------------
>
>                 Key: AIRFLOW-1400
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1400
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: Airflow 1.8
>            Reporter: Xi Wang
>
> When I set of the task with catchup=False, it threw a error as follow(logs/scheduler/my_dag_name):
> [2017-07-10 15:13:12,534] {jobs.py:354} DagFileProcessor373 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1171, in _process_dags
>     dag_run = self.create_dag_run(dag)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 776, in create_dag_run
>     if next_start <= now:
> TypeError: can''t compare datetime.datetime to NoneType
> It seems next_start was not defined properly, (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L777)
> Any help is appreciated.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26659,54,JIRA.13066720.1493133634000.17985.1508506860872@Atlassian.JIRA,2208,Junyoung Park (JIRA),JIRA.13066720.1493133634000@Atlassian.JIRA,,,2017-10-20 06:41:00-07,[jira] [Assigned] (AIRFLOW-1146) izip use in Python 3.4,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1146?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Junyoung Park reassigned AIRFLOW-1146:
--------------------------------------

    Assignee:     (was: Junyoung Park)

> izip use in Python 3.4
> ----------------------
>
>                 Key: AIRFLOW-1146
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1146
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hive_hooks
>    Affects Versions: Airflow 1.8
>            Reporter: Alexander Panzhin
>
> Python 3 no longer has itertools.izip, but it is still used in airflow/hooks/hive_hooks.py
> This causes all kinds of havoc.
> This needs fixed, if this is to be used on Python 3+



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26660,54,JIRA.13063355.1491959771000.18047.1508507940241@Atlassian.JIRA,2208,Junyoung Park (JIRA),JIRA.13063355.1491959771000@Atlassian.JIRA,,,2017-10-20 06:59:00-07,"[jira] [Assigned] (AIRFLOW-1101) Docs use the term ""Pipelines""
 instead of DAGs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1101?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Junyoung Park reassigned AIRFLOW-1101:
--------------------------------------

    Assignee: Junyoung Park

> Docs use the term ""Pipelines"" instead of DAGs
> ---------------------------------------------
>
>                 Key: AIRFLOW-1101
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1101
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Andrew Chen
>            Assignee: Junyoung Park
>
> It''s a bit confusing to use the words ""DAGs"" and ""pipelines"" interchangea=
bly in the documentation. Here are a couple examples where it is especially=
 confusing.
> https://airflow.incubator.apache.org/configuration.html#connections
> ""The pipeline code you will author will reference the =E2=80=98conn_id=E2=
=80=99 of the Connection objects.""
> ""Connections in Airflow pipelines can be created using environment variab=
les.""
> https://airflow.incubator.apache.org/configuration.html#scaling-out-with-=
celery
> ""If all your boxes have a common mount point, having your pipelines files=
 shared there should work as well""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
666965,23,JIRA.12976088.1465223118000.11758.1465286541122@Atlassian.JIRA,25074,Simeon Aladjem (JIRA),JIRA.12976088.1465223118000@Atlassian.JIRA,,,2016-06-07 01:02:21-07,"[jira] [Updated] (MILAGRO-4) [Standalone D-TA] Create an initial
 D-TA Proxy service","
     [ https://issues.apache.org/jira/browse/MILAGRO-4?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Simeon Aladjem updated MILAGRO-4:
---------------------------------
    Description: 
The D-TA Proxy exists in order to prevent direct access to the D-TA itself.

* The D-TA Proxy should be written in Golang in order to utilize this technology''s strengths
* The functionality of the D-TA Proxy is:
   ** To receive the requests for keys generation or time permits and verify that hey were made by a valid source
   ** Potentially to update a database with the events of generating Client Keys and Time Permits

  was:
The D-TA Proxy exists in order to prevent direct access to the D-TA itself.

The functionality of the D-TA Proxy is:
* To receive the requests for keys generation or time permits and verify that hey were made by a valid source
* Potentially to update a database with the events of generating Client Keys and Time Permits


> [Standalone D-TA] Create an initial D-TA Proxy service 
> -------------------------------------------------------
>
>                 Key: MILAGRO-4
>                 URL: https://issues.apache.org/jira/browse/MILAGRO-4
>             Project: Milagro
>          Issue Type: Task
>          Components: Standalone D-TA
>            Reporter: Simeon Aladjem
>
> The D-TA Proxy exists in order to prevent direct access to the D-TA itself.
> * The D-TA Proxy should be written in Golang in order to utilize this technology''s strengths
> * The functionality of the D-TA Proxy is:
>    ** To receive the requests for keys generation or time permits and verify that hey were made by a valid source
>    ** Potentially to update a database with the events of generating Client Keys and Time Permits



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26661,54,JIRA.13088499.1500489709000.19216.1508516880135@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13088499.1500489709000@Atlassian.JIRA,,,2017-10-20 09:28:00-07,"[jira] [Assigned] (AIRFLOW-1432) NVD3 Charts do not have labeled
 axes and units change dynamically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1432?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor reassigned AIRFLOW-1432:
------------------------------------------

    Assignee: Ash Berlin-Taylor

> NVD3 Charts do not have labeled axes and units change dynamically
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1432
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1432
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Ash Berlin-Taylor
>
> E.g. for the landing times chart, the y axis isn''t labeled (e.g. minutes/hours/days), and changes dynamically based on the data points in the chart/the pixel height of the chart. The y axis should be labeled in these charts.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26662,54,JIRA.13047391.1488390378000.19488.1508518740406@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13047391.1488390378000@Atlassian.JIRA,,,2017-10-20 09:59:00-07,"[jira] [Commented] (AIRFLOW-930) Task Duration Visualization
 Regression","
    [ https://issues.apache.org/jira/browse/AIRFLOW-930?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212886#comment-16212886 ] 

Ash Berlin-Taylor commented on AIRFLOW-930:
-------------------------------------------

No label on the Y axis is reported as AIRFLOW-1432.

> Task Duration Visualization Regression
> --------------------------------------
>
>                 Key: AIRFLOW-930
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-930
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Kevin Mandich
>
> I''m not sure if this is the right forum for this, but here goes:
> The charts in 1.8.0 seem like a major regression from the previous versions. Here are are some issues i''ve identified, ranked by decreasing severity.
> * There is no label on the y-axis. When looking at the charts, I don''t know what value I''m looking at.
> * Markers on the data points were removed. This makes it hard to see actual run times for tasks. It also removes the tremendous advantage of being able to quickly see when tasks failed.
> * The ""Hide All Series"" and ""Show All Series"" buttons have been removed. This makes viewing only a single task amongst many a very tedious process.
> * The default y-axis value is no longer 0, but instead the lowest value of the visible data points. This is a visualization anti-pattern and can be misleading (e.g. a trace whose y-values vary from 0.0091 to 0.0092 may appear to be increasing or decreasing with time, when in reality it''s basically staying the same)
> * When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two. What is the point of showing two separate charts in the first place?
> * Gridlines aren''t necessary, especially since you can mouse-over each data point to find the value. These add unnecessary clutter and make it hard to distinguish the signal from the noise.
> * A nitpick, but: why was the spline version of the charts removed? At the minimum it''d be nice to switch back and forth between spline and raw forms.
> Most of these issues are also relevant to the Task Tries and Landing Times visualizations, as well.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26663,54,JIRA.13047391.1488390378000.19548.1508519041524@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13047391.1488390378000@Atlassian.JIRA,,,2017-10-20 10:04:01-07,"[jira] [Comment Edited] (AIRFLOW-930) Task Duration Visualization
 Regression","
    [ https://issues.apache.org/jira/browse/AIRFLOW-930?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212886#comment-16212886 ] 

Ash Berlin-Taylor edited comment on AIRFLOW-930 at 10/20/17 5:03 PM:
---------------------------------------------------------------------

No label on the Y axis is reported as AIRFLOW-1432 (for which I''ve just submitted a pull request..


was (Author: ashb):
No label on the Y axis is reported as AIRFLOW-1432.

> Task Duration Visualization Regression
> --------------------------------------
>
>                 Key: AIRFLOW-930
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-930
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Kevin Mandich
>
> I''m not sure if this is the right forum for this, but here goes:
> The charts in 1.8.0 seem like a major regression from the previous versions. Here are are some issues i''ve identified, ranked by decreasing severity.
> * There is no label on the y-axis. When looking at the charts, I don''t know what value I''m looking at.
> * Markers on the data points were removed. This makes it hard to see actual run times for tasks. It also removes the tremendous advantage of being able to quickly see when tasks failed.
> * The ""Hide All Series"" and ""Show All Series"" buttons have been removed. This makes viewing only a single task amongst many a very tedious process.
> * The default y-axis value is no longer 0, but instead the lowest value of the visible data points. This is a visualization anti-pattern and can be misleading (e.g. a trace whose y-values vary from 0.0091 to 0.0092 may appear to be increasing or decreasing with time, when in reality it''s basically staying the same)
> * When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two. What is the point of showing two separate charts in the first place?
> * Gridlines aren''t necessary, especially since you can mouse-over each data point to find the value. These add unnecessary clutter and make it hard to distinguish the signal from the noise.
> * A nitpick, but: why was the spline version of the charts removed? At the minimum it''d be nice to switch back and forth between spline and raw forms.
> Most of these issues are also relevant to the Task Tries and Landing Times visualizations, as well.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26664,54,JIRA.13111040.1508519267000.19580.1508519280187@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13111040.1508519267000@Atlassian.JIRA,,,2017-10-20 10:08:00-07,"[jira] [Created] (AIRFLOW-1741) Task Duration shows two charts on
 first page load.","Ash Berlin-Taylor created AIRFLOW-1741:
------------------------------------------

             Summary: Task Duration shows two charts on first page load.
                 Key: AIRFLOW-1741
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1741
             Project: Apache Airflow
          Issue Type: Bug
          Components: ui
    Affects Versions: 1.8.0
            Reporter: Ash Berlin-Taylor
            Priority: Minor
             Fix For: 1.10.0


As reported in AIRFLOW-930 

> When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two.

This is because of an issue with the the chart code running a trigger too early - before the trigger callback has been registered.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26665,54,JIRA.13111043.1508519871000.19635.1508519880023@Atlassian.JIRA,2285,Andrew Loughran (JIRA),JIRA.13111043.1508519871000@Atlassian.JIRA,,,2017-10-20 10:18:00-07,"[jira] [Created] (AIRFLOW-1742) Cannot work out why queued tasks
 aren''t being processed.","Andrew Loughran created AIRFLOW-1742:
----------------------------------------

             Summary: Cannot work out why queued tasks aren''t being processed.
                 Key: AIRFLOW-1742
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1742
             Project: Apache Airflow
          Issue Type: Wish
            Reporter: Andrew Loughran


I have run both LocalExecutor and CeleryWorker with a remote postgresDB.

On the initial run, the DAG completed successfully, but subsequent runs the execution of the tasks halts.  The UI reports that the tasks are in the queue, but the executor fails to pick them up to process them.  This has resulted in stopped DAGS, and ultimately the only way forward was an `airflow resetdb` to get a fresh state, restart all the airflow services (scheduler, worker, webserver & flower), and a flush of the redis queue.

I''d like to help troubleshoot what could be causing this issue, but probably need assistance with how to setup the logs, and replicate the problem with the debugging turned on fully.  Once that''s done, I''m hoping it will be a more obvious fix.

Opening this ticket as I will try and start the work by myself and post the logs as attachments, but will require support from other developers as and when I''ve got the logs here.

Thanks,

Andy



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549385,277,JIRA.12986655.1467653918000.29090.1467746591623@Atlassian.JIRA,21272,Barbara Gomes (JIRA),JIRA.12986655.1467653918000@Atlassian.JIRA,,,2016-07-05 12:23:11-07,"[jira] [Commented] (IOTA-21) Refactor Built of performer to manage
 multi module dependency more efficient and scala style plugin for code
 quality","
    [ https://issues.apache.org/jira/browse/IOTA-21?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15363066#comment-15363066 ] 

Barbara Gomes commented on IOTA-21:
-----------------------------------

I don''t know if you saw that I edited my comment in the PR.

""Can you also remove the building settings for fey *lazy val FeybuildSettings*""

> Refactor Built of performer to manage multi module dependency more efficient and scala style plugin for code quality 
> ---------------------------------------------------------------------------------------------------------------------
>
>                 Key: IOTA-21
>                 URL: https://issues.apache.org/jira/browse/IOTA-21
>             Project: Iota
>          Issue Type: Improvement
>            Reporter: sandeep purohit
>            Assignee: sandeep purohit
>   Original Estimate: 3h
>  Remaining Estimate: 3h
>
> In this improvement refactoring the code to manage multi module dependencies more efficient way and add scala style plugin for code quality .



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26666,54,JIRA.13047391.1488390378000.19670.1508520060807@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13047391.1488390378000@Atlassian.JIRA,,,2017-10-20 10:21:00-07,"[jira] [Commented] (AIRFLOW-930) Task Duration Visualization
 Regression","
    [ https://issues.apache.org/jira/browse/AIRFLOW-930?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212912#comment-16212912 ] 

Ash Berlin-Taylor commented on AIRFLOW-930:
-------------------------------------------

And point 5 addressed in AIRFLOW-1741

> Task Duration Visualization Regression
> --------------------------------------
>
>                 Key: AIRFLOW-930
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-930
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Kevin Mandich
>
> I''m not sure if this is the right forum for this, but here goes:
> The charts in 1.8.0 seem like a major regression from the previous versions. Here are are some issues i''ve identified, ranked by decreasing severity.
> * There is no label on the y-axis. When looking at the charts, I don''t know what value I''m looking at.
> * Markers on the data points were removed. This makes it hard to see actual run times for tasks. It also removes the tremendous advantage of being able to quickly see when tasks failed.
> * The ""Hide All Series"" and ""Show All Series"" buttons have been removed. This makes viewing only a single task amongst many a very tedious process.
> * The default y-axis value is no longer 0, but instead the lowest value of the visible data points. This is a visualization anti-pattern and can be misleading (e.g. a trace whose y-values vary from 0.0091 to 0.0092 may appear to be increasing or decreasing with time, when in reality it''s basically staying the same)
> * When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two. What is the point of showing two separate charts in the first place?
> * Gridlines aren''t necessary, especially since you can mouse-over each data point to find the value. These add unnecessary clutter and make it hard to distinguish the signal from the noise.
> * A nitpick, but: why was the spline version of the charts removed? At the minimum it''d be nice to switch back and forth between spline and raw forms.
> Most of these issues are also relevant to the Task Tries and Landing Times visualizations, as well.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26667,54,JIRA.13110297.1508334610000.19657.1508520060537@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13110297.1508334610000@Atlassian.JIRA,,,2017-10-20 10:21:00-07,[jira] [Updated] (AIRFLOW-1731) Import custom config on PYTHONPATH,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1731?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1731:
-------------------------------------
    Priority: Blocker  (was: Major)

> Import custom config on PYTHONPATH
> ----------------------------------
>
>                 Key: AIRFLOW-1731
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1731
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: logging
>    Affects Versions: 1.9.0
>            Reporter: Fokko Driesprong
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Currently the PYTHONPATH does not contain the required path to import a custom config as described. This needs to be fixed and the instructions needs to be updated based on user feedback.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26668,54,JIRA.13111040.1508519267000.19716.1508520541916@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13111040.1508519267000@Atlassian.JIRA,,,2017-10-20 10:29:01-07,"[jira] [Updated] (AIRFLOW-1741) Task Duration shows two charts on
 first page load.","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1741?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor updated AIRFLOW-1741:
---------------------------------------
    External issue URL: https://github.com/apache/incubator-airflow/pull/2711

> Task Duration shows two charts on first page load.
> --------------------------------------------------
>
>                 Key: AIRFLOW-1741
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1741
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> As reported in AIRFLOW-930 
> > When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two.
> This is because of an issue with the the chart code running a trigger too early - before the trigger callback has been registered.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26669,54,JIRA.13111040.1508519267000.19711.1508520541862@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13111040.1508519267000@Atlassian.JIRA,,,2017-10-20 10:29:01-07,"[jira] [Assigned] (AIRFLOW-1741) Task Duration shows two charts on
 first page load.","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1741?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor reassigned AIRFLOW-1741:
------------------------------------------

    Assignee: Ash Berlin-Taylor

> Task Duration shows two charts on first page load.
> --------------------------------------------------
>
>                 Key: AIRFLOW-1741
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1741
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> As reported in AIRFLOW-930 
> > When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two.
> This is because of an issue with the the chart code running a trigger too early - before the trigger callback has been registered.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26670,54,JIRA.13088499.1500489709000.19721.1508520600589@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13088499.1500489709000@Atlassian.JIRA,,,2017-10-20 10:30:00-07,"[jira] [Commented] (AIRFLOW-1432) NVD3 Charts do not have labeled
 axes and units change dynamically","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1432?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212920#comment-16212920 ] 

Ash Berlin-Taylor commented on AIRFLOW-1432:
--------------------------------------------

Opened PR https://github.com/apache/incubator-airflow/pull/2710

> NVD3 Charts do not have labeled axes and units change dynamically
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1432
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1432
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Ash Berlin-Taylor
>
> E.g. for the landing times chart, the y axis isn''t labeled (e.g. minutes/hours/days), and changes dynamically based on the data points in the chart/the pixel height of the chart. The y axis should be labeled in these charts.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
666966,23,JIRA.12977190.1465469318000.38638.1465469360966@Atlassian.JIRA,25066,Tom Matecki (JIRA),JIRA.12977190.1465469318000@Atlassian.JIRA,,,2016-06-09 03:49:20-07,[jira] [Created] (MILAGRO-10) Milestones for MILAGRO project,"Tom Matecki created MILAGRO-10:
----------------------------------

             Summary: Milestones for MILAGRO project
                 Key: MILAGRO-10
                 URL: https://issues.apache.org/jira/browse/MILAGRO-10
             Project: Milagro
          Issue Type: Task
            Reporter: Tom Matecki


Documenting MILAGRO milestones



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26671,54,JIRA.13104708.1506332962000.19746.1508520780446@Atlassian.JIRA,2285,Andrew Loughran (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-20 10:33:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212927#comment-16212927 ] 

Andrew Loughran commented on AIRFLOW-1641:
------------------------------------------

I''d love to help resolve this bug.  Let me know how we can work on it together.

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>              Labels: queued, scheduler, stuck, task
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26672,54,JIRA.13110115.1508271682000.19756.1508520780599@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110115.1508271682000@Atlassian.JIRA,,,2017-10-20 10:33:00-07,"[jira] [Commented] (AIRFLOW-1728) Add networkUri, subnetworkUri and
 tags to DataprocClusterCreateOperator","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1728?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212930#comment-16212930 ] 

ASF subversion and git services commented on AIRFLOW-1728:
----------------------------------------------------------

Commit eb012a3c8daecdbe20c13958468fbd21ffbbbe3e in incubator-airflow''s branch refs/heads/master from jfantom
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=eb012a3 ]

[AIRFLOW-1728] Add networkUri, subnet, tags to Dataproc operator

Closes #2706 from jfantom/master


> Add networkUri, subnetworkUri and tags to DataprocClusterCreateOperator 
> ------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1728
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1728
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Jessica Chen Fan 
>            Assignee: Jessica Chen Fan 
>            Priority: Minor
>
> Add ability to specify subnetwork and firewall tags for easier networking when creating 
> dataproc clusters.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26673,54,JIRA.13110115.1508271682000.19759.1508520840754@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13110115.1508271682000@Atlassian.JIRA,,,2017-10-20 10:34:00-07,"[jira] [Resolved] (AIRFLOW-1728) Add networkUri, subnetworkUri and
 tags to DataprocClusterCreateOperator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1728?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1728.
--------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> Add networkUri, subnetworkUri and tags to DataprocClusterCreateOperator 
> ------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1728
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1728
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Jessica Chen Fan 
>            Assignee: Jessica Chen Fan 
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Add ability to specify subnetwork and firewall tags for easier networking when creating 
> dataproc clusters.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26674,54,JIRA.13104708.1506332962000.19777.1508520840940@Atlassian.JIRA,2285,Andrew Loughran (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-20 10:34:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212934#comment-16212934 ] 

Andrew Loughran commented on AIRFLOW-1641:
------------------------------------------

Running on AWS.

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>              Labels: queued, scheduler, stuck, task
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26675,54,JIRA.13111047.1508521809000.19857.1508521861133@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13111047.1508521809000@Atlassian.JIRA,,,2017-10-20 10:51:01-07,"[jira] [Created] (AIRFLOW-1743) Default config template should not
 contain ldap filters","Bolke de Bruin created AIRFLOW-1743:
---------------------------------------

             Summary: Default config template should not contain ldap filters
                 Key: AIRFLOW-1743
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1743
             Project: Apache Airflow
          Issue Type: Bug
          Components: authentication
            Reporter: Bolke de Bruin
             Fix For: 1.9.0


The config template functions as a template. In case a item is not in the airflow.cfg of the user it is used for default values.

This should not be the case for filters for ldap users, as that can create unexpected results such as users not getting the right access levels.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26676,54,JIRA.13104708.1506332962000.19941.1508522280385@Atlassian.JIRA,2188,Nick Bauer (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-20 10:58:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212960#comment-16212960 ] 

Nick Bauer commented on AIRFLOW-1641:
-------------------------------------

Not sure you to ping the author of this, but one thing that made our situation WAY better is to increase the dag bag loading timeout. We were experiencing timeouts when the task would start, and the first thing it does is load the dag it belongs to.  If that process timed out (ours was taking 6 minutes), especially when the server was under heavy load

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>              Labels: queued, scheduler, stuck, task
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26677,54,JIRA.13104708.1506332962000.19978.1508522640394@Atlassian.JIRA,2340,Andy Loughran (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-20 11:04:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212966#comment-16212966 ] 

Andy Loughran commented on AIRFLOW-1641:
----------------------------------------

How easy is that coding change; or is it a variable in airflow.cfg?

Sent from my iPhone



> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>              Labels: queued, scheduler, stuck, task
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26678,54,JIRA.13099337.1504306043000.20119.1508523540191@Atlassian.JIRA,2341,Jeffrey Bian (JIRA),JIRA.13099337.1504306043000@Atlassian.JIRA,,,2017-10-20 11:19:00-07,"[jira] [Commented] (AIRFLOW-1559) MySQL warnings about aborted
 connections, missing engine disposal","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1559?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16212993#comment-16212993 ] 

Jeffrey Bian commented on AIRFLOW-1559:
---------------------------------------

Proposed fix:

https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L354

At the beginning of the *helper* function, because the engine will be a ''copy'' in the subprocess, need to close them explicitly first and let the subsequent function implicitly recreate the resource via import settings etc.
{{settings.engine.dispose()}}

Then at the end of the function, do another round of 

{{settings.engine.dispose()}}

Comments?



> MySQL warnings about aborted connections, missing engine disposal
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1559
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1559
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: db
>            Reporter: Daniel Huang
>            Priority: Minor
>
> We''re seeing a flood of warnings about aborted connections in our MySQL logs. 
> {code}
> Aborted connection 56720 to db: ''airflow'' user: ''foo'' host: ''x.x.x.x'' (Got an error reading communication packets)
> {code}
> It appears this is because we''re not performing [engine disposal|http://docs.sqlalchemy.org/en/latest/core/connections.html#engine-disposal]. The most common source of this warning is from the scheduler, when it kicks off new processes to process the DAG files. Calling dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L403 greatly reduced these messages. However, the worker is still causing some of these, I assume from when we spin up processes to run tasks. We do call dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L1394-L1396, but I think it''s a bit early. Not sure if there''s a place we can put this cleanup to ensure it''s done everywhere.
> Quick script to reproduce this warning message:
> {code}
> from airflow import settings
> from airflow.models import Connection
> session = settings.Session()
> session.query(Connection).count()
> session.close()
> # not calling settings.engine.dispose()
> {code}
> Reproduced with Airflow 1.8.1, MySQL 5.7, and SQLAlchemy 1.1.13. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26679,54,JIRA.13111064.1508524368000.20256.1508524380627@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13111064.1508524368000@Atlassian.JIRA,,,2017-10-20 11:33:00-07,[jira] [Created] (AIRFLOW-1744) task.retries can be False,"Bolke de Bruin created AIRFLOW-1744:
---------------------------------------

             Summary: task.retries can be False 
                 Key: AIRFLOW-1744
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1744
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Bolke de Bruin


When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. 

It is proposed to set it to try_number in case try_number > 0 otherwise to 1.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26680,54,JIRA.13111064.1508524368000.20257.1508524440148@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13111064.1508524368000@Atlassian.JIRA,,,2017-10-20 11:34:00-07,[jira] [Updated] (AIRFLOW-1744) task.retries can be False,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1744?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1744:
------------------------------------
    Fix Version/s: 1.9.0

> task.retries can be False 
> --------------------------
>
>                 Key: AIRFLOW-1744
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1744
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>
> When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. 
> It is proposed to set it to try_number in case try_number > 0 otherwise to 1.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
875219,186,CAHYtTq=ZhQOsSUc_T-asTuJ1L3in_5xGq1+vApQ1K_MEFj4h+Q@mail.gmail.com,30825,Carlos Santana,NULL,,,2017-05-02 19:02:06-07,ApacheCon 2016: t-shirt ideas?,"I=E2=80=99m attending ApacheCon May 16-18, Who else is attending?

 I want to print a custom t-shirt with some message on the back that would
attract folks to ask me more about Apache OpenWhisk.

Anyone can help out with ideas?

Here are I just came up, very lame but to spark the discussion

- Need to burn your servers? Ask this guy
- Trigger waiting to be fire
- Web Action FTW!
- These are not the cold activations your looking for

Maybe we can print a few to wear there.


--Carlos
",f
26681,54,JIRA.13104708.1506332962000.20341.1508524802072@Atlassian.JIRA,2188,Nick Bauer (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-20 11:40:02-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213027#comment-16213027 ] 

Nick Bauer commented on AIRFLOW-1641:
-------------------------------------

yes, airflow.cfg:
dagbag_import_timeout = 30

30 is the default.

or via environment variables:
export AIRFLOW__CORE_DAGBAG_IMPORT_TIMEOUT=30

Take a look at the log for your queued task that is stuck.  It did not occur to me that a queued task would even have a log, but once I looked at it, it was easy to identify if it the issue was due to a timeout.

Even so, the task should fail instead of stay ""queued"", or retry depending on your settings, possibly email if that is setup.


> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>              Labels: queued, scheduler, stuck, task
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26682,54,JIRA.13111040.1508519267000.20390.1508525100636@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111040.1508519267000@Atlassian.JIRA,,,2017-10-20 11:45:00-07,"[jira] [Commented] (AIRFLOW-1741) Task Duration shows two charts on
 first page load.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1741?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213037#comment-16213037 ] 

ASF subversion and git services commented on AIRFLOW-1741:
----------------------------------------------------------

Commit 3d4feb60906a4e4edbd1dba4b397db1a0b5be8ed in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3d4feb6 ]

[AIRFLOW-1741] Correctly hide second chart on task duration page

A timing/load order bug meant that this "".trigger""
call was happening
too early, resulting in a trigger being sent
before the hook callback
had been registered, meaning we didn''t hide the
second chart correctly
on page load.

Closes #2711 from ashb/AIRFLOW-1741-task-duration-
charts-load


> Task Duration shows two charts on first page load.
> --------------------------------------------------
>
>                 Key: AIRFLOW-1741
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1741
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> As reported in AIRFLOW-930 
> > When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two.
> This is because of an issue with the the chart code running a trigger too early - before the trigger callback has been registered.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26683,54,JIRA.13111040.1508519267000.20393.1508525100876@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111040.1508519267000@Atlassian.JIRA,,,2017-10-20 11:45:00-07,"[jira] [Commented] (AIRFLOW-1741) Task Duration shows two charts on
 first page load.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1741?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213039#comment-16213039 ] 

ASF subversion and git services commented on AIRFLOW-1741:
----------------------------------------------------------

Commit 974b493fc929e46a67c1b68f2e7ef9581f2cde0d in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=974b493 ]

[AIRFLOW-1741] Correctly hide second chart on task duration page

A timing/load order bug meant that this "".trigger""
call was happening
too early, resulting in a trigger being sent
before the hook callback
had been registered, meaning we didn''t hide the
second chart correctly
on page load.

Closes #2711 from ashb/AIRFLOW-1741-task-duration-
charts-load

(cherry picked from commit 3d4feb60906a4e4edbd1dba4b397db1a0b5be8ed)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Task Duration shows two charts on first page load.
> --------------------------------------------------
>
>                 Key: AIRFLOW-1741
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1741
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> As reported in AIRFLOW-930 
> > When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two.
> This is because of an issue with the the chart code running a trigger too early - before the trigger callback has been registered.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26684,54,JIRA.13111040.1508519267000.20392.1508525100868@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111040.1508519267000@Atlassian.JIRA,,,2017-10-20 11:45:00-07,"[jira] [Commented] (AIRFLOW-1741) Task Duration shows two charts on
 first page load.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1741?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213038#comment-16213038 ] 

ASF subversion and git services commented on AIRFLOW-1741:
----------------------------------------------------------

Commit 3d4feb60906a4e4edbd1dba4b397db1a0b5be8ed in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3d4feb6 ]

[AIRFLOW-1741] Correctly hide second chart on task duration page

A timing/load order bug meant that this "".trigger""
call was happening
too early, resulting in a trigger being sent
before the hook callback
had been registered, meaning we didn''t hide the
second chart correctly
on page load.

Closes #2711 from ashb/AIRFLOW-1741-task-duration-
charts-load


> Task Duration shows two charts on first page load.
> --------------------------------------------------
>
>                 Key: AIRFLOW-1741
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1741
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> As reported in AIRFLOW-930 
> > When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two.
> This is because of an issue with the the chart code running a trigger too early - before the trigger callback has been registered.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26685,54,JIRA.13111040.1508519267000.20396.1508525100904@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111040.1508519267000@Atlassian.JIRA,,,2017-10-20 11:45:00-07,"[jira] [Commented] (AIRFLOW-1741) Task Duration shows two charts on
 first page load.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1741?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213040#comment-16213040 ] 

ASF subversion and git services commented on AIRFLOW-1741:
----------------------------------------------------------

Commit 974b493fc929e46a67c1b68f2e7ef9581f2cde0d in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=974b493 ]

[AIRFLOW-1741] Correctly hide second chart on task duration page

A timing/load order bug meant that this "".trigger""
call was happening
too early, resulting in a trigger being sent
before the hook callback
had been registered, meaning we didn''t hide the
second chart correctly
on page load.

Closes #2711 from ashb/AIRFLOW-1741-task-duration-
charts-load

(cherry picked from commit 3d4feb60906a4e4edbd1dba4b397db1a0b5be8ed)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Task Duration shows two charts on first page load.
> --------------------------------------------------
>
>                 Key: AIRFLOW-1741
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1741
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> As reported in AIRFLOW-930 
> > When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two.
> This is because of an issue with the the chart code running a trigger too early - before the trigger callback has been registered.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26686,54,JIRA.13111040.1508519267000.20399.1508525100934@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13111040.1508519267000@Atlassian.JIRA,,,2017-10-20 11:45:00-07,"[jira] [Resolved] (AIRFLOW-1741) Task Duration shows two charts on
 first page load.","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1741?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1741.
-------------------------------------
       Resolution: Fixed
    Fix Version/s:     (was: 1.10.0)
                   1.9.0

Issue resolved by pull request #2711
[https://github.com/apache/incubator-airflow/pull/2711]

> Task Duration shows two charts on first page load.
> --------------------------------------------------
>
>                 Key: AIRFLOW-1741
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1741
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> As reported in AIRFLOW-930 
> > When you first load the page, two charts are shown: Cumulative and Non-cumulative. However, when you toggle the ""Cumulative Duration"" button, one of the charts disappears, and you can now only switch between the two.
> This is because of an issue with the the chart code running a trigger too early - before the trigger callback has been registered.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26687,54,JIRA.13104708.1506332962000.20583.1508526720420@Atlassian.JIRA,2285,Andrew Loughran (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-20 12:12:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213078#comment-16213078 ] 

Andrew Loughran commented on AIRFLOW-1641:
------------------------------------------

Great to see some comments on this issue - thanks [~nickjbauer]!

I agree, and may raise a subsequent bug if I can reproduce it for a timed-out task to go into a failed state, rather than end up being stuck.  IF this is the case, it''s hopefull a case of handling that state management a bit tighter.

Cheers!

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>              Labels: queued, scheduler, stuck, task
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26688,54,JIRA.13111147.1508562846000.24274.1508562900028@Atlassian.JIRA,2305,William Pursell (JIRA),JIRA.13111147.1508562846000@Atlassian.JIRA,,,2017-10-20 22:15:00-07,"[jira] [Created] (AIRFLOW-1745) BashOperator ignores SIGPIPE in
 subprocess","William Pursell created AIRFLOW-1745:
----------------------------------------

             Summary: BashOperator ignores SIGPIPE in subprocess
                 Key: AIRFLOW-1745
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1745
             Project: Apache Airflow
          Issue Type: Bug
          Components: DagRun
    Affects Versions: Airflow 1.8
            Reporter: William Pursell
            Assignee: William Pursell


In Python 2, subprocesses ignore SIGPIPE (https://bugs.python.org/issue1652).  As a result, a simple bash operator with bash_command = ''yes | head'' may never terminate (depending on the implementation of yes).  Airflow should reset the signal disposition to default.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26689,54,JIRA.13111147.1508562846000.24328.1508563620162@Atlassian.JIRA,2305,William Pursell (JIRA),JIRA.13111147.1508562846000@Atlassian.JIRA,,,2017-10-20 22:27:00-07,"[jira] [Commented] (AIRFLOW-1745) BashOperator ignores SIGPIPE in
 subprocess","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1745?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213742#comment-16213742 ] 

William Pursell commented on AIRFLOW-1745:
------------------------------------------

Also, see https://stackoverflow.com/questions/22077881/yes-reporting-error-with-subprocess-communicate  and   https://stackoverflow.com/questions/23064636/python-subprocess-popen-blocks-with-shell-and-pipe/23089677#23089677  It''s not clear to me if signals other than SIGPIPE need be reset.

> BashOperator ignores SIGPIPE in subprocess
> ------------------------------------------
>
>                 Key: AIRFLOW-1745
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1745
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DagRun
>    Affects Versions: Airflow 1.8
>            Reporter: William Pursell
>            Assignee: William Pursell
>   Original Estimate: 10m
>  Remaining Estimate: 10m
>
> In Python 2, subprocesses ignore SIGPIPE (https://bugs.python.org/issue1652).  As a result, a simple bash operator with bash_command = ''yes | head'' may never terminate (depending on the implementation of yes).  Airflow should reset the signal disposition to default.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26690,54,JIRA.13111147.1508562846000.26439.1508574360146@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111147.1508562846000@Atlassian.JIRA,,,2017-10-21 01:26:00-07,"[jira] [Commented] (AIRFLOW-1745) BashOperator ignores SIGPIPE in
 subprocess","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1745?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213811#comment-16213811 ] 

ASF subversion and git services commented on AIRFLOW-1745:
----------------------------------------------------------

Commit ca961042c146d49504e00e4abefc7779f0747782 in incubator-airflow''s branch refs/heads/master from [~wrp]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=ca96104 ]

[AIRFLOW-1745] Restore default signal disposition

Restore defaults for SIGPIPE, SIGXFZ, and SIGXFSZ
Python 2.7 subprocess resets signal disposition
for these
signals to ignore, which can cause problems.  For
example,
a simple BashOperator executing ''yes | head'' may
never
terminate.  For details, see discussion at:
https://bugs.python.org/issue1652
https://stackoverflow.com/questions/22077881/yes-
reporting-error-with-subprocess-communicate
etc.

Closes #2714 from wrp/sigpipe


> BashOperator ignores SIGPIPE in subprocess
> ------------------------------------------
>
>                 Key: AIRFLOW-1745
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1745
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DagRun
>    Affects Versions: Airflow 1.8
>            Reporter: William Pursell
>            Assignee: William Pursell
>             Fix For: 1.9.0
>
>   Original Estimate: 10m
>  Remaining Estimate: 10m
>
> In Python 2, subprocesses ignore SIGPIPE (https://bugs.python.org/issue1652).  As a result, a simple bash operator with bash_command = ''yes | head'' may never terminate (depending on the implementation of yes).  Airflow should reset the signal disposition to default.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
875378,186,552B7197-99D8-46A4-A7E5-BD2D893DE1E1@gmail.com,30891,Rodric Rabbah,C503E080-4E4C-428C-913E-4E6F7FAD945A@adobe.com,,,2017-06-30 18:12:24-07,Re: Adding pages to confluence,"+1 for wiki on GitHub unless this runs afoul of the Apache processes. 

-r

",f
26691,54,JIRA.13111147.1508562846000.26444.1508574360183@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13111147.1508562846000@Atlassian.JIRA,,,2017-10-21 01:26:00-07,"[jira] [Resolved] (AIRFLOW-1745) BashOperator ignores SIGPIPE in
 subprocess","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1745?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1745.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2714
[https://github.com/apache/incubator-airflow/pull/2714]

> BashOperator ignores SIGPIPE in subprocess
> ------------------------------------------
>
>                 Key: AIRFLOW-1745
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1745
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DagRun
>    Affects Versions: Airflow 1.8
>            Reporter: William Pursell
>            Assignee: William Pursell
>             Fix For: 1.9.0
>
>   Original Estimate: 10m
>  Remaining Estimate: 10m
>
> In Python 2, subprocesses ignore SIGPIPE (https://bugs.python.org/issue1652).  As a result, a simple bash operator with bash_command = ''yes | head'' may never terminate (depending on the implementation of yes).  Airflow should reset the signal disposition to default.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26692,54,JIRA.13111147.1508562846000.26440.1508574360155@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111147.1508562846000@Atlassian.JIRA,,,2017-10-21 01:26:00-07,"[jira] [Commented] (AIRFLOW-1745) BashOperator ignores SIGPIPE in
 subprocess","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1745?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213812#comment-16213812 ] 

ASF subversion and git services commented on AIRFLOW-1745:
----------------------------------------------------------

Commit e021c9d0a4e45465f3702249bf5eadbe40bf9135 in incubator-airflow''s branch refs/heads/v1-9-test from [~wrp]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=e021c9d ]

[AIRFLOW-1745] Restore default signal disposition

Restore defaults for SIGPIPE, SIGXFZ, and SIGXFSZ
Python 2.7 subprocess resets signal disposition
for these
signals to ignore, which can cause problems.  For
example,
a simple BashOperator executing ''yes | head'' may
never
terminate.  For details, see discussion at:
https://bugs.python.org/issue1652
https://stackoverflow.com/questions/22077881/yes-
reporting-error-with-subprocess-communicate
etc.

Closes #2714 from wrp/sigpipe

(cherry picked from commit ca961042c146d49504e00e4abefc7779f0747782)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> BashOperator ignores SIGPIPE in subprocess
> ------------------------------------------
>
>                 Key: AIRFLOW-1745
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1745
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DagRun
>    Affects Versions: Airflow 1.8
>            Reporter: William Pursell
>            Assignee: William Pursell
>             Fix For: 1.9.0
>
>   Original Estimate: 10m
>  Remaining Estimate: 10m
>
> In Python 2, subprocesses ignore SIGPIPE (https://bugs.python.org/issue1652).  As a result, a simple bash operator with bash_command = ''yes | head'' may never terminate (depending on the implementation of yes).  Airflow should reset the signal disposition to default.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26693,54,JIRA.13111158.1508578157000.26560.1508578200029@Atlassian.JIRA,2342,Eyal Trabelsi (JIRA),JIRA.13111158.1508578157000@Atlassian.JIRA,,,2017-10-21 02:30:00-07,"[jira] [Created] (AIRFLOW-1746) Add a Nomad operator to trigger job
 from Airflow","Eyal Trabelsi created AIRFLOW-1746:
--------------------------------------

             Summary: Add a Nomad operator to trigger job from Airflow
                 Key: AIRFLOW-1746
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1746
             Project: Apache Airflow
          Issue Type: New Feature
          Components: contrib
            Reporter: Eyal Trabelsi


We recently face the need to trigger nomad jobs from Airflow and no operator are available for that. 
The way the operator works is to register a nomad job and dispatch the job , than check the status of the job using similar method like boto-core (https://github.com/boto/botocore/blob/5a07b477114b11e6dc5f676f5db810972565b113/botocore/docs/waiter.py)

The operator uses https://github.com/jrxFive/python-nomad which is a wrap over nomad rest api of nomad written in python.

Link to the PR : https://github.com/apache/incubator-airflow/pull/2708




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26694,54,JIRA.13111047.1508521809000.26897.1508588580374@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111047.1508521809000@Atlassian.JIRA,,,2017-10-21 05:23:00-07,"[jira] [Commented] (AIRFLOW-1743) Default config template should
 not contain ldap filters","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1743?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213877#comment-16213877 ] 

ASF subversion and git services commented on AIRFLOW-1743:
----------------------------------------------------------

Commit 16899a95b5cf08d1053892c532f4b494b2f7d0cc in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=16899a9 ]

[AIRFLOW-1743] Verify ldap filters correctly

The superuser and data profiler filter where set
by default
in the config template and could not be unset.

Closes #2712 from bolkedebruin/AIRFLOW-1743


> Default config template should not contain ldap filters
> -------------------------------------------------------
>
>                 Key: AIRFLOW-1743
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1743
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: authentication
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>
> The config template functions as a template. In case a item is not in the airflow.cfg of the user it is used for default values.
> This should not be the case for filters for ldap users, as that can create unexpected results such as users not getting the right access levels.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26695,54,JIRA.13111047.1508521809000.26901.1508588580424@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111047.1508521809000@Atlassian.JIRA,,,2017-10-21 05:23:00-07,"[jira] [Commented] (AIRFLOW-1743) Default config template should
 not contain ldap filters","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1743?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213879#comment-16213879 ] 

ASF subversion and git services commented on AIRFLOW-1743:
----------------------------------------------------------

Commit 27068450ab8d191c98ab087fb1ed816af6be908a in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2706845 ]

[AIRFLOW-1743] Verify ldap filters correctly

The superuser and data profiler filter where set
by default
in the config template and could not be unset.

Closes #2712 from bolkedebruin/AIRFLOW-1743

(cherry picked from commit 16899a95b5cf08d1053892c532f4b494b2f7d0cc)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Default config template should not contain ldap filters
> -------------------------------------------------------
>
>                 Key: AIRFLOW-1743
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1743
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: authentication
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>
> The config template functions as a template. In case a item is not in the airflow.cfg of the user it is used for default values.
> This should not be the case for filters for ldap users, as that can create unexpected results such as users not getting the right access levels.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26696,54,JIRA.13111047.1508521809000.26904.1508588580451@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111047.1508521809000@Atlassian.JIRA,,,2017-10-21 05:23:00-07,"[jira] [Commented] (AIRFLOW-1743) Default config template should
 not contain ldap filters","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1743?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213880#comment-16213880 ] 

ASF subversion and git services commented on AIRFLOW-1743:
----------------------------------------------------------

Commit 27068450ab8d191c98ab087fb1ed816af6be908a in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2706845 ]

[AIRFLOW-1743] Verify ldap filters correctly

The superuser and data profiler filter where set
by default
in the config template and could not be unset.

Closes #2712 from bolkedebruin/AIRFLOW-1743

(cherry picked from commit 16899a95b5cf08d1053892c532f4b494b2f7d0cc)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Default config template should not contain ldap filters
> -------------------------------------------------------
>
>                 Key: AIRFLOW-1743
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1743
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: authentication
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>
> The config template functions as a template. In case a item is not in the airflow.cfg of the user it is used for default values.
> This should not be the case for filters for ldap users, as that can create unexpected results such as users not getting the right access levels.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26697,54,JIRA.13111047.1508521809000.26906.1508588580470@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13111047.1508521809000@Atlassian.JIRA,,,2017-10-21 05:23:00-07,"[jira] [Resolved] (AIRFLOW-1743) Default config template should not
 contain ldap filters","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1743?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1743.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2712
[https://github.com/apache/incubator-airflow/pull/2712]

> Default config template should not contain ldap filters
> -------------------------------------------------------
>
>                 Key: AIRFLOW-1743
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1743
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: authentication
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>
> The config template functions as a template. In case a item is not in the airflow.cfg of the user it is used for default values.
> This should not be the case for filters for ldap users, as that can create unexpected results such as users not getting the right access levels.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26698,54,JIRA.13111047.1508521809000.26900.1508588580413@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111047.1508521809000@Atlassian.JIRA,,,2017-10-21 05:23:00-07,"[jira] [Commented] (AIRFLOW-1743) Default config template should
 not contain ldap filters","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1743?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16213878#comment-16213878 ] 

ASF subversion and git services commented on AIRFLOW-1743:
----------------------------------------------------------

Commit 16899a95b5cf08d1053892c532f4b494b2f7d0cc in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=16899a9 ]

[AIRFLOW-1743] Verify ldap filters correctly

The superuser and data profiler filter where set
by default
in the config template and could not be unset.

Closes #2712 from bolkedebruin/AIRFLOW-1743


> Default config template should not contain ldap filters
> -------------------------------------------------------
>
>                 Key: AIRFLOW-1743
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1743
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: authentication
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>
> The config template functions as a template. In case a item is not in the airflow.cfg of the user it is used for default values.
> This should not be the case for filters for ldap users, as that can create unexpected results such as users not getting the right access levels.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26699,54,JIRA.13088499.1500489709000.29750.1508692681817@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13088499.1500489709000@Atlassian.JIRA,,,2017-10-22 10:18:01-07,"[jira] [Commented] (AIRFLOW-1432) NVD3 Charts do not have labeled
 axes and units change dynamically","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1432?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214382#comment-16214382 ] 

ASF subversion and git services commented on AIRFLOW-1432:
----------------------------------------------------------

Commit 4d1466b655e21b7756f22b5c2a3937811d0c65cf in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4d1466b ]

[AIRFLOW-1432] Charts label for Y axis not visible

The NVD3 charts did _have_ labels on the y-axis
saying the unit (hours,
minutes etc) but they weren''t _visible_. nvd3.js
correctly places labels
on the xAxis, but it doesn''t correctly space them
on the vertical axis.

Closes #2710 from ashb/AIRFLOW-1432-chart-y-axes


> NVD3 Charts do not have labeled axes and units change dynamically
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1432
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1432
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Ash Berlin-Taylor
>
> E.g. for the landing times chart, the y axis isn''t labeled (e.g. minutes/hours/days), and changes dynamically based on the data points in the chart/the pixel height of the chart. The y axis should be labeled in these charts.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
613212,146,JIRA.12755301.1415956857000.535055.1416401253620@Atlassian.JIRA,14290,Amareshwari Sriramadasu (JIRA),JIRA.12755301.1415956857000@Atlassian.JIRA,,,2014-11-19 04:47:33-08,"[jira] [Commented] (LENS-21) Minor refactoring changes in Query
 Rewriter interface","
    [ https://issues.apache.org/jira/browse/LENS-21?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14217842#comment-14217842 ] 

Amareshwari Sriramadasu commented on LENS-21:
---------------------------------------------

+1 Patch looks fine. I see no changes from previous. Going ahead and committing

> Minor refactoring changes in Query Rewriter interface 
> ------------------------------------------------------
>
>                 Key: LENS-21
>                 URL: https://issues.apache.org/jira/browse/LENS-21
>             Project: Apache Lens
>          Issue Type: Improvement
>          Components: api
>    Affects Versions: 2.0
>            Reporter: Suma Shivaprasad
>            Assignee: Suma Shivaprasad
>             Fix For: 2.0
>
>         Attachments: LENS-34.1.patch, LENS-34.patch
>
>
> QueryRewrite interface  needs to be moved into server-api and some minor method signature changes



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26700,54,JIRA.13088499.1500489709000.29753.1508692681838@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13088499.1500489709000@Atlassian.JIRA,,,2017-10-22 10:18:01-07,"[jira] [Commented] (AIRFLOW-1432) NVD3 Charts do not have labeled
 axes and units change dynamically","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1432?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214383#comment-16214383 ] 

ASF subversion and git services commented on AIRFLOW-1432:
----------------------------------------------------------

Commit 4d1466b655e21b7756f22b5c2a3937811d0c65cf in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4d1466b ]

[AIRFLOW-1432] Charts label for Y axis not visible

The NVD3 charts did _have_ labels on the y-axis
saying the unit (hours,
minutes etc) but they weren''t _visible_. nvd3.js
correctly places labels
on the xAxis, but it doesn''t correctly space them
on the vertical axis.

Closes #2710 from ashb/AIRFLOW-1432-chart-y-axes


> NVD3 Charts do not have labeled axes and units change dynamically
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1432
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1432
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Ash Berlin-Taylor
>
> E.g. for the landing times chart, the y axis isn''t labeled (e.g. minutes/hours/days), and changes dynamically based on the data points in the chart/the pixel height of the chart. The y axis should be labeled in these charts.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26701,54,JIRA.13088499.1500489709000.29758.1508692740388@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13088499.1500489709000@Atlassian.JIRA,,,2017-10-22 10:19:00-07,"[jira] [Commented] (AIRFLOW-1432) NVD3 Charts do not have labeled
 axes and units change dynamically","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1432?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214384#comment-16214384 ] 

ASF subversion and git services commented on AIRFLOW-1432:
----------------------------------------------------------

Commit 70ffa43ad467e0c5ac9f8cca4971af5347c354fd in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=70ffa43 ]

[AIRFLOW-1432] Charts label for Y axis not visible

The NVD3 charts did _have_ labels on the y-axis
saying the unit (hours,
minutes etc) but they weren''t _visible_. nvd3.js
correctly places labels
on the xAxis, but it doesn''t correctly space them
on the vertical axis.

Closes #2710 from ashb/AIRFLOW-1432-chart-y-axes

(cherry picked from commit 4d1466b655e21b7756f22b5c2a3937811d0c65cf)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> NVD3 Charts do not have labeled axes and units change dynamically
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1432
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1432
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> E.g. for the landing times chart, the y axis isn''t labeled (e.g. minutes/hours/days), and changes dynamically based on the data points in the chart/the pixel height of the chart. The y axis should be labeled in these charts.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26702,54,JIRA.13088499.1500489709000.29759.1508692740399@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13088499.1500489709000@Atlassian.JIRA,,,2017-10-22 10:19:00-07,"[jira] [Commented] (AIRFLOW-1432) NVD3 Charts do not have labeled
 axes and units change dynamically","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1432?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214385#comment-16214385 ] 

ASF subversion and git services commented on AIRFLOW-1432:
----------------------------------------------------------

Commit 70ffa43ad467e0c5ac9f8cca4971af5347c354fd in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=70ffa43 ]

[AIRFLOW-1432] Charts label for Y axis not visible

The NVD3 charts did _have_ labels on the y-axis
saying the unit (hours,
minutes etc) but they weren''t _visible_. nvd3.js
correctly places labels
on the xAxis, but it doesn''t correctly space them
on the vertical axis.

Closes #2710 from ashb/AIRFLOW-1432-chart-y-axes

(cherry picked from commit 4d1466b655e21b7756f22b5c2a3937811d0c65cf)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> NVD3 Charts do not have labeled axes and units change dynamically
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1432
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1432
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> E.g. for the landing times chart, the y axis isn''t labeled (e.g. minutes/hours/days), and changes dynamically based on the data points in the chart/the pixel height of the chart. The y axis should be labeled in these charts.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26703,54,JIRA.13088499.1500489709000.29765.1508692740454@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13088499.1500489709000@Atlassian.JIRA,,,2017-10-22 10:19:00-07,"[jira] [Resolved] (AIRFLOW-1432) NVD3 Charts do not have labeled
 axes and units change dynamically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1432?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1432.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2710
[https://github.com/apache/incubator-airflow/pull/2710]

> NVD3 Charts do not have labeled axes and units change dynamically
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1432
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1432
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> E.g. for the landing times chart, the y axis isn''t labeled (e.g. minutes/hours/days), and changes dynamically based on the data points in the chart/the pixel height of the chart. The y axis should be labeled in these charts.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
666967,23,JIRA.12977198.1465473133000.38913.1465473140925@Atlassian.JIRA,25066,Tom Matecki (JIRA),JIRA.12977198.1465473133000@Atlassian.JIRA,,,2016-06-09 04:52:20-07,"[jira] [Created] (MILAGRO-11) Create ""Help wanted"" section on
 MILAGRO website","Tom Matecki created MILAGRO-11:
----------------------------------

             Summary: Create ""Help wanted"" section on MILAGRO website
                 Key: MILAGRO-11
                 URL: https://issues.apache.org/jira/browse/MILAGRO-11
             Project: Milagro
          Issue Type: Task
            Reporter: Tom Matecki
            Assignee: Tom Matecki


Link to MILAGRO JIRA board should be placed on MILAGRO website 

http://milagro.incubator.apache.org/index.html



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26704,54,JIRA.13109866.1508208719000.29774.1508693280068@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109866.1508208719000@Atlassian.JIRA,,,2017-10-22 10:28:00-07,[jira] [Commented] (AIRFLOW-1719) Fix small typo - your vs you,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1719?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214388#comment-16214388 ] 

ASF subversion and git services commented on AIRFLOW-1719:
----------------------------------------------------------

Commit dfe1d53f8a8c370d7f0fea408668c3f738189900 in incubator-airflow''s branch refs/heads/master from [~j450h1]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=dfe1d53 ]

[AIRFLOW-1719] Fix small typo

Closes #2689 from j450h1/master


> Fix small typo - your vs you
> ----------------------------
>
>                 Key: AIRFLOW-1719
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1719
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Jas Sohi
>
> Fix a small typo.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26705,54,JIRA.13109866.1508208719000.29777.1508693340203@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13109866.1508208719000@Atlassian.JIRA,,,2017-10-22 10:29:00-07,[jira] [Resolved] (AIRFLOW-1719) Fix small typo - your vs you,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1719?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1719.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2689
[https://github.com/apache/incubator-airflow/pull/2689]

> Fix small typo - your vs you
> ----------------------------
>
>                 Key: AIRFLOW-1719
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1719
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Jas Sohi
>             Fix For: 1.9.0
>
>
> Fix a small typo.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26706,54,JIRA.13109553.1508100330000.29793.1508693882030@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109553.1508100330000@Atlassian.JIRA,,,2017-10-22 10:38:02-07,"[jira] [Commented] (AIRFLOW-1716) Fix multiple __init__ definition
 in SimpleDag","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1716?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214393#comment-16214393 ] 

ASF subversion and git services commented on AIRFLOW-1716:
----------------------------------------------------------

Commit e05254f8731ac55e169cdc581a38b0d3fe06267d in incubator-airflow''s branch refs/heads/master from [~sanjay.pillai]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=e05254f ]

[AIRFLOW-1716] Fix multiple __init__ def in SimpleDag

Closes #2692 from MortalViews/master


> Fix multiple __init__ definition in SimpleDag
> ---------------------------------------------
>
>                 Key: AIRFLOW-1716
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1716
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG
>    Affects Versions: 1.9.0
>            Reporter: Sanjay Pillai
>            Assignee: Sanjay Pillai
>
> In airflow.utitls.dag_processing.SimpleDag  __init__ definition is repeated. 
> probably a result of incorrect merge conflict resolution. 
> removed the previous __init__ def and updated doc string. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26707,54,JIRA.13109553.1508100330000.29795.1508694182642@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109553.1508100330000@Atlassian.JIRA,,,2017-10-22 10:43:02-07,"[jira] [Commented] (AIRFLOW-1716) Fix multiple __init__ definition
 in SimpleDag","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1716?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214394#comment-16214394 ] 

ASF subversion and git services commented on AIRFLOW-1716:
----------------------------------------------------------

Commit 786e5223ac3c395dd31e748e6776fdaf3dc4880f in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=786e522 ]

Revert ""[AIRFLOW-1716] Fix multiple __init__ def in SimpleDag""

This reverts commit e05254f8731ac55e169cdc581a38b0d3fe06267d.


> Fix multiple __init__ definition in SimpleDag
> ---------------------------------------------
>
>                 Key: AIRFLOW-1716
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1716
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG
>    Affects Versions: 1.9.0
>            Reporter: Sanjay Pillai
>            Assignee: Sanjay Pillai
>
> In airflow.utitls.dag_processing.SimpleDag  __init__ definition is repeated. 
> probably a result of incorrect merge conflict resolution. 
> removed the previous __init__ def and updated doc string. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26708,54,JIRA.13095579.1503043773000.29811.1508695322115@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13095579.1503043773000@Atlassian.JIRA,,,2017-10-22 11:02:02-07,[jira] [Commented] (AIRFLOW-1520) S3Hook uses boto2,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1520?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214399#comment-16214399 ] 

ASF subversion and git services commented on AIRFLOW-1520:
----------------------------------------------------------

Commit cd3ad3f2e2108dfd2c416765c33a2e6dca576846 in incubator-airflow''s branch refs/heads/master from [~nzeilemaker]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=cd3ad3f ]

[AIRFLOW-1520] Boto3 S3Hook, S3Log

Closes #2532 from NielsZeilemaker/AIRFLOW-1520


> S3Hook uses boto2
> -----------------
>
>                 Key: AIRFLOW-1520
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1520
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>
> The S3Hook uses boto2 which does not support container roles. Therefore, we need to add permissions to the underlying ec2 instances instead.
> Upgrading to boto3 fixes this



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26709,54,JIRA.13095579.1503043773000.29817.1508695322169@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13095579.1503043773000@Atlassian.JIRA,,,2017-10-22 11:02:02-07,[jira] [Commented] (AIRFLOW-1520) S3Hook uses boto2,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1520?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214401#comment-16214401 ] 

ASF subversion and git services commented on AIRFLOW-1520:
----------------------------------------------------------

Commit 3865836563e0ce97fd57232f4b6ede2fff601595 in incubator-airflow''s branch refs/heads/v1-9-test from [~nzeilemaker]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3865836 ]

[AIRFLOW-1520] Boto3 S3Hook, S3Log

Closes #2532 from NielsZeilemaker/AIRFLOW-1520

(cherry picked from commit cd3ad3f2e2108dfd2c416765c33a2e6dca576846)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook uses boto2
> -----------------
>
>                 Key: AIRFLOW-1520
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1520
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>
> The S3Hook uses boto2 which does not support container roles. Therefore, we need to add permissions to the underlying ec2 instances instead.
> Upgrading to boto3 fixes this



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26710,54,JIRA.13095579.1503043773000.29816.1508695322161@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13095579.1503043773000@Atlassian.JIRA,,,2017-10-22 11:02:02-07,[jira] [Commented] (AIRFLOW-1520) S3Hook uses boto2,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1520?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214400#comment-16214400 ] 

ASF subversion and git services commented on AIRFLOW-1520:
----------------------------------------------------------

Commit cd3ad3f2e2108dfd2c416765c33a2e6dca576846 in incubator-airflow''s branch refs/heads/master from [~nzeilemaker]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=cd3ad3f ]

[AIRFLOW-1520] Boto3 S3Hook, S3Log

Closes #2532 from NielsZeilemaker/AIRFLOW-1520


> S3Hook uses boto2
> -----------------
>
>                 Key: AIRFLOW-1520
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1520
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>
> The S3Hook uses boto2 which does not support container roles. Therefore, we need to add permissions to the underlying ec2 instances instead.
> Upgrading to boto3 fixes this



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26711,54,JIRA.13095579.1503043773000.29822.1508695322207@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13095579.1503043773000@Atlassian.JIRA,,,2017-10-22 11:02:02-07,[jira] [Commented] (AIRFLOW-1520) S3Hook uses boto2,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1520?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214402#comment-16214402 ] 

ASF subversion and git services commented on AIRFLOW-1520:
----------------------------------------------------------

Commit 3865836563e0ce97fd57232f4b6ede2fff601595 in incubator-airflow''s branch refs/heads/v1-9-test from [~nzeilemaker]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3865836 ]

[AIRFLOW-1520] Boto3 S3Hook, S3Log

Closes #2532 from NielsZeilemaker/AIRFLOW-1520

(cherry picked from commit cd3ad3f2e2108dfd2c416765c33a2e6dca576846)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook uses boto2
> -----------------
>
>                 Key: AIRFLOW-1520
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1520
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>
> The S3Hook uses boto2 which does not support container roles. Therefore, we need to add permissions to the underlying ec2 instances instead.
> Upgrading to boto3 fixes this



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26712,54,JIRA.13095579.1503043773000.29826.1508695380208@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13095579.1503043773000@Atlassian.JIRA,,,2017-10-22 11:03:00-07,[jira] [Resolved] (AIRFLOW-1520) S3Hook uses boto2,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1520?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1520.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2532
[https://github.com/apache/incubator-airflow/pull/2532]

> S3Hook uses boto2
> -----------------
>
>                 Key: AIRFLOW-1520
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1520
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Niels Zeilemaker
>             Fix For: 1.9.0
>
>
> The S3Hook uses boto2 which does not support container roles. Therefore, we need to add permissions to the underlying ec2 instances instead.
> Upgrading to boto3 fixes this



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26713,54,JIRA.13047135.1488326347000.29831.1508695562360@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13047135.1488326347000@Atlassian.JIRA,,,2017-10-22 11:06:02-07,"[jira] [Commented] (AIRFLOW-926) jdbc connector is broken due to
 jaydebeapi api update","
    [ https://issues.apache.org/jira/browse/AIRFLOW-926?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214405#comment-16214405 ] 

ASF subversion and git services commented on AIRFLOW-926:
---------------------------------------------------------

Commit 21257e8f0643335d09c41f324c1d83ad1a8fc485 in incubator-airflow''s branch refs/heads/master from [~r-richmond]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=21257e8 ]

[AIRFLOW-926] Fix JDBC Hook

JayDeBeApi made a backwards incompatible change
This updates the JDBC Hook''s implementation
and changes the required JayDeBeApi to >= 1.1.1

Closes #2651 from r-richmond/AIRFLOW-926


> jdbc connector is broken due to jaydebeapi api update
> -----------------------------------------------------
>
>                 Key: AIRFLOW-926
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-926
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hooks
>    Affects Versions: Airflow 1.8, Airflow 1.7.1.3
>            Reporter: r-richmond
>             Fix For: 1.9.0
>
>
> marking this as major as JDBC hooks are broken but it probably belongs somewhere between major and minor.... feel free to adjust
> there is an incompability between jaydebapi==1.0.0 and airflow as it sits now. they changed their connect api. Its a small change but it breaks the jdbc hook for the moment. the jdbc hook [code|https://github.com/apache/incubator-airflow/blob/ff45d8f2218a8da9328161aa66d004c3db3b367e/airflow/hooks/jdbc_hook.py#L55] should look like this (working on a pull request right now but this is the fix + something for multiple jars that are comma separated)
> {code:python}
>         conn = jaydebeapi.connect(jclassname=jdbc_driver_name,
>                                   url=host,
>                                   driver_args=[str(login), str(psw)],
>                                   jars=jdbc_driver_loc.split(sep="",""))
> {code}
> anyways that should be changed and [setup.py|https://github.com/apache/incubator-airflow/blob/4f52db317f16b431ef06e61ffebd8cc6aecd50c1/setup.py#L143] should be updated to reflect
> {code}
> jdbc = [''jaydebeapi>=1.0.0'']
> {code}
> [jaydebeapi updated api|https://github.com/baztian/jaydebeapi/commit/2640445de036def2169514aa56e5ad9ced246b16#diff-a51683c99849d299a6cb9e6c4bcec328R349] jan 9 2017



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26714,54,JIRA.13047135.1488326347000.29838.1508695562450@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13047135.1488326347000@Atlassian.JIRA,,,2017-10-22 11:06:02-07,"[jira] [Commented] (AIRFLOW-926) jdbc connector is broken due to
 jaydebeapi api update","
    [ https://issues.apache.org/jira/browse/AIRFLOW-926?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214406#comment-16214406 ] 

ASF subversion and git services commented on AIRFLOW-926:
---------------------------------------------------------

Commit 07ed2968bbfb8c736ce6657190e887ba0a99f127 in incubator-airflow''s branch refs/heads/v1-9-test from [~r-richmond]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=07ed296 ]

[AIRFLOW-926] Fix JDBC Hook

JayDeBeApi made a backwards incompatible change
This updates the JDBC Hook''s implementation
and changes the required JayDeBeApi to >= 1.1.1

Closes #2651 from r-richmond/AIRFLOW-926

(cherry picked from commit 21257e8f0643335d09c41f324c1d83ad1a8fc485)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> jdbc connector is broken due to jaydebeapi api update
> -----------------------------------------------------
>
>                 Key: AIRFLOW-926
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-926
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hooks
>    Affects Versions: Airflow 1.8, Airflow 1.7.1.3
>            Reporter: r-richmond
>             Fix For: 1.9.0
>
>
> marking this as major as JDBC hooks are broken but it probably belongs somewhere between major and minor.... feel free to adjust
> there is an incompability between jaydebapi==1.0.0 and airflow as it sits now. they changed their connect api. Its a small change but it breaks the jdbc hook for the moment. the jdbc hook [code|https://github.com/apache/incubator-airflow/blob/ff45d8f2218a8da9328161aa66d004c3db3b367e/airflow/hooks/jdbc_hook.py#L55] should look like this (working on a pull request right now but this is the fix + something for multiple jars that are comma separated)
> {code:python}
>         conn = jaydebeapi.connect(jclassname=jdbc_driver_name,
>                                   url=host,
>                                   driver_args=[str(login), str(psw)],
>                                   jars=jdbc_driver_loc.split(sep="",""))
> {code}
> anyways that should be changed and [setup.py|https://github.com/apache/incubator-airflow/blob/4f52db317f16b431ef06e61ffebd8cc6aecd50c1/setup.py#L143] should be updated to reflect
> {code}
> jdbc = [''jaydebeapi>=1.0.0'']
> {code}
> [jaydebeapi updated api|https://github.com/baztian/jaydebeapi/commit/2640445de036def2169514aa56e5ad9ced246b16#diff-a51683c99849d299a6cb9e6c4bcec328R349] jan 9 2017



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26715,54,JIRA.13047135.1488326347000.29830.1508695562351@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13047135.1488326347000@Atlassian.JIRA,,,2017-10-22 11:06:02-07,"[jira] [Commented] (AIRFLOW-926) jdbc connector is broken due to
 jaydebeapi api update","
    [ https://issues.apache.org/jira/browse/AIRFLOW-926?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214404#comment-16214404 ] 

ASF subversion and git services commented on AIRFLOW-926:
---------------------------------------------------------

Commit 21257e8f0643335d09c41f324c1d83ad1a8fc485 in incubator-airflow''s branch refs/heads/master from [~r-richmond]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=21257e8 ]

[AIRFLOW-926] Fix JDBC Hook

JayDeBeApi made a backwards incompatible change
This updates the JDBC Hook''s implementation
and changes the required JayDeBeApi to >= 1.1.1

Closes #2651 from r-richmond/AIRFLOW-926


> jdbc connector is broken due to jaydebeapi api update
> -----------------------------------------------------
>
>                 Key: AIRFLOW-926
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-926
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hooks
>    Affects Versions: Airflow 1.8, Airflow 1.7.1.3
>            Reporter: r-richmond
>             Fix For: 1.9.0
>
>
> marking this as major as JDBC hooks are broken but it probably belongs somewhere between major and minor.... feel free to adjust
> there is an incompability between jaydebapi==1.0.0 and airflow as it sits now. they changed their connect api. Its a small change but it breaks the jdbc hook for the moment. the jdbc hook [code|https://github.com/apache/incubator-airflow/blob/ff45d8f2218a8da9328161aa66d004c3db3b367e/airflow/hooks/jdbc_hook.py#L55] should look like this (working on a pull request right now but this is the fix + something for multiple jars that are comma separated)
> {code:python}
>         conn = jaydebeapi.connect(jclassname=jdbc_driver_name,
>                                   url=host,
>                                   driver_args=[str(login), str(psw)],
>                                   jars=jdbc_driver_loc.split(sep="",""))
> {code}
> anyways that should be changed and [setup.py|https://github.com/apache/incubator-airflow/blob/4f52db317f16b431ef06e61ffebd8cc6aecd50c1/setup.py#L143] should be updated to reflect
> {code}
> jdbc = [''jaydebeapi>=1.0.0'']
> {code}
> [jaydebeapi updated api|https://github.com/baztian/jaydebeapi/commit/2640445de036def2169514aa56e5ad9ced246b16#diff-a51683c99849d299a6cb9e6c4bcec328R349] jan 9 2017



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26716,54,JIRA.13047135.1488326347000.29839.1508695562459@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13047135.1488326347000@Atlassian.JIRA,,,2017-10-22 11:06:02-07,"[jira] [Commented] (AIRFLOW-926) jdbc connector is broken due to
 jaydebeapi api update","
    [ https://issues.apache.org/jira/browse/AIRFLOW-926?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214407#comment-16214407 ] 

ASF subversion and git services commented on AIRFLOW-926:
---------------------------------------------------------

Commit 07ed2968bbfb8c736ce6657190e887ba0a99f127 in incubator-airflow''s branch refs/heads/v1-9-test from [~r-richmond]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=07ed296 ]

[AIRFLOW-926] Fix JDBC Hook

JayDeBeApi made a backwards incompatible change
This updates the JDBC Hook''s implementation
and changes the required JayDeBeApi to >= 1.1.1

Closes #2651 from r-richmond/AIRFLOW-926

(cherry picked from commit 21257e8f0643335d09c41f324c1d83ad1a8fc485)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> jdbc connector is broken due to jaydebeapi api update
> -----------------------------------------------------
>
>                 Key: AIRFLOW-926
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-926
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hooks
>    Affects Versions: Airflow 1.8, Airflow 1.7.1.3
>            Reporter: r-richmond
>             Fix For: 1.9.0
>
>
> marking this as major as JDBC hooks are broken but it probably belongs somewhere between major and minor.... feel free to adjust
> there is an incompability between jaydebapi==1.0.0 and airflow as it sits now. they changed their connect api. Its a small change but it breaks the jdbc hook for the moment. the jdbc hook [code|https://github.com/apache/incubator-airflow/blob/ff45d8f2218a8da9328161aa66d004c3db3b367e/airflow/hooks/jdbc_hook.py#L55] should look like this (working on a pull request right now but this is the fix + something for multiple jars that are comma separated)
> {code:python}
>         conn = jaydebeapi.connect(jclassname=jdbc_driver_name,
>                                   url=host,
>                                   driver_args=[str(login), str(psw)],
>                                   jars=jdbc_driver_loc.split(sep="",""))
> {code}
> anyways that should be changed and [setup.py|https://github.com/apache/incubator-airflow/blob/4f52db317f16b431ef06e61ffebd8cc6aecd50c1/setup.py#L143] should be updated to reflect
> {code}
> jdbc = [''jaydebeapi>=1.0.0'']
> {code}
> [jaydebeapi updated api|https://github.com/baztian/jaydebeapi/commit/2640445de036def2169514aa56e5ad9ced246b16#diff-a51683c99849d299a6cb9e6c4bcec328R349] jan 9 2017



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26717,54,JIRA.13047135.1488326347000.29849.1508695562531@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13047135.1488326347000@Atlassian.JIRA,,,2017-10-22 11:06:02-07,"[jira] [Resolved] (AIRFLOW-926) jdbc connector is broken due to
 jaydebeapi api update","
     [ https://issues.apache.org/jira/browse/AIRFLOW-926?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-926.
------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2651
[https://github.com/apache/incubator-airflow/pull/2651]

> jdbc connector is broken due to jaydebeapi api update
> -----------------------------------------------------
>
>                 Key: AIRFLOW-926
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-926
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hooks
>    Affects Versions: Airflow 1.8, Airflow 1.7.1.3
>            Reporter: r-richmond
>             Fix For: 1.9.0
>
>
> marking this as major as JDBC hooks are broken but it probably belongs somewhere between major and minor.... feel free to adjust
> there is an incompability between jaydebapi==1.0.0 and airflow as it sits now. they changed their connect api. Its a small change but it breaks the jdbc hook for the moment. the jdbc hook [code|https://github.com/apache/incubator-airflow/blob/ff45d8f2218a8da9328161aa66d004c3db3b367e/airflow/hooks/jdbc_hook.py#L55] should look like this (working on a pull request right now but this is the fix + something for multiple jars that are comma separated)
> {code:python}
>         conn = jaydebeapi.connect(jclassname=jdbc_driver_name,
>                                   url=host,
>                                   driver_args=[str(login), str(psw)],
>                                   jars=jdbc_driver_loc.split(sep="",""))
> {code}
> anyways that should be changed and [setup.py|https://github.com/apache/incubator-airflow/blob/4f52db317f16b431ef06e61ffebd8cc6aecd50c1/setup.py#L143] should be updated to reflect
> {code}
> jdbc = [''jaydebeapi>=1.0.0'']
> {code}
> [jaydebeapi updated api|https://github.com/baztian/jaydebeapi/commit/2640445de036def2169514aa56e5ad9ced246b16#diff-a51683c99849d299a6cb9e6c4bcec328R349] jan 9 2017



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26718,54,JIRA.13104708.1506332962000.29924.1508700603947@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-22 12:30:03-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214424#comment-16214424 ] 

Bolke de Bruin commented on AIRFLOW-1641:
-----------------------------------------

See https://github.com/apache/incubator-airflow/pull/2715 

Please test if this resolves your issue. When timeouts happen, the task will now fail. I havent tested it myself yet.



> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>              Labels: queued, scheduler, stuck, task
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26719,54,JIRA.13104708.1506332962000.29926.1508701380415@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-22 12:43:00-07,"[jira] [Work started] (AIRFLOW-1641) Task gets stuck in queued
 state","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1641 started by Bolke de Bruin.
-----------------------------------------------
> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26720,54,JIRA.13104708.1506332962000.29935.1508701382536@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-22 12:43:02-07,[jira] [Updated] (AIRFLOW-1641) Task gets stuck in queued state,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1641:
------------------------------------
    Fix Version/s: 1.9.0

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26721,54,JIRA.13104708.1506332962000.30015.1508705220213@Atlassian.JIRA,2188,Nick Bauer (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-22 13:47:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214443#comment-16214443 ] 

Nick Bauer commented on AIRFLOW-1641:
-------------------------------------

Thanks for trying to make a fix for this... but why would you submit a PR without testing your code?

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26722,54,JIRA.13104708.1506332962000.30049.1508706240174@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-22 14:04:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214452#comment-16214452 ] 

Bolke de Bruin commented on AIRFLOW-1641:
-----------------------------------------

Cause I need the feedback: I don''t experience the issue myself, it is pretty core in the scheduler what needs to be updated (the current patch doesn''t work correctly), next to that the automated testing helps to catch some errors (tests don''t pass currently).

Also in addition to what is written here you can also limit parallelism to workaround the issue. Staying in QUEUED means the task instance can''t even start. Most likely due to load or memory issues. This should be dealt with correctly (that is the real issue Im trying the solve in de patch), but it will fail your task anyways.




> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26723,54,JIRA.13104708.1506332962000.30115.1508708760172@Atlassian.JIRA,2188,Nick Bauer (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-22 14:46:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214462#comment-16214462 ] 

Nick Bauer commented on AIRFLOW-1641:
-------------------------------------

I''ll try to make some time to test this, but I just recently upgraded our code so our dags load faster.  it should be pretty easy to test however: Create a dag that takes a long time to load - like have it sleep. , and set your dagbag import to something small like 5 seconds.

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26724,54,JIRA.13012493.1476485289000.31902.1508742360275@Atlassian.JIRA,2309,Gauri K (JIRA),JIRA.13012493.1476485289000@Atlassian.JIRA,,,2017-10-23 00:06:00-07,"[jira] [Commented] (AIRFLOW-571) allow gunicorn config to be passed
 to airflow webserver","
    [ https://issues.apache.org/jira/browse/AIRFLOW-571?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214744#comment-16214744 ] 

Gauri K commented on AIRFLOW-571:
---------------------------------

Any updates on the same?

> allow gunicorn config to be passed to airflow webserver
> -------------------------------------------------------
>
>                 Key: AIRFLOW-571
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-571
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webserver
>            Reporter: Dennis O''Brien
>
> I have run into an issue when running airflow webserver behind a load balancer where redirects result in https requests forwarded to http.  I ran into a similar issue with Caravel which also uses gunicorn.  https://github.com/airbnb/caravel/issues/978  From that issue:
> {quote}
> When gunicorn is run on a different machine from the load balancer (nginx or ELB), it needs to be told explicitly to trust the X-Forwarded-* headers sent. gunicorn takes an option --forwarded-allow-ips which can either be a comma separated list of ip addresses, or ""*"" to trust all.
> {quote}
> I don''t see a simple way to inject custom arguments to the gunicorn call in `webserver()`.  Rather than making a special case to set --forwarded-allow-ips, it would be nice if the caller of `airflow webserver` could pass an additional gunicorn config file.
> The call to gunicorn is already including a -c and I''m not sure gunicorn will take multiple configs, so maybe we have to parse the config and include each name=value on the gunicorn command line.  Any suggestions on how best to allow this?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
631124,147,407C2A71.205@apache.org,21653,Gregor J. Rothfuss,407C18F1.1030103@wyona.com,29,Michael Wechner,2004-04-13 10:59:13-07,Re: Clone Proposal,"Michael Wechner wrote:
> whereas I guess the specific clone.xml imports the generic one (or is it 
> the opposite way around).

the other way round from what i can see

> Anyway, for the beginning we could do this for the default and blog 
> publication.

+1

> Also I think we should add two new publication instances:
> 
> my-default and my-blog where we have slight changes re the layout for 
> instance.

-1 in cvs, +1 in the build process. or even better, accessible from the 
admin area. should be an easy excercise for someone to have a flow page 
trigger the ant task.

-gregor

-- 
Gregor J. Rothfuss
Wyona Inc.  -   Open Source Content Management   -   Apache Lenya
http://wyona.com                   http://cocoon.apache.org/lenya
gregor.rothfuss@wyona.com                       gregor@apache.org

---------------------------------------------------------------------
To unsubscribe, e-mail: lenya-dev-unsubscribe@cocoon.apache.org
For additional commands, e-mail: lenya-dev-help@cocoon.apache.org


",f
26725,54,JIRA.13106861.1507094940000.32515.1508748120832@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13106861.1507094940000@Atlassian.JIRA,,,2017-10-23 01:42:00-07,"[jira] [Commented] (AIRFLOW-1677) Fix typo in
 example_qubole_operator","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1677?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214822#comment-16214822 ] 

ASF subversion and git services commented on AIRFLOW-1677:
----------------------------------------------------------

Commit 4386cd4145c15f03c61911d1c24c698d00db25b1 in incubator-airflow''s branch refs/heads/master from [~rupesh92]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4386cd4 ]

[AIRFLOW-1677] Fix typo in example_qubole_operator

Closes #2661 from rupesh92/AIRFLOW-1677


> Fix typo in example_qubole_operator
> -----------------------------------
>
>                 Key: AIRFLOW-1677
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1677
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rupesh Bansal
>            Assignee: Rupesh Bansal
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26726,54,JIRA.13106861.1507094940000.32514.1508748120825@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13106861.1507094940000@Atlassian.JIRA,,,2017-10-23 01:42:00-07,"[jira] [Commented] (AIRFLOW-1677) Fix typo in
 example_qubole_operator","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1677?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214821#comment-16214821 ] 

ASF subversion and git services commented on AIRFLOW-1677:
----------------------------------------------------------

Commit 4386cd4145c15f03c61911d1c24c698d00db25b1 in incubator-airflow''s branch refs/heads/master from [~rupesh92]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4386cd4 ]

[AIRFLOW-1677] Fix typo in example_qubole_operator

Closes #2661 from rupesh92/AIRFLOW-1677


> Fix typo in example_qubole_operator
> -----------------------------------
>
>                 Key: AIRFLOW-1677
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1677
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rupesh Bansal
>            Assignee: Rupesh Bansal
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26727,54,JIRA.13106861.1507094940000.32520.1508748180214@Atlassian.JIRA,1342,Sumit Maheshwari (JIRA),JIRA.13106861.1507094940000@Atlassian.JIRA,,,2017-10-23 01:43:00-07,"[jira] [Resolved] (AIRFLOW-1677) Fix typo in
 example_qubole_operator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1677?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sumit Maheshwari resolved AIRFLOW-1677.
---------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2661
[https://github.com/apache/incubator-airflow/pull/2661]

> Fix typo in example_qubole_operator
> -----------------------------------
>
>                 Key: AIRFLOW-1677
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1677
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rupesh Bansal
>            Assignee: Rupesh Bansal
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26728,54,JIRA.13105801.1506618471000.32927.1508752680734@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13105801.1506618471000@Atlassian.JIRA,,,2017-10-23 02:58:00-07,"[jira] [Commented] (AIRFLOW-1657) Handle failure of Qubole Operator
 for s3distcp hadoop command","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1657?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214904#comment-16214904 ] 

ASF subversion and git services commented on AIRFLOW-1657:
----------------------------------------------------------

Commit 263861ed2ccefc21665697af4903bdb021431dea in incubator-airflow''s branch refs/heads/master from [~rupesh92]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=263861e ]

[AIRFLOW-1657] Handle failing qubole operator

Closes #2643 from rupesh92/AIRFLOW-1657


> Handle failure of Qubole Operator for s3distcp hadoop command
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1657
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1657
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rupesh Bansal
>            Assignee: Rupesh Bansal
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26729,54,JIRA.13105801.1506618471000.32928.1508752680744@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13105801.1506618471000@Atlassian.JIRA,,,2017-10-23 02:58:00-07,"[jira] [Commented] (AIRFLOW-1657) Handle failure of Qubole Operator
 for s3distcp hadoop command","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1657?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214905#comment-16214905 ] 

ASF subversion and git services commented on AIRFLOW-1657:
----------------------------------------------------------

Commit 263861ed2ccefc21665697af4903bdb021431dea in incubator-airflow''s branch refs/heads/master from [~rupesh92]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=263861e ]

[AIRFLOW-1657] Handle failing qubole operator

Closes #2643 from rupesh92/AIRFLOW-1657


> Handle failure of Qubole Operator for s3distcp hadoop command
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1657
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1657
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rupesh Bansal
>            Assignee: Rupesh Bansal
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26730,54,JIRA.13105801.1506618471000.32942.1508752740683@Atlassian.JIRA,1342,Sumit Maheshwari (JIRA),JIRA.13105801.1506618471000@Atlassian.JIRA,,,2017-10-23 02:59:00-07,"[jira] [Resolved] (AIRFLOW-1657) Handle failure of Qubole Operator
 for s3distcp hadoop command","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1657?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sumit Maheshwari resolved AIRFLOW-1657.
---------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2643
[https://github.com/apache/incubator-airflow/pull/2643]

> Handle failure of Qubole Operator for s3distcp hadoop command
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1657
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1657
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rupesh Bansal
>            Assignee: Rupesh Bansal
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
875379,186,D8FDAF11-AA78-4ED5-B22A-56B370B2A431@adobe.com,30922,Felix Meschberger,C503E080-4E4C-428C-913E-4E6F7FAD945A@adobe.com,,,2017-06-30 22:23:43-07,Re: Adding pages to confluence,"SGkgVHlzb24NCg0KSSBoYXZlIGdyYW50ZWQgeW91IHdyaXRlIGFjY2Vzcw0KDQpSZWdhcmRzDQpG
ZWxpeA0KDQo+IEFtIDMwLjA2LjIwMTcgdW0gMTg6MDMgc2NocmllYiBUeXNvbiBOb3JyaXMgPHRu
b3JyaXNAYWRvYmUuY29tLklOVkFMSUQ+Og0KPiANCj4gSGkgLQ0KPiBJ4oCZbSB0cnlpbmcgdG8g
YWRkIGEgcGFnZSB0byBodHRwczovL2N3aWtpLmFwYWNoZS5vcmcvY29uZmx1ZW5jZS9kaXNwbGF5
L09QRU5XSElTSy9Qcm9wb3NhbHMNCj4gQnV0IGl0IHNlZW1zIGxpa2UgSSBkb27igJl0IGhhdmUg
cGVybWlzc2lvbj8NCj4gDQo+IFNob3VsZCBJIGluc3RlYWQgYWRkIGl0IHRvIEdpdEh1YiBhdCBo
dHRwczovL2dpdGh1Yi5jb20vYXBhY2hlL2luY3ViYXRvci1vcGVud2hpc2svd2lraSA/DQo+IA0K
PiBUaGFua3MNCj4gVHlzb24NCg0K

",f
26731,54,JIRA.13106671.1507028314000.32939.1508752740650@Atlassian.JIRA,2297,Andrew Jones (JIRA),JIRA.13106671.1507028314000@Atlassian.JIRA,,,2017-10-23 02:59:00-07,"[jira] [Commented] (AIRFLOW-1673) dagrun.dependency-check stat
 contains space in metric name","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1673?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214906#comment-16214906 ] 

Andrew Jones commented on AIRFLOW-1673:
---------------------------------------

Is there anything I can do to help get this merged? Thanks.

> dagrun.dependency-check stat contains space in metric name
> ----------------------------------------------------------
>
>                 Key: AIRFLOW-1673
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1673
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: 1.8.2
>            Reporter: Andrew Jones
>            Assignee: Andrew Jones
>            Priority: Minor
>              Labels: pull-request-available
>
> In {{models.py}}, we save a stat with the [following code|https://github.com/apache/incubator-airflow/blob/afd927a256d8ff97e59b28a3caf81ac0bf0d07f3/airflow/models.py#L4563]:
> {code}
> Stats.timing(""dagrun.dependency-check.{}.{}"".
>                      format(self.dag_id, self.execution_date), duration)
> {code}
> {{self.execution_date}} is introducing a space in the metric name, so what gets sent to stats is something like this:
> {code}
> airflow.dagrun.dependency-check.dagid.2017-09-25 00:00:00:32.253000|ms
> {code}
> A space isn''t valid here and should be removed.
> We could either remove the space from the datetime, save only the date, or remove the datetime from the stat name completely.
> Maybe just change {{self.execution_date}} to {{self.execution_date.isoformat()}}?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26732,54,JIRA.13110574.1508391320000.32987.1508752920206@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110574.1508391320000@Atlassian.JIRA,,,2017-10-23 03:02:00-07,"[jira] [Commented] (AIRFLOW-1736) Add HotelQuickly to Who Uses
 Airflow","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1736?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214917#comment-16214917 ] 

ASF subversion and git services commented on AIRFLOW-1736:
----------------------------------------------------------

Commit 1ef2b6c2d87ef9b1363136d62a598f2a54d2691e in incubator-airflow''s branch refs/heads/master from [~zinuzoid]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1ef2b6c ]

[AIRFLOW-1736] Add HotelQuickly to Who Uses Airflow

Closes #2705 from zinuzoid/AIRFLOW-1736


> Add HotelQuickly to Who Uses Airflow
> ------------------------------------
>
>                 Key: AIRFLOW-1736
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1736
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Jetsada Machom
>            Assignee: Jetsada Machom
>            Priority: Trivial
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26733,54,JIRA.13110574.1508391320000.32986.1508752920196@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110574.1508391320000@Atlassian.JIRA,,,2017-10-23 03:02:00-07,"[jira] [Commented] (AIRFLOW-1736) Add HotelQuickly to Who Uses
 Airflow","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1736?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16214916#comment-16214916 ] 

ASF subversion and git services commented on AIRFLOW-1736:
----------------------------------------------------------

Commit 1ef2b6c2d87ef9b1363136d62a598f2a54d2691e in incubator-airflow''s branch refs/heads/master from [~zinuzoid]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1ef2b6c ]

[AIRFLOW-1736] Add HotelQuickly to Who Uses Airflow

Closes #2705 from zinuzoid/AIRFLOW-1736


> Add HotelQuickly to Who Uses Airflow
> ------------------------------------
>
>                 Key: AIRFLOW-1736
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1736
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Jetsada Machom
>            Assignee: Jetsada Machom
>            Priority: Trivial
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26734,54,JIRA.13111372.1508757795000.33381.1508757840079@Atlassian.JIRA,2348,Ondrej Kokes (JIRA),JIRA.13111372.1508757795000@Atlassian.JIRA,,,2017-10-23 04:24:00-07,[jira] [Created] (AIRFLOW-1747) Incorrect documentation,"Ondrej Kokes created AIRFLOW-1747:
-------------------------------------

             Summary: Incorrect documentation
                 Key: AIRFLOW-1747
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1747
             Project: Apache Airflow
          Issue Type: Improvement
          Components: Documentation
            Reporter: Ondrej Kokes


Given that the PyPI package is now apache-airflow, not just airflow (as fixed in JIRAs 1185 and 1187), this needs to be reflected in the documentation. The rst code has been fixed, but the generated documentation is stale. *It''s still telling people to install {{airflow}}, not {{apache-airflow}}.*

* [The original documentation|https://airflow.apache.org/installation.html] still instructs people to use {{pip install airflow}}
* So does the [pythonhosted.org version|http://pythonhosted.org/airflow/installation.html] - this one is linked to from README.md in the projects, so I guess it''s the most visible piece of documentation (""Please visit the Airflow Platform documentation for help with installing Airflow, getting a quick start, or a more complete tutorial."")
* ReadTheDocs is fine

Pythonhosted seems to depend on what''s set up on PyPI, so that one depends on what repo the documentation is taken from. As the project was renamed, I guess the documentation is still taken from the old code.

As for the official website... I don''t know how that''s generated, but it could do with a refresh.

Let me know if I can help in any way - I unfortunately don''t know much about how these are setup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26735,54,JIRA.13111372.1508757795000.33382.1508757840095@Atlassian.JIRA,2348,Ondrej Kokes (JIRA),JIRA.13111372.1508757795000@Atlassian.JIRA,,,2017-10-23 04:24:00-07,[jira] [Updated] (AIRFLOW-1747) Stale documentation,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1747?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ondrej Kokes updated AIRFLOW-1747:
----------------------------------
    Summary: Stale documentation  (was: Incorrect documentation)

> Stale documentation
> -------------------
>
>                 Key: AIRFLOW-1747
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1747
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: Documentation
>            Reporter: Ondrej Kokes
>
> Given that the PyPI package is now apache-airflow, not just airflow (as fixed in JIRAs 1185 and 1187), this needs to be reflected in the documentation. The rst code has been fixed, but the generated documentation is stale. *It''s still telling people to install {{airflow}}, not {{apache-airflow}}.*
> * [The original documentation|https://airflow.apache.org/installation.html] still instructs people to use {{pip install airflow}}
> * So does the [pythonhosted.org version|http://pythonhosted.org/airflow/installation.html] - this one is linked to from README.md in the projects, so I guess it''s the most visible piece of documentation (""Please visit the Airflow Platform documentation for help with installing Airflow, getting a quick start, or a more complete tutorial."")
> * ReadTheDocs is fine
> Pythonhosted seems to depend on what''s set up on PyPI, so that one depends on what repo the documentation is taken from. As the project was renamed, I guess the documentation is still taken from the old code.
> As for the official website... I don''t know how that''s generated, but it could do with a refresh.
> Let me know if I can help in any way - I unfortunately don''t know much about how these are setup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
875512,186,CAAC1_d4=kYmUMnfz+z9H+nbCteFiM5ROHZ0jAawhm4=xDL4bUg@mail.gmail.com,30891,Rodric Rabbah,NULL,,,2017-08-01 17:49:52-07,authentication models for openwhisk,"I''m capturing below a summary of slack discussions around authentication
and identity management for openwhisk for the benefit of the dev list.

The discussion started around the IBM Bluemix login integration for
openwhisk and how the wsk CLI will be a plugin as part of an existing
bluemix CLI. In doing this, the goal is to maintain the platform
independence of the wsk CLI but allow a platform to provide its own
identity management.

To which, it was noted [1] that Adobe has had the same discussion within
their team since they require a different authentication method for their
OpenWhisk deployment. One of the questions that arose in the ensuing
discussion was: what would it take to make the wsk CLI plugable with
respect to authentication (rather than having each platform provider wrap
wsk for their own purposes).

It was noted that JWT (Json Web Tokens) may be one approach [2], or using
pre-existing OAuth providers to provide tokens (that expire after some
duration) [3]. So how can one use OAuth tokens to authenticate the wsk CLI?
Apropos this point, it may be necessary to modify the Authentication [4]
interface in the controller to allow for multiple identity provider models.
Some extensions in this space are already underway (see [5] for certificate
based authentication for example).

An example was proposed as follows [6]:
   currently CLI does
       req.Header.Add(""Authorization"", fmt.Sprintf(""Basic %%%%s"",
encodedAuthToken))
   that is one place it would be nice to make configurable, e.g.
       wsk --basic-auth
    and
       wsk --bearer-auth (or Authorization: Bearer)

And to possibly extend .wskprops as in [7] to augment the existing AUTH
property:
       AUTH=Bearer <token>
    or
       AUTH=Basic <basic:Credentials>
    which can be automatically configured via a new wsk CLI command as in
       wsk auth login
    which can print a URL that users open in the browser to authenticate,
then they come back to paste the Bearer Token. The CLI can add the token
into .wskprops.

It was noted in [7] that the Azure CLI works similarly. Here''s an example:
    az login
   To sign in, use a web browser to open the page https://aka.ms/devicelogin
and enter the code xxxxx to authenticate.

A prototype of this ""wsk login"" approach was previously prototyped as proof
of concept in [8] using GitHub as an OAuth provider. The flow if I recall
is:

    wsk login <provider> # one of github, google

this opens a browser which allows the user to authenticate and returns an
OAuth token. This was injected into the authentication database which then
allows the wsk CLI to use the token for subsequent operations.

-r (on behalf of several openwhiskers from our community slack channel -
typos all my own)

[1] https://openwhisk-team.slack.com/archives/C3TPCAQG1/p1501594586367600
[2] https://openwhisk-team.slack.com/archives/C3TPCAQG1/p1501598248729369
[3] https://openwhisk-team.slack.com/archives/C3TPCAQG1/p1501599810814266
[4]
https://github.com/apache/incubator-openwhisk/blob/master/core/controller/src/main/scala/whisk/core/controller/AuthenticatedRoute.scala
[5] https://github.com/apache/incubator-openwhisk/pull/2517
[6] https://openwhisk-team.slack.com/archives/C3TPCAQG1/p1501602885940574
[7] https://openwhisk-team.slack.com/archives/C3TPCAQG1/p1501604477993184
[8]
https://github.com/apache/incubator-openwhisk/compare/master...starpit:oauth_login?expand=1
(note that the secrets shows here are not valid and for illustration only)
",f
26736,54,JIRA.13111372.1508757795000.33383.1508757900072@Atlassian.JIRA,2348,Ondrej Kokes (JIRA),JIRA.13111372.1508757795000@Atlassian.JIRA,,,2017-10-23 04:25:00-07,[jira] [Updated] (AIRFLOW-1747) Stale documentation,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1747?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ondrej Kokes updated AIRFLOW-1747:
----------------------------------
    Priority: Minor  (was: Major)

> Stale documentation
> -------------------
>
>                 Key: AIRFLOW-1747
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1747
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: Documentation
>            Reporter: Ondrej Kokes
>            Priority: Minor
>
> Given that the PyPI package is now apache-airflow, not just airflow (as fixed in JIRAs 1185 and 1187), this needs to be reflected in the documentation. The rst code has been fixed, but the generated documentation is stale. *It''s still telling people to install {{airflow}}, not {{apache-airflow}}.*
> * [The original documentation|https://airflow.apache.org/installation.html] still instructs people to use {{pip install airflow}}
> * So does the [pythonhosted.org version|http://pythonhosted.org/airflow/installation.html] - this one is linked to from README.md in the projects, so I guess it''s the most visible piece of documentation (""Please visit the Airflow Platform documentation for help with installing Airflow, getting a quick start, or a more complete tutorial."")
> * ReadTheDocs is fine
> Pythonhosted seems to depend on what''s set up on PyPI, so that one depends on what repo the documentation is taken from. As the project was renamed, I guess the documentation is still taken from the old code.
> As for the official website... I don''t know how that''s generated, but it could do with a refresh.
> Let me know if I can help in any way - I unfortunately don''t know much about how these are setup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26737,54,JIRA.13104708.1506332962000.33527.1508759580427@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-23 04:53:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16215001#comment-16215001 ] 

Bolke de Bruin commented on AIRFLOW-1641:
-----------------------------------------

Patch should now be ready for real testing (tests pass)

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26738,54,JIRA.13111410.1508765341000.34171.1508765400235@Atlassian.JIRA,2349,Di Zhu (JIRA),JIRA.13111410.1508765341000@Atlassian.JIRA,,,2017-10-23 06:30:00-07,"[jira] [Created] (AIRFLOW-1748) Task is not being scheduled daily
 as expected in Airflow","Di Zhu created AIRFLOW-1748:
-------------------------------

             Summary: Task is not being scheduled daily as expected in Airflow
                 Key: AIRFLOW-1748
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1748
             Project: Apache Airflow
          Issue Type: Bug
          Components: DagRun
         Environment: v1.8.0
            Reporter: Di Zhu


A dag python file is defined as below:

```default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017, 10, 22, 11, 25),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 0,
    ''retry_delay'': timedelta(minutes=1),
    # ''queue'': ''bash_queue'',
    # ''pool'': ''backfill'',
    # ''priority_weight'': 10,
    # ''end_date'': datetime(2016, 1, 1),
}

dag = DAG(''test_jason_1'', default_args=default_args, schedule_interval=""@daily"")

# t1, t2 and t3 are examples of tasks created by instantiating operators
t1 = BashOperator(
    task_id=''print_date'',
    bash_command=''echo ""123"" && exit 1'',
    dag=dag)```

Current datetime from command line (same machine as where Airflow is installed) is:

[ec2-user@ip-10-0-0-XXX print_date]$ date
Mon Oct 23 11:24:06 UTC 2017

but after 11:25:00, this job is still not scheduled accordingly. Could anyone help correct me if i''m wrong? Thanks a lot!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26739,54,JIRA.13107977.1507556485000.34216.1508765940157@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13107977.1507556485000@Atlassian.JIRA,,,2017-10-23 06:39:00-07,"[jira] [Commented] (AIRFLOW-1695) Redshift Hook using boto3 & AWS
 Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1695?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16215146#comment-16215146 ] 

Andy Hadjigeorgiou commented on AIRFLOW-1695:
---------------------------------------------

PR here: https://github.com/apache/incubator-airflow/pull/2717

> Redshift Hook using boto3 & AWS Hook
> ------------------------------------
>
>                 Key: AIRFLOW-1695
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1695
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>
> I''d like to add AWS Redshift management capabilities working from the boto3 PR (https://github.com/apache/incubator-airflow/pull/2532). I propose creating a Redshift Hook (that extends AWS Hook class) that uses the boto3 client to manage clusters. Included will be the ability to image, shut-down, create clusters from images, and check cluster status.
> I''m currently working on this, will post PR soon.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549386,277,JIRA.12986655.1467653918000.29105.1467746711118@Atlassian.JIRA,8143,sandeep purohit (JIRA),JIRA.12986655.1467653918000@Atlassian.JIRA,,,2016-07-05 12:25:11-07,"[jira] [Commented] (IOTA-21) Refactor Built of performer to manage
 multi module dependency more efficient and scala style plugin for code
 quality","
    [ https://issues.apache.org/jira/browse/IOTA-21?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15363068#comment-15363068 ] 

sandeep purohit commented on IOTA-21:
-------------------------------------

I make changes and push it according you mention in comment please review it.

> Refactor Built of performer to manage multi module dependency more efficient and scala style plugin for code quality 
> ---------------------------------------------------------------------------------------------------------------------
>
>                 Key: IOTA-21
>                 URL: https://issues.apache.org/jira/browse/IOTA-21
>             Project: Iota
>          Issue Type: Improvement
>            Reporter: sandeep purohit
>            Assignee: sandeep purohit
>   Original Estimate: 3h
>  Remaining Estimate: 3h
>
> In this improvement refactoring the code to manage multi module dependencies more efficient way and add scala style plugin for code quality .



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26740,54,JIRA.13111410.1508765341000.34215.1508765940142@Atlassian.JIRA,2349,Di Zhu (JIRA),JIRA.13111410.1508765341000@Atlassian.JIRA,,,2017-10-23 06:39:00-07,"[jira] [Updated] (AIRFLOW-1748) Task is not being scheduled daily
 as expected in Airflow","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1748?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Di Zhu updated AIRFLOW-1748:
----------------------------
    Description: 
A dag python file is defined as below:

```
default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017, 10, 22, 11, 25),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 0,
    ''retry_delay'': timedelta(minutes=1),
    # ''queue'': ''bash_queue'',
    # ''pool'': ''backfill'',
    # ''priority_weight'': 10,
    # ''end_date'': datetime(2016, 1, 1),
}

dag = DAG(''test_jason_1'', default_args=default_args, schedule_interval=""@daily"")

# t1, t2 and t3 are examples of tasks created by instantiating operators
t1 = BashOperator(
    task_id=''print_date'',
    bash_command=''echo ""123"" && exit 1'',
    dag=dag)```

Current datetime from command line (same machine as where Airflow is installed) is:

```
[ec2-user@ip-10-0-0-XXX print_date]$ date
Mon Oct 23 11:24:06 UTC 2017
```

but after 11:25:00, this job is still not scheduled accordingly. Could anyone help correct me if i''m wrong? Thanks a lot!

  was:
A dag python file is defined as below:

```default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017, 10, 22, 11, 25),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 0,
    ''retry_delay'': timedelta(minutes=1),
    # ''queue'': ''bash_queue'',
    # ''pool'': ''backfill'',
    # ''priority_weight'': 10,
    # ''end_date'': datetime(2016, 1, 1),
}

dag = DAG(''test_jason_1'', default_args=default_args, schedule_interval=""@daily"")

# t1, t2 and t3 are examples of tasks created by instantiating operators
t1 = BashOperator(
    task_id=''print_date'',
    bash_command=''echo ""123"" && exit 1'',
    dag=dag)```

Current datetime from command line (same machine as where Airflow is installed) is:

[ec2-user@ip-10-0-0-XXX print_date]$ date
Mon Oct 23 11:24:06 UTC 2017

but after 11:25:00, this job is still not scheduled accordingly. Could anyone help correct me if i''m wrong? Thanks a lot!


> Task is not being scheduled daily as expected in Airflow
> --------------------------------------------------------
>
>                 Key: AIRFLOW-1748
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1748
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DagRun
>         Environment: v1.8.0
>            Reporter: Di Zhu
>
> A dag python file is defined as below:
> ```
> default_args = {
>     ''owner'': ''airflow'',
>     ''depends_on_past'': False,
>     ''start_date'': datetime(2017, 10, 22, 11, 25),
>     ''email'': [''airflow@airflow.com''],
>     ''email_on_failure'': False,
>     ''email_on_retry'': False,
>     ''retries'': 0,
>     ''retry_delay'': timedelta(minutes=1),
>     # ''queue'': ''bash_queue'',
>     # ''pool'': ''backfill'',
>     # ''priority_weight'': 10,
>     # ''end_date'': datetime(2016, 1, 1),
> }
> dag = DAG(''test_jason_1'', default_args=default_args, schedule_interval=""@daily"")
> # t1, t2 and t3 are examples of tasks created by instantiating operators
> t1 = BashOperator(
>     task_id=''print_date'',
>     bash_command=''echo ""123"" && exit 1'',
>     dag=dag)```
> Current datetime from command line (same machine as where Airflow is installed) is:
> ```
> [ec2-user@ip-10-0-0-XXX print_date]$ date
> Mon Oct 23 11:24:06 UTC 2017
> ```
> but after 11:25:00, this job is still not scheduled accordingly. Could anyone help correct me if i''m wrong? Thanks a lot!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26741,54,JIRA.13111456.1508773358000.35335.1508773380518@Atlassian.JIRA,2350,Nick McNutt (JIRA),JIRA.13111456.1508773358000@Atlassian.JIRA,,,2017-10-23 08:43:00-07,"[jira] [Created] (AIRFLOW-1749) AirflowConfigParser fails to
 override has_option from ConfigParser, causing broken LDAP config","Nick McNutt created AIRFLOW-1749:
------------------------------------

             Summary: AirflowConfigParser fails to override has_option from ConfigParser, causing broken LDAP config
                 Key: AIRFLOW-1749
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1749
             Project: Apache Airflow
          Issue Type: Bug
          Components: configuration
    Affects Versions: Airflow 2.0, Airflow 1.8
         Environment: Ubuntu 16.04
            Reporter: Nick McNutt
            Priority: Minor


In configuration.py, class {{AirflowConfigParser}} fails to override {{has_option}} from {{ConfigParser}}.  This breaks the following in ldap_auth.py:

{{if configuration.has_option(""ldap"", ""search_scope""):
            search_scope = SUBTREE if configuration.get(""ldap"", ""search_scope"") == ""SUBTREE"" else LEVEL}}

This code fails to consider whether any environment variable (e.g., {{AIRFLOW__LDAP__SEARCH_SCOPE}}) or command override''s are set, meaning that LDAP configuration cannot be entirely set up through environment variables.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26742,54,JIRA.13111532.1508792427000.38515.1508792464063@Atlassian.JIRA,2351,Mark Secada (JIRA),JIRA.13111532.1508792427000@Atlassian.JIRA,,,2017-10-23 14:01:04-07,"[jira] [Created] (AIRFLOW-1750)
 GoogleCloudStorageToBigQueryOperator 404 HttpError","Mark Secada created AIRFLOW-1750:
------------------------------------

             Summary: GoogleCloudStorageToBigQueryOperator 404 HttpError
                 Key: AIRFLOW-1750
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1750
             Project: Apache Airflow
          Issue Type: Bug
          Components: gcp
    Affects Versions: Airflow 1.8
         Environment: Python 2.7.13
            Reporter: Mark Secada
             Fix For: Airflow 1.8


I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:

{bash}
ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
Traceback (most recent call last):
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
    result = task_copy.execute(context=context)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
    schema_update_options=self.schema_update_options)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
    return self.run_with_configuration(configuration)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
    .insert(projectId=self.project_id, body=job_data) \
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
    raise HttpError(resp, content, uri=self.uri)
{bash}

My code''s here:

{code:python}
// Some comments here
t3 = GoogleCloudStorageToBigQueryOperator(
        task_id=''move_''+source+''_from_gcs_to_bq'',
        bucket=''mybucket'',
        source_objects=[''news/latest_headline_''+source+''.json''],
        destination_project_dataset_table=''mydataset.latest_news_headlines'',
        schema_object=''news/latest_headline_''+source+''.json'',
        source_format=''NEWLINE_DELIMITED_JSON'',
        write_disposition=''WRITE_APPEND''
        dag=dag)
{code}





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26743,54,JIRA.13111532.1508792427000.38593.1508792466331@Atlassian.JIRA,2351,Mark Secada (JIRA),JIRA.13111532.1508792427000@Atlassian.JIRA,,,2017-10-23 14:01:06-07,"[jira] [Updated] (AIRFLOW-1750)
 GoogleCloudStorageToBigQueryOperator 404 HttpError","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1750?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark Secada updated AIRFLOW-1750:
---------------------------------
    Description: 
I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:

{code:bash}
ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
Traceback (most recent call last):
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
    result = task_copy.execute(context=context)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
    schema_update_options=self.schema_update_options)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
    return self.run_with_configuration(configuration)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
    .insert(projectId=self.project_id, body=job_data) \
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
    raise HttpError(resp, content, uri=self.uri)
{code}

My code''s here:

{code:python}
// Some comments here
t3 = GoogleCloudStorageToBigQueryOperator(
        task_id=''move_''+source+''_from_gcs_to_bq'',
        bucket=''mybucket'',
        source_objects=[''news/latest_headline_''+source+''.json''],
        destination_project_dataset_table=''mydataset.latest_news_headlines'',
        schema_object=''news/latest_headline_''+source+''.json'',
        source_format=''NEWLINE_DELIMITED_JSON'',
        write_disposition=''WRITE_APPEND''
        dag=dag)
{code}



  was:
I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:

{bash}
ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
Traceback (most recent call last):
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
    result = task_copy.execute(context=context)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
    schema_update_options=self.schema_update_options)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
    return self.run_with_configuration(configuration)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
    .insert(projectId=self.project_id, body=job_data) \
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
    raise HttpError(resp, content, uri=self.uri)
{bash}

My code''s here:

{code:python}
// Some comments here
t3 = GoogleCloudStorageToBigQueryOperator(
        task_id=''move_''+source+''_from_gcs_to_bq'',
        bucket=''mybucket'',
        source_objects=[''news/latest_headline_''+source+''.json''],
        destination_project_dataset_table=''mydataset.latest_news_headlines'',
        schema_object=''news/latest_headline_''+source+''.json'',
        source_format=''NEWLINE_DELIMITED_JSON'',
        write_disposition=''WRITE_APPEND''
        dag=dag)
{code}




> GoogleCloudStorageToBigQueryOperator 404 HttpError
> --------------------------------------------------
>
>                 Key: AIRFLOW-1750
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1750
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>    Affects Versions: Airflow 1.8
>         Environment: Python 2.7.13
>            Reporter: Mark Secada
>             Fix For: Airflow 1.8
>
>
> I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:
> {code:bash}
> ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
> Traceback (most recent call last):
>   File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
>     result = task_copy.execute(context=context)
>   File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
>     schema_update_options=self.schema_update_options)
>   File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
>     return self.run_with_configuration(configuration)
>   File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
>     .insert(projectId=self.project_id, body=job_data) \
>   File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
>     return wrapped(*args, **kwargs)
>   File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
>     raise HttpError(resp, content, uri=self.uri)
> {code}
> My code''s here:
> {code:python}
> // Some comments here
> t3 = GoogleCloudStorageToBigQueryOperator(
>         task_id=''move_''+source+''_from_gcs_to_bq'',
>         bucket=''mybucket'',
>         source_objects=[''news/latest_headline_''+source+''.json''],
>         destination_project_dataset_table=''mydataset.latest_news_headlines'',
>         schema_object=''news/latest_headline_''+source+''.json'',
>         source_format=''NEWLINE_DELIMITED_JSON'',
>         write_disposition=''WRITE_APPEND''
>         dag=dag)
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26744,54,JIRA.13111532.1508792427000.38599.1508792520183@Atlassian.JIRA,2351,Mark Secada (JIRA),JIRA.13111532.1508792427000@Atlassian.JIRA,,,2017-10-23 14:02:00-07,"[jira] [Updated] (AIRFLOW-1750)
 GoogleCloudStorageToBigQueryOperator 404 HttpError","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1750?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark Secada updated AIRFLOW-1750:
---------------------------------
    Description: 
I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:

{code:bash}
ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
Traceback (most recent call last):
  File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
    result = task_copy.execute(context=context)
  File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
    schema_update_options=self.schema_update_options)
  File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
    return self.run_with_configuration(configuration)
  File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
    .insert(projectId=self.project_id, body=job_data) \
  File ""/Users/myname/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/Users/myname/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
    raise HttpError(resp, content, uri=self.uri)
{code}

My code for the task is here:

{code:python}
// Some comments here
t3 = GoogleCloudStorageToBigQueryOperator(
        task_id=''move_''+source+''_from_gcs_to_bq'',
        bucket=''mybucket'',
        source_objects=[''news/latest_headline_''+source+''.json''],
        destination_project_dataset_table=''mydataset.latest_news_headlines'',
        schema_object=''news/latest_headline_''+source+''.json'',
        source_format=''NEWLINE_DELIMITED_JSON'',
        write_disposition=''WRITE_APPEND''
        dag=dag)
{code}



  was:
I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:

{code:bash}
ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
Traceback (most recent call last):
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
    result = task_copy.execute(context=context)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
    schema_update_options=self.schema_update_options)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
    return self.run_with_configuration(configuration)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
    .insert(projectId=self.project_id, body=job_data) \
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
    return wrapped(*args, **kwargs)
  File ""/Users/marksecada/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
    raise HttpError(resp, content, uri=self.uri)
{code}

My code''s here:

{code:python}
// Some comments here
t3 = GoogleCloudStorageToBigQueryOperator(
        task_id=''move_''+source+''_from_gcs_to_bq'',
        bucket=''mybucket'',
        source_objects=[''news/latest_headline_''+source+''.json''],
        destination_project_dataset_table=''mydataset.latest_news_headlines'',
        schema_object=''news/latest_headline_''+source+''.json'',
        source_format=''NEWLINE_DELIMITED_JSON'',
        write_disposition=''WRITE_APPEND''
        dag=dag)
{code}




> GoogleCloudStorageToBigQueryOperator 404 HttpError
> --------------------------------------------------
>
>                 Key: AIRFLOW-1750
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1750
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>    Affects Versions: Airflow 1.8
>         Environment: Python 2.7.13
>            Reporter: Mark Secada
>             Fix For: Airflow 1.8
>
>
> I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:
> {code:bash}
> ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
> Traceback (most recent call last):
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
>     result = task_copy.execute(context=context)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
>     schema_update_options=self.schema_update_options)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
>     return self.run_with_configuration(configuration)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
>     .insert(projectId=self.project_id, body=job_data) \
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
>     return wrapped(*args, **kwargs)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
>     raise HttpError(resp, content, uri=self.uri)
> {code}
> My code for the task is here:
> {code:python}
> // Some comments here
> t3 = GoogleCloudStorageToBigQueryOperator(
>         task_id=''move_''+source+''_from_gcs_to_bq'',
>         bucket=''mybucket'',
>         source_objects=[''news/latest_headline_''+source+''.json''],
>         destination_project_dataset_table=''mydataset.latest_news_headlines'',
>         schema_object=''news/latest_headline_''+source+''.json'',
>         source_format=''NEWLINE_DELIMITED_JSON'',
>         write_disposition=''WRITE_APPEND''
>         dag=dag)
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
730778,24,154570223150.13192.13897499722127086849.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-12-24 17:43:51-08,[GitHub] vrakesh commented on issue #13719: new label shapes problem,"vrakesh commented on issue #13719: new label shapes problem
URL: https://github.com/apache/incubator-mxnet/issues/13719#issuecomment-449785308
 
 
   @mxnet-label-bot  add [NDArray]
   @youhangtian Thank you for  reporting the issue. 
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
26745,54,JIRA.13111532.1508792427000.38672.1508793120284@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13111532.1508792427000@Atlassian.JIRA,,,2017-10-23 14:12:00-07,"[jira] [Commented] (AIRFLOW-1750)
 GoogleCloudStorageToBigQueryOperator 404 HttpError","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1750?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16215848#comment-16215848 ] 

Chris Riccomini commented on AIRFLOW-1750:
------------------------------------------

It looks to me like the project id is not being properly set. Have you checked your hook definition, service account, etc? The URL listed in the stack trace has two slashes after `projects`, indicating that no project_id was set.

> GoogleCloudStorageToBigQueryOperator 404 HttpError
> --------------------------------------------------
>
>                 Key: AIRFLOW-1750
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1750
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>    Affects Versions: Airflow 1.8
>         Environment: Python 2.7.13
>            Reporter: Mark Secada
>             Fix For: Airflow 1.8
>
>
> I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:
> {code:bash}
> ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
> Traceback (most recent call last):
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
>     result = task_copy.execute(context=context)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
>     schema_update_options=self.schema_update_options)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
>     return self.run_with_configuration(configuration)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
>     .insert(projectId=self.project_id, body=job_data) \
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
>     return wrapped(*args, **kwargs)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
>     raise HttpError(resp, content, uri=self.uri)
> {code}
> My code for the task is here:
> {code:python}
> // Some comments here
> t3 = GoogleCloudStorageToBigQueryOperator(
>         task_id=''move_''+source+''_from_gcs_to_bq'',
>         bucket=''mybucket'',
>         source_objects=[''news/latest_headline_''+source+''.json''],
>         destination_project_dataset_table=''mydataset.latest_news_headlines'',
>         schema_object=''news/latest_headline_''+source+''.json'',
>         source_format=''NEWLINE_DELIMITED_JSON'',
>         write_disposition=''WRITE_APPEND''
>         dag=dag)
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26746,54,JIRA.13111595.1508812757000.40881.1508812800019@Atlassian.JIRA,2217,Yifei Hong (JIRA),JIRA.13111595.1508812757000@Atlassian.JIRA,,,2017-10-23 19:40:00-07,"[jira] [Created] (AIRFLOW-1751) SqlAlchemy Error when backfilling
 dag and connecting to SQL Server","Yifei Hong created AIRFLOW-1751:
-----------------------------------

             Summary: SqlAlchemy Error when backfilling dag and connecting to SQL Server
                 Key: AIRFLOW-1751
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1751
             Project: Apache Airflow
          Issue Type: Bug
          Components: backfill, db
    Affects Versions: Airflow 1.8
         Environment: Linux
            Reporter: Yifei Hong
         Attachments: db.png, error.log, web.png

Airflow works well for me when using sqlite as the backend database.

In order to use LocalExecutor to test on parallel runs (pseudo), I set up the db connection to MS SQL Server with mssql+pyodbc. The connection is good and all metadata tables are created. However, when trying to backfill the dag run, I always encounter the same sqlalchemy.exc.StatementError for any example dag.

airflow backfill example_bash_operator -s 2017-10-18

sqlalchemy.exc.StatementError: (exceptions.AttributeError) ''module'' object has no attribute ''BinaryNull'' [SQL: u''INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?)''] [parameters: [{''end_date'': None, ''run_id'': u''backfill_2017-10-18T00:00:00'', ''execution_date'': datetime.datetime(2017, 10, 18, 0, 0), ''external_trigger'': False, ''state'': u''running'', ''conf'': None, ''start_date'': datetime.datetime(2017, 10, 23, 21, 25, 9, 399710), ''dag_id'': ''example_bash_operator''}]]

When I tried to trigger the run on web GUI, it seems to be fine and I can see the record got inserted into dag_run table successfully.

!web.png|thumbnail!

!db.png|thumbnail!
 




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26747,54,JIRA.13111595.1508812757000.40882.1508812920046@Atlassian.JIRA,2217,Yifei Hong (JIRA),JIRA.13111595.1508812757000@Atlassian.JIRA,,,2017-10-23 19:42:00-07,"[jira] [Updated] (AIRFLOW-1751) SqlAlchemy Error when backfilling
 dag and connecting to SQL Server","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1751?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Yifei Hong updated AIRFLOW-1751:
--------------------------------
    Description: 
Airflow works well for me when using sqlite as the backend database.

In order to use LocalExecutor to test on parallel runs (pseudo), I set up the db connection to MS SQL Server with mssql+pyodbc. The connection is good and all metadata tables are created. However, when trying to backfill the dag run, I always encounter the same sqlalchemy.exc.StatementError for any example dag.

airflow backfill example_bash_operator -s 2017-10-18

sqlalchemy.exc.StatementError: (exceptions.AttributeError) ''module'' object has no attribute ''BinaryNull'' [SQL: u''INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?)''] [parameters: [{''end_date'': None, ''run_id'': u''backfill_2017-10-18T00:00:00'', ''execution_date'': datetime.datetime(2017, 10, 18, 0, 0), ''external_trigger'': False, ''state'': u''running'', ''conf'': None, ''start_date'': datetime.datetime(2017, 10, 23, 21, 25, 9, 399710), ''dag_id'': ''example_bash_operator''}]]

When I tried to trigger the run on web GUI, it seems to be fine and I can see the record got inserted into dag_run table successfully.

[^web.png]
[^db.png]


  was:
Airflow works well for me when using sqlite as the backend database.

In order to use LocalExecutor to test on parallel runs (pseudo), I set up the db connection to MS SQL Server with mssql+pyodbc. The connection is good and all metadata tables are created. However, when trying to backfill the dag run, I always encounter the same sqlalchemy.exc.StatementError for any example dag.

airflow backfill example_bash_operator -s 2017-10-18

sqlalchemy.exc.StatementError: (exceptions.AttributeError) ''module'' object has no attribute ''BinaryNull'' [SQL: u''INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?)''] [parameters: [{''end_date'': None, ''run_id'': u''backfill_2017-10-18T00:00:00'', ''execution_date'': datetime.datetime(2017, 10, 18, 0, 0), ''external_trigger'': False, ''state'': u''running'', ''conf'': None, ''start_date'': datetime.datetime(2017, 10, 23, 21, 25, 9, 399710), ''dag_id'': ''example_bash_operator''}]]

When I tried to trigger the run on web GUI, it seems to be fine and I can see the record got inserted into dag_run table successfully.

!web.png|thumbnail!

!db.png|thumbnail!
 



> SqlAlchemy Error when backfilling dag and connecting to SQL Server
> ------------------------------------------------------------------
>
>                 Key: AIRFLOW-1751
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1751
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: backfill, db
>    Affects Versions: Airflow 1.8
>         Environment: Linux
>            Reporter: Yifei Hong
>         Attachments: db.png, error.log, web.png
>
>
> Airflow works well for me when using sqlite as the backend database.
> In order to use LocalExecutor to test on parallel runs (pseudo), I set up the db connection to MS SQL Server with mssql+pyodbc. The connection is good and all metadata tables are created. However, when trying to backfill the dag run, I always encounter the same sqlalchemy.exc.StatementError for any example dag.
> airflow backfill example_bash_operator -s 2017-10-18
> sqlalchemy.exc.StatementError: (exceptions.AttributeError) ''module'' object has no attribute ''BinaryNull'' [SQL: u''INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?)''] [parameters: [{''end_date'': None, ''run_id'': u''backfill_2017-10-18T00:00:00'', ''execution_date'': datetime.datetime(2017, 10, 18, 0, 0), ''external_trigger'': False, ''state'': u''running'', ''conf'': None, ''start_date'': datetime.datetime(2017, 10, 23, 21, 25, 9, 399710), ''dag_id'': ''example_bash_operator''}]]
> When I tried to trigger the run on web GUI, it seems to be fine and I can see the record got inserted into dag_run table successfully.
> [^web.png]
> [^db.png]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26748,54,JIRA.13111595.1508812757000.40886.1508812980218@Atlassian.JIRA,2217,Yifei Hong (JIRA),JIRA.13111595.1508812757000@Atlassian.JIRA,,,2017-10-23 19:43:00-07,"[jira] [Updated] (AIRFLOW-1751) SqlAlchemy Error when backfilling
 dag and connecting to SQL Server","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1751?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Yifei Hong updated AIRFLOW-1751:
--------------------------------
    Description: 
Airflow works well for me when using sqlite as the backend database.

In order to use LocalExecutor to test on parallel runs (pseudo), I set up the db connection to MS SQL Server with mssql+pyodbc. The connection is good and all metadata tables are created. However, when trying to backfill the dag run, I always encounter the same sqlalchemy.exc.StatementError for any example dag. Error log is attached with the full stack trace.

airflow backfill example_bash_operator -s 2017-10-18

sqlalchemy.exc.StatementError: (exceptions.AttributeError) ''module'' object has no attribute ''BinaryNull'' [SQL: u''INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?)''] [parameters: [{''end_date'': None, ''run_id'': u''backfill_2017-10-18T00:00:00'', ''execution_date'': datetime.datetime(2017, 10, 18, 0, 0), ''external_trigger'': False, ''state'': u''running'', ''conf'': None, ''start_date'': datetime.datetime(2017, 10, 23, 21, 25, 9, 399710), ''dag_id'': ''example_bash_operator''}]]

When I tried to trigger the run on web GUI, it seems to be fine and I can see the record got inserted into dag_run table successfully.

[^web.png]
[^db.png]


  was:
Airflow works well for me when using sqlite as the backend database.

In order to use LocalExecutor to test on parallel runs (pseudo), I set up the db connection to MS SQL Server with mssql+pyodbc. The connection is good and all metadata tables are created. However, when trying to backfill the dag run, I always encounter the same sqlalchemy.exc.StatementError for any example dag.

airflow backfill example_bash_operator -s 2017-10-18

sqlalchemy.exc.StatementError: (exceptions.AttributeError) ''module'' object has no attribute ''BinaryNull'' [SQL: u''INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?)''] [parameters: [{''end_date'': None, ''run_id'': u''backfill_2017-10-18T00:00:00'', ''execution_date'': datetime.datetime(2017, 10, 18, 0, 0), ''external_trigger'': False, ''state'': u''running'', ''conf'': None, ''start_date'': datetime.datetime(2017, 10, 23, 21, 25, 9, 399710), ''dag_id'': ''example_bash_operator''}]]

When I tried to trigger the run on web GUI, it seems to be fine and I can see the record got inserted into dag_run table successfully.

[^web.png]
[^db.png]



> SqlAlchemy Error when backfilling dag and connecting to SQL Server
> ------------------------------------------------------------------
>
>                 Key: AIRFLOW-1751
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1751
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: backfill, db
>    Affects Versions: Airflow 1.8
>         Environment: Linux
>            Reporter: Yifei Hong
>         Attachments: db.png, error.log, web.png
>
>
> Airflow works well for me when using sqlite as the backend database.
> In order to use LocalExecutor to test on parallel runs (pseudo), I set up the db connection to MS SQL Server with mssql+pyodbc. The connection is good and all metadata tables are created. However, when trying to backfill the dag run, I always encounter the same sqlalchemy.exc.StatementError for any example dag. Error log is attached with the full stack trace.
> airflow backfill example_bash_operator -s 2017-10-18
> sqlalchemy.exc.StatementError: (exceptions.AttributeError) ''module'' object has no attribute ''BinaryNull'' [SQL: u''INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?)''] [parameters: [{''end_date'': None, ''run_id'': u''backfill_2017-10-18T00:00:00'', ''execution_date'': datetime.datetime(2017, 10, 18, 0, 0), ''external_trigger'': False, ''state'': u''running'', ''conf'': None, ''start_date'': datetime.datetime(2017, 10, 23, 21, 25, 9, 399710), ''dag_id'': ''example_bash_operator''}]]
> When I tried to trigger the run on web GUI, it seems to be fine and I can see the record got inserted into dag_run table successfully.
> [^web.png]
> [^db.png]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
730779,24,154570286537.19249.13008642615965273708.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-12-24 17:54:25-08,"[GitHub] ascust closed issue #13660: A question about the mechanism of
 autograd","ascust closed issue #13660: A question about the mechanism of autograd
URL: https://github.com/apache/incubator-mxnet/issues/13660
 
 
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
730780,24,154571460759.10270.15004120417274330169.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-12-24 21:10:07-08,"[GitHub] lanking520 commented on a change in pull request #13450:
 [MXNET-862] Basic maven jenkins pipeline","lanking520 commented on a change in pull request #13450: [MXNET-862] Basic maven jenkins pipeline
URL: https://github.com/apache/incubator-mxnet/pull/13450#discussion_r243875330
 
 

 ##########
 File path: scala-package/dev/buildkey.py
 ##########
 @@ -0,0 +1,151 @@
+#!/usr/bin/env python3
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+import os
+import json
+import logging
+import subprocess
+
+HOME = os.environ[''HOME'']
+KEY_PATH = os.path.join(HOME, "".m2"")
+
+
+''''''
+This file would do the following items:
+    Import keys from AWS Credential services
+    Create settings.xml in .m2 with pass phrase
+    Create security-settings.xml in .m2 with master password
+    Import keys.asc the encrypted keys in gpg
+''''''
+
+
+def getCredentials():
+    import boto3
+    import botocore
+    endpoint_url = os.environ[''MAVEN_PUBLISH_SECRET_ENDPOINT_URL'']
+    secret_creds_name = os.environ[''MAVEN_PUBLISH_SECRET_NAME_CREDENTIALS'']
+    secret_key_name = os.environ[''MAVEN_PUBLISH_SECRET_NAME_GPG'']
+    region_name = os.environ[''DOCKERHUB_SECRET_ENDPOINT_REGION'']
+
+    session = boto3.Session()
+    client = session.client(
+        service_name=''secretsmanager'',
+        region_name=region_name,
+        endpoint_url=endpoint_url
+    )
+    try:
+        get_secret_value_response = client.get_secret_value(
+            SecretId=secret_creds_name
+        )
+        get_secret_key_response = client.get_secret_value(
+            SecretId=secret_key_name
+        )
+    except botocore.exceptions.ClientError as client_error:
+        if client_error.response[''Error''][''Code''] == ''ResourceNotFoundException'':
+            name = (secret_key_name if get_secret_value_response
+                    else secret_creds_name)
+            logging.exception(""The requested secret %%%%s was not found"", name)
+        elif client_error.response[''Error''][''Code''] == ''InvalidRequestException'':
+            logging.exception(""The request was invalid due to:"")
+        elif client_error.response[''Error''][''Code''] == ''InvalidParameterException'':
+            logging.exception(""The request had invalid params:"")
+        else:
+            raise
+    else:
+        secret = get_secret_value_response[''SecretString'']
+        secret_dict = json.loads(secret)
+        secret_key = get_secret_key_response[''SecretString'']
+        return secret_dict, secret_key
+
+
+def importASC(key, gpgPassphrase):
+    filename = os.path.join(KEY_PATH, ""key.asc"")
+    with open(filename, ''w'') as f:
+        f.write(key)
+    subprocess.run([''gpg2'', ''--batch'', ''--yes'',
+                    ''--passphrase=\""{}\""''.format(gpgPassphrase),
+                    ""--import"", ""{}"".format(filename)])
+
+
+def encryptMasterPSW(password):
+    result = subprocess.run([''mvn'', ''--encrypt-master-password'', password],
+                            stdout=subprocess.PIPE)
+    return str(result.stdout)[2:-3]
+
+
+def encryptPSW(password):
+    result = subprocess.run([''mvn'', ''--encrypt-password'', password],
+                            stdout=subprocess.PIPE)
+    return str(result.stdout)[2:-3]
+
+
+def masterPSW(password):
+    with open(os.path.join(KEY_PATH, ""settings-security.xml""), ""w"") as f:
+        f.write(""<settingsSecurity>\n <master>{}</master>\n</settingsSecurity>""
+                .format(password))
+
+
+def severPSW(username, password, gpgPassphrase):
 
 Review comment:
   The Apache server

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
26749,54,JIRA.13111687.1508850966000.45057.1508851020295@Atlassian.JIRA,2352,Mattias Hammarsten (JIRA),JIRA.13111687.1508850966000@Atlassian.JIRA,,,2017-10-24 06:17:00-07,"[jira] [Created] (AIRFLOW-1752) Wrong username is stored in airflow
 when logging in with contrib ldap","Mattias Hammarsten created AIRFLOW-1752:
-------------------------------------------

             Summary: Wrong username is stored in airflow when logging in with contrib ldap
                 Key: AIRFLOW-1752
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1752
             Project: Apache Airflow
          Issue Type: Bug
          Components: authentication
    Affects Versions: 1.8.2
            Reporter: Mattias Hammarsten


When logging in with airflow.contrib.auth.backends.ldap_auth username gets stored (to airflow.users table) from user input rather than the username stored in ldap.

This leads to problems when the username entered in the gui has different case from the username in ldap.

1) The user logs in with username and password
2) ldap_auth authenticates the user - case insesitive
3) ldap_auth stores whatever username entered in the gui to airflow.users table
4) ldap_auth checks for group membership by doing a case-sensitive comparison between the stored username and the username from ldap.

If the user entered the username with a different case compared to ldap at the first login, then group membership checks fail for all future logins.

This also potentially affects dag filtering.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26750,54,JIRA.13111532.1508792427000.45504.1508853180969@Atlassian.JIRA,2351,Mark Secada (JIRA),JIRA.13111532.1508792427000@Atlassian.JIRA,,,2017-10-24 06:53:00-07,"[jira] [Commented] (AIRFLOW-1750)
 GoogleCloudStorageToBigQueryOperator 404 HttpError","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1750?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16216946#comment-16216946 ] 

Mark Secada commented on AIRFLOW-1750:
--------------------------------------

[~criccomini]

Regarding my service account: I set up my connection to Google Cloud Services through the webserver. I have no problems with the FileToGoogleCloudStorageOperator, or the BigQueryOperator. 

I''ve also specified my project_id in destination_project_dataset_table before and received the same error.

> GoogleCloudStorageToBigQueryOperator 404 HttpError
> --------------------------------------------------
>
>                 Key: AIRFLOW-1750
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1750
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>    Affects Versions: Airflow 1.8
>         Environment: Python 2.7.13
>            Reporter: Mark Secada
>             Fix For: Airflow 1.8
>
>
> I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:
> {code:bash}
> ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
> Traceback (most recent call last):
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
>     result = task_copy.execute(context=context)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
>     schema_update_options=self.schema_update_options)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
>     return self.run_with_configuration(configuration)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
>     .insert(projectId=self.project_id, body=job_data) \
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
>     return wrapped(*args, **kwargs)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
>     raise HttpError(resp, content, uri=self.uri)
> {code}
> My code for the task is here:
> {code:python}
> // Some comments here
> t3 = GoogleCloudStorageToBigQueryOperator(
>         task_id=''move_''+source+''_from_gcs_to_bq'',
>         bucket=''mybucket'',
>         source_objects=[''news/latest_headline_''+source+''.json''],
>         destination_project_dataset_table=''mydataset.latest_news_headlines'',
>         schema_object=''news/latest_headline_''+source+''.json'',
>         source_format=''NEWLINE_DELIMITED_JSON'',
>         write_disposition=''WRITE_APPEND''
>         dag=dag)
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26751,54,JIRA.13111746.1508863133000.46898.1508863140287@Atlassian.JIRA,2353,Lakshman Udayakantha (JIRA),JIRA.13111746.1508863133000@Atlassian.JIRA,,,2017-10-24 09:39:00-07,[jira] [Created] (AIRFLOW-1753) Can''t install on windows 10,"Lakshman Udayakantha created AIRFLOW-1753:
---------------------------------------------

             Summary: Can''t install on windows 10
                 Key: AIRFLOW-1753
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1753
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.8.0
            Reporter: Lakshman Udayakantha


When I installed airflow using ""pip install airflow command"" two errors pop up.

1.  link.exe failed with exit status 1158
2.\x86_amd64\\cl.exe'' failed with exit status 2

first issue can be solved by reffering https://stackoverflow.com/questions/43858836/python-installing-clarifai-vs14-0-link-exe-failed-with-exit-status-1158/44563421#44563421.

But second issue is still there. there was no any solution by googling also. how to prevent that issue and install airflow on windows 10 X64.





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26752,54,JIRA.13111758.1508865186000.47509.1508865240374@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13111758.1508865186000@Atlassian.JIRA,,,2017-10-24 10:14:00-07,"[jira] [Created] (AIRFLOW-1754) Add GCP logging download for
 Dataflow operator","Chris Riccomini created AIRFLOW-1754:
----------------------------------------

             Summary: Add GCP logging download for Dataflow operator
                 Key: AIRFLOW-1754
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1754
             Project: Apache Airflow
          Issue Type: New Feature
            Reporter: Chris Riccomini
             Fix For: 1.10.0


Based on conversation in AIRFLOW-1732 and https://github.com/apache/incubator-airflow/pull/2702, there is useful logging that occurs for Dataflow on the server-side (i.e. it''s not visible simply by piping client logs to the Airflow log file).

We should add a method to fetch logs from GCP logging (stack driver?), so we can spool server side logging into the Dataflow operator for debugging purposes.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26753,54,JIRA.13110477.1508360188000.47533.1508865300798@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110477.1508360188000@Atlassian.JIRA,,,2017-10-24 10:15:00-07,[jira] [Commented] (AIRFLOW-1732) Improve Dataflow Hook Logging,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1732?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217295#comment-16217295 ] 

ASF subversion and git services commented on AIRFLOW-1732:
----------------------------------------------------------

Commit 1475e67078a518bfff59dbf05a1b9c0d98e0218c in incubator-airflow''s branch refs/heads/master from [~TrevorEdwards]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1475e67 ]

[AIRFLOW-1732] Improve dataflow hook logging

Closes #2702 from TrevorEdwards/1732


> Improve Dataflow Hook Logging
> -----------------------------
>
>                 Key: AIRFLOW-1732
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1732
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Trevor Edwards
>            Assignee: Trevor Edwards
>
> Logging output for Dataflow hook could be a bit more useful. Namely:
> # Log the command that is used for opening a dataflow subprocess
> # If the dataflow subprocess experiences an error, log that error at warning (instead of debug)
> Currently, errors are extremely opaque, only showing the exit code. The command used is unknown and the error is logged, but it is at the debug level which makes it difficult to find.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26754,54,JIRA.13110477.1508360188000.47559.1508865361296@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13110477.1508360188000@Atlassian.JIRA,,,2017-10-24 10:16:01-07,[jira] [Updated] (AIRFLOW-1732) Improve Dataflow Hook Logging,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1732?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1732:
-------------------------------------
    Fix Version/s: 1.10.0

> Improve Dataflow Hook Logging
> -----------------------------
>
>                 Key: AIRFLOW-1732
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1732
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Trevor Edwards
>            Assignee: Trevor Edwards
>             Fix For: 1.10.0
>
>
> Logging output for Dataflow hook could be a bit more useful. Namely:
> # Log the command that is used for opening a dataflow subprocess
> # If the dataflow subprocess experiences an error, log that error at warning (instead of debug)
> Currently, errors are extremely opaque, only showing the exit code. The command used is unknown and the error is logged, but it is at the debug level which makes it difficult to find.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
730817,24,154577174841.23785.1001463601681662454.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-12-25 13:02:28-08,[GitHub] vrakesh commented on issue #13724: libmxnet_predict.so runtime error,"vrakesh commented on issue #13724: libmxnet_predict.so runtime error
URL: https://github.com/apache/incubator-mxnet/issues/13724#issuecomment-449873608
 
 
   @bryanlinnan Sounds excellent could you elaborate further on the root cause and steps used to build. So that I can try reproduce it on my end.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
26755,54,JIRA.13110477.1508360188000.47563.1508865361347@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13110477.1508360188000@Atlassian.JIRA,,,2017-10-24 10:16:01-07,[jira] [Resolved] (AIRFLOW-1732) Improve Dataflow Hook Logging,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1732?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1732.
--------------------------------------
    Resolution: Fixed

> Improve Dataflow Hook Logging
> -----------------------------
>
>                 Key: AIRFLOW-1732
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1732
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Trevor Edwards
>            Assignee: Trevor Edwards
>             Fix For: 1.10.0
>
>
> Logging output for Dataflow hook could be a bit more useful. Namely:
> # Log the command that is used for opening a dataflow subprocess
> # If the dataflow subprocess experiences an error, log that error at warning (instead of debug)
> Currently, errors are extremely opaque, only showing the exit code. The command used is unknown and the error is logged, but it is at the debug level which makes it difficult to find.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26756,54,JIRA.13081281.1498007085000.47590.1508865421032@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13081281.1498007085000@Atlassian.JIRA,,,2017-10-24 10:17:01-07,"[jira] [Commented] (AIRFLOW-1330) Connection.parse_from_uri doesn''t
 work for google_cloud_platform and so on","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1330?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217303#comment-16217303 ] 

ASF subversion and git services commented on AIRFLOW-1330:
----------------------------------------------------------

Commit 6e5e9d282e2048c16cde75d7dabff74cd5717646 in incubator-airflow''s branch refs/heads/v1-9-test from [~mrkm4ntr]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6e5e9d2 ]

[AIRFLOW-1330] Add conn_type argument to CLI when adding connection

Closes #2525 from mrkm4ntr/airflow-1330


> Connection.parse_from_uri doesn''t work for google_cloud_platform and so on
> --------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1330
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1330
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli
>            Reporter: Yu Ishikawa
>            Assignee: Shintaro Murakami
>             Fix For: 1.10.0
>
>
> h2. Overview
> {{Connection.parse_from_uri}} doesn''t work for some types like {{google_cloud_platform}} whose type name includes under scores. Since `urllib.parse.urlparse()` which is used in {{Connection.parse_from_url}} doesn''t support a schema name which include under scores.
> So, airflow''s CLI doesn''t work when a given connection URI includes under scores like {{google_cloud_platform://XXXXX}}.
> h3. Workaround
> https://medium.com/@yuu.ishikawa/apache-airflow-how-to-add-a-connection-to-google-cloud-with-cli-af2cc8df138d



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26757,54,JIRA.13081281.1498007085000.47597.1508865480136@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13081281.1498007085000@Atlassian.JIRA,,,2017-10-24 10:18:00-07,"[jira] [Updated] (AIRFLOW-1330) Connection.parse_from_uri doesn''t
 work for google_cloud_platform and so on","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1330?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1330:
-------------------------------------
    Fix Version/s:     (was: 1.10.0)
                   1.9.0

> Connection.parse_from_uri doesn''t work for google_cloud_platform and so on
> --------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1330
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1330
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli
>            Reporter: Yu Ishikawa
>            Assignee: Shintaro Murakami
>             Fix For: 1.9.0
>
>
> h2. Overview
> {{Connection.parse_from_uri}} doesn''t work for some types like {{google_cloud_platform}} whose type name includes under scores. Since `urllib.parse.urlparse()` which is used in {{Connection.parse_from_url}} doesn''t support a schema name which include under scores.
> So, airflow''s CLI doesn''t work when a given connection URI includes under scores like {{google_cloud_platform://XXXXX}}.
> h3. Workaround
> https://medium.com/@yuu.ishikawa/apache-airflow-how-to-add-a-connection-to-google-cloud-with-cli-af2cc8df138d



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26758,54,JIRA.13104708.1506332962000.47906.1508866741080@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-24 10:39:01-07,[jira] [Updated] (AIRFLOW-1641) Task gets stuck in queued state,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1641:
-------------------------------------
    Priority: Blocker  (was: Major)

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
621343,146,JIRA.12839495.1434966146000.134541.1434966480669@Atlassian.JIRA,23783,Ankeet Maini (JIRA),JIRA.12839495.1434966146000@Atlassian.JIRA,,,2015-06-22 02:48:00-07,"[jira] [Updated] (LENS-629) A new, improved web client.","
     [ https://issues.apache.org/jira/browse/LENS-629?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ankeet Maini updated LENS-629:
------------------------------
    Attachment: results.png
                query-status.png
                query results.png
                default-view.png
                Data discovery.png
                autocomplete-2.png
                autocomplete-1.png

Adding some screenshots of the envisaged front end client.

> A new, improved web client.
> ---------------------------
>
>                 Key: LENS-629
>                 URL: https://issues.apache.org/jira/browse/LENS-629
>             Project: Apache Lens
>          Issue Type: Improvement
>            Reporter: Ankeet Maini
>         Attachments: Data discovery.png, autocomplete-1.png, autocomplete-2.png, default-view.png, query results.png, query-status.png, results.png
>
>
> A new web client app to fire queries, discover cubes, native tables and to view results.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26759,54,JIRA.13057410.1489976523000.47931.1508866860558@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13057410.1489976523000@Atlassian.JIRA,,,2017-10-24 10:41:00-07,"[jira] [Updated] (AIRFLOW-1013) airflow/jobs.py:manage_slas()
 exception for @once dag","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1013?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1013:
------------------------------------
    Priority: Major  (was: Blocker)

> airflow/jobs.py:manage_slas() exception for @once dag
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1013
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1013
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8, 1.8.1, 1.8.0, 1.8.2
>            Reporter: Ruslan Dautkhanov
>            Assignee: Muhammad Ahmmad
>              Labels: dagrun, once, scheduler, sla
>             Fix For: 1.9.0
>
>
> Getting following exception 
> {noformat}
> [2017-03-19 20:16:25,786] {jobs.py:354} DagFileProcessor2638 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 1175, in _process_dags
>     self.manage_slas(dag)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 595, in manage_slas
>     while dttm < datetime.now():
> TypeError: can''t compare datetime.datetime to NoneType
> {noformat}
> Exception is in airflow/jobs.py:manage_slas() :
> https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/jobs.py#L595
> {code}
>         ts = datetime.now()
>         SlaMiss = models.SlaMiss
>         for ti in max_tis:
>             task = dag.get_task(ti.task_id)
>             dttm = ti.execution_date
>             if task.sla:
>                 dttm = dag.following_schedule(dttm)
>   >>>           while dttm < datetime.now():          <<< here
>                     following_schedule = dag.following_schedule(dttm)
>                     if following_schedule + task.sla < datetime.now():
>                         session.merge(models.SlaMiss(
>                             task_id=ti.task_id,
> {code}
> It seems that dag.following_schedule() returns None for @once dag?
> Here''s how dag is defined:
> {code}
> main_dag = DAG(
>     dag_id                         = ''DISCOVER-Oracle-Load'',
>     default_args                   = default_args,           
>     user_defined_macros            = dag_macros,       
>     start_date                     = datetime.now(),         
>     catchup                        = False,                  
>     schedule_interval              = ''@once'',                
>     concurrency                    = 2,                      
>     max_active_runs                = 1,                      
>     dagrun_timeout                 = timedelta(days=4),      
> )
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26760,54,JIRA.13057410.1489976523000.47941.1508866860654@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13057410.1489976523000@Atlassian.JIRA,,,2017-10-24 10:41:00-07,"[jira] [Updated] (AIRFLOW-1013) airflow/jobs.py:manage_slas()
 exception for @once dag","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1013?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1013:
-------------------------------------
    Priority: Critical  (was: Major)

> airflow/jobs.py:manage_slas() exception for @once dag
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1013
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1013
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8, 1.8.1, 1.8.0, 1.8.2
>            Reporter: Ruslan Dautkhanov
>            Assignee: Muhammad Ahmmad
>            Priority: Critical
>              Labels: dagrun, once, scheduler, sla
>             Fix For: 1.9.0
>
>
> Getting following exception 
> {noformat}
> [2017-03-19 20:16:25,786] {jobs.py:354} DagFileProcessor2638 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 1175, in _process_dags
>     self.manage_slas(dag)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/opt/cloudera/parcels/Anaconda/lib/python2.7/site-packages/airflow/jobs.py"", line 595, in manage_slas
>     while dttm < datetime.now():
> TypeError: can''t compare datetime.datetime to NoneType
> {noformat}
> Exception is in airflow/jobs.py:manage_slas() :
> https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/jobs.py#L595
> {code}
>         ts = datetime.now()
>         SlaMiss = models.SlaMiss
>         for ti in max_tis:
>             task = dag.get_task(ti.task_id)
>             dttm = ti.execution_date
>             if task.sla:
>                 dttm = dag.following_schedule(dttm)
>   >>>           while dttm < datetime.now():          <<< here
>                     following_schedule = dag.following_schedule(dttm)
>                     if following_schedule + task.sla < datetime.now():
>                         session.merge(models.SlaMiss(
>                             task_id=ti.task_id,
> {code}
> It seems that dag.following_schedule() returns None for @once dag?
> Here''s how dag is defined:
> {code}
> main_dag = DAG(
>     dag_id                         = ''DISCOVER-Oracle-Load'',
>     default_args                   = default_args,           
>     user_defined_macros            = dag_macros,       
>     start_date                     = datetime.now(),         
>     catchup                        = False,                  
>     schedule_interval              = ''@once'',                
>     concurrency                    = 2,                      
>     max_active_runs                = 1,                      
>     dagrun_timeout                 = timedelta(days=4),      
> )
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26761,54,JIRA.13111064.1508524368000.48042.1508867521143@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13111064.1508524368000@Atlassian.JIRA,,,2017-10-24 10:52:01-07,[jira] [Updated] (AIRFLOW-1744) task.retries can be False,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1744?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1744:
-------------------------------------
    Priority: Blocker  (was: Major)

> task.retries can be False 
> --------------------------
>
>                 Key: AIRFLOW-1744
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1744
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. 
> It is proposed to set it to try_number in case try_number > 0 otherwise to 1.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26762,54,JIRA.13111064.1508524368000.48572.1508870820933@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111064.1508524368000@Atlassian.JIRA,,,2017-10-24 11:47:00-07,[jira] [Commented] (AIRFLOW-1744) task.retries can be False,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1744?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217433#comment-16217433 ] 

ASF subversion and git services commented on AIRFLOW-1744:
----------------------------------------------------------

Commit f271d437a6d168521d6d6b06e43ce9262ba3dccf in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f271d43 ]

[AIRFLOW-1744] Make sure max_tries can be set

task.retries can be False. Which is not acceptable
for
and integer field.

Closes #2713 from bolkedebruin/AIRFLOW-1744


> task.retries can be False 
> --------------------------
>
>                 Key: AIRFLOW-1744
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1744
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. 
> It is proposed to set it to try_number in case try_number > 0 otherwise to 1.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
574910,135,2115949106.1134157364363.JavaMail.jira@ajax.apache.org,22134,Craig Russell (JIRA),1561588258.1134157088920.JavaMail.jira@ajax.apache.org,22134,Craig Russell (JIRA),2005-12-09 11:42:44-08,[jira] Commented: (JDO-257) Change return type of makePersistent from void to Object,"    [ http://issues.apache.org/jira/browse/JDO-257?page=comments#action_12359931 ] 

Craig Russell commented on JDO-257:
-----------------------------------

The javadoc for these methods is weak. Someone needs to add more information regarding the return values and the fact that detached instances are not returned but values are merged from the detached instance to the persistent instances.

> Change return type of makePersistent from void to Object
> --------------------------------------------------------
>
>          Key: JDO-257
>          URL: http://issues.apache.org/jira/browse/JDO-257
>      Project: JDO
>         Type: New Feature
>   Components: api20
>     Reporter: Craig Russell
>     Assignee: Craig Russell
>      Fix For: JDO 2 beta
>  Attachments: persistencemanager.patch
>
> The signatures of makePersistent and makePersistentAll are changed to return the persistent instances. This change was adopted by the expert group 8-Dec-2005.

-- 
This message is automatically generated by JIRA.
-
If you think it was sent incorrectly contact one of the administrators:
   http://issues.apache.org/jira/secure/Administrators.jspa
-
For more information on JIRA, see:
   http://www.atlassian.com/software/jira


",f
26763,54,JIRA.13111064.1508524368000.48573.1508870820942@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111064.1508524368000@Atlassian.JIRA,,,2017-10-24 11:47:00-07,[jira] [Commented] (AIRFLOW-1744) task.retries can be False,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1744?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217434#comment-16217434 ] 

ASF subversion and git services commented on AIRFLOW-1744:
----------------------------------------------------------

Commit f271d437a6d168521d6d6b06e43ce9262ba3dccf in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f271d43 ]

[AIRFLOW-1744] Make sure max_tries can be set

task.retries can be False. Which is not acceptable
for
and integer field.

Closes #2713 from bolkedebruin/AIRFLOW-1744


> task.retries can be False 
> --------------------------
>
>                 Key: AIRFLOW-1744
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1744
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. 
> It is proposed to set it to try_number in case try_number > 0 otherwise to 1.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26764,54,JIRA.13111064.1508524368000.48581.1508870821011@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111064.1508524368000@Atlassian.JIRA,,,2017-10-24 11:47:01-07,[jira] [Commented] (AIRFLOW-1744) task.retries can be False,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1744?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217437#comment-16217437 ] 

ASF subversion and git services commented on AIRFLOW-1744:
----------------------------------------------------------

Commit 6144c6f0292600a51e87aa7970746466453c4b3f in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6144c6f ]

[AIRFLOW-1744] Make sure max_tries can be set

task.retries can be False. Which is not acceptable
for
and integer field.

Closes #2713 from bolkedebruin/AIRFLOW-1744

(cherry picked from commit f271d437a6d168521d6d6b06e43ce9262ba3dccf)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> task.retries can be False 
> --------------------------
>
>                 Key: AIRFLOW-1744
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1744
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. 
> It is proposed to set it to try_number in case try_number > 0 otherwise to 1.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26765,54,JIRA.13111064.1508524368000.48579.1508870820993@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111064.1508524368000@Atlassian.JIRA,,,2017-10-24 11:47:00-07,[jira] [Commented] (AIRFLOW-1744) task.retries can be False,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1744?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217436#comment-16217436 ] 

ASF subversion and git services commented on AIRFLOW-1744:
----------------------------------------------------------

Commit 6144c6f0292600a51e87aa7970746466453c4b3f in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6144c6f ]

[AIRFLOW-1744] Make sure max_tries can be set

task.retries can be False. Which is not acceptable
for
and integer field.

Closes #2713 from bolkedebruin/AIRFLOW-1744

(cherry picked from commit f271d437a6d168521d6d6b06e43ce9262ba3dccf)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> task.retries can be False 
> --------------------------
>
>                 Key: AIRFLOW-1744
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1744
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. 
> It is proposed to set it to try_number in case try_number > 0 otherwise to 1.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26766,54,JIRA.13111064.1508524368000.48583.1508870821027@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13111064.1508524368000@Atlassian.JIRA,,,2017-10-24 11:47:01-07,[jira] [Resolved] (AIRFLOW-1744) task.retries can be False,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1744?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1744.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2713
[https://github.com/apache/incubator-airflow/pull/2713]

> task.retries can be False 
> --------------------------
>
>                 Key: AIRFLOW-1744
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1744
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> When adding the max_tries field task.retries can be False (e.g. in case of a faulty day). At least Postgres will not accept ""False"" for an integer field. 
> It is proposed to set it to try_number in case try_number > 0 otherwise to 1.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26767,54,JIRA.13104708.1506332962000.48620.1508871060439@Atlassian.JIRA,2188,Nick Bauer (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-24 11:51:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217446#comment-16217446 ] 

Nick Bauer commented on AIRFLOW-1641:
-------------------------------------

[~criccomini] - Not sure I agree with the priority as a blocker, unless there are other definitive root causes.  If we agree that this is due to the dagbag import timeout, then the workaround is to increase your dagbag import timeout setting, OR optimize your dags so the load faster.

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
875513,186,CAHYtTq=T8a1S9gCO98G9b9bJvOBv4oTL=xH3nuX1KQOp3rpgog@mail.gmail.com,30825,Carlos Santana,37826AF1-C507-44E5-9219-763A21157EA8@adobe.com,30894,Michael Marth,2017-08-02 15:04:32-07,Re: Performance tests for OpenWhisk,"Following up here I requested a new repo incubator-openwhisk-performance
INFRA ticket here: https://issues.apache.org/jira/browse/INFRA-14778


On Wed, May 3, 2017 at 2:57 PM Michael Marth <mmarth@adobe.com> wrote:

> Markus,
>
> Quick update: sent the below to users@infra. So far no reaction. The
> archive is here [1] but Bertrand tells me only ASF member have access  -
> for whatever reason.
>
> Michael
>
> [1]
> https://lists.apache.org/thread.html/70999f9233dac9b416ef9dedc97c0ef196a9=
38c05d6a407b94ba3479@%%%%3Cusers.infra.apache.org%%%%3E
>
>
> On Fri, Apr 28, 2017 at 2:23 PM, Michael Marth <mmarth@adobe.com<mailto:
> mmarth@adobe.com>> wrote:
> Dear Infra team,
>
> I am enquiring on behalf of the OpenWhisk project (currently in Incubator=
)
> [1].
>
> We would like to periodically run performance tests on a distributed
> environment (OpenWhisk typically runs on more than 1 machine). So we are
> basically looking for an ability to spin up/tear down a number of (virtua=
l)
> machines and exclusively use them for a certain amount of time (so that t=
he
> VMs are not shared and the performance test results are comparable over
> time).
> The order of magnitude would be ~5-10 VMs for 1 hour 3 times a week.
>
> I would like to find out if there is an ASF-supported mechanism to do tha=
t.
> For example, can Infra provide such infrastructure? Or is there a cloud
> provider (like Azure) that might sponsor such efforts with VMs? Or maybe
> there is an established way for commercial companies that are interested =
in
> an ASF project to sponsor (fund) such tests?
>
> If none of the above exists, then it would also be helpful for us to get =
to
> know how other projects run such sort of tests.
>
> Thanks a lot!
> Michael
>
>
> [1]
>
> https://lists.apache.org/thread.html/b66ab5b438f2db5cdc8c5f5eabece201b4ad=
090058fa3a9a3bd09d12@%%%%3Cdev.openwhisk.apache.org%%%%3E
>
>
>
>
> From: Markus Th=C3=B6mmes <markusthoemmes@me.com<mailto:markusthoemmes@me=
.com>>
> Reply-To: ""dev@openwhisk.apache.org<mailto:dev@openwhisk.apache.org>"" <
> dev@openwhisk.apache.org<mailto:dev@openwhisk.apache.org>>
> Date: Wednesday 26 April 2017 12:59
> To: ""dev@openwhisk.apache.org<mailto:dev@openwhisk.apache.org>"" <
> dev@openwhisk.apache.org<mailto:dev@openwhisk.apache.org>>
> Subject: Re: Performance tests for OpenWhisk
>
> Hi Michael,
>
> yeah that sounds pretty much spot on. I''d like to have at least 2 VMs wit=
h
> 4+ cores and 8GB memory. One VM would host the management stack while one
> would be dedicated to an Invoker only. That way we could assert
> single-invoker performance the easiest.
>
> Thanks for helping!
>
> Cheers,
> Markus
>
> Am 26. April 2017 um 11:36 schrieb Michael Marth <mmarth@adobe.com<mailto=
:
> mmarth@adobe.com>>:
>
> Markus,
>
> Does what I describe reflect what you are looking for?
> If yes, I am happy to ask on infra.
>
> Let me know
> Michael
>
>
>
> On 26/04/17 07:52, ""Bertrand Delacretaz"" <bdelacretaz@apache.org<mailto:
> bdelacretaz@apache.org>> wrote:
>
> Hi Michael,
>
> On Tue, Apr 25, 2017 at 6:52 PM, Michael Marth <mmarth@adobe.com<mailto:
> mmarth@adobe.com>> wrote:
> ...Maybe our mentors can chime in. Has this been discussed in the ASF
> board or so?...
>
> Best would be to ask the ASF infrastructure team via
> users@infra.apache.org<mailto:users@infra.apache.org> - briefly describe
> what you need to see what''s
> possible.
>
> -Bertrand
>
",f
26768,54,JIRA.13104708.1506332962000.48666.1508871360353@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-24 11:56:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217453#comment-16217453 ] 

Chris Riccomini commented on AIRFLOW-1641:
------------------------------------------

In this case I''m using Blocker as a proxy for ""we want to include this in 1.9.0"". Agree there are work arounds.

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26769,54,JIRA.13104708.1506332962000.51243.1508885222386@Atlassian.JIRA,2188,Nick Bauer (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-24 15:47:02-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16217836#comment-16217836 ] 

Nick Bauer commented on AIRFLOW-1641:
-------------------------------------

Ok, no problem.  Is there any docs on how the various jira elements are used for this project?  My organization is interested in getting involved.

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26770,54,JIRA.13111827.1508885615000.51287.1508885640144@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13111827.1508885615000@Atlassian.JIRA,,,2017-10-24 15:54:00-07,"[jira] [Created] (AIRFLOW-1755) URL Prefix for both Flower and Web
 admin","Semet created AIRFLOW-1755:
------------------------------

             Summary: URL Prefix for both Flower and Web admin
                 Key: AIRFLOW-1755
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1755
             Project: Apache Airflow
          Issue Type: Improvement
          Components: cli, core
    Affects Versions: Airflow 2.0
            Reporter: Semet
            Assignee: Semet
            Priority: Minor


Similar to AIRFLOW-339.
Should also fix AIRFLOW-964.

I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
{code}
# Root URL to use for the web server
web_server_url_prefix: /flower
...
# The root URL for Flower
flower_url_prefix = /flower
{code}

This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26771,54,JIRA.13111829.1508886107000.51338.1508886120294@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-24 16:02:00-07,"[jira] [Created] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","Colin Son created AIRFLOW-1756:
----------------------------------

             Summary: S3 Task Handler Cannot Read Logs With New S3Hook
                 Key: AIRFLOW-1756
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.9.0
            Reporter: Colin Son


With the changes to the S3Hook, it seems like it cannot read the S3 task logs. This is because the Key object is no longer an object. 

In the `s3_read` in the S3TaskHandler.py:

{code}
s3_key = self.hook.get_key(remote_log_location)
if s3_key:
    return s3_key.get_contents_as_string().decode()
{code}

Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26772,54,JIRA.13111829.1508886107000.51339.1508886180052@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-24 16:03:00-07,"[jira] [Updated] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Colin Son updated AIRFLOW-1756:
-------------------------------
    Description: 
With the changes to the S3Hook, it seems like it cannot read the S3 task logs.

In the `s3_read` in the S3TaskHandler.py:

{code}
s3_key = self.hook.get_key(remote_log_location)
if s3_key:
    return s3_key.get_contents_as_string().decode()
{code}

Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 

  was:
With the changes to the S3Hook, it seems like it cannot read the S3 task logs. This is because the Key object is no longer an object. 

In the `s3_read` in the S3TaskHandler.py:

{code}
s3_key = self.hook.get_key(remote_log_location)
if s3_key:
    return s3_key.get_contents_as_string().decode()
{code}

Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 


> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
875514,186,CAMPvFO8_jdxNaU8j9veph02O3A0MPzoyKy0qcZmRPJfRfM7nLQ@mail.gmail.com,30911,Dascalita Dragos,CAHYtTq=T8a1S9gCO98G9b9bJvOBv4oTL=xH3nuX1KQOp3rpgog@mail.gmail.com,30825,Carlos Santana,2017-08-03 09:27:47-07,Re: Performance tests for OpenWhisk,"+1 to the idea of having 1 machine dedicated to running action containers
only.
We''ve also taken Markus'' repo a step further with a distributed testing
tool named Locust.io [1] . The tests are at [2].
I''ve also used Tsung [3] on other projects (it''s based on Erlang). Tsung
provides the best performance I''ve seen out there. It scales to millions of
concurrent users with only a few machines. But b/c of Erlang, I''ve been
having issues running a cluster in docker containers. Locust.io is much
easier to setup and unlike Tsung, which provides an XML to describe the
tests, Locust.io supports Python and Go scripts.

For running performance tests in a single machine we don''t need a
distributed testing tool, but I would still build on such a tool b/c I find
it valuable to reuse the same scripts to generate more load in bigger
OpenWhisk clusters.

[1] - http://locust.io/
[2] - https://github.com/adobe-apiplatform/openwhisk-performance-tests
[3] - http://tsung.erlang-projects.org/



On Thu, Aug 3, 2017 at 1:04 AM Carlos Santana <csantana23@gmail.com> wrote:

> Following up here I requested a new repo incubator-openwhisk-performance
> INFRA ticket here: https://issues.apache.org/jira/browse/INFRA-14778
>
>
> On Wed, May 3, 2017 at 2:57 PM Michael Marth <mmarth@adobe.com> wrote:
>
> > Markus,
> >
> > Quick update: sent the below to users@infra. So far no reaction. The
> > archive is here [1] but Bertrand tells me only ASF member have access  =
-
> > for whatever reason.
> >
> > Michael
> >
> > [1]
> >
> https://lists.apache.org/thread.html/70999f9233dac9b416ef9dedc97c0ef196a9=
38c05d6a407b94ba3479@%%%%3Cusers.infra.apache.org%%%%3E
> >
> >
> > On Fri, Apr 28, 2017 at 2:23 PM, Michael Marth <mmarth@adobe.com<mailto=
:
> > mmarth@adobe.com>> wrote:
> > Dear Infra team,
> >
> > I am enquiring on behalf of the OpenWhisk project (currently in
> Incubator)
> > [1].
> >
> > We would like to periodically run performance tests on a distributed
> > environment (OpenWhisk typically runs on more than 1 machine). So we ar=
e
> > basically looking for an ability to spin up/tear down a number of
> (virtual)
> > machines and exclusively use them for a certain amount of time (so that
> the
> > VMs are not shared and the performance test results are comparable over
> > time).
> > The order of magnitude would be ~5-10 VMs for 1 hour 3 times a week.
> >
> > I would like to find out if there is an ASF-supported mechanism to do
> that.
> > For example, can Infra provide such infrastructure? Or is there a cloud
> > provider (like Azure) that might sponsor such efforts with VMs? Or mayb=
e
> > there is an established way for commercial companies that are intereste=
d
> in
> > an ASF project to sponsor (fund) such tests?
> >
> > If none of the above exists, then it would also be helpful for us to ge=
t
> to
> > know how other projects run such sort of tests.
> >
> > Thanks a lot!
> > Michael
> >
> >
> > [1]
> >
> >
> https://lists.apache.org/thread.html/b66ab5b438f2db5cdc8c5f5eabece201b4ad=
090058fa3a9a3bd09d12@%%%%3Cdev.openwhisk.apache.org%%%%3E
> >
> >
> >
> >
> > From: Markus Th=C3=B6mmes <markusthoemmes@me.com<mailto:markusthoemmes@=
me.com
> >>
> > Reply-To: ""dev@openwhisk.apache.org<mailto:dev@openwhisk.apache.org>"" <
> > dev@openwhisk.apache.org<mailto:dev@openwhisk.apache.org>>
> > Date: Wednesday 26 April 2017 12:59
> > To: ""dev@openwhisk.apache.org<mailto:dev@openwhisk.apache.org>"" <
> > dev@openwhisk.apache.org<mailto:dev@openwhisk.apache.org>>
> > Subject: Re: Performance tests for OpenWhisk
> >
> > Hi Michael,
> >
> > yeah that sounds pretty much spot on. I''d like to have at least 2 VMs
> with
> > 4+ cores and 8GB memory. One VM would host the management stack while o=
ne
> > would be dedicated to an Invoker only. That way we could assert
> > single-invoker performance the easiest.
> >
> > Thanks for helping!
> >
> > Cheers,
> > Markus
> >
> > Am 26. April 2017 um 11:36 schrieb Michael Marth <mmarth@adobe.com
> <mailto:
> > mmarth@adobe.com>>:
> >
> > Markus,
> >
> > Does what I describe reflect what you are looking for?
> > If yes, I am happy to ask on infra.
> >
> > Let me know
> > Michael
> >
> >
> >
> > On 26/04/17 07:52, ""Bertrand Delacretaz"" <bdelacretaz@apache.org<mailto=
:
> > bdelacretaz@apache.org>> wrote:
> >
> > Hi Michael,
> >
> > On Tue, Apr 25, 2017 at 6:52 PM, Michael Marth <mmarth@adobe.com<mailto=
:
> > mmarth@adobe.com>> wrote:
> > ...Maybe our mentors can chime in. Has this been discussed in the ASF
> > board or so?...
> >
> > Best would be to ask the ASF infrastructure team via
> > users@infra.apache.org<mailto:users@infra.apache.org> - briefly describ=
e
> > what you need to see what''s
> > possible.
> >
> > -Bertrand
> >
>
",f
26773,54,JIRA.13111827.1508885615000.53893.1508919300141@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13111827.1508885615000@Atlassian.JIRA,,,2017-10-25 01:15:00-07,"[jira] [Commented] (AIRFLOW-1755) URL Prefix for both Flower and
 Web admin","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1755?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16218239#comment-16218239 ] 

Semet commented on AIRFLOW-1755:
--------------------------------

Pull Request: https://github.com/apache/incubator-airflow/pull/2723

> URL Prefix for both Flower and Web admin
> ----------------------------------------
>
>                 Key: AIRFLOW-1755
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1755
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: cli, core
>    Affects Versions: Airflow 2.0
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>
> Similar to AIRFLOW-339.
> Should also fix AIRFLOW-964.
> I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
> {code}
> # Root URL to use for the web server
> web_server_url_prefix: /flower
> ...
> # The root URL for Flower
> flower_url_prefix = /flower
> {code}
> This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26774,54,JIRA.13111829.1508886107000.54133.1508923025299@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 02:17:05-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16218280#comment-16218280 ] 

Ash Berlin-Taylor commented on AIRFLOW-1756:
--------------------------------------------

I''m going to test this later today, but I can''t see where get_key is being returned as a dict. In both master and v1-9-test the s3hook.get_key function looks like this:

{code}
    def get_key(self, key, bucket_name=None):
        """"""
        Returns a boto3.S3.Key object

        :param key: the path to the key
        :type key: str
        :param bucket_name: the name of the bucket
        :type bucket_name: str
        """"""
        if not bucket_name:
            (bucket_name, key) = self.parse_s3_url(key)
            
        return self.get_conn().get_object(Bucket=bucket_name, Key=key)
{code}

Do you have a full stack trace of the error please?

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26775,54,JIRA.12991339.1469074175000.54678.1508929142874@Atlassian.JIRA,2165,Steen Manniche (JIRA),JIRA.12991339.1469074175000@Atlassian.JIRA,,,2017-10-25 03:59:02-07,[jira] [Commented] (AIRFLOW-351) Failed to clear downstream tasks,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-351?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16218399#comment-16218399 ] 

Steen Manniche commented on AIRFLOW-351:
----------------------------------------

The question has also popped up on [stackoverflow|https://stackoverflow.com/questions/29267261/object-new-thread-lock-is-not-safe-use-thread-lock-new] in relation to the use of {{deep_copy}}. In general, if a user has any import that actively uses threads, she will see this error message which, as demonstrated in the present issue, takes some time and effort to track down. A simple {{try...except}} wrapping of [this line|https://github.com/apache/incubator-airflow/blob/cfc2f73c445074e1e09d6ef6a056cd2b33a945da/airflow/models.py#L3477] could help debugging immensely:

{{{
diff --git a/airflow/models.py b/airflow/models.py
index e3c52b5..53dfbb2 100755
--- a/airflow/models.py
+++ b/airflow/models.py
@@ -3474,7 +3474,10 @@ class DAG(BaseDag, LoggingMixin):
         memo[id(self)] = result
         for k, v in list(self.__dict__.items()):
             if k not in (''user_defined_macros'', ''user_defined_filters'', ''params''):
-                setattr(result, k, copy.deepcopy(v, memo))
+                try:
+                    setattr(result, k, copy.deepcopy(v, memo))
+                except TypeError as te:
+                    raise AirflowException(''Failed to deep_copy the value %%%%s: %%%%s''%%%%(v, te))
 
         result.user_defined_macros = self.user_defined_macros
         result.user_defined_filters = self.user_defined_filters
}}}

> Failed to clear downstream tasks
> --------------------------------
>
>                 Key: AIRFLOW-351
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-351
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: models, subdag, webserver
>    Affects Versions: Airflow 1.7.1.3
>            Reporter: Adinata
>         Attachments: dag_error.py, error_on_clear_dag.txt, ubuntu-14-packages.log, ubuntu-16-oops.log, ubuntu-16-packages.log
>
>
> {code}
>                           ____/ (  (    )   )  \___
>                          /( (  (  )   _    ))  )   )\
>                        ((     (   )(    )  )   (   )  )
>                      ((/  ( _(   )   (   _) ) (  () )  )
>                     ( (  ( (_)   ((    (   )  .((_ ) .  )_
>                    ( (  )    (      (  )    )   ) . ) (   )
>                   (  (   (  (   ) (  _  ( _) ).  ) . ) ) ( )
>                   ( (  (   ) (  )   (  ))     ) _)(   )  )  )
>                  ( (  ( \ ) (    (_  ( ) ( )  )   ) )  )) ( )
>                   (  (   (  (   (_ ( ) ( _    )  ) (  )  )   )
>                  ( (  ( (  (  )     (_  )  ) )  _)   ) _( ( )
>                   ((  (   )(    (     _    )   _) _(_ (  (_ )
>                    (_((__(_(__(( ( ( |  ) ) ) )_))__))_)___)
>                    ((__)        \\||lll|l||///          \_))
>                             (   /(/ (  )  ) )\   )
>                           (    ( ( ( | | ) ) )\   )
>                            (   /(| / ( )) ) ) )) )
>                          (     ( ((((_(|)_)))))     )
>                           (      ||\(|(|)|/||     )
>                         (        |(||(||)||||        )
>                           (     //|/l|||)|\\ \     )
>                         (/ / //  /|//||||\\  \ \  \ _)
> -------------------------------------------------------------------------------
> Node: 9889a7c79e9b
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1817, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1477, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1381, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1475, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1461, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 68, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 367, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 118, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 167, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1017, in clear
>     include_upstream=upstream)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 2870, in sub_dag
>     dag = copy.deepcopy(self)
>   File ""/usr/lib/python2.7/copy.py"", line 174, in deepcopy
>     y = copier(memo)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 2856, in __deepcopy__
>     setattr(result, k, copy.deepcopy(v, memo))
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 174, in deepcopy
>     y = copier(memo)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1974, in __deepcopy__
>     setattr(result, k, copy.deepcopy(v, memo))
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 230, in _deepcopy_list
>     y.append(deepcopy(a, memo))
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 329, in _reconstruct
>     y = callable(*args)
>   File ""/usr/lib/python2.7/copy_reg.py"", line 93, in __newobj__
>     return cls.__new__(cls, *args)
> TypeError: object.__new__(thread.lock) is not safe, use thread.lock.__new__()
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26776,54,JIRA.12991339.1469074175000.54681.1508929142903@Atlassian.JIRA,2165,Steen Manniche (JIRA),JIRA.12991339.1469074175000@Atlassian.JIRA,,,2017-10-25 03:59:02-07,"[jira] [Comment Edited] (AIRFLOW-351) Failed to clear downstream
 tasks","
    [ https://issues.apache.org/jira/browse/AIRFLOW-351?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16218399#comment-16218399 ] 

Steen Manniche edited comment on AIRFLOW-351 at 10/25/17 10:58 AM:
-------------------------------------------------------------------

The question has also popped up on [stackoverflow|https://stackoverflow.com/questions/29267261/object-new-thread-lock-is-not-safe-use-thread-lock-new] in relation to the use of {{deep_copy}}. In general, if a user has any import that actively uses threads, she will see this error message which, as demonstrated in the present issue, takes some time and effort to track down. A simple {{try...except}} wrapping of [this line|https://github.com/apache/incubator-airflow/blob/cfc2f73c445074e1e09d6ef6a056cd2b33a945da/airflow/models.py#L3477] could help debugging immensely:

{code}
diff --git a/airflow/models.py b/airflow/models.py
index e3c52b5..53dfbb2 100755
--- a/airflow/models.py
+++ b/airflow/models.py
@@ -3474,7 +3474,10 @@ class DAG(BaseDag, LoggingMixin):
         memo[id(self)] = result
         for k, v in list(self.__dict__.items()):
             if k not in (''user_defined_macros'', ''user_defined_filters'', ''params''):
-                setattr(result, k, copy.deepcopy(v, memo))
+                try:
+                    setattr(result, k, copy.deepcopy(v, memo))
+                except TypeError as te:
+                    raise AirflowException(''Failed to deep_copy the value %%%%s: %%%%s''%%%%(v, te))
 
         result.user_defined_macros = self.user_defined_macros
         result.user_defined_filters = self.user_defined_filters
{code}


was (Author: steen.manniche):
The question has also popped up on [stackoverflow|https://stackoverflow.com/questions/29267261/object-new-thread-lock-is-not-safe-use-thread-lock-new] in relation to the use of {{deep_copy}}. In general, if a user has any import that actively uses threads, she will see this error message which, as demonstrated in the present issue, takes some time and effort to track down. A simple {{try...except}} wrapping of [this line|https://github.com/apache/incubator-airflow/blob/cfc2f73c445074e1e09d6ef6a056cd2b33a945da/airflow/models.py#L3477] could help debugging immensely:

{{{
diff --git a/airflow/models.py b/airflow/models.py
index e3c52b5..53dfbb2 100755
--- a/airflow/models.py
+++ b/airflow/models.py
@@ -3474,7 +3474,10 @@ class DAG(BaseDag, LoggingMixin):
         memo[id(self)] = result
         for k, v in list(self.__dict__.items()):
             if k not in (''user_defined_macros'', ''user_defined_filters'', ''params''):
-                setattr(result, k, copy.deepcopy(v, memo))
+                try:
+                    setattr(result, k, copy.deepcopy(v, memo))
+                except TypeError as te:
+                    raise AirflowException(''Failed to deep_copy the value %%%%s: %%%%s''%%%%(v, te))
 
         result.user_defined_macros = self.user_defined_macros
         result.user_defined_filters = self.user_defined_filters
}}}

> Failed to clear downstream tasks
> --------------------------------
>
>                 Key: AIRFLOW-351
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-351
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: models, subdag, webserver
>    Affects Versions: Airflow 1.7.1.3
>            Reporter: Adinata
>         Attachments: dag_error.py, error_on_clear_dag.txt, ubuntu-14-packages.log, ubuntu-16-oops.log, ubuntu-16-packages.log
>
>
> {code}
>                           ____/ (  (    )   )  \___
>                          /( (  (  )   _    ))  )   )\
>                        ((     (   )(    )  )   (   )  )
>                      ((/  ( _(   )   (   _) ) (  () )  )
>                     ( (  ( (_)   ((    (   )  .((_ ) .  )_
>                    ( (  )    (      (  )    )   ) . ) (   )
>                   (  (   (  (   ) (  _  ( _) ).  ) . ) ) ( )
>                   ( (  (   ) (  )   (  ))     ) _)(   )  )  )
>                  ( (  ( \ ) (    (_  ( ) ( )  )   ) )  )) ( )
>                   (  (   (  (   (_ ( ) ( _    )  ) (  )  )   )
>                  ( (  ( (  (  )     (_  )  ) )  _)   ) _( ( )
>                   ((  (   )(    (     _    )   _) _(_ (  (_ )
>                    (_((__(_(__(( ( ( |  ) ) ) )_))__))_)___)
>                    ((__)        \\||lll|l||///          \_))
>                             (   /(/ (  )  ) )\   )
>                           (    ( ( ( | | ) ) )\   )
>                            (   /(| / ( )) ) ) )) )
>                          (     ( ((((_(|)_)))))     )
>                           (      ||\(|(|)|/||     )
>                         (        |(||(||)||||        )
>                           (     //|/l|||)|\\ \     )
>                         (/ / //  /|//||||\\  \ \  \ _)
> -------------------------------------------------------------------------------
> Node: 9889a7c79e9b
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1817, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1477, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1381, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1475, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1461, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 68, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 367, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 118, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 167, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1017, in clear
>     include_upstream=upstream)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 2870, in sub_dag
>     dag = copy.deepcopy(self)
>   File ""/usr/lib/python2.7/copy.py"", line 174, in deepcopy
>     y = copier(memo)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 2856, in __deepcopy__
>     setattr(result, k, copy.deepcopy(v, memo))
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 174, in deepcopy
>     y = copier(memo)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1974, in __deepcopy__
>     setattr(result, k, copy.deepcopy(v, memo))
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 230, in _deepcopy_list
>     y.append(deepcopy(a, memo))
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
>     state = deepcopy(state, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
>     y = copier(x, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
>     y[deepcopy(key, memo)] = deepcopy(value, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
>     y = _reconstruct(x, rv, 1, memo)
>   File ""/usr/lib/python2.7/copy.py"", line 329, in _reconstruct
>     y = callable(*args)
>   File ""/usr/lib/python2.7/copy_reg.py"", line 93, in __newobj__
>     return cls.__new__(cls, *args)
> TypeError: object.__new__(thread.lock) is not safe, use thread.lock.__new__()
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26777,54,JIRA.13037521.1485304666000.55308.1508934780875@Atlassian.JIRA,2355,Sanjay Kumar (JIRA),JIRA.13037521.1485304666000@Atlassian.JIRA,,,2017-10-25 05:33:00-07,"[jira] [Commented] (AIRFLOW-800) Initialize default Google BigQuery
 Connection with valid conn_type","
    [ https://issues.apache.org/jira/browse/AIRFLOW-800?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16218508#comment-16218508 ] 

Sanjay Kumar commented on AIRFLOW-800:
--------------------------------------

Will work on this JIRA. Have submitted a pull request for the same

> Initialize default Google BigQuery Connection with valid conn_type
> ------------------------------------------------------------------
>
>                 Key: AIRFLOW-800
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-800
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: utils
>            Reporter: Wilson Lian
>            Assignee: Wilson Lian
>            Priority: Minor
>
> {{airflow initdb}} creates a connection with conn_id=''bigquery_default'' and conn_type=''bigquery''. However, bigquery is not a valid conn_type, according to models.Connection._types, and BigQuery connections should use the google_cloud_platform conn_type.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26778,54,JIRA.13037521.1485304666000.55311.1508934780894@Atlassian.JIRA,2355,Sanjay Kumar (JIRA),JIRA.13037521.1485304666000@Atlassian.JIRA,,,2017-10-25 05:33:00-07,"[jira] [Assigned] (AIRFLOW-800) Initialize default Google BigQuery
 Connection with valid conn_type","
     [ https://issues.apache.org/jira/browse/AIRFLOW-800?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sanjay Kumar reassigned AIRFLOW-800:
------------------------------------

    Assignee: Sanjay Kumar  (was: Wilson Lian)

> Initialize default Google BigQuery Connection with valid conn_type
> ------------------------------------------------------------------
>
>                 Key: AIRFLOW-800
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-800
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: utils
>            Reporter: Wilson Lian
>            Assignee: Sanjay Kumar
>            Priority: Minor
>
> {{airflow initdb}} creates a connection with conn_id=''bigquery_default'' and conn_type=''bigquery''. However, bigquery is not a valid conn_type, according to models.Connection._types, and BigQuery connections should use the google_cloud_platform conn_type.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26779,54,JIRA.13037521.1485304666000.55399.1508935140478@Atlassian.JIRA,2355,Sanjay Kumar (JIRA),JIRA.13037521.1485304666000@Atlassian.JIRA,,,2017-10-25 05:39:00-07,"[jira] [Comment Edited] (AIRFLOW-800) Initialize default Google
 BigQuery Connection with valid conn_type","
    [ https://issues.apache.org/jira/browse/AIRFLOW-800?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16218508#comment-16218508 ] 

Sanjay Kumar edited comment on AIRFLOW-800 at 10/25/17 12:38 PM:
-----------------------------------------------------------------

Will work on this JIRA if there are no issues


was (Author: 88.sanjay@gmail.com):
Will work on this JIRA. Have submitted a pull request for the same

> Initialize default Google BigQuery Connection with valid conn_type
> ------------------------------------------------------------------
>
>                 Key: AIRFLOW-800
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-800
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: utils
>            Reporter: Wilson Lian
>            Assignee: Sanjay Kumar
>            Priority: Minor
>
> {{airflow initdb}} creates a connection with conn_id=''bigquery_default'' and conn_type=''bigquery''. However, bigquery is not a valid conn_type, according to models.Connection._types, and BigQuery connections should use the google_cloud_platform conn_type.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26780,54,JIRA.13112002.1508946012000.57466.1508946060361@Atlassian.JIRA,1356,Tomasz Bartczak (JIRA),JIRA.13112002.1508946012000@Atlassian.JIRA,,,2017-10-25 08:41:00-07,"[jira] [Created] (AIRFLOW-1757) Contrib.SparkSubmitOperator should
 allow --packages parameter","Tomasz Bartczak created AIRFLOW-1757:
----------------------------------------

             Summary: Contrib.SparkSubmitOperator should allow --packages parameter
                 Key: AIRFLOW-1757
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1757
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Tomasz Bartczak
            Priority: Trivial


We miss two options from spark-submit - to pass packages to exclude and to pass custom repositories.

I have a PR ready for that.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26781,54,JIRA.12990672.1468912243000.58159.1508949420500@Atlassian.JIRA,2356,Kevin Lam (JIRA),JIRA.12990672.1468912243000@Atlassian.JIRA,,,2017-10-25 09:37:00-07,"[jira] [Commented] (AIRFLOW-342)  exception in ''airflow scheduler''
 : Connection reset by peer","
    [ https://issues.apache.org/jira/browse/AIRFLOW-342?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219016#comment-16219016 ] 

Kevin Lam commented on AIRFLOW-342:
-----------------------------------

Same issue with airflow 1.8.2rc2, celery 3.1.23, using kube-airflow: https://github.com/mumoshu/kube-airflow

>  exception in ''airflow scheduler'' : Connection reset by peer
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-342
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-342
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery, scheduler
>    Affects Versions: Airflow 1.7.1.3
>         Environment: OS: Red Hat Enterprise Linux Server 7.2 (Maipo)
> Python: 2.7.5
> Airflow: 1.7.1.3
>            Reporter: Hila Visan
>            Assignee: Hila Visan
>
> ''airflow scheduler'' command throws an exception when running it. 
> Despite the exception, the workers run the tasks from the queues as expected.
> Error details:
>  
> [2016-06-30 19:00:10,130] {jobs.py:758} ERROR - [Errno 104] Connection reset by peer
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 755, in _execute
>     executor.heartbeat()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/base_executor.py"", line 107, in heartbeat
>     self.sync()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/celery_executor.py"", line 74, in sync
>     state = async.state
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 394, in state
>     return self._get_task_meta()[''status'']
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 339, in _get_task_meta
>     return self._maybe_set_cache(self.backend.get_task_meta(self.id))
>   File ""/usr/lib/python2.7/site-packages/celery/backends/amqp.py"", line 163, in get_task_meta
>     binding.declare()
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 521, in declare
>    self.exchange.declare(nowait)
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 174, in declare
>     nowait=nowait, passive=passive,
>   File ""/usr/lib/python2.7/site-packages/amqp/channel.py"", line 615, in exchange_declare
>     self._send_method((40, 10), args)
>   File ""/usr/lib/python2.7/site-packages/amqp/abstract_channel.py"", line 56, in _send_method
>     self.channel_id, method_sig, args, content,
>   File ""/usr/lib/python2.7/site-packages/amqp/method_framing.py"", line 221, in write_method
>     write_frame(1, channel, payload)
>   File ""/usr/lib/python2.7/site-packages/amqp/transport.py"", line 182, in write_frame
>     frame_type, channel, size, payload, 0xce,
>   File ""/usr/lib64/python2.7/socket.py"", line 224, in meth
>     return getattr(self._sock,name)(*args)
> error: [Errno 104] Connection reset by peer
> [2016-06-30 19:00:10,131] {jobs.py:759} ERROR - Tachycardia!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26782,54,JIRA.13111829.1508886107000.59032.1508952840566@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 10:34:00-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219148#comment-16219148 ] 

Colin Son commented on AIRFLOW-1756:
------------------------------------

It''s in the Boto3 Documentation: http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.get_object

Here is the Boto S3 Documentation: http://boto.cloudhackers.com/en/latest/ref/s3.html#boto.s3.bucket.Bucket.get_key

In the [AIRFLOW-1520], the S3Hook was changed to use the AWSHook, which uses boto3. 

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26783,54,JIRA.13111829.1508886107000.59052.1508952900605@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 10:35:00-07,"[jira] [Comment Edited] (AIRFLOW-1756) S3 Task Handler Cannot Read
 Logs With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219148#comment-16219148 ] 

Colin Son edited comment on AIRFLOW-1756 at 10/25/17 5:34 PM:
--------------------------------------------------------------

[~ashb]

It''s in the Boto3 Documentation: http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.get_object

Here is the Boto S3 Documentation: http://boto.cloudhackers.com/en/latest/ref/s3.html#boto.s3.bucket.Bucket.get_key

In the [AIRFLOW-1520], the S3Hook was changed to use the AWSHook, which uses boto3. 


was (Author: ccsn1234):
It''s in the Boto3 Documentation: http://boto3.readthedocs.io/en/latest/reference/services/s3.html#S3.Client.get_object

Here is the Boto S3 Documentation: http://boto.cloudhackers.com/en/latest/ref/s3.html#boto.s3.bucket.Bucket.get_key

In the [AIRFLOW-1520], the S3Hook was changed to use the AWSHook, which uses boto3. 

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26784,54,JIRA.13111829.1508886107000.59122.1508953380044@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 10:43:00-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219166#comment-16219166 ] 

Colin Son commented on AIRFLOW-1756:
------------------------------------

[~ashb]

The AWS Hook will create a boto3 client whenever you call `self.get_conn()`: 

{code}
    def get_client_type(self, client_type, region_name=None):
        aws_access_key_id, aws_secret_access_key, region_name, endpoint_url = \
            self._get_credentials(region_name)

        return boto3.client(
            client_type,
            region_name=region_name,
            aws_access_key_id=aws_access_key_id,
            aws_secret_access_key=aws_secret_access_key,
            endpoint_url=endpoint_url
        )
{code}

And using `get_object` using the Boto3 Client will return a dict, and not a boto3.S3.Key object. 

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26785,54,JIRA.13111532.1508792427000.60632.1508960880379@Atlassian.JIRA,2351,Mark Secada (JIRA),JIRA.13111532.1508792427000@Atlassian.JIRA,,,2017-10-25 12:48:00-07,"[jira] [Closed] (AIRFLOW-1750) GoogleCloudStorageToBigQueryOperator
 404 HttpError","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1750?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark Secada closed AIRFLOW-1750.
--------------------------------
    Resolution: Fixed

> GoogleCloudStorageToBigQueryOperator 404 HttpError
> --------------------------------------------------
>
>                 Key: AIRFLOW-1750
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1750
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>    Affects Versions: Airflow 1.8
>         Environment: Python 2.7.13
>            Reporter: Mark Secada
>             Fix For: Airflow 1.8
>
>
> I''m trying to write a DAG which uploads JSON files to GoogleCloudStorage and then moves them to BigQuery. I was able to upload these files to GoogleCloudStorage, but when I run this second task, I get a 404 HttpError. The error looks like this:
> {code:bash}
> ERROR - <HttpError 404 when requesting https://www.googleapis.com/bigquery/v2/projects//jobs?alt=json returned ""Not Found"">
> Traceback (most recent call last):
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/models.py"", line 1374, in run
>     result = task_copy.execute(context=context)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/operators/gcs_to_bq.py"", line 153, in execute
>     schema_update_options=self.schema_update_options)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 476, in run_load
>     return self.run_with_configuration(configuration)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/airflow/contrib/hooks/bigquery_hook.py"", line 498, in run_with_configuration
>     .insert(projectId=self.project_id, body=job_data) \
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/oauth2client/util.py"", line 135, in positional_wrapper
>     return wrapped(*args, **kwargs)
>   File ""/Users/myname/anaconda/lib/python2.7/site-packages/googleapiclient/http.py"", line 838, in execute
>     raise HttpError(resp, content, uri=self.uri)
> {code}
> My code for the task is here:
> {code:python}
> // Some comments here
> t3 = GoogleCloudStorageToBigQueryOperator(
>         task_id=''move_''+source+''_from_gcs_to_bq'',
>         bucket=''mybucket'',
>         source_objects=[''news/latest_headline_''+source+''.json''],
>         destination_project_dataset_table=''mydataset.latest_news_headlines'',
>         schema_object=''news/latest_headline_''+source+''.json'',
>         source_format=''NEWLINE_DELIMITED_JSON'',
>         write_disposition=''WRITE_APPEND''
>         dag=dag)
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26786,54,JIRA.13111829.1508886107000.60770.1508961720245@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 13:02:00-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219423#comment-16219423 ] 

Ash Berlin-Taylor commented on AIRFLOW-1756:
--------------------------------------------

Ah right, yes boto3 is lower level than boto2. (We didn''t get as far as testing this part today)

I wonder if get_key should return a s3.Object instead http://boto3.readthedocs.io/en/latest/reference/services/s3.html#object -- I feel that is closest to what the old API did.

This will need fixing before 1.9.0 goes out. Can you update the fix version on this ticket so we don''t forget, please?

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26787,54,JIRA.13111829.1508886107000.60816.1508962020165@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 13:07:00-07,"[jira] [Updated] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Colin Son updated AIRFLOW-1756:
-------------------------------
    Fix Version/s: 1.9.0

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26788,54,JIRA.13111829.1508886107000.60915.1508962381265@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 13:13:01-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219446#comment-16219446 ] 

Colin Son commented on AIRFLOW-1756:
------------------------------------

[~ashb]

Sounds good. Can I increase the priority of this ticket, since reading task logs (that are rotated to S3) is very critical when detecting errors, debugging, etc?

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549387,277,JIRA.12986655.1467653918000.29129.1467746831011@Atlassian.JIRA,3026,ASF GitHub Bot (JIRA),JIRA.12986655.1467653918000@Atlassian.JIRA,,,2016-07-05 12:27:11-07,"[jira] [Commented] (IOTA-21) Refactor Built of performer to manage
 multi module dependency more efficient and scala style plugin for code
 quality","
    [ https://issues.apache.org/jira/browse/IOTA-21?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15363075#comment-15363075 ] 

ASF GitHub Bot commented on IOTA-21:
------------------------------------

Github user phalodi commented on the issue:

    https://github.com/apache/incubator-iota/pull/5
  
    @barbaragomes I remove fey setting from build settings and also add organization name to basic setting so every project have it.


> Refactor Built of performer to manage multi module dependency more efficient and scala style plugin for code quality 
> ---------------------------------------------------------------------------------------------------------------------
>
>                 Key: IOTA-21
>                 URL: https://issues.apache.org/jira/browse/IOTA-21
>             Project: Iota
>          Issue Type: Improvement
>            Reporter: sandeep purohit
>            Assignee: sandeep purohit
>   Original Estimate: 3h
>  Remaining Estimate: 3h
>
> In this improvement refactoring the code to manage multi module dependencies more efficient way and add scala style plugin for code quality .



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26789,54,JIRA.13111829.1508886107000.61050.1508962981705@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 13:23:01-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219459#comment-16219459 ] 

Colin Son commented on AIRFLOW-1756:
------------------------------------

[~ashb]

And another issue that I wanted to point out is that the unit tests for the S3TaskHandler are broken. You can find them in here: incubator-airflow/tests/utils/log/test_logging.py

This particular test is not part of a python package, so the tests don''t run. And the tests fail because S3TaskHandler object is not initialized correctly (since it needs the correct args for the __init__). And the method names that it is trying to test are all spelled incorrectly. So the unit tests should be revisited as well. 

Thanks!

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26790,54,JIRA.13111829.1508886107000.61058.1508963160957@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 13:26:00-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219461#comment-16219461 ] 

Ash Berlin-Taylor commented on AIRFLOW-1756:
--------------------------------------------

Ohhh that explains how this got broken.

Priority can be increased, but it doesn''t have much bearing on the process :)

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26791,54,JIRA.13111829.1508886107000.61059.1508963160979@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 13:26:00-07,"[jira] [Comment Edited] (AIRFLOW-1756) S3 Task Handler Cannot Read
 Logs With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219446#comment-16219446 ] 

Colin Son edited comment on AIRFLOW-1756 at 10/25/17 8:25 PM:
--------------------------------------------------------------

[~ashb]

Sounds good. Can I increase the priority of this ticket, since reading task logs (that are rotated to S3) is very critical when detecting errors, debugging, etc?

We look forward to seeing this fix in 1.9.0.


was (Author: ccsn1234):
[~ashb]

Sounds good. Can I increase the priority of this ticket, since reading task logs (that are rotated to S3) is very critical when detecting errors, debugging, etc?

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26792,54,JIRA.13111829.1508886107000.61098.1508963220605@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 13:27:00-07,"[jira] [Updated] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Colin Son updated AIRFLOW-1756:
-------------------------------
    Priority: Critical  (was: Major)

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
574911,135,232609089.1134176229628.JavaMail.jira@ajax.apache.org,22135,Michelle Caisse (JIRA),NULL,,,2005-12-09 16:57:09-08,[jira] Created: (JDO-258) Write tests for attach/detach lifecycle assertions,"Write tests for attach/detach lifecycle assertions
--------------------------------------------------

         Key: JDO-258
         URL: http://issues.apache.org/jira/browse/JDO-258
     Project: JDO
        Type: New Feature
  Components: tck20  
    Reporter: Michelle Caisse


A5.6.2-3 [A persistent-nontransactional-dirty instance transitions to hollow if it is the parameter of evict or evictAll. This allows the application to remove instances from the set of instances whose state is to be committed to the datastore.]

A5.6.2-5 [The persistent-nontransactional-dirty instances will transition according to the RetainValues flag. With the RetainValues flag set to true, persistent-nontransactional-dirty instances will transition to persistent-nontransactional. With the RetainValues flag set to false, persistent-nontransactional-dirty instances will transition to hollow. ]

A5.6.2-7 [The persistent-nontransactional-dirty instances will transition according to the RestoreValues flag. With the RestoreValues flag set to true, persistent-nontransactional-dirty instances will make no state transition, but the fields will be restored to their values as of the beginning of the transaction, and any changes made within the transaction will be discarded. With the RestoreValues flag set to false, persistent-nontransactional-dirty instances will transition to hollow.]

A5.6.2-9 [The persistent-nontransactional-dirty instances will transition according to the RetainValues flag. With the RetainValues flag set to true, persistent-nontransactional-dirty instances will transition to persistent-nontransactional. With the RetainValues flag set to false, persistent-nontransactional-dirty instances will transition to hollow.]

A5.6.2-11 [With the RestoreValues flag set to true, persistent-nontransactional-dirty instances will make no state transition, but the fields will be restored to their values as of the beginning of the transaction, and any changes made within the transaction will be discarded. With the RestoreValues flag set to false, persistent-nontransactional-dirty instances will transition to hollow.] 

-- 
This message is automatically generated by JIRA.
-
If you think it was sent incorrectly contact one of the administrators:
   http://issues.apache.org/jira/secure/Administrators.jspa
-
For more information on JIRA, see:
   http://www.atlassian.com/software/jira


",f
26793,54,JIRA.13111829.1508886107000.61097.1508963220597@Atlassian.JIRA,2354,Colin Son (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-25 13:27:00-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16219463#comment-16219463 ] 

Colin Son commented on AIRFLOW-1756:
------------------------------------

[~ashb]

Thanks so much for your help. I look forward to seeing this fixed in the 1.9.0 release. 

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26794,54,JIRA.13111595.1508812757000.63847.1508981520122@Atlassian.JIRA,2217,Yifei Hong (JIRA),JIRA.13111595.1508812757000@Atlassian.JIRA,,,2017-10-25 18:32:00-07,"[jira] [Closed] (AIRFLOW-1751) SqlAlchemy Error when backfilling
 dag and connecting to SQL Server","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1751?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Yifei Hong closed AIRFLOW-1751.
-------------------------------
    Resolution: Fixed

> SqlAlchemy Error when backfilling dag and connecting to SQL Server
> ------------------------------------------------------------------
>
>                 Key: AIRFLOW-1751
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1751
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: backfill, db
>    Affects Versions: Airflow 1.8
>         Environment: Linux
>            Reporter: Yifei Hong
>         Attachments: db.png, error.log, web.png
>
>
> Airflow works well for me when using sqlite as the backend database.
> In order to use LocalExecutor to test on parallel runs (pseudo), I set up the db connection to MS SQL Server with mssql+pyodbc. The connection is good and all metadata tables are created. However, when trying to backfill the dag run, I always encounter the same sqlalchemy.exc.StatementError for any example dag. Error log is attached with the full stack trace.
> airflow backfill example_bash_operator -s 2017-10-18
> sqlalchemy.exc.StatementError: (exceptions.AttributeError) ''module'' object has no attribute ''BinaryNull'' [SQL: u''INSERT INTO dag_run (dag_id, execution_date, start_date, end_date, state, run_id, external_trigger, conf) OUTPUT inserted.id VALUES (?, ?, ?, ?, ?, ?, ?, ?)''] [parameters: [{''end_date'': None, ''run_id'': u''backfill_2017-10-18T00:00:00'', ''execution_date'': datetime.datetime(2017, 10, 18, 0, 0), ''external_trigger'': False, ''state'': u''running'', ''conf'': None, ''start_date'': datetime.datetime(2017, 10, 23, 21, 25, 9, 399710), ''dag_id'': ''example_bash_operator''}]]
> When I tried to trigger the run on web GUI, it seems to be fine and I can see the record got inserted into dag_run table successfully.
> [^web.png]
> [^db.png]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26795,54,JIRA.13110016.1508253770000.65184.1509001500235@Atlassian.JIRA,1752,Feng Lu (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-26 00:05:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16220060#comment-16220060 ] 

Feng Lu commented on AIRFLOW-1723:
----------------------------------

Re-opening this JIRA issue to track the process of moving sendgrid from core to contrib.

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26796,54,JIRA.13110016.1508253770000.65185.1509001500240@Atlassian.JIRA,1752,Feng Lu (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-26 00:05:00-07,[jira] [Reopened] (AIRFLOW-1723) Support sendgrid in email backend,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Feng Lu reopened AIRFLOW-1723:
------------------------------

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26797,54,JIRA.13057719.1490049741000.65416.1509003300706@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13057719.1490049741000@Atlassian.JIRA,,,2017-10-26 00:35:00-07,"[jira] [Work started] (AIRFLOW-1018) Scheduler DAG processes can
 not log to stdout","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1018?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1018 started by Bolke de Bruin.
-----------------------------------------------
> Scheduler DAG processes can not log to stdout
> ---------------------------------------------
>
>                 Key: AIRFLOW-1018
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1018
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>         Environment: Airflow 1.8.0
>            Reporter: Vincent Poulain
>            Assignee: Bolke de Bruin
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. 
> Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container.
> When we specify /dev/stdout it raises:
> ""OSError: [Errno 20] Not a directory: ''/dev/stdout/2017-03-19''""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26798,54,JIRA.12971085.1463648643000.67097.1509014400393@Atlassian.JIRA,2165,Steen Manniche (JIRA),JIRA.12971085.1463648643000@Atlassian.JIRA,,,2017-10-26 03:40:00-07,"[jira] [Commented] (AIRFLOW-137) Airflow does not respect
 ''max_active_runs'' when task from multiple dag runs cleared","
    [ https://issues.apache.org/jira/browse/AIRFLOW-137?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16220289#comment-16220289 ] 

Steen Manniche commented on AIRFLOW-137:
----------------------------------------

[~mremes] & [~mrchrisrodriguez]: Shouldn''t the case you are mentioning be created as a new issue? I think that this issue has a different definition and should be closed.

> Airflow does not respect ''max_active_runs'' when task from multiple dag runs cleared
> -----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-137
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-137
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Tomasz Bartczak
>            Priority: Minor
>
> Also requested at https://github.com/apache/incubator-airflow/issues/1442
> Dear Airflow Maintainers,
> Environment
> Before I tell you about my issue, let me describe my Airflow environment:
> Please fill out any appropriate fields:
>     Airflow version: 1.7.0
>     Airflow components: webserver, mysql, scheduler with celery executor
>     Python Version: 2.7.6
>     Operating System: Linux Ubuntu 3.19.0-26-generic Scheduler runs with --num-runs and get restarted around every minute or so
> Description of Issue
> Now that you know a little about me, let me tell you about the issue I am having:
>     What did you expect to happen?
>     After running ''airflow clear -t spark_final_observations2csv -s 2016-04-07T01:00:00 -e 2016-04-11T01:00:00 MODELLING_V6'' I expected that this task gets executed in all dag-runs in specified by given time-range - respecting ''max_active_runs''
>     Dag configuration:
>     concurrency= 3,
>     max_active_runs = 2,
>     What happened instead?
>     Airflow at first started executing 3 of those tasks, which already violates ''max_active_runs'', but it looks like ''concurrency'' was the applied limit here.
>     3_running_2_pending
> After first task was done - airflow scheduled all other tasks, making it 5 running dags at the same time that violates all specified limit.
> In the GUI we saw red warning (5/2 Dags running ;-) )
> Reproducing the Issue
> max_active_runs is respected in a day-to-day basis - when of the tasks was stuck - airflow didn''t start more than 2 dags concurrently.
> [screenshots in the original issue: https://github.com/apache/incubator-airflow/issues/1442]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26799,54,JIRA.13042742.1487030758000.69915.1509037620283@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13042742.1487030758000@Atlassian.JIRA,,,2017-10-26 10:07:00-07,"[jira] [Commented] (AIRFLOW-873) Tests forcing availability of
 hive_metastore module","
    [ https://issues.apache.org/jira/browse/AIRFLOW-873?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16220794#comment-16220794 ] 

Andy Hadjigeorgiou commented on AIRFLOW-873:
--------------------------------------------

What was the resolution here (noticed it is closed). Is there a workaround that has been successful?

> Tests forcing availability of hive_metastore module
> ---------------------------------------------------
>
>                 Key: AIRFLOW-873
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-873
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: krishnabhupatiraju
>            Assignee: krishnabhupatiraju
>            Priority: Minor
>
> PR 2012 leads to an attempt to import hive_metastore any time unit tests are run, even if these aren''t the unit tests being run. hive_metastore is not a required module for Airflow, but now any machine without it can''t run any local unit tests! I put the traceback I''m seeing below.
> The unit test file causing this import needs to be guarded so it only runs in if its dependencies are available (see hive_operator.py in the same tests directory for one example, or the explicit skip guards in the postgres/mysql tests that ensure they only run in the right travis environment).
> ======================================================================
> ERROR: Failure: ImportError (No module named ''hive_metastore'')
> ----------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/failure.py"", line 39, in runTest
>     raise self.exc_val.with_traceback(self.tb)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/loader.py"", line 418, in loadTestsFromName
>     addr.filename, addr.module)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/importer.py"", line 47, in importFromPath
>     return self.importFromDir(dir_path, fqname)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/importer.py"", line 94, in importFromDir
>     mod = load_module(part_fqname, fh, filename, desc)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/imp.py"", line 244, in load_module
>     return load_package(name, filename)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/imp.py"", line 216, in load_package
>     return _load(spec)
>   File ""<frozen importlib._bootstrap>"", line 693, in _load
>   File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
>   File ""<frozen importlib._bootstrap_external>"", line 665, in exec_module
>   File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
>   File ""/Users/jlowin/git/airflow/tests/__init__.py"", line 24, in <module>
>     from .operators import *
>   File ""/Users/jlowin/git/airflow/tests/operators/__init__.py"", line 20, in <module>
>     from .s3_to_hive_operator import *
>   File ""/Users/jlowin/git/airflow/tests/operators/s3_to_hive_operator.py"", line 25, in <module>
>     from airflow.operators.s3_to_hive_operator import S3ToHiveTransfer
>   File ""/Users/jlowin/git/airflow/airflow/operators/s3_to_hive_operator.py"", line 27, in <module>
>     from airflow.hooks.hive_hooks import HiveCliHook
>   File ""/Users/jlowin/git/airflow/airflow/hooks/hive_hooks.py"", line 28, in <module>
>     import hive_metastore
> ImportError: No module named ''hive_metastore''



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26800,54,JIRA.13094009.1502427343000.72639.1509049921779@Atlassian.JIRA,2357,Maximilian Roos (JIRA),JIRA.13094009.1502427343000@Atlassian.JIRA,,,2017-10-26 13:32:01-07,"[jira] [Commented] (AIRFLOW-1503) AssertionError: INTERNAL: No
 default project is specified","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1503?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221188#comment-16221188 ] 

Maximilian Roos commented on AIRFLOW-1503:
------------------------------------------

Is there a reason we need to put this connection in the DB, rather than using `google_cloud_default`? I don''t think there''s any additional information that the connection params provide. 

This is an example of what we have to do in order to run BQ tasks without manually configuring through the UI (or let me know if we''re making a mistake): https://stackoverflow.com/questions/45626406/assertionerror-internal-no-default-project-is-specified

> AssertionError: INTERNAL: No default project is specified
> ---------------------------------------------------------
>
>                 Key: AIRFLOW-1503
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1503
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>    Affects Versions: Airflow 1.8
>         Environment: Unix platform
>            Reporter: chaitanya
>            Priority: Minor
>              Labels: beginner
>
> Hi ,
> New to airflow. Tried to run BigQuery query and store the result in another table. Getting the following error. 
> Please let me know where to default project. 
> Code: 
> sql_bigquery = BigQueryOperator(
>         task_id=''sql_bigquery'',
>         use_legacy_sql=False,
>         write_disposition=''WRITE_TRUNCATE'',
>         allow_large_results=True,
>         bql=''''''
>             #standardSQL
>                 SELECT ID, Name, Group, Mark, RATIO_TO_REPORT(Mark) OVER(PARTITION BY Group) AS percent FROM `tensile-site-168620.temp.marks`
>                 '''''',
>         destination_dataset_table=''temp.percentage'',
>         dag=dag
>         )
> Error Message: 
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 585, in test
>     ti.run(ignore_task_deps=True, ignore_ti_state=True, test_mode=True)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in run
>     result = task_copy.execute(context=context)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/operators/bigquery_operator.py"", line 82, in execute
>     self.allow_large_results, self.udf_config, self.use_legacy_sql)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/bigquery_hook.py"", line 228, in run_query
>     default_project_id=self.project_id)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/bigquery_hook.py"", line 917, in _split_tablename
>     assert default_project_id is not None, ""INTERNAL: No default project is specified""
> AssertionError: INTERNAL: No default project is specified



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26801,54,JIRA.13112413.1509054762000.73413.1509054780045@Atlassian.JIRA,2357,Maximilian Roos (JIRA),JIRA.13112413.1509054762000@Atlassian.JIRA,,,2017-10-26 14:53:00-07,[jira] [Created] (AIRFLOW-1758) Print full traceback on errors,"Maximilian Roos created AIRFLOW-1758:
----------------------------------------

             Summary: Print full traceback on errors
                 Key: AIRFLOW-1758
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1758
             Project: Apache Airflow
          Issue Type: Improvement
          Components: cli
            Reporter: Maximilian Roos
            Priority: Minor


Currently when there is a failure during a run, it''s difficult to see what the cause was. Could we at least print the python stack trace? 

As an example: 
```
[2017-10-26 21:43:38,155] {models.py:1563} ERROR - DataFlow failed with return code 1
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1461, in _run_raw_task
    result = task_copy.execute(context=context)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/operators/dataflow_operator.py"", line 192, in execute
    self.py_file, self.py_options)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 155, in start_python_dataflow
    task_id, variables, dataflow, name, [""python""] + py_options)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 141, in _start_dataflow
    _Dataflow(cmd).wait_for_done()
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 122, in wait_for_done
    self._proc.returncode))
Exception: DataFlow failed with return code 1
```

I then need to jump into a repl and attempt simulate the command that airflow would have run, which is both difficult and error prone. (Or is there a simpler way of doing this??)

I then get a better stack-trace:

```
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.py"", line 328, in run
    return self.runner.run(self)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py"", line 283, in run
    self.dataflow_client.create_job(self.job), self)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py"", line 168, in wrapper
    return fun(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 423, in create_job
    self.create_job_description(job)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 446, in create_job_description
    job.options, file_copy=self._gcs_file_copy)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 347, in stage_job_resources
    build_setup_args)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 439, in _build_setup_package
    os.chdir(os.path.dirname(setup_file))
OSError: [Errno 2] No such file or directory: ''''
```

Somewhat related to: https://issues.apache.org/jira/browse/AIRFLOW-174

(I''m using the DataFlowPythonOperator at the moment, but I suspect the issue is wider)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26802,54,JIRA.13112413.1509054762000.73420.1509054900078@Atlassian.JIRA,2357,Maximilian Roos (JIRA),JIRA.13112413.1509054762000@Atlassian.JIRA,,,2017-10-26 14:55:00-07,[jira] [Updated] (AIRFLOW-1758) Print full traceback on errors,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1758?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Maximilian Roos updated AIRFLOW-1758:
-------------------------------------
    Description: 
Currently when there is a failure during a run, it''s difficult to see what the cause was. Could we at least print the python stack trace? 

As an example: 
{code:python}
[2017-10-26 21:43:38,155] {models.py:1563} ERROR - DataFlow failed with return code 1
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1461, in _run_raw_task
    result = task_copy.execute(context=context)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/operators/dataflow_operator.py"", line 192, in execute
    self.py_file, self.py_options)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 155, in start_python_dataflow
    task_id, variables, dataflow, name, [""python""] + py_options)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 141, in _start_dataflow
    _Dataflow(cmd).wait_for_done()
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 122, in wait_for_done
    self._proc.returncode))
Exception: DataFlow failed with return code 1
{code}


I then need to jump into a repl and attempt simulate the command that airflow would have run, which is both difficult and error prone. (Or is there a simpler way of doing this??)

I then get a better stack-trace:

{code:python}
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.py"", line 328, in run
    return self.runner.run(self)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py"", line 283, in run
    self.dataflow_client.create_job(self.job), self)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py"", line 168, in wrapper
    return fun(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 423, in create_job
    self.create_job_description(job)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 446, in create_job_description
    job.options, file_copy=self._gcs_file_copy)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 347, in stage_job_resources
    build_setup_args)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 439, in _build_setup_package
    os.chdir(os.path.dirname(setup_file))
OSError: [Errno 2] No such file or directory: ''''
{code}

Somewhat related to: https://issues.apache.org/jira/browse/AIRFLOW-174

(I''m using the DataFlowPythonOperator at the moment, but I suspect the issue is wider)

  was:
Currently when there is a failure during a run, it''s difficult to see what the cause was. Could we at least print the python stack trace? 

As an example: 
```
[2017-10-26 21:43:38,155] {models.py:1563} ERROR - DataFlow failed with return code 1
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1461, in _run_raw_task
    result = task_copy.execute(context=context)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/operators/dataflow_operator.py"", line 192, in execute
    self.py_file, self.py_options)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 155, in start_python_dataflow
    task_id, variables, dataflow, name, [""python""] + py_options)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 141, in _start_dataflow
    _Dataflow(cmd).wait_for_done()
  File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 122, in wait_for_done
    self._proc.returncode))
Exception: DataFlow failed with return code 1
```

I then need to jump into a repl and attempt simulate the command that airflow would have run, which is both difficult and error prone. (Or is there a simpler way of doing this??)

I then get a better stack-trace:

```
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.py"", line 328, in run
    return self.runner.run(self)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py"", line 283, in run
    self.dataflow_client.create_job(self.job), self)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py"", line 168, in wrapper
    return fun(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 423, in create_job
    self.create_job_description(job)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 446, in create_job_description
    job.options, file_copy=self._gcs_file_copy)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 347, in stage_job_resources
    build_setup_args)
  File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 439, in _build_setup_package
    os.chdir(os.path.dirname(setup_file))
OSError: [Errno 2] No such file or directory: ''''
```

Somewhat related to: https://issues.apache.org/jira/browse/AIRFLOW-174

(I''m using the DataFlowPythonOperator at the moment, but I suspect the issue is wider)


> Print full traceback on errors
> ------------------------------
>
>                 Key: AIRFLOW-1758
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1758
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: cli
>            Reporter: Maximilian Roos
>            Priority: Minor
>
> Currently when there is a failure during a run, it''s difficult to see what the cause was. Could we at least print the python stack trace? 
> As an example: 
> {code:python}
> [2017-10-26 21:43:38,155] {models.py:1563} ERROR - DataFlow failed with return code 1
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1461, in _run_raw_task
>     result = task_copy.execute(context=context)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/operators/dataflow_operator.py"", line 192, in execute
>     self.py_file, self.py_options)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 155, in start_python_dataflow
>     task_id, variables, dataflow, name, [""python""] + py_options)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 141, in _start_dataflow
>     _Dataflow(cmd).wait_for_done()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 122, in wait_for_done
>     self._proc.returncode))
> Exception: DataFlow failed with return code 1
> {code}
> I then need to jump into a repl and attempt simulate the command that airflow would have run, which is both difficult and error prone. (Or is there a simpler way of doing this??)
> I then get a better stack-trace:
> {code:python}
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.py"", line 328, in run
>     return self.runner.run(self)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py"", line 283, in run
>     self.dataflow_client.create_job(self.job), self)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py"", line 168, in wrapper
>     return fun(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 423, in create_job
>     self.create_job_description(job)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 446, in create_job_description
>     job.options, file_copy=self._gcs_file_copy)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 347, in stage_job_resources
>     build_setup_args)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 439, in _build_setup_package
>     os.chdir(os.path.dirname(setup_file))
> OSError: [Errno 2] No such file or directory: ''''
> {code}
> Somewhat related to: https://issues.apache.org/jira/browse/AIRFLOW-174
> (I''m using the DataFlowPythonOperator at the moment, but I suspect the issue is wider)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26803,54,JIRA.13112413.1509054762000.73476.1509055500488@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13112413.1509054762000@Atlassian.JIRA,,,2017-10-26 15:05:00-07,[jira] [Commented] (AIRFLOW-1758) Print full traceback on errors,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1758?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221308#comment-16221308 ] 

Chris Riccomini commented on AIRFLOW-1758:
------------------------------------------

Is this fixed by AIRFLOW-1732?

> Print full traceback on errors
> ------------------------------
>
>                 Key: AIRFLOW-1758
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1758
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: cli
>            Reporter: Maximilian Roos
>            Priority: Minor
>
> Currently when there is a failure during a run, it''s difficult to see what the cause was. Could we at least print the python stack trace? 
> As an example: 
> {code:python}
> [2017-10-26 21:43:38,155] {models.py:1563} ERROR - DataFlow failed with return code 1
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1461, in _run_raw_task
>     result = task_copy.execute(context=context)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/operators/dataflow_operator.py"", line 192, in execute
>     self.py_file, self.py_options)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 155, in start_python_dataflow
>     task_id, variables, dataflow, name, [""python""] + py_options)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 141, in _start_dataflow
>     _Dataflow(cmd).wait_for_done()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 122, in wait_for_done
>     self._proc.returncode))
> Exception: DataFlow failed with return code 1
> {code}
> I then need to jump into a repl and attempt simulate the command that airflow would have run, which is both difficult and error prone. (Or is there a simpler way of doing this??)
> I then get a better stack-trace:
> {code:python}
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.py"", line 328, in run
>     return self.runner.run(self)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py"", line 283, in run
>     self.dataflow_client.create_job(self.job), self)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py"", line 168, in wrapper
>     return fun(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 423, in create_job
>     self.create_job_description(job)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 446, in create_job_description
>     job.options, file_copy=self._gcs_file_copy)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 347, in stage_job_resources
>     build_setup_args)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 439, in _build_setup_package
>     os.chdir(os.path.dirname(setup_file))
> OSError: [Errno 2] No such file or directory: ''''
> {code}
> Somewhat related to: https://issues.apache.org/jira/browse/AIRFLOW-174
> (I''m using the DataFlowPythonOperator at the moment, but I suspect the issue is wider)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26804,54,JIRA.13110016.1508253770000.73574.1509056460202@Atlassian.JIRA,2357,Maximilian Roos (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-26 15:21:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221329#comment-16221329 ] 

Maximilian Roos commented on AIRFLOW-1723:
------------------------------------------

This makes the `sendgrid` library a hard dependency. Was that intended?

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26805,54,JIRA.13112413.1509054762000.73639.1509056820084@Atlassian.JIRA,2357,Maximilian Roos (JIRA),JIRA.13112413.1509054762000@Atlassian.JIRA,,,2017-10-26 15:27:00-07,[jira] [Commented] (AIRFLOW-1758) Print full traceback on errors,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1758?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221346#comment-16221346 ] 

Maximilian Roos commented on AIRFLOW-1758:
------------------------------------------

It is fixed! 
I was on the alpha release rather than master; after testing in master this is good.
Thanks!

> Print full traceback on errors
> ------------------------------
>
>                 Key: AIRFLOW-1758
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1758
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: cli
>            Reporter: Maximilian Roos
>            Priority: Minor
>
> Currently when there is a failure during a run, it''s difficult to see what the cause was. Could we at least print the python stack trace? 
> As an example: 
> {code:python}
> [2017-10-26 21:43:38,155] {models.py:1563} ERROR - DataFlow failed with return code 1
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1461, in _run_raw_task
>     result = task_copy.execute(context=context)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/operators/dataflow_operator.py"", line 192, in execute
>     self.py_file, self.py_options)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 155, in start_python_dataflow
>     task_id, variables, dataflow, name, [""python""] + py_options)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 141, in _start_dataflow
>     _Dataflow(cmd).wait_for_done()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 122, in wait_for_done
>     self._proc.returncode))
> Exception: DataFlow failed with return code 1
> {code}
> I then need to jump into a repl and attempt simulate the command that airflow would have run, which is both difficult and error prone. (Or is there a simpler way of doing this??)
> I then get a better stack-trace:
> {code:python}
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.py"", line 328, in run
>     return self.runner.run(self)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py"", line 283, in run
>     self.dataflow_client.create_job(self.job), self)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py"", line 168, in wrapper
>     return fun(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 423, in create_job
>     self.create_job_description(job)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 446, in create_job_description
>     job.options, file_copy=self._gcs_file_copy)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 347, in stage_job_resources
>     build_setup_args)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 439, in _build_setup_package
>     os.chdir(os.path.dirname(setup_file))
> OSError: [Errno 2] No such file or directory: ''''
> {code}
> Somewhat related to: https://issues.apache.org/jira/browse/AIRFLOW-174
> (I''m using the DataFlowPythonOperator at the moment, but I suspect the issue is wider)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26806,54,JIRA.13112413.1509054762000.73641.1509056880621@Atlassian.JIRA,2357,Maximilian Roos (JIRA),JIRA.13112413.1509054762000@Atlassian.JIRA,,,2017-10-26 15:28:00-07,[jira] [Resolved] (AIRFLOW-1758) Print full traceback on errors,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1758?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Maximilian Roos resolved AIRFLOW-1758.
--------------------------------------
    Resolution: Fixed

> Print full traceback on errors
> ------------------------------
>
>                 Key: AIRFLOW-1758
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1758
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: cli
>            Reporter: Maximilian Roos
>            Priority: Minor
>
> Currently when there is a failure during a run, it''s difficult to see what the cause was. Could we at least print the python stack trace? 
> As an example: 
> {code:python}
> [2017-10-26 21:43:38,155] {models.py:1563} ERROR - DataFlow failed with return code 1
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1461, in _run_raw_task
>     result = task_copy.execute(context=context)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/operators/dataflow_operator.py"", line 192, in execute
>     self.py_file, self.py_options)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 155, in start_python_dataflow
>     task_id, variables, dataflow, name, [""python""] + py_options)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 141, in _start_dataflow
>     _Dataflow(cmd).wait_for_done()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/contrib/hooks/gcp_dataflow_hook.py"", line 122, in wait_for_done
>     self._proc.returncode))
> Exception: DataFlow failed with return code 1
> {code}
> I then need to jump into a repl and attempt simulate the command that airflow would have run, which is both difficult and error prone. (Or is there a simpler way of doing this??)
> I then get a better stack-trace:
> {code:python}
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/pipeline.py"", line 328, in run
>     return self.runner.run(self)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/dataflow_runner.py"", line 283, in run
>     self.dataflow_client.create_job(self.job), self)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/utils/retry.py"", line 168, in wrapper
>     return fun(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 423, in create_job
>     self.create_job_description(job)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/apiclient.py"", line 446, in create_job_description
>     job.options, file_copy=self._gcs_file_copy)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 347, in stage_job_resources
>     build_setup_args)
>   File ""/usr/local/lib/python2.7/dist-packages/apache_beam/runners/dataflow/internal/dependency.py"", line 439, in _build_setup_package
>     os.chdir(os.path.dirname(setup_file))
> OSError: [Errno 2] No such file or directory: ''''
> {code}
> Somewhat related to: https://issues.apache.org/jira/browse/AIRFLOW-174
> (I''m using the DataFlowPythonOperator at the moment, but I suspect the issue is wider)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26807,54,JIRA.13110016.1508253770000.74613.1509064020182@Atlassian.JIRA,1752,Feng Lu (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-26 17:27:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221505#comment-16221505 ] 

Feng Lu commented on AIRFLOW-1723:
----------------------------------

[~m@maxroos.com] Certainly not intend to make sendgrid a hard dependency,  [PR 2727 ]will make sendgrid an optional component. 

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26808,54,JIRA.13112445.1509064628000.74667.1509064680171@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13112445.1509064628000@Atlassian.JIRA,,,2017-10-26 17:38:00-07,[jira] [Created] (AIRFLOW-1759) Create PostgresTableSensor,"Ace Haidrey created AIRFLOW-1759:
------------------------------------

             Summary: Create PostgresTableSensor
                 Key: AIRFLOW-1759
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1759
             Project: Apache Airflow
          Issue Type: New Feature
          Components: operators
            Reporter: Ace Haidrey
            Assignee: Ace Haidrey
            Priority: Minor


Like the HivePartitionSensor or sensors of that sort, create a PostgresTableSensor that makes sure a postgres table exists before operating on it.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26809,54,JIRA.13108831.1507796837000.74725.1509064980481@Atlassian.JIRA,2358,yonghwee (JIRA),JIRA.13108831.1507796837000@Atlassian.JIRA,,,2017-10-26 17:43:00-07,[jira] [Commented] (AIRFLOW-1705) Error: Already running on PID,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1705?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221519#comment-16221519 ] 

yonghwee commented on AIRFLOW-1705:
-----------------------------------

It seems like already running airflow on your server. Just kill process of this airflow webserver ( that is written in pid file ) for solving problem. 

> Error: Already running on PID
> -----------------------------
>
>                 Key: AIRFLOW-1705
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1705
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Igor Cherepanov
>
> what cause this error?
> {code:java}
> ic@ic-P900:~/airflow/dags$ airflow webserver
> [2017-10-12 10:20:47,109] {__init__.py:57} INFO - Using executor SequentialExecutor
> [2017-10-12 10:20:47,196] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt
> [2017-10-12 10:20:47,226] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt
>   ____________       _____________
>  ____    |__( )_________  __/__  /________      __
> ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
> ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
>  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
>  
> /usr/local/lib/python3.5/dist-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead.
>   .format(x=modname), ExtDeprecationWarning
> [2017-10-12 10:20:47,692] [3690] {models.py:167} INFO - Filling up the DagBag from /home/ic/airflow/dags
> Running the Gunicorn Server with:
> Workers: 4 sync
> Host: 0.0.0.0:8080
> Timeout: 120
> Logfiles: - -
> =================================================================            
> [2017-10-12 10:20:48,464] {__init__.py:57} INFO - Using executor SequentialExecutor
> [2017-10-12 10:20:48,548] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt
> [2017-10-12 10:20:48,577] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt
> [2017-10-12 10:20:48 +0200] [3697] [INFO] Starting gunicorn 19.3.0
> Error: Already running on PID 2446 (or pid file ''/home/ic/airflow/airflow-webserver.pid'' is stale)
> {code}
> thanks!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26810,54,JIRA.13024677.1480572641000.77322.1509090720969@Atlassian.JIRA,2359,Jesper Baasch-Larsen (JIRA),JIRA.13024677.1480572641000@Atlassian.JIRA,,,2017-10-27 00:52:00-07,"[jira] [Commented] (AIRFLOW-663) Improve time units for task
 duration and landing times charts","
    [ https://issues.apache.org/jira/browse/AIRFLOW-663?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221895#comment-16221895 ] 

Jesper Baasch-Larsen commented on AIRFLOW-663:
----------------------------------------------

Maybe there is a regression on this issue. Specifically I do not see any units on the y-axis in Airflow 1.8.2.

> Improve time units for task duration and landing times charts
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-663
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-663
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: ui
>            Reporter: Vijay Bhat
>            Assignee: Vijay Bhat
>
> The task duration and landing time charts display time interval values in hours. This is not the appropriate unit for tasks that execute on smaller time scales (~minutes, ~seconds), and the chart is unreadable in those cases. Convert the time values to the appropriate units and update the y axis label to show the unit.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
1103825,215,20040605011621.029898421D@bugzilla.spamassassin.org,36968,bugzilla-daemon,NULL,,,2004-06-04 18:16:21-07,[Bug 3208] 3.0.0 bugs to fix before release,"http://bugzilla.spamassassin.org/show_bug.cgi?id=3208

Bug 3208 depends on bug 3438, which changed state.

Bug 3438 Summary: Conflicting API documentation
http://bugzilla.spamassassin.org/show_bug.cgi?id=3438

           What    |Old Value                   |New Value
----------------------------------------------------------------------------
             Status|NEW                         |RESOLVED
         Resolution|                            |FIXED





------- You are receiving this mail because: -------
You are the assignee for the bug, or are watching the assignee.

",f
26811,54,JIRA.13024677.1480572641000.77584.1509093900590@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13024677.1480572641000@Atlassian.JIRA,,,2017-10-27 01:45:00-07,"[jira] [Commented] (AIRFLOW-663) Improve time units for task
 duration and landing times charts","
    [ https://issues.apache.org/jira/browse/AIRFLOW-663?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221954#comment-16221954 ] 

Ash Berlin-Taylor commented on AIRFLOW-663:
-------------------------------------------

There was a regression, yes, but it has been fixed in AIRFLOW-1432 which will be in 1.9.0

> Improve time units for task duration and landing times charts
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-663
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-663
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: ui
>            Reporter: Vijay Bhat
>            Assignee: Vijay Bhat
>
> The task duration and landing time charts display time interval values in hours. This is not the appropriate unit for tasks that execute on smaller time scales (~minutes, ~seconds), and the chart is unreadable in those cases. Convert the time values to the appropriate units and update the y axis label to show the unit.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26812,54,JIRA.13104708.1506332962000.77730.1509094980683@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-27 02:03:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221984#comment-16221984 ] 

ASF subversion and git services commented on AIRFLOW-1641:
----------------------------------------------------------

Commit 2abead7049806482047e29d123a109b444c00355 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2abead7 ]

[AIRFLOW-1641] Handle executor events in the scheduler

While in Backfills we do handle the executor
state,
we do not in the Scheduler. In case there is an
unspecified
error (e.g. a timeout, airflow command failure)
tasks
can get stuck.

Closes #2715 from bolkedebruin/AIRFLOW-1641


> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26813,54,JIRA.13104708.1506332962000.77729.1509094980673@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-27 02:03:00-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16221983#comment-16221983 ] 

ASF subversion and git services commented on AIRFLOW-1641:
----------------------------------------------------------

Commit 2abead7049806482047e29d123a109b444c00355 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2abead7 ]

[AIRFLOW-1641] Handle executor events in the scheduler

While in Backfills we do handle the executor
state,
we do not in the Scheduler. In case there is an
unspecified
error (e.g. a timeout, airflow command failure)
tasks
can get stuck.

Closes #2715 from bolkedebruin/AIRFLOW-1641


> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26814,54,JIRA.13112584.1509106580000.79137.1509106620104@Atlassian.JIRA,1940,Niels Zeilemaker (JIRA),JIRA.13112584.1509106580000@Atlassian.JIRA,,,2017-10-27 05:17:00-07,"[jira] [Created] (AIRFLOW-1760) Implement http_basic authentication
 for experimental api","Niels Zeilemaker created AIRFLOW-1760:
-----------------------------------------

             Summary: Implement http_basic authentication for experimental api
                 Key: AIRFLOW-1760
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1760
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Niels Zeilemaker
            Assignee: Niels Zeilemaker






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26815,54,JIRA.13104708.1506332962000.80045.1509112621158@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-27 06:57:01-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222409#comment-16222409 ] 

ASF subversion and git services commented on AIRFLOW-1641:
----------------------------------------------------------

Commit 73549763eac74142b7c4018422bb2f8c897b45a8 in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=7354976 ]

[AIRFLOW-1641] Handle executor events in the scheduler

While in Backfills we do handle the executor
state,
we do not in the Scheduler. In case there is an
unspecified
error (e.g. a timeout, airflow command failure)
tasks
can get stuck.

Closes #2715 from bolkedebruin/AIRFLOW-1641

(cherry picked from commit 2abead7049806482047e29d123a109b444c00355)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26816,54,JIRA.13104708.1506332962000.80055.1509112621261@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-27 06:57:01-07,[jira] [Commented] (AIRFLOW-1641) Task gets stuck in queued state,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222410#comment-16222410 ] 

ASF subversion and git services commented on AIRFLOW-1641:
----------------------------------------------------------

Commit 73549763eac74142b7c4018422bb2f8c897b45a8 in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=7354976 ]

[AIRFLOW-1641] Handle executor events in the scheduler

While in Backfills we do handle the executor
state,
we do not in the Scheduler. In case there is an
unspecified
error (e.g. a timeout, airflow command failure)
tasks
can get stuck.

Closes #2715 from bolkedebruin/AIRFLOW-1641

(cherry picked from commit 2abead7049806482047e29d123a109b444c00355)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26817,54,JIRA.13104708.1506332962000.80068.1509112740922@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13104708.1506332962000@Atlassian.JIRA,,,2017-10-27 06:59:00-07,[jira] [Closed] (AIRFLOW-1641) Task gets stuck in queued state,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1641?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin closed AIRFLOW-1641.
-----------------------------------
    Resolution: Fixed

> Task gets stuck in queued state
> -------------------------------
>
>                 Key: AIRFLOW-1641
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1641
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.0
>         Environment: Linux
>            Reporter: Mas
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>              Labels: queued, scheduler, stuck, task
>             Fix For: 1.9.0
>
>
> Hello,
> I have one dag with ~20 tasks. 
> The dags runs daily and some tasks can sometime last for hours, depending on the processed data behind.
> There are some interactions with AWS and a remote DB.
> I only use LocalExecutor.
> What this issue is about, is the fact that sometime (randomly, and without any clear reason) one of the tasks (here also, it is random) gets stuck in ""queued"" state and never starts running. 
> The manual workaround is to restart the task manually by clearing it.
> Does anyone have ideas about the issue behind, and how to avoid it for the future? 
> Thanks in advance for your help.
> PS: other people are facing the same behaviour: [link|https://stackoverflow.com/questions/45853013/airflow-tasks-get-stuck-at-queued-status-and-never-gets-running]



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26818,54,JIRA.13110297.1508334610000.80096.1509113040469@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110297.1508334610000@Atlassian.JIRA,,,2017-10-27 07:04:00-07,"[jira] [Commented] (AIRFLOW-1731) Import custom config on
 PYTHONPATH","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1731?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222418#comment-16222418 ] 

ASF subversion and git services commented on AIRFLOW-1731:
----------------------------------------------------------

Commit 635ab01a76d2187738b9b11f1b06c31e7b7dcf33 in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=635ab01 ]

[AIRFLOW-1731] Set pythonpath for logging

Before initializing the logging framework, we want
to set the python
path so the logging config can be found.

Closes #2721 from Fokko/AIRFLOW-1731-import-
pythonpath


> Import custom config on PYTHONPATH
> ----------------------------------
>
>                 Key: AIRFLOW-1731
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1731
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: logging
>    Affects Versions: 1.9.0
>            Reporter: Fokko Driesprong
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Currently the PYTHONPATH does not contain the required path to import a custom config as described. This needs to be fixed and the instructions needs to be updated based on user feedback.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26819,54,JIRA.13110297.1508334610000.80098.1509113040490@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110297.1508334610000@Atlassian.JIRA,,,2017-10-27 07:04:00-07,"[jira] [Commented] (AIRFLOW-1731) Import custom config on
 PYTHONPATH","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1731?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222419#comment-16222419 ] 

ASF subversion and git services commented on AIRFLOW-1731:
----------------------------------------------------------

Commit 635ab01a76d2187738b9b11f1b06c31e7b7dcf33 in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=635ab01 ]

[AIRFLOW-1731] Set pythonpath for logging

Before initializing the logging framework, we want
to set the python
path so the logging config can be found.

Closes #2721 from Fokko/AIRFLOW-1731-import-
pythonpath


> Import custom config on PYTHONPATH
> ----------------------------------
>
>                 Key: AIRFLOW-1731
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1731
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: logging
>    Affects Versions: 1.9.0
>            Reporter: Fokko Driesprong
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Currently the PYTHONPATH does not contain the required path to import a custom config as described. This needs to be fixed and the instructions needs to be updated based on user feedback.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
666968,23,20160621122551.1ea6edb9@bifrost.webthing.com,4432,Nick Kew,fa1e6ba17bde4df2b5cdb5d2b54a981f@git.apache.org,25018,Mike Hewitt,2016-06-21 04:25:51-07,"Re: incubator-milagro-mfa-server git commit: fixed link to milagro
 documentation","On Tue, 21 Jun 2016 11:05:00 +0000 (UTC)
niq@apache.org wrote:

> Repository: incubator-milagro-mfa-server
> Updated Branches:
>   refs/heads/master fba4c5be7 -> 9fa974fbf
> 
> 
> fixed link to milagro documentation

Heads-up: I think this has been a source of some confusion
regarding github-apache working as per
https://wiki.apache.org/general/GithubMirror

This is one of two essentially-identical pull requests from
MikeHewitt, making the same (trivial) fix in two repos.  I had
previously pulled the other (in the js-client repo), so seeing
this one I thought it had already been pulled.  I think
Tom and maybe others may have shared my confusion there.

Sorry folks.  Must get used to working with a project in
multiple different repos!

-- 
Nick Kew

",f
26820,54,JIRA.13110297.1508334610000.80103.1509113040540@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110297.1508334610000@Atlassian.JIRA,,,2017-10-27 07:04:00-07,"[jira] [Commented] (AIRFLOW-1731) Import custom config on
 PYTHONPATH","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1731?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222421#comment-16222421 ] 

ASF subversion and git services commented on AIRFLOW-1731:
----------------------------------------------------------

Commit f07eb3106ff73de9cbd2f46a0354ec981846a0be in incubator-airflow''s branch refs/heads/v1-9-test from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f07eb31 ]

[AIRFLOW-1731] Set pythonpath for logging

Before initializing the logging framework, we want
to set the python
path so the logging config can be found.

Closes #2721 from Fokko/AIRFLOW-1731-import-
pythonpath

(cherry picked from commit 635ab01a76d2187738b9b11f1b06c31e7b7dcf33)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Import custom config on PYTHONPATH
> ----------------------------------
>
>                 Key: AIRFLOW-1731
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1731
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: logging
>    Affects Versions: 1.9.0
>            Reporter: Fokko Driesprong
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Currently the PYTHONPATH does not contain the required path to import a custom config as described. This needs to be fixed and the instructions needs to be updated based on user feedback.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26821,54,JIRA.13110297.1508334610000.80101.1509113040519@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110297.1508334610000@Atlassian.JIRA,,,2017-10-27 07:04:00-07,"[jira] [Commented] (AIRFLOW-1731) Import custom config on
 PYTHONPATH","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1731?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222420#comment-16222420 ] 

ASF subversion and git services commented on AIRFLOW-1731:
----------------------------------------------------------

Commit f07eb3106ff73de9cbd2f46a0354ec981846a0be in incubator-airflow''s branch refs/heads/v1-9-test from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f07eb31 ]

[AIRFLOW-1731] Set pythonpath for logging

Before initializing the logging framework, we want
to set the python
path so the logging config can be found.

Closes #2721 from Fokko/AIRFLOW-1731-import-
pythonpath

(cherry picked from commit 635ab01a76d2187738b9b11f1b06c31e7b7dcf33)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Import custom config on PYTHONPATH
> ----------------------------------
>
>                 Key: AIRFLOW-1731
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1731
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: logging
>    Affects Versions: 1.9.0
>            Reporter: Fokko Driesprong
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Currently the PYTHONPATH does not contain the required path to import a custom config as described. This needs to be fixed and the instructions needs to be updated based on user feedback.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26822,54,JIRA.13110297.1508334610000.80106.1509113040568@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13110297.1508334610000@Atlassian.JIRA,,,2017-10-27 07:04:00-07,[jira] [Resolved] (AIRFLOW-1731) Import custom config on PYTHONPATH,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1731?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1731.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2721
[https://github.com/apache/incubator-airflow/pull/2721]

> Import custom config on PYTHONPATH
> ----------------------------------
>
>                 Key: AIRFLOW-1731
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1731
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: logging
>    Affects Versions: 1.9.0
>            Reporter: Fokko Driesprong
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Currently the PYTHONPATH does not contain the required path to import a custom config as described. This needs to be fixed and the instructions needs to be updated based on user feedback.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26823,54,JIRA.13112623.1509117243000.80731.1509117300258@Atlassian.JIRA,1279,Siddharth Anand (JIRA),JIRA.13112623.1509117243000@Atlassian.JIRA,,,2017-10-27 08:15:00-07,[jira] [Created] (AIRFLOW-1761) Fix a typo in scheduler.rst,"Siddharth Anand created AIRFLOW-1761:
----------------------------------------

             Summary: Fix a typo in scheduler.rst
                 Key: AIRFLOW-1761
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1761
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Martial Hue
            Priority: Trivial






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26824,54,JIRA.13112623.1509117243000.80735.1509117360355@Atlassian.JIRA,1279,Siddharth Anand (JIRA),JIRA.13112623.1509117243000@Atlassian.JIRA,,,2017-10-27 08:16:00-07,[jira] [Assigned] (AIRFLOW-1761) Fix a typo in scheduler.rst,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1761?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Siddharth Anand reassigned AIRFLOW-1761:
----------------------------------------

              Assignee: Martial Hue
    External issue URL: https://github.com/apache/incubator-airflow/pull/2707

> Fix a typo in scheduler.rst
> ---------------------------
>
>                 Key: AIRFLOW-1761
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1761
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Martial Hue
>            Assignee: Martial Hue
>            Priority: Trivial
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26825,54,JIRA.13112623.1509117243000.80758.1509117540441@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112623.1509117243000@Atlassian.JIRA,,,2017-10-27 08:19:00-07,[jira] [Commented] (AIRFLOW-1761) Fix a typo in scheduler.rst,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1761?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222536#comment-16222536 ] 

ASF subversion and git services commented on AIRFLOW-1761:
----------------------------------------------------------

Commit efdc4d3b418a27d7808b17026ce271fe416646f0 in incubator-airflow''s branch refs/heads/master from [~mhue]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=efdc4d3 ]

[AIRFLOW-1761] Fix type in scheduler.rst

Closes #2707 from mhue/patch-1


> Fix a typo in scheduler.rst
> ---------------------------
>
>                 Key: AIRFLOW-1761
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1761
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Martial Hue
>            Assignee: Martial Hue
>            Priority: Trivial
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26826,54,JIRA.13112623.1509117243000.80770.1509117660137@Atlassian.JIRA,1279,Siddharth Anand (JIRA),JIRA.13112623.1509117243000@Atlassian.JIRA,,,2017-10-27 08:21:00-07,[jira] [Closed] (AIRFLOW-1761) Fix a typo in scheduler.rst,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1761?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Siddharth Anand closed AIRFLOW-1761.
------------------------------------
    Resolution: Fixed

> Fix a typo in scheduler.rst
> ---------------------------
>
>                 Key: AIRFLOW-1761
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1761
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Martial Hue
>            Assignee: Martial Hue
>            Priority: Trivial
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26827,54,JIRA.13042742.1487030758000.80974.1509119040736@Atlassian.JIRA,2305,William Pursell (JIRA),JIRA.13042742.1487030758000@Atlassian.JIRA,,,2017-10-27 08:44:00-07,"[jira] [Commented] (AIRFLOW-873) Tests forcing availability of
 hive_metastore module","
    [ https://issues.apache.org/jira/browse/AIRFLOW-873?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222573#comment-16222573 ] 

William Pursell commented on AIRFLOW-873:
-----------------------------------------

+1  To Andy''s comment.  What is the resolution?  How does one run run_unit_tests.sh?

> Tests forcing availability of hive_metastore module
> ---------------------------------------------------
>
>                 Key: AIRFLOW-873
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-873
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: krishnabhupatiraju
>            Assignee: krishnabhupatiraju
>            Priority: Minor
>
> PR 2012 leads to an attempt to import hive_metastore any time unit tests are run, even if these aren''t the unit tests being run. hive_metastore is not a required module for Airflow, but now any machine without it can''t run any local unit tests! I put the traceback I''m seeing below.
> The unit test file causing this import needs to be guarded so it only runs in if its dependencies are available (see hive_operator.py in the same tests directory for one example, or the explicit skip guards in the postgres/mysql tests that ensure they only run in the right travis environment).
> ======================================================================
> ERROR: Failure: ImportError (No module named ''hive_metastore'')
> ----------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/failure.py"", line 39, in runTest
>     raise self.exc_val.with_traceback(self.tb)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/loader.py"", line 418, in loadTestsFromName
>     addr.filename, addr.module)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/importer.py"", line 47, in importFromPath
>     return self.importFromDir(dir_path, fqname)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/importer.py"", line 94, in importFromDir
>     mod = load_module(part_fqname, fh, filename, desc)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/imp.py"", line 244, in load_module
>     return load_package(name, filename)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/imp.py"", line 216, in load_package
>     return _load(spec)
>   File ""<frozen importlib._bootstrap>"", line 693, in _load
>   File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
>   File ""<frozen importlib._bootstrap_external>"", line 665, in exec_module
>   File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
>   File ""/Users/jlowin/git/airflow/tests/__init__.py"", line 24, in <module>
>     from .operators import *
>   File ""/Users/jlowin/git/airflow/tests/operators/__init__.py"", line 20, in <module>
>     from .s3_to_hive_operator import *
>   File ""/Users/jlowin/git/airflow/tests/operators/s3_to_hive_operator.py"", line 25, in <module>
>     from airflow.operators.s3_to_hive_operator import S3ToHiveTransfer
>   File ""/Users/jlowin/git/airflow/airflow/operators/s3_to_hive_operator.py"", line 27, in <module>
>     from airflow.hooks.hive_hooks import HiveCliHook
>   File ""/Users/jlowin/git/airflow/airflow/hooks/hive_hooks.py"", line 28, in <module>
>     import hive_metastore
> ImportError: No module named ''hive_metastore''



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26828,54,JIRA.13110016.1508253770000.82954.1509139800964@Atlassian.JIRA,2361,Marcin Szymanski (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-27 14:30:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222865#comment-16222865 ] 

Marcin Szymanski commented on AIRFLOW-1723:
-------------------------------------------

A small request while you''re doing this. Can you please add {{**kwargs}} to {{send_email}} and push it down to the backend. It could be useful for calling {{Personalization.add_custom_arguments}}, either directly with {{kwargs}} or with a dictionary from {{kwargs\[''custom_arguments''\]}}.

I can prepare prepare a new feature once this is sorted 

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26829,54,JIRA.13110016.1508253770000.83174.1509140941840@Atlassian.JIRA,1752,Feng Lu (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-27 14:49:01-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222897#comment-16222897 ] 

Feng Lu commented on AIRFLOW-1723:
----------------------------------

[~ms32035] certainly not a problem. My concern is that it unfortunately breaks the airflow.utils.email.send_mail interface as send_mail_smtp doesn''t seem to support custom_args. WDYT? 

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26830,54,JIRA.13109034.1507839702000.83181.1509140941909@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13109034.1507839702000@Atlassian.JIRA,,,2017-10-27 14:49:01-07,"[jira] [Updated] (AIRFLOW-1711) Ldap Attributes not always a ""list""
 part 2","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1711?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1711:
-------------------------------------
    Priority: Blocker  (was: Major)

> Ldap Attributes not always a ""list"" part 2
> ------------------------------------------
>
>                 Key: AIRFLOW-1711
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1711
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.7.1
>         Environment: Linux + Active Directory
>            Reporter: Steve Jacobs
>            Priority: Blocker
>
> in the LDAP auth module
> `group_contains_user` checks for `resp[''attributes''].get(user_name_attr)[0] == username`
> Some Ldaps apparently have this as a simple string
> `resp[''attributes''].get(user_name_attr) == username` 
> also should be checked. 
> But really a test should be done to see if the return is a ''list'' and perform the check differently. If its not a list, python will check both arguments and exit with an error. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
666982,23,JIRA.12994601.1470234525000.212425.1470234560474@Atlassian.JIRA,25078,kealan mccusker (JIRA),JIRA.12994601.1470234525000@Atlassian.JIRA,,,2016-08-03 07:29:20-07,"[jira] [Created] (MILAGRO-12) Update the crypto code in milagro-mfa
 to use milagro-crypto-c","kealan mccusker created MILAGRO-12:
--------------------------------------

             Summary: Update the crypto code in milagro-mfa to use milagro-crypto-c
                 Key: MILAGRO-12
                 URL: https://issues.apache.org/jira/browse/MILAGRO-12
             Project: Milagro
          Issue Type: Task
          Components: Server
            Reporter: kealan mccusker
            Priority: Minor






--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26831,54,JIRA.13057719.1490049741000.83176.1509140941861@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13057719.1490049741000@Atlassian.JIRA,,,2017-10-27 14:49:01-07,"[jira] [Updated] (AIRFLOW-1018) Scheduler DAG processes can not log
 to stdout","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1018?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1018:
-------------------------------------
    Priority: Blocker  (was: Critical)

> Scheduler DAG processes can not log to stdout
> ---------------------------------------------
>
>                 Key: AIRFLOW-1018
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1018
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>         Environment: Airflow 1.8.0
>            Reporter: Vincent Poulain
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. 
> Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container.
> When we specify /dev/stdout it raises:
> ""OSError: [Errno 20] Not a directory: ''/dev/stdout/2017-03-19''""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26832,54,JIRA.13110016.1508253770000.83210.1509141060266@Atlassian.JIRA,2361,Marcin Szymanski (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-27 14:51:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222900#comment-16222900 ] 

Marcin Szymanski commented on AIRFLOW-1723:
-------------------------------------------

Adding {{**kwargs}} there as well, without any handling inside resolves the issue.

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26833,54,JIRA.13110016.1508253770000.83223.1509141240255@Atlassian.JIRA,2361,Marcin Szymanski (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-27 14:54:00-07,"[jira] [Comment Edited] (AIRFLOW-1723) Support sendgrid in email
 backend","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222900#comment-16222900 ] 

Marcin Szymanski edited comment on AIRFLOW-1723 at 10/27/17 9:53 PM:
---------------------------------------------------------------------

Adding {{**kwargs}} there as well, without any handling inside resolves the issue. Optionally, there could be warning, that unsupported parameters are passed.


was (Author: ms32035):
Adding {{**kwargs}} there as well, without any handling inside resolves the issue.

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26834,54,JIRA.13110016.1508253770000.83489.1509142200440@Atlassian.JIRA,1752,Feng Lu (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-27 15:10:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222940#comment-16222940 ] 

Feng Lu commented on AIRFLOW-1723:
----------------------------------

How about this, once this PR is merged, I''ll reply the email thread on ""ImportError: No module named sendgrid"" in the dev mailing list and aske for the inclusion of **kwargs? 
I can definitely see the value of including **kwargs for more flexibility as individual email backend may want to handle message with its specific way.  



> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26835,54,JIRA.13110016.1508253770000.83509.1509142380456@Atlassian.JIRA,2361,Marcin Szymanski (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-27 15:13:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16222943#comment-16222943 ] 

Marcin Szymanski commented on AIRFLOW-1723:
-------------------------------------------

Ok, thx

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
666983,23,JIRA.12995581.1470647517000.247356.1470647540430@Atlassian.JIRA,25079,Alessandro Budroni (JIRA),JIRA.12995581.1470647517000@Atlassian.JIRA,,,2016-08-08 02:12:20-07,"[jira] [Created] (MILAGRO-13) Update the code in milagro-tls with
 the dev branch in milagro-crypto-c","Alessandro Budroni created MILAGRO-13:
-----------------------------------------

             Summary: Update the code in milagro-tls with the dev branch in milagro-crypto-c
                 Key: MILAGRO-13
                 URL: https://issues.apache.org/jira/browse/MILAGRO-13
             Project: Milagro
          Issue Type: Task
          Components: crypto, TLS
            Reporter: Alessandro Budroni
            Priority: Minor






--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26836,54,JIRA.13112766.1509171008000.85985.1509171060024@Atlassian.JIRA,2362,Nathan McIntyre (JIRA),JIRA.13112766.1509171008000@Atlassian.JIRA,,,2017-10-27 23:11:00-07,"[jira] [Created] (AIRFLOW-1762) SSHHook neglects key_file in
 create_tunnel()","Nathan McIntyre created AIRFLOW-1762:
----------------------------------------

             Summary: SSHHook neglects key_file in create_tunnel()
                 Key: AIRFLOW-1762
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1762
             Project: Apache Airflow
          Issue Type: Bug
          Components: contrib
    Affects Versions: Airflow 1.8, 1.9.0
            Reporter: Nathan McIntyre
            Assignee: Nathan McIntyre


In contrib/hooks/ssh_hook.py, the ssh command created by the create_tunnel() method does not use the key_file attribute. This prevents the creation of tunnels where a key file is required. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26837,54,JIRA.13063355.1491959771000.86983.1509187080373@Atlassian.JIRA,2208,Junyoung Park (JIRA),JIRA.13063355.1491959771000@Atlassian.JIRA,,,2017-10-28 03:38:00-07,"[jira] [Assigned] (AIRFLOW-1101) Docs use the term ""Pipelines""
 instead of DAGs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1101?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Junyoung Park reassigned AIRFLOW-1101:
--------------------------------------

    Assignee:     (was: Junyoung Park)

> Docs use the term ""Pipelines"" instead of DAGs
> ---------------------------------------------
>
>                 Key: AIRFLOW-1101
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1101
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Andrew Chen
>
> It''s a bit confusing to use the words ""DAGs"" and ""pipelines"" interchangea=
bly in the documentation. Here are a couple examples where it is especially=
 confusing.
> https://airflow.incubator.apache.org/configuration.html#connections
> ""The pipeline code you will author will reference the =E2=80=98conn_id=E2=
=80=99 of the Connection objects.""
> ""Connections in Airflow pipelines can be created using environment variab=
les.""
> https://airflow.incubator.apache.org/configuration.html#scaling-out-with-=
celery
> ""If all your boxes have a common mount point, having your pipelines files=
 shared there should work as well""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26838,54,JIRA.13110554.1508379412000.87481.1509196140854@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110554.1508379412000@Atlassian.JIRA,,,2017-10-28 06:09:00-07,"[jira] [Commented] (AIRFLOW-1734) Sqoop Operator contains logic
 errors & needs options to pass more sqoop options","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1734?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16223492#comment-16223492 ] 

ASF subversion and git services commented on AIRFLOW-1734:
----------------------------------------------------------

Commit 1d531555ecd594ee7ec2c5d3fc87f8d4bcc2c27e in incubator-airflow''s branch refs/heads/master from [~ahaidrey]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1d53155 ]

[AIRFLOW-1734][Airflow 1734] Sqoop hook/operator enhancements

Closes #2703 from Acehaidrey/sqoop_contrib_fixes


> Sqoop Operator contains logic errors & needs options to pass more sqoop options
> -------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1734
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1734
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Ace Haidrey
>            Assignee: Ace Haidrey
>              Labels: patch
>             Fix For: 1.9.0
>
>
> After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.
> I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
> https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796
> Here is my PR with change by change explanation:
> https://github.com/apache/incubator-airflow/pull/2703/files#diff-8e77f042c2e060bbfd60828431a91e9bL131



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26839,54,JIRA.13110554.1508379412000.87497.1509196141016@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13110554.1508379412000@Atlassian.JIRA,,,2017-10-28 06:09:01-07,"[jira] [Resolved] (AIRFLOW-1734) Sqoop Operator contains logic
 errors & needs options to pass more sqoop options","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1734?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1734.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2703
[https://github.com/apache/incubator-airflow/pull/2703]

> Sqoop Operator contains logic errors & needs options to pass more sqoop options
> -------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1734
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1734
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Ace Haidrey
>            Assignee: Ace Haidrey
>              Labels: patch
>             Fix For: 1.9.0
>
>
> After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.
> I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
> https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796
> Here is my PR with change by change explanation:
> https://github.com/apache/incubator-airflow/pull/2703/files#diff-8e77f042c2e060bbfd60828431a91e9bL131



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26840,54,JIRA.13110554.1508379412000.87491.1509196140958@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110554.1508379412000@Atlassian.JIRA,,,2017-10-28 06:09:00-07,"[jira] [Commented] (AIRFLOW-1734) Sqoop Operator contains logic
 errors & needs options to pass more sqoop options","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1734?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16223494#comment-16223494 ] 

ASF subversion and git services commented on AIRFLOW-1734:
----------------------------------------------------------

Commit f6810c9b4f6d821f3fd7ae775606a4295ab8cb20 in incubator-airflow''s branch refs/heads/v1-9-test from [~ahaidrey]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f6810c9 ]

[AIRFLOW-1734][Airflow 1734] Sqoop hook/operator enhancements

Closes #2703 from Acehaidrey/sqoop_contrib_fixes

(cherry picked from commit 1d531555ecd594ee7ec2c5d3fc87f8d4bcc2c27e)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Sqoop Operator contains logic errors & needs options to pass more sqoop options
> -------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1734
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1734
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Ace Haidrey
>            Assignee: Ace Haidrey
>              Labels: patch
>             Fix For: 1.9.0
>
>
> After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.
> I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
> https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796
> Here is my PR with change by change explanation:
> https://github.com/apache/incubator-airflow/pull/2703/files#diff-8e77f042c2e060bbfd60828431a91e9bL131



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26841,54,JIRA.13112002.1508946012000.87530.1509196440412@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112002.1508946012000@Atlassian.JIRA,,,2017-10-28 06:14:00-07,"[jira] [Commented] (AIRFLOW-1757) Contrib.SparkSubmitOperator
 should allow --packages parameter","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1757?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16223503#comment-16223503 ] 

ASF subversion and git services commented on AIRFLOW-1757:
----------------------------------------------------------

Commit b3c247d3bfd43a292f728608bc5aaba772f40f33 in incubator-airflow''s branch refs/heads/master from [~kretes]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=b3c247d ]

[AIRFLOW-1757] Add missing options to SparkSubmitOperator

add ''exclude-packages'' and ''repositories'' as
options
to SparkSubmitOperator as they were missing

Closes #2725 from kretes/AIRFLOW-1757-spark-new-
options


> Contrib.SparkSubmitOperator should allow --packages parameter
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1757
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1757
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Tomasz Bartczak
>            Priority: Trivial
>
> We miss two options from spark-submit - to pass packages to exclude and to pass custom repositories.
> I have a PR ready for that.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26842,54,JIRA.13112002.1508946012000.87533.1509196440440@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112002.1508946012000@Atlassian.JIRA,,,2017-10-28 06:14:00-07,"[jira] [Commented] (AIRFLOW-1757) Contrib.SparkSubmitOperator
 should allow --packages parameter","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1757?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16223505#comment-16223505 ] 

ASF subversion and git services commented on AIRFLOW-1757:
----------------------------------------------------------

Commit 4e06ee554291798fabfd3c8c6ff3943219162c7b in incubator-airflow''s branch refs/heads/v1-9-test from [~kretes]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4e06ee5 ]

[AIRFLOW-1757] Add missing options to SparkSubmitOperator

add ''exclude-packages'' and ''repositories'' as
options
to SparkSubmitOperator as they were missing

Closes #2725 from kretes/AIRFLOW-1757-spark-new-
options

(cherry picked from commit b3c247d3bfd43a292f728608bc5aaba772f40f33)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Contrib.SparkSubmitOperator should allow --packages parameter
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1757
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1757
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Tomasz Bartczak
>            Priority: Trivial
>
> We miss two options from spark-submit - to pass packages to exclude and to pass custom repositories.
> I have a PR ready for that.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26843,54,JIRA.13112002.1508946012000.87528.1509196440397@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112002.1508946012000@Atlassian.JIRA,,,2017-10-28 06:14:00-07,"[jira] [Commented] (AIRFLOW-1757) Contrib.SparkSubmitOperator
 should allow --packages parameter","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1757?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16223502#comment-16223502 ] 

ASF subversion and git services commented on AIRFLOW-1757:
----------------------------------------------------------

Commit b3c247d3bfd43a292f728608bc5aaba772f40f33 in incubator-airflow''s branch refs/heads/master from [~kretes]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=b3c247d ]

[AIRFLOW-1757] Add missing options to SparkSubmitOperator

add ''exclude-packages'' and ''repositories'' as
options
to SparkSubmitOperator as they were missing

Closes #2725 from kretes/AIRFLOW-1757-spark-new-
options


> Contrib.SparkSubmitOperator should allow --packages parameter
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1757
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1757
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Tomasz Bartczak
>            Priority: Trivial
>
> We miss two options from spark-submit - to pass packages to exclude and to pass custom repositories.
> I have a PR ready for that.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26844,54,JIRA.13112002.1508946012000.87531.1509196440422@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112002.1508946012000@Atlassian.JIRA,,,2017-10-28 06:14:00-07,"[jira] [Commented] (AIRFLOW-1757) Contrib.SparkSubmitOperator
 should allow --packages parameter","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1757?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16223504#comment-16223504 ] 

ASF subversion and git services commented on AIRFLOW-1757:
----------------------------------------------------------

Commit 4e06ee554291798fabfd3c8c6ff3943219162c7b in incubator-airflow''s branch refs/heads/v1-9-test from [~kretes]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4e06ee5 ]

[AIRFLOW-1757] Add missing options to SparkSubmitOperator

add ''exclude-packages'' and ''repositories'' as
options
to SparkSubmitOperator as they were missing

Closes #2725 from kretes/AIRFLOW-1757-spark-new-
options

(cherry picked from commit b3c247d3bfd43a292f728608bc5aaba772f40f33)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Contrib.SparkSubmitOperator should allow --packages parameter
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1757
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1757
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Tomasz Bartczak
>            Priority: Trivial
>
> We miss two options from spark-submit - to pass packages to exclude and to pass custom repositories.
> I have a PR ready for that.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26845,54,JIRA.13112002.1508946012000.87537.1509196500352@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13112002.1508946012000@Atlassian.JIRA,,,2017-10-28 06:15:00-07,"[jira] [Resolved] (AIRFLOW-1757) Contrib.SparkSubmitOperator should
 allow --packages parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1757?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1757.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2725
[https://github.com/apache/incubator-airflow/pull/2725]

> Contrib.SparkSubmitOperator should allow --packages parameter
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1757
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1757
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Tomasz Bartczak
>            Priority: Trivial
>             Fix For: 1.9.0
>
>
> We miss two options from spark-submit - to pass packages to exclude and to pass custom repositories.
> I have a PR ready for that.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26846,54,JIRA.13112850.1509220003000.89191.1509220020242@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13112850.1509220003000@Atlassian.JIRA,,,2017-10-28 12:47:00-07,"[jira] [Created] (AIRFLOW-1763) S3 Task Handler unit tests not
 running","Andy Hadjigeorgiou created AIRFLOW-1763:
-------------------------------------------

             Summary: S3 Task Handler unit tests not running
                 Key: AIRFLOW-1763
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1763
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Andy Hadjigeorgiou
            Assignee: Andy Hadjigeorgiou


S3 Task Handler tests are not being run, because the folder isn''t being discovered due to it not being a python package. Furthermore, these tests are broken (lots of updates to s3_task_handler since tests were run), so they need to be fixed. (different initialization, changed method names and slightly altered return values.





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26847,54,JIRA.13112850.1509220003000.89194.1509220320033@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13112850.1509220003000@Atlassian.JIRA,,,2017-10-28 12:52:00-07,"[jira] [Work started] (AIRFLOW-1763) S3 Task Handler unit tests not
 running","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1763?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1763 started by Andy Hadjigeorgiou.
---------------------------------------------------
> S3 Task Handler unit tests not running
> --------------------------------------
>
>                 Key: AIRFLOW-1763
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1763
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>
> S3 Task Handler tests are not being run, because the folder isn''t being discovered due to it not being a python package. Furthermore, these tests are broken (lots of updates to s3_task_handler since tests were run), so they need to be fixed. (different initialization, changed method names and slightly altered return values.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26848,54,JIRA.13112850.1509220003000.89246.1509222720427@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13112850.1509220003000@Atlassian.JIRA,,,2017-10-28 13:32:00-07,"[jira] [Commented] (AIRFLOW-1763) S3 Task Handler unit tests not
 running","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1763?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16223726#comment-16223726 ] 

Andy Hadjigeorgiou commented on AIRFLOW-1763:
---------------------------------------------

PR for this can be found here: https://github.com/apache/incubator-airflow/pull/2732

> S3 Task Handler unit tests not running
> --------------------------------------
>
>                 Key: AIRFLOW-1763
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1763
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>
> S3 Task Handler tests are not being run, because the folder isn''t being discovered due to it not being a python package. Furthermore, these tests are broken (lots of updates to s3_task_handler since tests were run), so they need to be fixed. (different initialization, changed method names and slightly altered return values.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26849,54,JIRA.13089723.1500934036000.89904.1509240543116@Atlassian.JIRA,2358,yonghwee (JIRA),JIRA.13089723.1500934036000@Atlassian.JIRA,,,2017-10-28 18:29:03-07,"[jira] [Assigned] (AIRFLOW-1449) D3 elements on the dashboard have
 absolute links which make it difficult to use a proxy","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1449?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

yonghwee reassigned AIRFLOW-1449:
---------------------------------

    Assignee: yonghwee

> D3 elements on the dashboard have absolute links which make it difficult to use a proxy
> ---------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1449
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1449
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: Airflow 1.8
>            Reporter: Adarsh Kyadige
>            Assignee: yonghwee
>            Priority: Minor
>
> For example, in dags_list.html, line 279, window.location is being set to an absolute url with an on-click event listener, which causes issues when Airflow dashboard is exposed using a proxy server with a rewritten URL. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26850,54,JIRA.13089723.1500934036000.89906.1509240543138@Atlassian.JIRA,2358,yonghwee (JIRA),JIRA.13089723.1500934036000@Atlassian.JIRA,,,2017-10-28 18:29:03-07,"[jira] [Assigned] (AIRFLOW-1449) D3 elements on the dashboard have
 absolute links which make it difficult to use a proxy","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1449?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

yonghwee reassigned AIRFLOW-1449:
---------------------------------

    Assignee:     (was: yonghwee)

> D3 elements on the dashboard have absolute links which make it difficult to use a proxy
> ---------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1449
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1449
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: Airflow 1.8
>            Reporter: Adarsh Kyadige
>            Priority: Minor
>
> For example, in dags_list.html, line 279, window.location is being set to an absolute url with an on-click event listener, which causes issues when Airflow dashboard is exposed using a proxy server with a rewritten URL. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26851,54,JIRA.13112766.1509171008000.91709.1509287220155@Atlassian.JIRA,2362,Nathan McIntyre (JIRA),JIRA.13112766.1509171008000@Atlassian.JIRA,,,2017-10-29 07:27:00-07,"[jira] [Updated] (AIRFLOW-1762) Use key_file in
 SSHHook.create_tunnel","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1762?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Nathan McIntyre updated AIRFLOW-1762:
-------------------------------------
    Summary: Use key_file in SSHHook.create_tunnel  (was: SSHHook neglects key_file in create_tunnel())

> Use key_file in SSHHook.create_tunnel
> -------------------------------------
>
>                 Key: AIRFLOW-1762
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1762
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.8, 1.9.0
>            Reporter: Nathan McIntyre
>            Assignee: Nathan McIntyre
>              Labels: patch
>
> In contrib/hooks/ssh_hook.py, the ssh command created by the create_tunnel() method does not use the key_file attribute. This prevents the creation of tunnels where a key file is required. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
1103826,215,20040605011711.516AC8421D@bugzilla.spamassassin.org,36968,bugzilla-daemon,NULL,,,2004-06-04 18:17:11-07,[Bug 3474] spamd looks up groups incorrectly,"http://bugzilla.spamassassin.org/show_bug.cgi?id=3474





------- Additional Comments From jm@jmason.org  2004-06-04 18:17 -------
yeesh!  how did that get in there ;)  +1



------- You are receiving this mail because: -------
You are the assignee for the bug, or are watching the assignee.

",f
26852,54,JIRA.13112766.1509171008000.91729.1509287762645@Atlassian.JIRA,2362,Nathan McIntyre (JIRA),JIRA.13112766.1509171008000@Atlassian.JIRA,,,2017-10-29 07:36:02-07,[jira] [Updated] (AIRFLOW-1762) Use key_file in create_tunnel(),"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1762?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Nathan McIntyre updated AIRFLOW-1762:
-------------------------------------
    Summary: Use key_file in create_tunnel()  (was: Use key_file in SSHHook.create_tunnel)

> Use key_file in create_tunnel()
> -------------------------------
>
>                 Key: AIRFLOW-1762
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1762
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.8, 1.9.0
>            Reporter: Nathan McIntyre
>            Assignee: Nathan McIntyre
>              Labels: patch
>
> In contrib/hooks/ssh_hook.py, the ssh command created by the create_tunnel() method does not use the key_file attribute. This prevents the creation of tunnels where a key file is required. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26853,54,JIRA.13111829.1508886107000.92716.1509306720173@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-29 12:52:00-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16224170#comment-16224170 ] 

Andy Hadjigeorgiou commented on AIRFLOW-1756:
---------------------------------------------

Fix for tests here: https://github.com/andyxhadji/incubator-airflow/tree/AIRFLOW-1763

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26854,54,JIRA.13111829.1508886107000.92723.1509306780153@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-29 12:53:00-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16224171#comment-16224171 ] 

Andy Hadjigeorgiou commented on AIRFLOW-1756:
---------------------------------------------

Sorry - PR here: https://github.com/apache/incubator-airflow/pull/2732

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26855,54,JIRA.13111829.1508886107000.92727.1509306840147@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-10-29 12:54:00-07,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16224172#comment-16224172 ] 

Andy Hadjigeorgiou commented on AIRFLOW-1756:
---------------------------------------------

Note - PR is only an update to fix the tests, not fix to the logging issue.

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26856,54,JIRA.13112995.1509361230000.96486.1509361260108@Atlassian.JIRA,1940,Niels Zeilemaker (JIRA),JIRA.13112995.1509361230000@Atlassian.JIRA,,,2017-10-30 04:01:00-07,"[jira] [Created] (AIRFLOW-1764) Web Interface should not use
 experimental api","Niels Zeilemaker created AIRFLOW-1764:
-----------------------------------------

             Summary: Web Interface should not use experimental api
                 Key: AIRFLOW-1764
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1764
             Project: Apache Airflow
          Issue Type: Bug
          Components: api
            Reporter: Niels Zeilemaker
            Assignee: Niels Zeilemaker
            Priority: Minor






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26857,54,JIRA.13112995.1509361230000.96499.1509361320391@Atlassian.JIRA,1940,Niels Zeilemaker (JIRA),JIRA.13112995.1509361230000@Atlassian.JIRA,,,2017-10-30 04:02:00-07,"[jira] [Updated] (AIRFLOW-1764) Web Interface should not use
 experimental api","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1764?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Niels Zeilemaker updated AIRFLOW-1764:
--------------------------------------
    Description: The web interface should not use the experimental api as the authentication options differ between the two. This means that the latest_runs call should be moved into the web interface.

> Web Interface should not use experimental api
> ---------------------------------------------
>
>                 Key: AIRFLOW-1764
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1764
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>            Priority: Minor
>
> The web interface should not use the experimental api as the authentication options differ between the two. This means that the latest_runs call should be moved into the web interface.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26858,54,JIRA.13110554.1508379412000.101508.1509384660706@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13110554.1508379412000@Atlassian.JIRA,,,2017-10-30 10:31:00-07,"[jira] [Commented] (AIRFLOW-1734) Sqoop Operator contains logic
 errors & needs options to pass more sqoop options","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1734?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225379#comment-16225379 ] 

Ace Haidrey commented on AIRFLOW-1734:
--------------------------------------

Awesome, thanks [~bolke]

> Sqoop Operator contains logic errors & needs options to pass more sqoop options
> -------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1734
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1734
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Ace Haidrey
>            Assignee: Ace Haidrey
>              Labels: patch
>             Fix For: 1.9.0
>
>
> After taking a look at the Sqoop Operator I have found a number of errors with it. I will describe them in depth on the PR I will make, but it includes assign wrong parameters, not hiding the users raw password in the logs (though a method to do that had been created in the hook, wrong if conditions, and the inability to add other sqoop options that would make sense to have the option to pass those.
> I have made some remarks in past commits about it too in the case I''m misinterpreting but regardless there are definitely errors.
> https://github.com/apache/incubator-airflow/pull/2177#pullrequestreview-70046796
> Here is my PR with change by change explanation:
> https://github.com/apache/incubator-airflow/pull/2703/files#diff-8e77f042c2e060bbfd60828431a91e9bL131



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26859,54,JIRA.13110016.1508253770000.102075.1509386160635@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-30 10:56:00-07,[jira] [Commented] (AIRFLOW-1723) Support sendgrid in email backend,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225430#comment-16225430 ] 

ASF subversion and git services commented on AIRFLOW-1723:
----------------------------------------------------------

Commit 574e1c63d9f818be07978e5deda413bf50b6c667 in incubator-airflow''s branch refs/heads/master from [~fenglu]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=574e1c6 ]

[AIRFLOW-1723] Make sendgrid a plugin

Closes #2727 from fenglu-g/master


> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26860,54,JIRA.13110016.1508253770000.102103.1509386221160@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13110016.1508253770000@Atlassian.JIRA,,,2017-10-30 10:57:01-07,[jira] [Resolved] (AIRFLOW-1723) Support sendgrid in email backend,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1723?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1723.
--------------------------------------
    Resolution: Fixed

> Support sendgrid in email backend 
> ----------------------------------
>
>                 Key: AIRFLOW-1723
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1723
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: utils
>    Affects Versions: 1.9.0
>            Reporter: Feng Lu
>            Assignee: Feng Lu
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Current airflow email backend only supports SMTP, this PR aims to extend email backend and integrate with sendgrid. The airflow config file is also updated to include a [sendgrid] section where user can specify api_key and mail_from. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26861,54,JIRA.13113100.1509386655000.102230.1509386700382@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-10-30 11:05:00-07,"[jira] [Created] (AIRFLOW-1765) Default API auth backed should deny
 all.","Ash Berlin-Taylor created AIRFLOW-1765:
------------------------------------------

             Summary: Default API auth backed should deny all.
                 Key: AIRFLOW-1765
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
             Project: Apache Airflow
          Issue Type: Bug
          Components: api, authentication
    Affects Versions: 1.8.2
            Reporter: Ash Berlin-Taylor
            Priority: Critical
             Fix For: 1.9.0


It has been discovered that the experimental API in the default configuration is not protected behind any authentication.

This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26862,54,JIRA.13113100.1509386655000.102246.1509386760140@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-10-30 11:06:00-07,"[jira] [Commented] (AIRFLOW-1765) Default API auth backed should
 deny all.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225454#comment-16225454 ] 

Ash Berlin-Taylor commented on AIRFLOW-1765:
--------------------------------------------

The /dags page needs to not use the experimental API before we can deny by default.

> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>            Priority: Critical
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
1103827,215,20040605011843.8912483E3E@bugzilla.spamassassin.org,36968,bugzilla-daemon,NULL,,,2004-06-04 18:18:43-07,[Bug 3442] [review] flock locking creates and doesn''t clean up lock files,"http://bugzilla.spamassassin.org/show_bug.cgi?id=3442





------- Additional Comments From jm@jmason.org  2004-06-04 18:18 -------
still looking for name suggestions btw ;)



------- You are receiving this mail because: -------
You are the assignee for the bug, or are watching the assignee.

",f
26863,54,JIRA.13109034.1507839702000.102766.1509388562661@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109034.1507839702000@Atlassian.JIRA,,,2017-10-30 11:36:02-07,"[jira] [Commented] (AIRFLOW-1711) Ldap Attributes not always a
 ""list"" part 2","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1711?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225508#comment-16225508 ] 

ASF subversion and git services commented on AIRFLOW-1711:
----------------------------------------------------------

Commit abcf1d584c66ab4f0a4c8c2c56c74104d9a50903 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=abcf1d5 ]

[AIRFLOW-1711] Use ldap3 dict for group membership

Certain schemas for group membership return a
string
instead of a list. Instead of using a check we now
use the entries API from ldap3.

Closes #2731 from bolkedebruin/AIRFLOW-1711


> Ldap Attributes not always a ""list"" part 2
> ------------------------------------------
>
>                 Key: AIRFLOW-1711
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1711
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.7.1
>         Environment: Linux + Active Directory
>            Reporter: Steve Jacobs
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> in the LDAP auth module
> `group_contains_user` checks for `resp[''attributes''].get(user_name_attr)[0] == username`
> Some Ldaps apparently have this as a simple string
> `resp[''attributes''].get(user_name_attr) == username` 
> also should be checked. 
> But really a test should be done to see if the return is a ''list'' and perform the check differently. If its not a list, python will check both arguments and exit with an error. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26864,54,JIRA.13109034.1507839702000.102764.1509388561960@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109034.1507839702000@Atlassian.JIRA,,,2017-10-30 11:36:01-07,"[jira] [Commented] (AIRFLOW-1711) Ldap Attributes not always a
 ""list"" part 2","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1711?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225507#comment-16225507 ] 

ASF subversion and git services commented on AIRFLOW-1711:
----------------------------------------------------------

Commit abcf1d584c66ab4f0a4c8c2c56c74104d9a50903 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=abcf1d5 ]

[AIRFLOW-1711] Use ldap3 dict for group membership

Certain schemas for group membership return a
string
instead of a list. Instead of using a check we now
use the entries API from ldap3.

Closes #2731 from bolkedebruin/AIRFLOW-1711


> Ldap Attributes not always a ""list"" part 2
> ------------------------------------------
>
>                 Key: AIRFLOW-1711
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1711
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.7.1
>         Environment: Linux + Active Directory
>            Reporter: Steve Jacobs
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> in the LDAP auth module
> `group_contains_user` checks for `resp[''attributes''].get(user_name_attr)[0] == username`
> Some Ldaps apparently have this as a simple string
> `resp[''attributes''].get(user_name_attr) == username` 
> also should be checked. 
> But really a test should be done to see if the return is a ''list'' and perform the check differently. If its not a list, python will check both arguments and exit with an error. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26865,54,JIRA.13109034.1507839702000.102770.1509388562712@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109034.1507839702000@Atlassian.JIRA,,,2017-10-30 11:36:02-07,"[jira] [Commented] (AIRFLOW-1711) Ldap Attributes not always a
 ""list"" part 2","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1711?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225509#comment-16225509 ] 

ASF subversion and git services commented on AIRFLOW-1711:
----------------------------------------------------------

Commit 40a936b67e8449bfb5ba67507cc4a774d8991b51 in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=40a936b ]

[AIRFLOW-1711] Use ldap3 dict for group membership

Certain schemas for group membership return a
string
instead of a list. Instead of using a check we now
use the entries API from ldap3.

Closes #2731 from bolkedebruin/AIRFLOW-1711

(cherry picked from commit abcf1d584c66ab4f0a4c8c2c56c74104d9a50903)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Ldap Attributes not always a ""list"" part 2
> ------------------------------------------
>
>                 Key: AIRFLOW-1711
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1711
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.7.1
>         Environment: Linux + Active Directory
>            Reporter: Steve Jacobs
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> in the LDAP auth module
> `group_contains_user` checks for `resp[''attributes''].get(user_name_attr)[0] == username`
> Some Ldaps apparently have this as a simple string
> `resp[''attributes''].get(user_name_attr) == username` 
> also should be checked. 
> But really a test should be done to see if the return is a ''list'' and perform the check differently. If its not a list, python will check both arguments and exit with an error. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26866,54,JIRA.13109034.1507839702000.102775.1509388562802@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109034.1507839702000@Atlassian.JIRA,,,2017-10-30 11:36:02-07,"[jira] [Commented] (AIRFLOW-1711) Ldap Attributes not always a
 ""list"" part 2","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1711?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225511#comment-16225511 ] 

ASF subversion and git services commented on AIRFLOW-1711:
----------------------------------------------------------

Commit 40a936b67e8449bfb5ba67507cc4a774d8991b51 in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=40a936b ]

[AIRFLOW-1711] Use ldap3 dict for group membership

Certain schemas for group membership return a
string
instead of a list. Instead of using a check we now
use the entries API from ldap3.

Closes #2731 from bolkedebruin/AIRFLOW-1711

(cherry picked from commit abcf1d584c66ab4f0a4c8c2c56c74104d9a50903)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Ldap Attributes not always a ""list"" part 2
> ------------------------------------------
>
>                 Key: AIRFLOW-1711
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1711
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.7.1
>         Environment: Linux + Active Directory
>            Reporter: Steve Jacobs
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> in the LDAP auth module
> `group_contains_user` checks for `resp[''attributes''].get(user_name_attr)[0] == username`
> Some Ldaps apparently have this as a simple string
> `resp[''attributes''].get(user_name_attr) == username` 
> also should be checked. 
> But really a test should be done to see if the return is a ''list'' and perform the check differently. If its not a list, python will check both arguments and exit with an error. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
926024,195,153152466906.8618.857130179881809622.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-13 16:31:09-07,"[GitHub] rdhabalia removed a comment on issue #2159: fix: conflicting
 jackson transitive dependency for worker","rdhabalia removed a comment on issue #2159: fix: conflicting jackson transitive dependency for worker
URL: https://github.com/apache/incubator-pulsar/pull/2159#issuecomment-404978879
 
 
   retest this please

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
26867,54,JIRA.13109034.1507839702000.102781.1509388562861@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13109034.1507839702000@Atlassian.JIRA,,,2017-10-30 11:36:02-07,"[jira] [Resolved] (AIRFLOW-1711) Ldap Attributes not always a
 ""list"" part 2","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1711?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1711.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2731
[https://github.com/apache/incubator-airflow/pull/2731]

> Ldap Attributes not always a ""list"" part 2
> ------------------------------------------
>
>                 Key: AIRFLOW-1711
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1711
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.7.1
>         Environment: Linux + Active Directory
>            Reporter: Steve Jacobs
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> in the LDAP auth module
> `group_contains_user` checks for `resp[''attributes''].get(user_name_attr)[0] == username`
> Some Ldaps apparently have this as a simple string
> `resp[''attributes''].get(user_name_attr) == username` 
> also should be checked. 
> But really a test should be done to see if the return is a ''list'' and perform the check differently. If its not a list, python will check both arguments and exit with an error. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26868,54,JIRA.13112995.1509361230000.102857.1509389102113@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13112995.1509361230000@Atlassian.JIRA,,,2017-10-30 11:45:02-07,"[jira] [Updated] (AIRFLOW-1764) Web Interface should not use
 experimental api","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1764?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1764:
-------------------------------------
    Fix Version/s: 1.9.0

> Web Interface should not use experimental api
> ---------------------------------------------
>
>                 Key: AIRFLOW-1764
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1764
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> The web interface should not use the experimental api as the authentication options differ between the two. This means that the latest_runs call should be moved into the web interface.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26869,54,JIRA.13112995.1509361230000.102853.1509389102062@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13112995.1509361230000@Atlassian.JIRA,,,2017-10-30 11:45:02-07,"[jira] [Updated] (AIRFLOW-1764) Web Interface should not use
 experimental api","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1764?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1764:
-------------------------------------
    Priority: Blocker  (was: Minor)

> Web Interface should not use experimental api
> ---------------------------------------------
>
>                 Key: AIRFLOW-1764
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1764
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> The web interface should not use the experimental api as the authentication options differ between the two. This means that the latest_runs call should be moved into the web interface.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26870,54,JIRA.13113100.1509386655000.102851.1509389102043@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-10-30 11:45:02-07,"[jira] [Updated] (AIRFLOW-1765) Default API auth backed should deny
 all.","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1765:
-------------------------------------
    Priority: Blocker  (was: Critical)

> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>            Priority: Blocker
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26871,54,JIRA.13113100.1509386655000.103164.1509389580129@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-10-30 11:53:00-07,"[jira] [Commented] (AIRFLOW-1765) Default API auth backed should
 deny all.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225539#comment-16225539 ] 

Bolke de Bruin commented on AIRFLOW-1765:
-----------------------------------------

Airflow, out of the box, does not do authentication. Whether that is good or not remains to be seen, but we should only adjust the API.

> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>            Priority: Blocker
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
574963,135,Pine.LNX.4.61.0512152351510.7109@kongo,13012,Henri Yandell,3FF9CFA0-6531-4B68-809A-9DCC7524E0DC@Sun.COM,,,2005-12-15 20:53:12-08,Re: Subversion repository,"
Sorry for the lack of reply until now, I''ve only just got back online 
since Brian mentioned this to me.

How does Saturday, 20:00 US/Eastern time sound?

Hen
[ASF SVN gopher]

On Wed, 14 Dec 2005, Craig L Russell wrote:

> Hey,
>
> We''re  going to be moving the repo from incubator to db so the url for 
> checkout and commit will change. We (Apache infra) are planning on using the 
> svn move command so all the history will be preserved.
>
> Once the change takes place the old repo won''t work any more. You can check 
> out from the new repo. If you have changes in an active workspace you will 
> need to svn switch it to the new repository. Or perhaps better, check in 
> before the move.
>
> I''d like to plan for the move to happen over this coming weekend.
>
> If this message affects you and it is incomprehensible, please let me know.
>
> Thanks,
>
> Craig
>
> Craig Russell
> Architect, Sun Java Enterprise System http://java.sun.com/products/jdo
> 408 276-5638 mailto:Craig.Russell@sun.com
> P.S. A good JDO? O, Gasp!
>
>

",f
26872,54,JIRA.13113100.1509386655000.103222.1509389882458@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-10-30 11:58:02-07,"[jira] [Updated] (AIRFLOW-1765) Default API auth backed should deny
 all.","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1765:
------------------------------------
    Issue Type: Improvement  (was: Bug)

> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>            Priority: Blocker
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26873,54,JIRA.13108844.1507799461000.103350.1509390480700@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13108844.1507799461000@Atlassian.JIRA,,,2017-10-30 12:08:00-07,"[jira] [Commented] (AIRFLOW-1706) Scheduler is failed on startup
 with MS SQL Server as backend","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1706?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
225568#comment-16225568 ]=20

ASF subversion and git services commented on AIRFLOW-1706:
----------------------------------------------------------

Commit 9e209bf301e5f16da6535e3acd828cab9c3e1bb7 in incubator-airflow''s bran=
ch refs/heads/v1-9-test from k.privezentsev
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D9e2=
09bf ]

[AIRFLOW-1706] Fix query error for MSSQL backend

MSSQL doesn''t support key word ''is'' as synonym for
''=3D''

Closes #2733 from
patsak/fix/illegal_query_for_mssql

(cherry picked from commit c800632bb40882d06344be1ebe0022e0a50ff121)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Scheduler is failed on startup with MS SQL Server as backend
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1706
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1706
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0
>            Reporter: Konstantin Privezentsev
>            Priority: Minor
>              Labels: newbie, patch
>             Fix For: 1.9.0
>
>         Attachments: 0001-fix-query-error-for-mssql-backend.patch
>
>
> Actual for commit - 21e94c7d1594c5e0
> Scheduler log:
> {noformat}
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1497}} INFO - =
Starting the scheduler
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1510}} INFO - =
Processing files using up to 2 processes at a time
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1511}} INFO - =
Running execute loop for -1 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1512}} INFO - =
Processing each file at most -1 times
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1513}} INFO - =
Process each file at most once every 0 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1514}} INFO - =
Checking for new files in /opt/airflow/dags every 300 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1517}} INFO - =
Searching for files in /opt/airflow/dags
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1519}} INFO - =
There are 1 files in /opt/airflow/dags
> airflow_scheduler_1  | [2017-10-12 08:45:49,628] {{jobs.py:1580}} INFO - =
Resetting orphaned tasks for active dag runs
> airflow_scheduler_1  | [2017-10-12 08:45:49,634] {{jobs.py:1538}} INFO - =
Exited execute loop
> airflow_scheduler_1  | Traceback (most recent call last):
> airflow_scheduler_1  |   File ""/usr/local/bin/airflow"", line 27, in <modu=
le>
> airflow_scheduler_1  |     args.func(args)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/bin/cli.py"", line 828, in scheduler
> airflow_scheduler_1  |     job.run()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 197, in run
> airflow_scheduler_1  |     self._execute()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 1536, in _execute
> airflow_scheduler_1  |     self._execute_helper(processor_manager)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 1581, in _execute_helper
> airflow_scheduler_1  |     self.reset_state_for_orphaned_tasks(session=3D=
session)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/utils/db.py"", line 50, in wrapper
> airflow_scheduler_1  |     result =3D func(*args, **kwargs)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 246, in reset_state_for_orphaned_tasks
> airflow_scheduler_1  |     TI.state.in_(resettable_states))).all()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2703, in all
> airflow_scheduler_1  |     return list(self)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2855, in __iter__
> airflow_scheduler_1  |     return self._execute_and_instances(context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2878, in _execute_and_instances
> airflow_scheduler_1  |     result =3D conn.execute(querycontext.statement=
, self._params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 945, in execute
> airflow_scheduler_1  |     return meth(self, multiparams, params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/sql/elements.py"", line 263, in _execute_on_connection
> airflow_scheduler_1  |     return connection._execute_clauseelement(self,=
 multiparams, params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1053, in _execute_clauseelement
> airflow_scheduler_1  |     compiled_sql, distilled_params
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1189, in _execute_context
> airflow_scheduler_1  |     context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1402, in _handle_dbapi_exception
> airflow_scheduler_1  |     exc_info
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/util/compat.py"", line 203, in raise_from_cause
> airflow_scheduler_1  |     reraise(type(exception), exception, tb=3Dexc_t=
b, cause=3Dcause)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1182, in _execute_context
> airflow_scheduler_1  |     context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/default.py"", line 470, in do_execute
> airflow_scheduler_1  |     cursor.execute(statement, parameters)
> airflow_scheduler_1  | sqlalchemy.exc.ProgrammingError: (pyodbc.Programmi=
ngError) (''42000'', ""[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL =
Server]Incorrect syntax near ''0''. (102) (SQLExecDirectW)"") [SQL: u''SELECT t=
ask_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task=
_instance_dag_id, task_instance.execution_date AS task_instance_execution_d=
ate, task_instance.start_date AS task_instance_start_date, task_instance.en=
d_date AS task_instance_end_date, task_instance.duration AS task_instance_d=
uration, task_instance.state AS task_instance_state, task_instance.try_numb=
er AS task_instance_try_number, task_instance.max_tries AS task_instance_ma=
x_tries, task_instance.hostname AS task_instance_hostname, task_instance.un=
ixname AS task_instance_unixname, task_instance.job_id AS task_instance_job=
_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_=
instance_queue, task_instance.priority_weight AS task_instance_priority_wei=
ght, task_instance.operator AS task_instance_operator, task_instance.queued=
_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid =
\nFROM task_instance JOIN dag_run ON task_instance.dag_id =3D dag_run.dag_i=
d AND task_instance.execution_date =3D dag_run.execution_date \nWHERE dag_r=
un.state =3D ? AND dag_run.external_trigger IS 0 AND dag_run.run_id NOT LIK=
E ? AND task_instance.state IN (?, ?)''] [parameters: (u''running'', u''backfil=
l_%%%%'', u''scheduled'', u''queued'')]
> {noformat}
> Patch in attachment



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26874,54,JIRA.13108844.1507799461000.103353.1509390480720@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13108844.1507799461000@Atlassian.JIRA,,,2017-10-30 12:08:00-07,"[jira] [Resolved] (AIRFLOW-1706) Scheduler is failed on startup
 with MS SQL Server as backend","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1706?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1706.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2733
[https://github.com/apache/incubator-airflow/pull/2733]

> Scheduler is failed on startup with MS SQL Server as backend
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1706
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1706
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0
>            Reporter: Konstantin Privezentsev
>            Priority: Minor
>              Labels: newbie, patch
>             Fix For: 1.9.0
>
>         Attachments: 0001-fix-query-error-for-mssql-backend.patch
>
>
> Actual for commit - 21e94c7d1594c5e0
> Scheduler log:
> {noformat}
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1497}} INFO - =
Starting the scheduler
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1510}} INFO - =
Processing files using up to 2 processes at a time
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1511}} INFO - =
Running execute loop for -1 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1512}} INFO - =
Processing each file at most -1 times
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1513}} INFO - =
Process each file at most once every 0 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1514}} INFO - =
Checking for new files in /opt/airflow/dags every 300 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1517}} INFO - =
Searching for files in /opt/airflow/dags
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1519}} INFO - =
There are 1 files in /opt/airflow/dags
> airflow_scheduler_1  | [2017-10-12 08:45:49,628] {{jobs.py:1580}} INFO - =
Resetting orphaned tasks for active dag runs
> airflow_scheduler_1  | [2017-10-12 08:45:49,634] {{jobs.py:1538}} INFO - =
Exited execute loop
> airflow_scheduler_1  | Traceback (most recent call last):
> airflow_scheduler_1  |   File ""/usr/local/bin/airflow"", line 27, in <modu=
le>
> airflow_scheduler_1  |     args.func(args)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/bin/cli.py"", line 828, in scheduler
> airflow_scheduler_1  |     job.run()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 197, in run
> airflow_scheduler_1  |     self._execute()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 1536, in _execute
> airflow_scheduler_1  |     self._execute_helper(processor_manager)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 1581, in _execute_helper
> airflow_scheduler_1  |     self.reset_state_for_orphaned_tasks(session=3D=
session)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/utils/db.py"", line 50, in wrapper
> airflow_scheduler_1  |     result =3D func(*args, **kwargs)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 246, in reset_state_for_orphaned_tasks
> airflow_scheduler_1  |     TI.state.in_(resettable_states))).all()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2703, in all
> airflow_scheduler_1  |     return list(self)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2855, in __iter__
> airflow_scheduler_1  |     return self._execute_and_instances(context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2878, in _execute_and_instances
> airflow_scheduler_1  |     result =3D conn.execute(querycontext.statement=
, self._params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 945, in execute
> airflow_scheduler_1  |     return meth(self, multiparams, params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/sql/elements.py"", line 263, in _execute_on_connection
> airflow_scheduler_1  |     return connection._execute_clauseelement(self,=
 multiparams, params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1053, in _execute_clauseelement
> airflow_scheduler_1  |     compiled_sql, distilled_params
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1189, in _execute_context
> airflow_scheduler_1  |     context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1402, in _handle_dbapi_exception
> airflow_scheduler_1  |     exc_info
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/util/compat.py"", line 203, in raise_from_cause
> airflow_scheduler_1  |     reraise(type(exception), exception, tb=3Dexc_t=
b, cause=3Dcause)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1182, in _execute_context
> airflow_scheduler_1  |     context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/default.py"", line 470, in do_execute
> airflow_scheduler_1  |     cursor.execute(statement, parameters)
> airflow_scheduler_1  | sqlalchemy.exc.ProgrammingError: (pyodbc.Programmi=
ngError) (''42000'', ""[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL =
Server]Incorrect syntax near ''0''. (102) (SQLExecDirectW)"") [SQL: u''SELECT t=
ask_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task=
_instance_dag_id, task_instance.execution_date AS task_instance_execution_d=
ate, task_instance.start_date AS task_instance_start_date, task_instance.en=
d_date AS task_instance_end_date, task_instance.duration AS task_instance_d=
uration, task_instance.state AS task_instance_state, task_instance.try_numb=
er AS task_instance_try_number, task_instance.max_tries AS task_instance_ma=
x_tries, task_instance.hostname AS task_instance_hostname, task_instance.un=
ixname AS task_instance_unixname, task_instance.job_id AS task_instance_job=
_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_=
instance_queue, task_instance.priority_weight AS task_instance_priority_wei=
ght, task_instance.operator AS task_instance_operator, task_instance.queued=
_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid =
\nFROM task_instance JOIN dag_run ON task_instance.dag_id =3D dag_run.dag_i=
d AND task_instance.execution_date =3D dag_run.execution_date \nWHERE dag_r=
un.state =3D ? AND dag_run.external_trigger IS 0 AND dag_run.run_id NOT LIK=
E ? AND task_instance.state IN (?, ?)''] [parameters: (u''running'', u''backfil=
l_%%%%'', u''scheduled'', u''queued'')]
> {noformat}
> Patch in attachment



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26875,54,JIRA.13108844.1507799461000.103348.1509390480686@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13108844.1507799461000@Atlassian.JIRA,,,2017-10-30 12:08:00-07,"[jira] [Commented] (AIRFLOW-1706) Scheduler is failed on startup
 with MS SQL Server as backend","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1706?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
225567#comment-16225567 ]=20

ASF subversion and git services commented on AIRFLOW-1706:
----------------------------------------------------------

Commit c800632bb40882d06344be1ebe0022e0a50ff121 in incubator-airflow''s bran=
ch refs/heads/master from k.privezentsev
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3Dc80=
0632 ]

[AIRFLOW-1706] Fix query error for MSSQL backend

MSSQL doesn''t support key word ''is'' as synonym for
''=3D''

Closes #2733 from
patsak/fix/illegal_query_for_mssql


> Scheduler is failed on startup with MS SQL Server as backend
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1706
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1706
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0
>            Reporter: Konstantin Privezentsev
>            Priority: Minor
>              Labels: newbie, patch
>             Fix For: 1.9.0
>
>         Attachments: 0001-fix-query-error-for-mssql-backend.patch
>
>
> Actual for commit - 21e94c7d1594c5e0
> Scheduler log:
> {noformat}
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1497}} INFO - =
Starting the scheduler
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1510}} INFO - =
Processing files using up to 2 processes at a time
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1511}} INFO - =
Running execute loop for -1 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,554] {{jobs.py:1512}} INFO - =
Processing each file at most -1 times
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1513}} INFO - =
Process each file at most once every 0 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1514}} INFO - =
Checking for new files in /opt/airflow/dags every 300 seconds
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1517}} INFO - =
Searching for files in /opt/airflow/dags
> airflow_scheduler_1  | [2017-10-12 08:45:49,555] {{jobs.py:1519}} INFO - =
There are 1 files in /opt/airflow/dags
> airflow_scheduler_1  | [2017-10-12 08:45:49,628] {{jobs.py:1580}} INFO - =
Resetting orphaned tasks for active dag runs
> airflow_scheduler_1  | [2017-10-12 08:45:49,634] {{jobs.py:1538}} INFO - =
Exited execute loop
> airflow_scheduler_1  | Traceback (most recent call last):
> airflow_scheduler_1  |   File ""/usr/local/bin/airflow"", line 27, in <modu=
le>
> airflow_scheduler_1  |     args.func(args)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/bin/cli.py"", line 828, in scheduler
> airflow_scheduler_1  |     job.run()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 197, in run
> airflow_scheduler_1  |     self._execute()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 1536, in _execute
> airflow_scheduler_1  |     self._execute_helper(processor_manager)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 1581, in _execute_helper
> airflow_scheduler_1  |     self.reset_state_for_orphaned_tasks(session=3D=
session)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/utils/db.py"", line 50, in wrapper
> airflow_scheduler_1  |     result =3D func(*args, **kwargs)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/air=
flow/jobs.py"", line 246, in reset_state_for_orphaned_tasks
> airflow_scheduler_1  |     TI.state.in_(resettable_states))).all()
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2703, in all
> airflow_scheduler_1  |     return list(self)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2855, in __iter__
> airflow_scheduler_1  |     return self._execute_and_instances(context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/orm/query.py"", line 2878, in _execute_and_instances
> airflow_scheduler_1  |     result =3D conn.execute(querycontext.statement=
, self._params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 945, in execute
> airflow_scheduler_1  |     return meth(self, multiparams, params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/sql/elements.py"", line 263, in _execute_on_connection
> airflow_scheduler_1  |     return connection._execute_clauseelement(self,=
 multiparams, params)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1053, in _execute_clauseelement
> airflow_scheduler_1  |     compiled_sql, distilled_params
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1189, in _execute_context
> airflow_scheduler_1  |     context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1402, in _handle_dbapi_exception
> airflow_scheduler_1  |     exc_info
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/util/compat.py"", line 203, in raise_from_cause
> airflow_scheduler_1  |     reraise(type(exception), exception, tb=3Dexc_t=
b, cause=3Dcause)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/base.py"", line 1182, in _execute_context
> airflow_scheduler_1  |     context)
> airflow_scheduler_1  |   File ""/usr/local/lib/python2.7/dist-packages/sql=
alchemy/engine/default.py"", line 470, in do_execute
> airflow_scheduler_1  |     cursor.execute(statement, parameters)
> airflow_scheduler_1  | sqlalchemy.exc.ProgrammingError: (pyodbc.Programmi=
ngError) (''42000'', ""[42000] [Microsoft][ODBC Driver 13 for SQL Server][SQL =
Server]Incorrect syntax near ''0''. (102) (SQLExecDirectW)"") [SQL: u''SELECT t=
ask_instance.task_id AS task_instance_task_id, task_instance.dag_id AS task=
_instance_dag_id, task_instance.execution_date AS task_instance_execution_d=
ate, task_instance.start_date AS task_instance_start_date, task_instance.en=
d_date AS task_instance_end_date, task_instance.duration AS task_instance_d=
uration, task_instance.state AS task_instance_state, task_instance.try_numb=
er AS task_instance_try_number, task_instance.max_tries AS task_instance_ma=
x_tries, task_instance.hostname AS task_instance_hostname, task_instance.un=
ixname AS task_instance_unixname, task_instance.job_id AS task_instance_job=
_id, task_instance.pool AS task_instance_pool, task_instance.queue AS task_=
instance_queue, task_instance.priority_weight AS task_instance_priority_wei=
ght, task_instance.operator AS task_instance_operator, task_instance.queued=
_dttm AS task_instance_queued_dttm, task_instance.pid AS task_instance_pid =
\nFROM task_instance JOIN dag_run ON task_instance.dag_id =3D dag_run.dag_i=
d AND task_instance.execution_date =3D dag_run.execution_date \nWHERE dag_r=
un.state =3D ? AND dag_run.external_trigger IS 0 AND dag_run.run_id NOT LIK=
E ? AND task_instance.state IN (?, ?)''] [parameters: (u''running'', u''backfil=
l_%%%%'', u''scheduled'', u''queued'')]
> {noformat}
> Patch in attachment



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26876,54,JIRA.13113100.1509386655000.103901.1509392043224@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-10-30 12:34:03-07,"[jira] [Updated] (AIRFLOW-1765) Default API auth backed should deny
 all.","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1765:
-------------------------------------
    Priority: Major  (was: Blocker)

> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26877,54,JIRA.13112995.1509361230000.103899.1509392043207@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13112995.1509361230000@Atlassian.JIRA,,,2017-10-30 12:34:03-07,"[jira] [Updated] (AIRFLOW-1764) Web Interface should not use
 experimental api","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1764?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1764:
-------------------------------------
    Priority: Major  (was: Blocker)

> Web Interface should not use experimental api
> ---------------------------------------------
>
>                 Key: AIRFLOW-1764
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1764
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>             Fix For: 1.9.0
>
>
> The web interface should not use the experimental api as the authentication options differ between the two. This means that the latest_runs call should be moved into the web interface.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26878,54,JIRA.13107977.1507556485000.103920.1509392220325@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13107977.1507556485000@Atlassian.JIRA,,,2017-10-30 12:37:00-07,"[jira] [Commented] (AIRFLOW-1695) Redshift Hook using boto3 & AWS
 Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1695?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225614#comment-16225614 ] 

ASF subversion and git services commented on AIRFLOW-1695:
----------------------------------------------------------

Commit bfddae72411f7f7be67bc18d2b867ddc093d34f1 in incubator-airflow''s branch refs/heads/v1-9-test from [~andyxhadji]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=bfddae7 ]

[AIRFLOW-1695] Add RedshiftHook using boto3

Adds RedshiftHook class, allowing for management
of AWS Redshift
clusters and snapshots using boto3 library. Also
adds new test file and
unit tests for class methods.

Closes #2717 from andyxhadji/1695

(cherry picked from commit 4fb7a90b36ec1daf169a65aa4adf28a31b30fbc5)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Redshift Hook using boto3 & AWS Hook
> ------------------------------------
>
>                 Key: AIRFLOW-1695
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1695
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> I''d like to add AWS Redshift management capabilities working from the boto3 PR (https://github.com/apache/incubator-airflow/pull/2532). I propose creating a Redshift Hook (that extends AWS Hook class) that uses the boto3 client to manage clusters. Included will be the ability to image, shut-down, create clusters from images, and check cluster status.
> I''m currently working on this, will post PR soon.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
621361,146,JIRA.12821500.1429182573000.135668.1434981301650@Atlassian.JIRA,23736,Rajat Khandelwal (JIRA),JIRA.12821500.1429182573000@Atlassian.JIRA,,,2015-06-22 06:55:01-07,"[jira] [Updated] (LENS-513) add jar should be able to take regex
 path and should be able to add multiple jars","
     [ https://issues.apache.org/jira/browse/LENS-513?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Rajat Khandelwal updated LENS-513:
----------------------------------
    Status: Patch Available  (was: Reopened)

> add jar should be able to take regex path and should be able to add multiple jars
> ---------------------------------------------------------------------------------
>
>                 Key: LENS-513
>                 URL: https://issues.apache.org/jira/browse/LENS-513
>             Project: Apache Lens
>          Issue Type: Improvement
>          Components: client
>    Affects Versions: 2.2
>            Reporter: Rajat Khandelwal
>            Assignee: Yash Sharma
>              Labels: newbie
>             Fix For: 2.2
>
>         Attachments: LENS-513.16.patch, LENS-513.28.patch
>
>




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26879,54,JIRA.13107977.1507556485000.103924.1509392220358@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13107977.1507556485000@Atlassian.JIRA,,,2017-10-30 12:37:00-07,"[jira] [Resolved] (AIRFLOW-1695) Redshift Hook using boto3 & AWS
 Hook","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1695?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1695.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2717
[https://github.com/apache/incubator-airflow/pull/2717]

> Redshift Hook using boto3 & AWS Hook
> ------------------------------------
>
>                 Key: AIRFLOW-1695
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1695
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> I''d like to add AWS Redshift management capabilities working from the boto3 PR (https://github.com/apache/incubator-airflow/pull/2532). I propose creating a Redshift Hook (that extends AWS Hook class) that uses the boto3 client to manage clusters. Included will be the ability to image, shut-down, create clusters from images, and check cluster status.
> I''m currently working on this, will post PR soon.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26880,54,JIRA.13107977.1507556485000.103919.1509392220316@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13107977.1507556485000@Atlassian.JIRA,,,2017-10-30 12:37:00-07,"[jira] [Commented] (AIRFLOW-1695) Redshift Hook using boto3 & AWS
 Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1695?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225613#comment-16225613 ] 

ASF subversion and git services commented on AIRFLOW-1695:
----------------------------------------------------------

Commit 4fb7a90b36ec1daf169a65aa4adf28a31b30fbc5 in incubator-airflow''s branch refs/heads/master from [~andyxhadji]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4fb7a90 ]

[AIRFLOW-1695] Add RedshiftHook using boto3

Adds RedshiftHook class, allowing for management
of AWS Redshift
clusters and snapshots using boto3 library. Also
adds new test file and
unit tests for class methods.

Closes #2717 from andyxhadji/1695


> Redshift Hook using boto3 & AWS Hook
> ------------------------------------
>
>                 Key: AIRFLOW-1695
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1695
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> I''d like to add AWS Redshift management capabilities working from the boto3 PR (https://github.com/apache/incubator-airflow/pull/2532). I propose creating a Redshift Hook (that extends AWS Hook class) that uses the boto3 client to manage clusters. Included will be the ability to image, shut-down, create clusters from images, and check cluster status.
> I''m currently working on this, will post PR soon.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26881,54,JIRA.13057719.1490049741000.103954.1509392460903@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13057719.1490049741000@Atlassian.JIRA,,,2017-10-30 12:41:00-07,"[jira] [Commented] (AIRFLOW-1018) Scheduler DAG processes can not
 log to stdout","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1018?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225622#comment-16225622 ] 

ASF subversion and git services commented on AIRFLOW-1018:
----------------------------------------------------------

Commit 4ee4e474b835b4f5f557226ba01b8cdfeb7d0789 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4ee4e47 ]

[AIRFLOW-1018] Make processor use logging framework

Until now, the dga processor had its own logging
implementation,
making it hard to adjust for certain use cases
like working
in a container.

This patch moves everything to the standard
logging framework.

Closes #2728 from bolkedebruin/AIRFLOW-1018


> Scheduler DAG processes can not log to stdout
> ---------------------------------------------
>
>                 Key: AIRFLOW-1018
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1018
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>         Environment: Airflow 1.8.0
>            Reporter: Vincent Poulain
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. 
> Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container.
> When we specify /dev/stdout it raises:
> ""OSError: [Errno 20] Not a directory: ''/dev/stdout/2017-03-19''""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26882,54,JIRA.13057719.1490049741000.103960.1509392460982@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13057719.1490049741000@Atlassian.JIRA,,,2017-10-30 12:41:00-07,"[jira] [Commented] (AIRFLOW-1018) Scheduler DAG processes can not
 log to stdout","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1018?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225623#comment-16225623 ] 

ASF subversion and git services commented on AIRFLOW-1018:
----------------------------------------------------------

Commit 4ee4e474b835b4f5f557226ba01b8cdfeb7d0789 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4ee4e47 ]

[AIRFLOW-1018] Make processor use logging framework

Until now, the dga processor had its own logging
implementation,
making it hard to adjust for certain use cases
like working
in a container.

This patch moves everything to the standard
logging framework.

Closes #2728 from bolkedebruin/AIRFLOW-1018


> Scheduler DAG processes can not log to stdout
> ---------------------------------------------
>
>                 Key: AIRFLOW-1018
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1018
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>         Environment: Airflow 1.8.0
>            Reporter: Vincent Poulain
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. 
> Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container.
> When we specify /dev/stdout it raises:
> ""OSError: [Errno 20] Not a directory: ''/dev/stdout/2017-03-19''""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
926025,195,153152662504.24464.5756842120436545517.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-13 17:03:45-07,"[GitHub] sijie closed pull request #2160: Add numFunctionWorkers and
 externalServices to cluster spec","sijie closed pull request #2160: Add numFunctionWorkers and externalServices to cluster spec
URL: https://github.com/apache/incubator-pulsar/pull/2160
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won''t show otherwise due to GitHub magic):

diff --git a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsThreadRuntimeTest.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/CassandraContainer.java
similarity index 51%%%%
rename from tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsThreadRuntimeTest.java
rename to tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/CassandraContainer.java
index 5772ee313f..ebfa000764 100644
--- a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsThreadRuntimeTest.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/CassandraContainer.java
@@ -16,28 +16,33 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.pulsar.tests.integration.functions.runtime;
+package org.apache.pulsar.tests.containers;
 
 import lombok.extern.slf4j.Slf4j;
-import org.testcontainers.containers.Container.ExecResult;
-import org.testng.annotations.BeforeClass;
+import org.testcontainers.containers.wait.strategy.HostPortWaitStrategy;
 
 /**
- * Run the runtime test cases in thread mode.
+ * Cassandra Container.
  */
 @Slf4j
-public class PulsarFunctionsThreadRuntimeTest extends PulsarFunctionsRuntimeTest {
+public class CassandraContainer<SelfT extends ChaosContainer<SelfT>> extends ChaosContainer<SelfT> {
 
-    public PulsarFunctionsThreadRuntimeTest() {
-        super(RuntimeFactory.THREAD);
-    }
+    public static final String NAME = ""cassandra"";
+    public static final int PORT = 9042;
 
-    @BeforeClass
-    public void setupCluster() throws Exception {
-        super.setupCluster(RuntimeFactory.THREAD.toString());
-        pulsarCluster.startFunctionWorkersWithThreadContainerFactory(1);
-        ExecResult result = pulsarCluster.getAnyWorker().execCmd(""cat"", ""/pulsar/conf/functions_worker.yml"");
-        log.info(""Functions Worker Config : \n{}"", result.getStdout());
+    public CassandraContainer(String clusterName) {
+        super(clusterName, ""cassandra:3"");
     }
 
+    @Override
+    protected void configure() {
+        super.configure();
+        this.withNetworkAliases(NAME)
+            .withExposedPorts(PORT)
+            .withCreateContainerCmdModifier(createContainerCmd -> {
+                createContainerCmd.withHostName(NAME);
+                createContainerCmd.withName(clusterName + ""-"" + NAME);
+            })
+            .waitingFor(new HostPortWaitStrategy());
+    }
 }
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaContainer.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaContainer.java
new file mode 100644
index 0000000000..83b5e42ba0
--- /dev/null
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaContainer.java
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pulsar.tests.containers;
+
+import lombok.extern.slf4j.Slf4j;
+import org.testcontainers.containers.BindMode;
+import org.testcontainers.containers.wait.strategy.HostPortWaitStrategy;
+
+/**
+ * Cassandra Container.
+ */
+@Slf4j
+public class KafkaContainer<SelfT extends ChaosContainer<SelfT>> extends ChaosContainer<SelfT> {
+
+    public static final String NAME = ""kafka"";
+    public static final int INTERNAL_PORT = 9092;
+    public static final int PORT = 9093;
+
+    public KafkaContainer(String clusterName) {
+        super(clusterName, ""confluentinc/cp-kafka:4.1.1"");
+    }
+
+    @Override
+    protected void configure() {
+        super.configure();
+        this.withNetworkAliases(NAME)
+            .withExposedPorts(INTERNAL_PORT, PORT)
+            .withClasspathResourceMapping(
+                ""kafka-zookeeper.properties"", ""/zookeeper.properties"",
+                BindMode.READ_ONLY)
+            .withCommand(""sh"", ""-c"", ""zookeeper-server-start /zookeeper.properties & /etc/confluent/docker/run"")
+            .withEnv(""KAFKA_LISTENERS"",
+                ""INTERNAL://kafka:"" + INTERNAL_PORT + "",PLAINTEXT://"" + ""0.0.0.0"" + "":"" + PORT)
+            .withEnv(""KAFKA_ZOOKEEPER_CONNECT"", ""localhost:2181"")
+            .withEnv(""KAFKA_LISTENER_SECURITY_PROTOCOL_MAP"", ""INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT"")
+            .withEnv(""KAFKA_INTER_BROKER_LISTENER_NAME"", ""INTERNAL"")
+            .withEnv(""KAFKA_BROKER_ID"", ""1"")
+            .withEnv(""KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR"", ""1"")
+            .withEnv(""KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS"", ""1"")
+            .withEnv(""KAFKA_LOG_FLUSH_INTERVAL_MESSAGES"", Long.MAX_VALUE + """")
+            .withCreateContainerCmdModifier(createContainerCmd -> {
+                createContainerCmd.withHostName(NAME);
+                createContainerCmd.withName(clusterName + ""-"" + NAME);
+            })
+            .waitingFor(new HostPortWaitStrategy());
+    }
+}
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaProxyContainer.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaProxyContainer.java
new file mode 100644
index 0000000000..052db7e8b0
--- /dev/null
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaProxyContainer.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pulsar.tests.containers;
+
+import lombok.extern.slf4j.Slf4j;
+import org.testcontainers.containers.SocatContainer;
+import org.testcontainers.containers.wait.strategy.HostPortWaitStrategy;
+
+/**
+ * Cassandra Container.
+ */
+@Slf4j
+public class KafkaProxyContainer extends SocatContainer {
+
+    public static final String NAME = ""kafka-proxy"";
+
+    private final String clusterName;
+
+    public KafkaProxyContainer(String clusterName) {
+        super();
+        this.clusterName = clusterName;
+    }
+
+    @Override
+    protected void configure() {
+        super.configure();
+        this.withNetworkAliases(NAME)
+            .withTarget(KafkaContainer.PORT, KafkaContainer.NAME)
+            .withCreateContainerCmdModifier(createContainerCmd -> {
+                createContainerCmd.withHostName(NAME);
+                createContainerCmd.withName(clusterName + ""-"" + NAME);
+            })
+            .waitingFor(new HostPortWaitStrategy());
+    }
+}
diff --git a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsProcessRuntimeTest.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/FunctionRuntimeType.java
similarity index 50%%%%
rename from tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsProcessRuntimeTest.java
rename to tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/FunctionRuntimeType.java
index ebc6e9974d..a3efd312f4 100644
--- a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsProcessRuntimeTest.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/FunctionRuntimeType.java
@@ -16,26 +16,12 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.pulsar.tests.integration.functions.runtime;
-
-import lombok.extern.slf4j.Slf4j;
-import org.testcontainers.containers.Container;
-import org.testng.annotations.BeforeClass;
+package org.apache.pulsar.tests.topologies;
 
 /**
- * Run runtime tests in process mode.
+ * Runtime type to run functions.
  */
-@Slf4j
-public class PulsarFunctionsProcessRuntimeTest extends PulsarFunctionsRuntimeTest {
-    public PulsarFunctionsProcessRuntimeTest() {
-        super(RuntimeFactory.PROCESS);
-    }
-
-    @BeforeClass
-    public void setupCluster() throws Exception {
-        super.setupCluster(RuntimeFactory.PROCESS.toString());
-        pulsarCluster.startFunctionWorkersWithProcessContainerFactory(1);
-        Container.ExecResult result = pulsarCluster.getAnyWorker().execCmd(""cat"", ""/pulsar/conf/functions_worker.yml"");
-        log.info(""Functions Worker Config : \n{}"", result.getStdout());
-    }
+public enum FunctionRuntimeType {
+    PROCESS,
+    THREAD
 }
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarCluster.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarCluster.java
index d0c9eb42a3..6983a1c00c 100644
--- a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarCluster.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarCluster.java
@@ -21,20 +21,15 @@
 import static com.google.common.base.Preconditions.checkArgument;
 import static org.apache.pulsar.tests.containers.PulsarContainer.CS_PORT;
 
-import com.github.dockerjava.api.command.CreateContainerCmd;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 
-import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.CompletableFuture;
-import java.util.function.Consumer;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
 import lombok.Getter;
@@ -49,7 +44,6 @@
 import org.testcontainers.containers.Container.ExecResult;
 import org.testcontainers.containers.GenericContainer;
 import org.testcontainers.containers.Network;
-import org.testcontainers.containers.output.Slf4jLogConsumer;
 
 /**
  * Pulsar Cluster in containers.
@@ -167,6 +161,27 @@ public void start() throws Exception {
         log.info(""Pulsar cluster {} is up running:"", clusterName);
         log.info(""\tBinary Service Url : {}"", getPlainTextServiceUrl());
         log.info(""\tHttp Service Url : {}"", getHttpServiceUrl());
+
+        // start function workers
+        if (spec.numFunctionWorkers() > 0) {
+            switch (spec.functionRuntimeType()) {
+                case THREAD:
+                    startFunctionWorkersWithThreadContainerFactory(spec.numFunctionWorkers());
+                    break;
+                case PROCESS:
+                    startFunctionWorkersWithProcessContainerFactory(spec.numFunctionWorkers());
+                    break;
+            }
+        }
+
+        // start external services
+        Map<String, GenericContainer<?>> externalServices = spec.externalServices;
+        if (null != externalServices) {
+            externalServices.entrySet().forEach(service -> {
+                service.getValue().start();
+                log.info(""Successfully start external service {}."", service.getKey());
+            });
+        }
     }
 
     private static <T extends PulsarContainer> Map<String, T> runNumContainers(String serviceName,
@@ -186,13 +201,15 @@ public void start() throws Exception {
     }
 
     public void stop() {
-
-        Stream<GenericContainer> list1 = Stream.of(proxyContainer, csContainer, zkContainer);
-        Stream<GenericContainer> list2 =
-            Stream.of(workerContainers.values(), brokerContainers.values(), bookieContainers.values())
-                .flatMap(Collection::stream);
-        Stream<GenericContainer> list3 = Stream.concat(list1, list2);
-        list3.parallel().forEach(GenericContainer::stop);
+        Stream.of(proxyContainer, csContainer, zkContainer).parallel().forEach(GenericContainer::stop);
+        workerContainers.values().parallelStream().forEach(GenericContainer::stop);
+        brokerContainers.values().parallelStream().forEach(GenericContainer::stop);
+        bookieContainers.values().parallelStream().forEach(GenericContainer::stop);
+        if (null != spec.externalServices()) {
+            spec.externalServices().values()
+                .parallelStream()
+                .forEach(GenericContainer::stop);
+        }
 
         try {
             network.close();
@@ -201,7 +218,7 @@ public void stop() {
         }
     }
 
-    public void startFunctionWorkersWithProcessContainerFactory(int numFunctionWorkers) {
+    private void startFunctionWorkersWithProcessContainerFactory(int numFunctionWorkers) {
         String serviceUrl = ""pulsar://pulsar-broker-0:"" + PulsarContainer.BROKER_PORT;
         String httpServiceUrl = ""http://pulsar-broker-0:"" + PulsarContainer.BROKER_HTTP_PORT;
         workerContainers.putAll(runNumContainers(
@@ -225,7 +242,7 @@ public void startFunctionWorkersWithProcessContainerFactory(int numFunctionWorke
         ));
     }
 
-    public void startFunctionWorkersWithThreadContainerFactory(int numFunctionWorkers) {
+    private void startFunctionWorkersWithThreadContainerFactory(int numFunctionWorkers) {
         String serviceUrl = ""pulsar://pulsar-broker-0:"" + PulsarContainer.BROKER_PORT;
         String httpServiceUrl = ""http://pulsar-broker-0:"" + PulsarContainer.BROKER_HTTP_PORT;
         workerContainers.putAll(runNumContainers(
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterSpec.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterSpec.java
index 85c9f5124d..fd830e56c1 100644
--- a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterSpec.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterSpec.java
@@ -18,11 +18,15 @@
  */
 package org.apache.pulsar.tests.topologies;
 
+import java.util.Map;
 import lombok.Builder;
 import lombok.Builder.Default;
 import lombok.Getter;
 import lombok.Setter;
 import lombok.experimental.Accessors;
+import org.apache.pulsar.tests.containers.ChaosContainer;
+import org.testcontainers.containers.GenericContainer;
+import org.testng.collections.Maps;
 
 /**
  * Spec to build a pulsar cluster.
@@ -64,6 +68,30 @@
     @Default
     int numProxies = 1;
 
+    /**
+     * Returns number of function workers.
+     *
+     * @return number of function workers.
+     */
+    @Default
+    int numFunctionWorkers = 0;
+
+    /**
+     * Returns the function runtime type.
+     *
+     * @return the function runtime type.
+     */
+    @Default
+    FunctionRuntimeType functionRuntimeType = FunctionRuntimeType.PROCESS;
+
+    /**
+     * Returns the list of external services to start with
+     * this cluster.
+     *
+     * @return the list of external services to start with the cluster.
+     */
+    Map<String, GenericContainer<?>> externalServices = Maps.newHashMap();
+
     /**
      * Returns the flag whether to enable/disable container log.
      *
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterTestBase.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterTestBase.java
index c2e4b88cb9..24e91842e5 100644
--- a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterTestBase.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterTestBase.java
@@ -62,7 +62,6 @@
     @BeforeClass
     public void setupCluster() throws Exception {
         this.setupCluster("""");
-        pulsarCluster.startFunctionWorkersWithProcessContainerFactory(1);
     }
 
     public void setupCluster(String namePrefix) throws Exception {
@@ -75,7 +74,7 @@ public void setupCluster(String namePrefix) throws Exception {
         setupCluster(spec);
     }
 
-    private void setupCluster(PulsarClusterSpec spec) throws Exception {
+    protected void setupCluster(PulsarClusterSpec spec) throws Exception {
         log.info(""Setting up cluster {} with {} bookies, {} brokers"",
             spec.clusterName(), spec.numBookies(), spec.numBrokers());
 
diff --git a/tests/integration-tests-topologies/src/main/resources/kafka-zookeeper.properties b/tests/integration-tests-topologies/src/main/resources/kafka-zookeeper.properties
new file mode 100644
index 0000000000..f7d1f92f22
--- /dev/null
+++ b/tests/integration-tests-topologies/src/main/resources/kafka-zookeeper.properties
@@ -0,0 +1,36 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+clientPort=2181
+dataDir=/var/lib/zookeeper/data
+dataLogDir=/var/lib/zookeeper/log
\ No newline at end of file
diff --git a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/PulsarFunctionsTestBase.java b/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/PulsarFunctionsTestBase.java
index 180e7f9375..8d224043e1 100644
--- a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/PulsarFunctionsTestBase.java
+++ b/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/PulsarFunctionsTestBase.java
@@ -18,9 +18,14 @@
  */
 package org.apache.pulsar.tests.integration.functions;
 
+import static java.util.stream.Collectors.joining;
+
+import java.util.stream.Stream;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.pulsar.tests.integration.functions.runtime.PulsarFunctionsRuntimeTest;
 import org.apache.pulsar.tests.integration.functions.utils.CommandGenerator.Runtime;
+import org.apache.pulsar.tests.topologies.FunctionRuntimeType;
+import org.apache.pulsar.tests.topologies.PulsarClusterSpec;
 import org.apache.pulsar.tests.topologies.PulsarClusterTestBase;
 import org.testcontainers.containers.Container.ExecResult;
 import org.testng.annotations.BeforeClass;
@@ -32,6 +37,34 @@
 @Slf4j
 public abstract class PulsarFunctionsTestBase extends PulsarClusterTestBase  {
 
+    protected final FunctionRuntimeType functionRuntimeType;
+
+    public PulsarFunctionsTestBase() {
+        this(FunctionRuntimeType.PROCESS);
+    }
+
+    protected PulsarFunctionsTestBase(FunctionRuntimeType functionRuntimeType) {
+        this.functionRuntimeType = functionRuntimeType;
+    }
+
+    @BeforeClass
+    @Override
+    public void setupCluster() throws Exception {
+        PulsarClusterSpec spec = PulsarClusterSpec.builder()
+            .clusterName(Stream.of(this.getClass().getSimpleName(), randomName(5))
+                .filter(s -> s != null && !s.isEmpty())
+                .collect(joining(""-"")))
+            .functionRuntimeType(functionRuntimeType)
+            .numFunctionWorkers(2)
+            .build();
+
+        super.setupCluster(spec);
+    }
+
+    //
+    // Common Variables used by functions test
+    //
+
     public static final String EXCLAMATION_JAVA_CLASS =
         ""org.apache.pulsar.functions.api.examples.ExclamationFunction"";
 
diff --git a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsRuntimeTest.java b/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsRuntimeTest.java
index f656886b46..6f498218ba 100644
--- a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsRuntimeTest.java
+++ b/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsRuntimeTest.java
@@ -31,24 +31,29 @@
 import org.apache.pulsar.tests.integration.functions.PulsarFunctionsTestBase;
 import org.apache.pulsar.tests.integration.functions.utils.CommandGenerator;
 import org.apache.pulsar.tests.integration.functions.utils.CommandGenerator.Runtime;
+import org.apache.pulsar.tests.topologies.FunctionRuntimeType;
 import org.apache.pulsar.tests.topologies.PulsarCluster;
 import org.testcontainers.containers.Container.ExecResult;
+import org.testng.annotations.DataProvider;
+import org.testng.annotations.Factory;
 import org.testng.annotations.Test;
 
 /**
  * The tests that run over different container mode.
  */
-public abstract class PulsarFunctionsRuntimeTest extends PulsarFunctionsTestBase {
+public class PulsarFunctionsRuntimeTest extends PulsarFunctionsTestBase {
 
-    public enum RuntimeFactory {
-        PROCESS,
-        THREAD
+    @DataProvider(name = ""FunctionRuntimeTypes"")
+    public static Object[][] getData() {
+        return new Object[][] {
+            { FunctionRuntimeType.PROCESS },
+            { FunctionRuntimeType.THREAD }
+        };
     }
 
-    private final RuntimeFactory runtimeFactory;
-
-    public PulsarFunctionsRuntimeTest(RuntimeFactory runtimeFactory) {
-        this.runtimeFactory = runtimeFactory;
+    @Factory(dataProvider = ""FunctionRuntimeTypes"")
+    PulsarFunctionsRuntimeTest(FunctionRuntimeType functionRuntimeType) {
+        super(functionRuntimeType);
     }
 
     //
@@ -57,7 +62,7 @@ public PulsarFunctionsRuntimeTest(RuntimeFactory runtimeFactory) {
 
     @Test(dataProvider = ""FunctionRuntimes"")
     public void testExclamationFunction(Runtime runtime) throws Exception {
-        if (runtimeFactory == RuntimeFactory.THREAD && runtime == Runtime.PYTHON) {
+        if (functionRuntimeType == FunctionRuntimeType.THREAD && runtime == Runtime.PYTHON) {
             // python can only run on process mode
             return;
         }


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
926026,195,153152662565.24575.8268959379559450760@gitbox.apache.org,13134,sijie,NULL,,,2018-07-13 17:03:45-07,"[incubator-pulsar] branch master updated: Add numFunctionWorkers
 and externalServices to cluster spec (#2160)","This is an automated email from the ASF dual-hosted git repository.

sijie pushed a commit to branch master
in repository https://gitbox.apache.org/repos/asf/incubator-pulsar.git


The following commit(s) were added to refs/heads/master by this push:
     new 10f273a  Add numFunctionWorkers and externalServices to cluster spec (#2160)
10f273a is described below

commit 10f273aa00e03f83e77847d8d4b855cc40a6f23b
Author: Sijie Guo <guosijie@gmail.com>
AuthorDate: Fri Jul 13 17:03:43 2018 -0700

    Add numFunctionWorkers and externalServices to cluster spec (#2160)
    
    *Motivation*
    
    ClusterSpec is used for defining how a cluster looks like for integration
    testing. Add `numFunctionWorker` and `externalServices` in cluster spec,
    so we have a common place for setting up a cluster.
    
    *Changes*
    
    - `numFunctionWorkers`: define how many function workers to run in the cluster. When a cluster is created from the cluster spec, it will start
    the same number of worker containers.
    
    - `functionRuntimeType`: define how the worker will invoke functions,
    whether it is in process mode or thread mode.
    
    - `externalServices`: define whether there are more external services to
    run along with the cluster. for example, we need cassandra or kafka for
    testing connectors, and we need s3 mock for testing offloaders.
---
 .../tests/containers/CassandraContainer.java}      | 33 +++++++-----
 .../pulsar/tests/containers/KafkaContainer.java    | 63 ++++++++++++++++++++++
 .../tests/containers/KafkaProxyContainer.java      | 51 ++++++++++++++++++
 .../tests/topologies/FunctionRuntimeType.java}     | 24 ++-------
 .../pulsar/tests/topologies/PulsarCluster.java     | 47 ++++++++++------
 .../pulsar/tests/topologies/PulsarClusterSpec.java | 28 ++++++++++
 .../tests/topologies/PulsarClusterTestBase.java    |  3 +-
 .../src/main/resources/kafka-zookeeper.properties  | 36 +++++++++++++
 .../functions/PulsarFunctionsTestBase.java         | 33 ++++++++++++
 .../runtime/PulsarFunctionsRuntimeTest.java        | 23 ++++----
 10 files changed, 282 insertions(+), 59 deletions(-)

diff --git a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsThreadRuntimeTest.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/CassandraContainer.java
similarity index 51%%%%
rename from tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsThreadRuntimeTest.java
rename to tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/CassandraContainer.java
index 5772ee3..ebfa000 100644
--- a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsThreadRuntimeTest.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/CassandraContainer.java
@@ -16,28 +16,33 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.pulsar.tests.integration.functions.runtime;
+package org.apache.pulsar.tests.containers;
 
 import lombok.extern.slf4j.Slf4j;
-import org.testcontainers.containers.Container.ExecResult;
-import org.testng.annotations.BeforeClass;
+import org.testcontainers.containers.wait.strategy.HostPortWaitStrategy;
 
 /**
- * Run the runtime test cases in thread mode.
+ * Cassandra Container.
  */
 @Slf4j
-public class PulsarFunctionsThreadRuntimeTest extends PulsarFunctionsRuntimeTest {
+public class CassandraContainer<SelfT extends ChaosContainer<SelfT>> extends ChaosContainer<SelfT> {
 
-    public PulsarFunctionsThreadRuntimeTest() {
-        super(RuntimeFactory.THREAD);
-    }
+    public static final String NAME = ""cassandra"";
+    public static final int PORT = 9042;
 
-    @BeforeClass
-    public void setupCluster() throws Exception {
-        super.setupCluster(RuntimeFactory.THREAD.toString());
-        pulsarCluster.startFunctionWorkersWithThreadContainerFactory(1);
-        ExecResult result = pulsarCluster.getAnyWorker().execCmd(""cat"", ""/pulsar/conf/functions_worker.yml"");
-        log.info(""Functions Worker Config : \n{}"", result.getStdout());
+    public CassandraContainer(String clusterName) {
+        super(clusterName, ""cassandra:3"");
     }
 
+    @Override
+    protected void configure() {
+        super.configure();
+        this.withNetworkAliases(NAME)
+            .withExposedPorts(PORT)
+            .withCreateContainerCmdModifier(createContainerCmd -> {
+                createContainerCmd.withHostName(NAME);
+                createContainerCmd.withName(clusterName + ""-"" + NAME);
+            })
+            .waitingFor(new HostPortWaitStrategy());
+    }
 }
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaContainer.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaContainer.java
new file mode 100644
index 0000000..83b5e42
--- /dev/null
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaContainer.java
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pulsar.tests.containers;
+
+import lombok.extern.slf4j.Slf4j;
+import org.testcontainers.containers.BindMode;
+import org.testcontainers.containers.wait.strategy.HostPortWaitStrategy;
+
+/**
+ * Cassandra Container.
+ */
+@Slf4j
+public class KafkaContainer<SelfT extends ChaosContainer<SelfT>> extends ChaosContainer<SelfT> {
+
+    public static final String NAME = ""kafka"";
+    public static final int INTERNAL_PORT = 9092;
+    public static final int PORT = 9093;
+
+    public KafkaContainer(String clusterName) {
+        super(clusterName, ""confluentinc/cp-kafka:4.1.1"");
+    }
+
+    @Override
+    protected void configure() {
+        super.configure();
+        this.withNetworkAliases(NAME)
+            .withExposedPorts(INTERNAL_PORT, PORT)
+            .withClasspathResourceMapping(
+                ""kafka-zookeeper.properties"", ""/zookeeper.properties"",
+                BindMode.READ_ONLY)
+            .withCommand(""sh"", ""-c"", ""zookeeper-server-start /zookeeper.properties & /etc/confluent/docker/run"")
+            .withEnv(""KAFKA_LISTENERS"",
+                ""INTERNAL://kafka:"" + INTERNAL_PORT + "",PLAINTEXT://"" + ""0.0.0.0"" + "":"" + PORT)
+            .withEnv(""KAFKA_ZOOKEEPER_CONNECT"", ""localhost:2181"")
+            .withEnv(""KAFKA_LISTENER_SECURITY_PROTOCOL_MAP"", ""INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT"")
+            .withEnv(""KAFKA_INTER_BROKER_LISTENER_NAME"", ""INTERNAL"")
+            .withEnv(""KAFKA_BROKER_ID"", ""1"")
+            .withEnv(""KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR"", ""1"")
+            .withEnv(""KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS"", ""1"")
+            .withEnv(""KAFKA_LOG_FLUSH_INTERVAL_MESSAGES"", Long.MAX_VALUE + """")
+            .withCreateContainerCmdModifier(createContainerCmd -> {
+                createContainerCmd.withHostName(NAME);
+                createContainerCmd.withName(clusterName + ""-"" + NAME);
+            })
+            .waitingFor(new HostPortWaitStrategy());
+    }
+}
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaProxyContainer.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaProxyContainer.java
new file mode 100644
index 0000000..052db7e
--- /dev/null
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/containers/KafkaProxyContainer.java
@@ -0,0 +1,51 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.pulsar.tests.containers;
+
+import lombok.extern.slf4j.Slf4j;
+import org.testcontainers.containers.SocatContainer;
+import org.testcontainers.containers.wait.strategy.HostPortWaitStrategy;
+
+/**
+ * Cassandra Container.
+ */
+@Slf4j
+public class KafkaProxyContainer extends SocatContainer {
+
+    public static final String NAME = ""kafka-proxy"";
+
+    private final String clusterName;
+
+    public KafkaProxyContainer(String clusterName) {
+        super();
+        this.clusterName = clusterName;
+    }
+
+    @Override
+    protected void configure() {
+        super.configure();
+        this.withNetworkAliases(NAME)
+            .withTarget(KafkaContainer.PORT, KafkaContainer.NAME)
+            .withCreateContainerCmdModifier(createContainerCmd -> {
+                createContainerCmd.withHostName(NAME);
+                createContainerCmd.withName(clusterName + ""-"" + NAME);
+            })
+            .waitingFor(new HostPortWaitStrategy());
+    }
+}
diff --git a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsProcessRuntimeTest.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/FunctionRuntimeType.java
similarity index 50%%%%
rename from tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsProcessRuntimeTest.java
rename to tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/FunctionRuntimeType.java
index ebc6e99..a3efd31 100644
--- a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsProcessRuntimeTest.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/FunctionRuntimeType.java
@@ -16,26 +16,12 @@
  * specific language governing permissions and limitations
  * under the License.
  */
-package org.apache.pulsar.tests.integration.functions.runtime;
-
-import lombok.extern.slf4j.Slf4j;
-import org.testcontainers.containers.Container;
-import org.testng.annotations.BeforeClass;
+package org.apache.pulsar.tests.topologies;
 
 /**
- * Run runtime tests in process mode.
+ * Runtime type to run functions.
  */
-@Slf4j
-public class PulsarFunctionsProcessRuntimeTest extends PulsarFunctionsRuntimeTest {
-    public PulsarFunctionsProcessRuntimeTest() {
-        super(RuntimeFactory.PROCESS);
-    }
-
-    @BeforeClass
-    public void setupCluster() throws Exception {
-        super.setupCluster(RuntimeFactory.PROCESS.toString());
-        pulsarCluster.startFunctionWorkersWithProcessContainerFactory(1);
-        Container.ExecResult result = pulsarCluster.getAnyWorker().execCmd(""cat"", ""/pulsar/conf/functions_worker.yml"");
-        log.info(""Functions Worker Config : \n{}"", result.getStdout());
-    }
+public enum FunctionRuntimeType {
+    PROCESS,
+    THREAD
 }
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarCluster.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarCluster.java
index d0c9eb4..6983a1c 100644
--- a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarCluster.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarCluster.java
@@ -21,20 +21,15 @@ package org.apache.pulsar.tests.topologies;
 import static com.google.common.base.Preconditions.checkArgument;
 import static org.apache.pulsar.tests.containers.PulsarContainer.CS_PORT;
 
-import com.github.dockerjava.api.command.CreateContainerCmd;
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
 
-import java.util.ArrayList;
-import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.CompletableFuture;
-import java.util.function.Consumer;
 import java.util.function.Function;
-import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
 import lombok.Getter;
@@ -49,7 +44,6 @@ import org.apache.pulsar.tests.containers.ZKContainer;
 import org.testcontainers.containers.Container.ExecResult;
 import org.testcontainers.containers.GenericContainer;
 import org.testcontainers.containers.Network;
-import org.testcontainers.containers.output.Slf4jLogConsumer;
 
 /**
  * Pulsar Cluster in containers.
@@ -167,6 +161,27 @@ public class PulsarCluster {
         log.info(""Pulsar cluster {} is up running:"", clusterName);
         log.info(""\tBinary Service Url : {}"", getPlainTextServiceUrl());
         log.info(""\tHttp Service Url : {}"", getHttpServiceUrl());
+
+        // start function workers
+        if (spec.numFunctionWorkers() > 0) {
+            switch (spec.functionRuntimeType()) {
+                case THREAD:
+                    startFunctionWorkersWithThreadContainerFactory(spec.numFunctionWorkers());
+                    break;
+                case PROCESS:
+                    startFunctionWorkersWithProcessContainerFactory(spec.numFunctionWorkers());
+                    break;
+            }
+        }
+
+        // start external services
+        Map<String, GenericContainer<?>> externalServices = spec.externalServices;
+        if (null != externalServices) {
+            externalServices.entrySet().forEach(service -> {
+                service.getValue().start();
+                log.info(""Successfully start external service {}."", service.getKey());
+            });
+        }
     }
 
     private static <T extends PulsarContainer> Map<String, T> runNumContainers(String serviceName,
@@ -186,13 +201,15 @@ public class PulsarCluster {
     }
 
     public void stop() {
-
-        Stream<GenericContainer> list1 = Stream.of(proxyContainer, csContainer, zkContainer);
-        Stream<GenericContainer> list2 =
-            Stream.of(workerContainers.values(), brokerContainers.values(), bookieContainers.values())
-                .flatMap(Collection::stream);
-        Stream<GenericContainer> list3 = Stream.concat(list1, list2);
-        list3.parallel().forEach(GenericContainer::stop);
+        Stream.of(proxyContainer, csContainer, zkContainer).parallel().forEach(GenericContainer::stop);
+        workerContainers.values().parallelStream().forEach(GenericContainer::stop);
+        brokerContainers.values().parallelStream().forEach(GenericContainer::stop);
+        bookieContainers.values().parallelStream().forEach(GenericContainer::stop);
+        if (null != spec.externalServices()) {
+            spec.externalServices().values()
+                .parallelStream()
+                .forEach(GenericContainer::stop);
+        }
 
         try {
             network.close();
@@ -201,7 +218,7 @@ public class PulsarCluster {
         }
     }
 
-    public void startFunctionWorkersWithProcessContainerFactory(int numFunctionWorkers) {
+    private void startFunctionWorkersWithProcessContainerFactory(int numFunctionWorkers) {
         String serviceUrl = ""pulsar://pulsar-broker-0:"" + PulsarContainer.BROKER_PORT;
         String httpServiceUrl = ""http://pulsar-broker-0:"" + PulsarContainer.BROKER_HTTP_PORT;
         workerContainers.putAll(runNumContainers(
@@ -225,7 +242,7 @@ public class PulsarCluster {
         ));
     }
 
-    public void startFunctionWorkersWithThreadContainerFactory(int numFunctionWorkers) {
+    private void startFunctionWorkersWithThreadContainerFactory(int numFunctionWorkers) {
         String serviceUrl = ""pulsar://pulsar-broker-0:"" + PulsarContainer.BROKER_PORT;
         String httpServiceUrl = ""http://pulsar-broker-0:"" + PulsarContainer.BROKER_HTTP_PORT;
         workerContainers.putAll(runNumContainers(
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterSpec.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterSpec.java
index 85c9f51..fd830e5 100644
--- a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterSpec.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterSpec.java
@@ -18,11 +18,15 @@
  */
 package org.apache.pulsar.tests.topologies;
 
+import java.util.Map;
 import lombok.Builder;
 import lombok.Builder.Default;
 import lombok.Getter;
 import lombok.Setter;
 import lombok.experimental.Accessors;
+import org.apache.pulsar.tests.containers.ChaosContainer;
+import org.testcontainers.containers.GenericContainer;
+import org.testng.collections.Maps;
 
 /**
  * Spec to build a pulsar cluster.
@@ -65,6 +69,30 @@ public class PulsarClusterSpec {
     int numProxies = 1;
 
     /**
+     * Returns number of function workers.
+     *
+     * @return number of function workers.
+     */
+    @Default
+    int numFunctionWorkers = 0;
+
+    /**
+     * Returns the function runtime type.
+     *
+     * @return the function runtime type.
+     */
+    @Default
+    FunctionRuntimeType functionRuntimeType = FunctionRuntimeType.PROCESS;
+
+    /**
+     * Returns the list of external services to start with
+     * this cluster.
+     *
+     * @return the list of external services to start with the cluster.
+     */
+    Map<String, GenericContainer<?>> externalServices = Maps.newHashMap();
+
+    /**
      * Returns the flag whether to enable/disable container log.
      *
      * @return the flag whether to enable/disable container log.
diff --git a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterTestBase.java b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterTestBase.java
index c2e4b88..24e9184 100644
--- a/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterTestBase.java
+++ b/tests/integration-tests-topologies/src/main/java/org/apache/pulsar/tests/topologies/PulsarClusterTestBase.java
@@ -62,7 +62,6 @@ public abstract class PulsarClusterTestBase {
     @BeforeClass
     public void setupCluster() throws Exception {
         this.setupCluster("""");
-        pulsarCluster.startFunctionWorkersWithProcessContainerFactory(1);
     }
 
     public void setupCluster(String namePrefix) throws Exception {
@@ -75,7 +74,7 @@ public abstract class PulsarClusterTestBase {
         setupCluster(spec);
     }
 
-    private void setupCluster(PulsarClusterSpec spec) throws Exception {
+    protected void setupCluster(PulsarClusterSpec spec) throws Exception {
         log.info(""Setting up cluster {} with {} bookies, {} brokers"",
             spec.clusterName(), spec.numBookies(), spec.numBrokers());
 
diff --git a/tests/integration-tests-topologies/src/main/resources/kafka-zookeeper.properties b/tests/integration-tests-topologies/src/main/resources/kafka-zookeeper.properties
new file mode 100644
index 0000000..f7d1f92
--- /dev/null
+++ b/tests/integration-tests-topologies/src/main/resources/kafka-zookeeper.properties
@@ -0,0 +1,36 @@
+#
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+#
+
+#
+# Licensed under the Apache License, Version 2.0 (the ""License"");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+clientPort=2181
+dataDir=/var/lib/zookeeper/data
+dataLogDir=/var/lib/zookeeper/log
\ No newline at end of file
diff --git a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/PulsarFunctionsTestBase.java b/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/PulsarFunctionsTestBase.java
index 180e7f9..8d22404 100644
--- a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/PulsarFunctionsTestBase.java
+++ b/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/PulsarFunctionsTestBase.java
@@ -18,9 +18,14 @@
  */
 package org.apache.pulsar.tests.integration.functions;
 
+import static java.util.stream.Collectors.joining;
+
+import java.util.stream.Stream;
 import lombok.extern.slf4j.Slf4j;
 import org.apache.pulsar.tests.integration.functions.runtime.PulsarFunctionsRuntimeTest;
 import org.apache.pulsar.tests.integration.functions.utils.CommandGenerator.Runtime;
+import org.apache.pulsar.tests.topologies.FunctionRuntimeType;
+import org.apache.pulsar.tests.topologies.PulsarClusterSpec;
 import org.apache.pulsar.tests.topologies.PulsarClusterTestBase;
 import org.testcontainers.containers.Container.ExecResult;
 import org.testng.annotations.BeforeClass;
@@ -32,6 +37,34 @@ import org.testng.annotations.DataProvider;
 @Slf4j
 public abstract class PulsarFunctionsTestBase extends PulsarClusterTestBase  {
 
+    protected final FunctionRuntimeType functionRuntimeType;
+
+    public PulsarFunctionsTestBase() {
+        this(FunctionRuntimeType.PROCESS);
+    }
+
+    protected PulsarFunctionsTestBase(FunctionRuntimeType functionRuntimeType) {
+        this.functionRuntimeType = functionRuntimeType;
+    }
+
+    @BeforeClass
+    @Override
+    public void setupCluster() throws Exception {
+        PulsarClusterSpec spec = PulsarClusterSpec.builder()
+            .clusterName(Stream.of(this.getClass().getSimpleName(), randomName(5))
+                .filter(s -> s != null && !s.isEmpty())
+                .collect(joining(""-"")))
+            .functionRuntimeType(functionRuntimeType)
+            .numFunctionWorkers(2)
+            .build();
+
+        super.setupCluster(spec);
+    }
+
+    //
+    // Common Variables used by functions test
+    //
+
     public static final String EXCLAMATION_JAVA_CLASS =
         ""org.apache.pulsar.functions.api.examples.ExclamationFunction"";
 
diff --git a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsRuntimeTest.java b/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsRuntimeTest.java
index f656886..6f49821 100644
--- a/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsRuntimeTest.java
+++ b/tests/integration/semantics/src/test/java/org/apache/pulsar/tests/integration/functions/runtime/PulsarFunctionsRuntimeTest.java
@@ -31,24 +31,29 @@ import org.apache.pulsar.client.api.SubscriptionType;
 import org.apache.pulsar.tests.integration.functions.PulsarFunctionsTestBase;
 import org.apache.pulsar.tests.integration.functions.utils.CommandGenerator;
 import org.apache.pulsar.tests.integration.functions.utils.CommandGenerator.Runtime;
+import org.apache.pulsar.tests.topologies.FunctionRuntimeType;
 import org.apache.pulsar.tests.topologies.PulsarCluster;
 import org.testcontainers.containers.Container.ExecResult;
+import org.testng.annotations.DataProvider;
+import org.testng.annotations.Factory;
 import org.testng.annotations.Test;
 
 /**
  * The tests that run over different container mode.
  */
-public abstract class PulsarFunctionsRuntimeTest extends PulsarFunctionsTestBase {
+public class PulsarFunctionsRuntimeTest extends PulsarFunctionsTestBase {
 
-    public enum RuntimeFactory {
-        PROCESS,
-        THREAD
+    @DataProvider(name = ""FunctionRuntimeTypes"")
+    public static Object[][] getData() {
+        return new Object[][] {
+            { FunctionRuntimeType.PROCESS },
+            { FunctionRuntimeType.THREAD }
+        };
     }
 
-    private final RuntimeFactory runtimeFactory;
-
-    public PulsarFunctionsRuntimeTest(RuntimeFactory runtimeFactory) {
-        this.runtimeFactory = runtimeFactory;
+    @Factory(dataProvider = ""FunctionRuntimeTypes"")
+    PulsarFunctionsRuntimeTest(FunctionRuntimeType functionRuntimeType) {
+        super(functionRuntimeType);
     }
 
     //
@@ -57,7 +62,7 @@ public abstract class PulsarFunctionsRuntimeTest extends PulsarFunctionsTestBase
 
     @Test(dataProvider = ""FunctionRuntimes"")
     public void testExclamationFunction(Runtime runtime) throws Exception {
-        if (runtimeFactory == RuntimeFactory.THREAD && runtime == Runtime.PYTHON) {
+        if (functionRuntimeType == FunctionRuntimeType.THREAD && runtime == Runtime.PYTHON) {
             // python can only run on process mode
             return;
         }


",t
26883,54,JIRA.13057719.1490049741000.103966.1509392461065@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13057719.1490049741000@Atlassian.JIRA,,,2017-10-30 12:41:01-07,"[jira] [Commented] (AIRFLOW-1018) Scheduler DAG processes can not
 log to stdout","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1018?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225624#comment-16225624 ] 

ASF subversion and git services commented on AIRFLOW-1018:
----------------------------------------------------------

Commit ef775d4f8aca13075eed7e207c07e5941ff68539 in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=ef775d4 ]

[AIRFLOW-1018] Make processor use logging framework

Until now, the dga processor had its own logging
implementation,
making it hard to adjust for certain use cases
like working
in a container.

This patch moves everything to the standard
logging framework.

Closes #2728 from bolkedebruin/AIRFLOW-1018

(cherry picked from commit 4ee4e474b835b4f5f557226ba01b8cdfeb7d0789)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Scheduler DAG processes can not log to stdout
> ---------------------------------------------
>
>                 Key: AIRFLOW-1018
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1018
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>         Environment: Airflow 1.8.0
>            Reporter: Vincent Poulain
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. 
> Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container.
> When we specify /dev/stdout it raises:
> ""OSError: [Errno 20] Not a directory: ''/dev/stdout/2017-03-19''""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26884,54,JIRA.13057719.1490049741000.103972.1509392461119@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13057719.1490049741000@Atlassian.JIRA,,,2017-10-30 12:41:01-07,"[jira] [Commented] (AIRFLOW-1018) Scheduler DAG processes can not
 log to stdout","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1018?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225625#comment-16225625 ] 

ASF subversion and git services commented on AIRFLOW-1018:
----------------------------------------------------------

Commit ef775d4f8aca13075eed7e207c07e5941ff68539 in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=ef775d4 ]

[AIRFLOW-1018] Make processor use logging framework

Until now, the dga processor had its own logging
implementation,
making it hard to adjust for certain use cases
like working
in a container.

This patch moves everything to the standard
logging framework.

Closes #2728 from bolkedebruin/AIRFLOW-1018

(cherry picked from commit 4ee4e474b835b4f5f557226ba01b8cdfeb7d0789)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Scheduler DAG processes can not log to stdout
> ---------------------------------------------
>
>                 Key: AIRFLOW-1018
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1018
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>         Environment: Airflow 1.8.0
>            Reporter: Vincent Poulain
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. 
> Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container.
> When we specify /dev/stdout it raises:
> ""OSError: [Errno 20] Not a directory: ''/dev/stdout/2017-03-19''""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26885,54,JIRA.13057719.1490049741000.103979.1509392461188@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13057719.1490049741000@Atlassian.JIRA,,,2017-10-30 12:41:01-07,"[jira] [Resolved] (AIRFLOW-1018) Scheduler DAG processes can not
 log to stdout","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1018?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1018.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2728
[https://github.com/apache/incubator-airflow/pull/2728]

> Scheduler DAG processes can not log to stdout
> ---------------------------------------------
>
>                 Key: AIRFLOW-1018
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1018
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>         Environment: Airflow 1.8.0
>            Reporter: Vincent Poulain
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> Each DAG has its own log file for the scheduler and we can specify the directory with child_process_log_directory param. 
> Unfortunately we can not change device / by specifying /dev/stdout for example. That is very useful when we execute Airflow in a container.
> When we specify /dev/stdout it raises:
> ""OSError: [Errno 20] Not a directory: ''/dev/stdout/2017-03-19''""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26886,54,JIRA.13113100.1509386655000.104967.1509399960495@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-10-30 14:46:00-07,"[jira] [Commented] (AIRFLOW-1765) Default API auth backed should
 deny all.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16225789#comment-16225789 ] 

Ash Berlin-Taylor commented on AIRFLOW-1765:
--------------------------------------------

Fair points.

I think the surprising thing to me was that it defaults to allow/isn''t mentioned in https://airflow.apache.org/security.html -- and I think from the code there''s no way to not make it open by default other than maybe to mis-configure an api auth backend? I don''t have the code in front of me and it''s late so I might be way off on this.

My plan will be to create 2, or 3 auth api backends. A ""denyAll"" (and make this the default now), a ""allowAll"" to get the old behaviour back. It might also be worth creating a ""sessionAuth"" wich just needs a valid login using whatever mechanism the front end allows. (#3 is probably optional for closing this hole)

Suitable doc updates to go with this.

Sound reasonable?

> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26887,54,JIRA.13113264.1509443325000.109439.1509443340289@Atlassian.JIRA,2273,Rupesh Bansal (JIRA),JIRA.13113264.1509443325000@Atlassian.JIRA,,,2017-10-31 02:49:00-07,"[jira] [Created] (AIRFLOW-1766) Handle failure of Qubole commands
 when connection is set from CLI","Rupesh Bansal created AIRFLOW-1766:
--------------------------------------

             Summary: Handle failure of Qubole commands when connection is set from CLI
                 Key: AIRFLOW-1766
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1766
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Rupesh Bansal
            Assignee: Rupesh Bansal






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706882,24,153193359137.27618.7816912003188194168.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 10:06:31-07,"[GitHub] larroy commented on a change in pull request #11798: flaky: Disable
 kvstore test","larroy commented on a change in pull request #11798: flaky: Disable kvstore test
URL: https://github.com/apache/incubator-mxnet/pull/11798#discussion_r203457714
 
 

 ##########
 File path: Jenkinsfile
 ##########
 @@ -1011,6 +1011,9 @@ try {
         }
       }
     },
+    /*  Disabled due to master build failure:
+     *  http://jenkins.mxnet-ci.amazon-ml.com/blue/organizations/jenkins/incubator-mxnet/detail/master/1221/pipeline/
+
 
 Review comment:
   Done

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
26888,54,JIRA.13113100.1509386655000.110389.1509453301598@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-10-31 05:35:01-07,"[jira] [Commented] (AIRFLOW-1765) Default API auth backed should
 deny all.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16226700#comment-16226700 ] 

Ash Berlin-Taylor commented on AIRFLOW-1765:
--------------------------------------------

I have created two PRs that address fix this in different ways. Only one should be used and the other closed unmerged.

- https://github.com/apache/incubator-airflow/pull/2736 - default backend denies all, added an allow_all backend
- https://github.com/apache/incubator-airflow/pull/2737 - default backend still allows_all, added a deny_all backend.

In cases both there remains a airflow.api.auth.backend.default so that existing config''s won''t suddenly break.

> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26889,54,JIRA.13080590.1497705847000.111211.1509458580435@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13080590.1497705847000@Atlassian.JIRA,,,2017-10-31 07:03:00-07,"[jira] [Commented] (AIRFLOW-1315) Add Qubole File and Partition
 Sensors","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1315?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16226832#comment-16226832 ] 

ASF subversion and git services commented on AIRFLOW-1315:
----------------------------------------------------------

Commit c5776375fdb8cc54edc0c67d623b3ea02a0f325a in incubator-airflow''s branch refs/heads/master from [~msumit]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=c577637 ]

[AIRFLOW-1315] Add Qubole File & Partition Sensors

Closes #2401 from msumit/AIRFLOW-1315


> Add Qubole File and Partition Sensors
> -------------------------------------
>
>                 Key: AIRFLOW-1315
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1315
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Sumit Maheshwari
>            Assignee: Sumit Maheshwari
>              Labels: contrib, qds, sensors
>             Fix For: 1.9.0
>
>
> Support to call Qubole''s File and Partitions sensors from Airflow. APIs are listed here http://docs.qubole.com/en/latest/rest-api/sensor_api/index.html



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26890,54,JIRA.13080590.1497705847000.111215.1509458580457@Atlassian.JIRA,1342,Sumit Maheshwari (JIRA),JIRA.13080590.1497705847000@Atlassian.JIRA,,,2017-10-31 07:03:00-07,"[jira] [Resolved] (AIRFLOW-1315) Add Qubole File and Partition
 Sensors","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1315?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sumit Maheshwari resolved AIRFLOW-1315.
---------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2401
[https://github.com/apache/incubator-airflow/pull/2401]

> Add Qubole File and Partition Sensors
> -------------------------------------
>
>                 Key: AIRFLOW-1315
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1315
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Sumit Maheshwari
>            Assignee: Sumit Maheshwari
>              Labels: contrib, qds, sensors
>             Fix For: 1.9.0
>
>
> Support to call Qubole''s File and Partitions sensors from Airflow. APIs are listed here http://docs.qubole.com/en/latest/rest-api/sensor_api/index.html



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26891,54,JIRA.13080590.1497705847000.111213.1509458580447@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13080590.1497705847000@Atlassian.JIRA,,,2017-10-31 07:03:00-07,"[jira] [Commented] (AIRFLOW-1315) Add Qubole File and Partition
 Sensors","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1315?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16226833#comment-16226833 ] 

ASF subversion and git services commented on AIRFLOW-1315:
----------------------------------------------------------

Commit c5776375fdb8cc54edc0c67d623b3ea02a0f325a in incubator-airflow''s branch refs/heads/master from [~msumit]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=c577637 ]

[AIRFLOW-1315] Add Qubole File & Partition Sensors

Closes #2401 from msumit/AIRFLOW-1315


> Add Qubole File and Partition Sensors
> -------------------------------------
>
>                 Key: AIRFLOW-1315
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1315
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Sumit Maheshwari
>            Assignee: Sumit Maheshwari
>              Labels: contrib, qds, sensors
>             Fix For: 1.9.0
>
>
> Support to call Qubole''s File and Partitions sensors from Airflow. APIs are listed here http://docs.qubole.com/en/latest/rest-api/sensor_api/index.html



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26892,54,JIRA.13090376.1501110696000.111990.1509462420059@Atlassian.JIRA,2364,Paul English (JIRA),JIRA.13090376.1501110696000@Atlassian.JIRA,,,2017-10-31 08:07:00-07,"[jira] [Commented] (AIRFLOW-1468) Airflow job in queue state when
 worker is different from master","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1468?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16226949#comment-16226949 ] 

Paul English commented on AIRFLOW-1468:
---------------------------------------

Also seeing this issue with airflow 1.8.2 using the CeleryExecutor

> Airflow job in queue state when worker is different from master
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1468
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1468
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ram
>
> Hi
> I setup the master/worker infrastructure. When i schedule my job with master as worker the job ran successfully. But when worker is different machine then i am seeing different behavior.
> In logs folder on remote work log is created and able see the output of bash operator and log showing the message like below
> [2017-07-26 15:51:14,892] {base_task_runner.py:95} INFO - Subtask: [2017-07-26 15:51:14,891] {bash_operator.py:94} INFO - Wed Jul 26 15:51:14 PDT 2017
> [2017-07-26 15:51:14,892] {base_task_runner.py:95} INFO - Subtask: [2017-07-26 15:51:14,892] {bash_operator.py:97} INFO - Command exited with return code 0
> [2017-07-26 15:51:19,056] {jobs.py:2083} INFO - Task exited with return code 0
> [2017-07-26 15:51:26,516] {models.py:167} INFO - Filling up the DagBag from /home/skathane/airflow/dags/test.py
> [2017-07-26 15:51:26,605] {base_task_runner.py:112} INFO - Running: [''bash'', ''-c'', u''airflow run test t2 2017-07-26T15:51:10.186620 --job_id 92 --raw -sd DAGS_FOLDER/test.py'']
> [2017-07-26 15:51:27,077] {base_task_runner.py:95} INFO - Subtask: [2017-07-26 15:51:27,077] {__init__.py:57} INFO - Using executor CeleryExecutor
> It seems task executed successfully but it not terminated and in Queue state only. In log further i am seeing below error.
> [2017-07-26 15:51:27,396] {base_task_runner.py:95} INFO - Subtask: [2017-07-26 15:51:27,396] {models.py:1120} I{color:red}*NFO - Dependencies not met for <TaskInstance: test.t2 2017-07-26 15:51:10.186620 [success]>, dependency ''Task Instance State'' FAILED: Task is in the ''success'' state which is not a valid state for execution. The task must be cleared in order to be run.
> *{color}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26893,54,JIRA.12994308.1470151190000.113288.1509470640333@Atlassian.JIRA,2247,Stephan Erb (JIRA),JIRA.12994308.1470151190000@Atlassian.JIRA,,,2017-10-31 10:24:00-07,[jira] [Commented] (AIRFLOW-387) Dropped postgres connections,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-387?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16227150#comment-16227150 ] 

Stephan Erb commented on AIRFLOW-387:
-------------------------------------

I have opened a pull request in order to close database sessions when no longer needed: https://github.com/apache/incubator-airflow/pull/2739

> Dropped postgres connections
> ----------------------------
>
>                 Key: AIRFLOW-387
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-387
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: Airflow 1.7.1.3
>         Environment: Running aiflow with postgresql+psycopg2, Postgres 9.1.19 backend.
>            Reporter: Andreas Merkel
>
> With the {{postgresql+psycopg2}} driver, we observe dropped connections in the database logs on the postgres server:
> {code}
> 2016-08-02 14:49:26 UTC LOG:  could not receive data from client: Connection reset by peer
> 2016-08-02 14:49:26 UTC LOG:  unexpected EOF on client connection
> {code}
> Three of these messages appear even if Airflow is started without any DAGs. If there are DAGs, the messages appear whenever the DAG is executed (even for simple DAGs like a Python operator that does nothing).
> On the Airflow side, everything works fine; it''s just the messages in the log indicating that Airflow is somehow dropping the connections to Postgres without closing them properly.
> h2. Environment
> {{airflow.cfg}}:
> {code}
> [core]
> airflow_home = $PWD
> dags_folder = $PWD/dags
> base_log_folder = $PWD/airflow_logs
> plugins_folder = $PWD/plugins
> executor = LocalExecutor
> sql_alchemy_conn = postgresql+psycopg2://postgres:postgres@127.0.0.1/airflow
> parallelism = 32
> dag_concurrency = 16
> max_active_runs_per_dag = 16
> load_examples = False
> donot_pickle = False
> fernet_key =  ; provided via environment
> dags_are_paused_at_creation = False
> [webserver]
> expose_config = true
> authenticate = False
> filter_by_owner = False
> workers = 4
> worker_class = sync
> [scheduler]
> job_heartbeat_sec = 5
> scheduler_heartbeat_sec = 5
> [celery]
> celeryd_concurrency = 1  ; Oddly enough, this is needed though we don''t use celery.
> {code}
> Installed packages:
> {code}
> airflow==1.7.1.3
> alembic==0.8.6
> Babel==1.3
> cffi==1.6.0
> chartkick==0.4.2
> croniter==0.3.12
> cryptography==1.3.2
> dill==0.2.5
> docutils==0.12
> enum34==1.1.6
> Flask==0.10.1
> Flask-Admin==1.4.0
> Flask-Cache==0.13.1
> Flask-Login==0.2.11
> Flask-WTF==0.12
> funcsigs==0.4
> future==0.15.2
> gunicorn==19.3.0
> idna==2.1
> ipaddress==1.0.16
> itsdangerous==0.24
> Jinja2==2.8
> lockfile==0.12.2
> Mako==1.0.4
> Markdown==2.6.6
> MarkupSafe==0.23
> numpy==1.11.0
> pandas==0.18.1
> psycopg2==2.6.1
> pyasn1==0.1.9
> pycparser==2.14
> Pygments==2.1.3
> python-daemon==2.1.1
> python-dateutil==2.5.3
> python-editor==1.0.1
> pytz==2016.4
> requests==2.10.0
> setproctitle==1.1.10
> six==1.10.0
> smoketest-dag==0.7
> SQLAlchemy==1.0.13
> thrift==0.9.3
> Werkzeug==0.11.10
> WTForms==2.1
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26894,54,JIRA.13113367.1509471930000.113499.1509471961599@Atlassian.JIRA,1504,David Klosowski (JIRA),JIRA.13113367.1509471930000@Atlassian.JIRA,,,2017-10-31 10:46:01-07,[jira] [Created] (AIRFLOW-1767) Airflow Scheduler no longer works,"David Klosowski created AIRFLOW-1767:
----------------------------------------

             Summary: Airflow Scheduler no longer works
                 Key: AIRFLOW-1767
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1767
             Project: Apache Airflow
          Issue Type: Bug
          Components: scheduler
    Affects Versions: 1.9.0, 1.10.0
         Environment: CeleryExecutor, Docker, 3 Workers
            Reporter: David Klosowski
            Priority: Blocker


The Airflow Scheduler no longer schedules DAGs after this commit on master:

https://github.com/apache/incubator-airflow/commit/73549763eac74142b7c4018422bb2f8c897b45a8







--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26895,54,JIRA.13113367.1509471930000.113556.1509472080968@Atlassian.JIRA,1504,David Klosowski (JIRA),JIRA.13113367.1509471930000@Atlassian.JIRA,,,2017-10-31 10:48:00-07,"[jira] [Updated] (AIRFLOW-1767) Airflow Scheduler no longer
 schedules DAGs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1767?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

David Klosowski updated AIRFLOW-1767:
-------------------------------------
    Summary: Airflow Scheduler no longer schedules DAGs  (was: Airflow Scheduler no longer works)

> Airflow Scheduler no longer schedules DAGs
> ------------------------------------------
>
>                 Key: AIRFLOW-1767
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1767
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0, 1.10.0
>         Environment: CeleryExecutor, Docker, 3 Workers
>            Reporter: David Klosowski
>            Priority: Blocker
>
> The Airflow Scheduler no longer schedules DAGs after this commit on master:
> https://github.com/apache/incubator-airflow/commit/73549763eac74142b7c4018422bb2f8c897b45a8



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26896,54,JIRA.13113367.1509471930000.113589.1509472140323@Atlassian.JIRA,1504,David Klosowski (JIRA),JIRA.13113367.1509471930000@Atlassian.JIRA,,,2017-10-31 10:49:00-07,"[jira] [Updated] (AIRFLOW-1767) Airflow Scheduler no longer
 schedules DAGs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1767?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

David Klosowski updated AIRFLOW-1767:
-------------------------------------
    Description: 
The Airflow Scheduler no longer schedules DAGs after this commit on master:

https://github.com/apache/incubator-airflow/commit/73549763eac74142b7c4018422bb2f8c897b45a8

Workers never receive any tasks and the scheduler never adjusts DAG state.




  was:
The Airflow Scheduler no longer schedules DAGs after this commit on master:

https://github.com/apache/incubator-airflow/commit/73549763eac74142b7c4018422bb2f8c897b45a8






> Airflow Scheduler no longer schedules DAGs
> ------------------------------------------
>
>                 Key: AIRFLOW-1767
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1767
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0, 1.10.0
>         Environment: CeleryExecutor, Docker, 3 Workers
>            Reporter: David Klosowski
>            Priority: Blocker
>
> The Airflow Scheduler no longer schedules DAGs after this commit on master:
> https://github.com/apache/incubator-airflow/commit/73549763eac74142b7c4018422bb2f8c897b45a8
> Workers never receive any tasks and the scheduler never adjusts DAG state.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26897,54,JIRA.13104409.1506114351000.113883.1509474000129@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13104409.1506114351000@Atlassian.JIRA,,,2017-10-31 11:20:00-07,[jira] [Commented] (AIRFLOW-1634) Add task_concurrency feature,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1634?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16227263#comment-16227263 ] 

ASF subversion and git services commented on AIRFLOW-1634:
----------------------------------------------------------

Commit 87afe8901559d4aa8b74179e980ca63fd1dedcb5 in incubator-airflow''s branch refs/heads/v1-9-test from [~saguziel]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=87afe89 ]

[AIRFLOW-1634] Adds task_concurrency feature

This adds a feature to limit the concurrency of
individual tasks. The
default will be to not change existing behavior.

Closes #2624 from saguziel/aguziel-task-
concurrency

(cherry picked from commit cfc2f73c445074e1e09d6ef6a056cd2b33a945da)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Add task_concurrency feature
> ----------------------------
>
>                 Key: AIRFLOW-1634
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1634
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Alex Guziel
>            Assignee: Alex Guziel
>
> This would allow a concurrency limit to be placed on individual tasks (which are represented by task_id, dag_id tuple).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26898,54,JIRA.13113367.1509471930000.113888.1509474182304@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13113367.1509471930000@Atlassian.JIRA,,,2017-10-31 11:23:02-07,"[jira] [Resolved] (AIRFLOW-1767) Airflow Scheduler no longer
 schedules DAGs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1767?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1767.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

An earlier commit was missing from v1-9-test, that has been resolved now.

> Airflow Scheduler no longer schedules DAGs
> ------------------------------------------
>
>                 Key: AIRFLOW-1767
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1767
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0
>         Environment: CeleryExecutor, Docker, 3 Workers
>            Reporter: David Klosowski
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> The Airflow Scheduler no longer schedules DAGs after this commit on master:
> https://github.com/apache/incubator-airflow/commit/73549763eac74142b7c4018422bb2f8c897b45a8
> Workers never receive any tasks and the scheduler never adjusts DAG state.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
631167,147,20040416170937.17641.qmail@nagoya.betaversion.org,23834,bugzilla,NULL,,,2004-04-16 10:09:37-07,"DO NOT REPLY [Bug 27470]  - 
    Upgrade all source files to ASF 2.0 license","DO NOT REPLY TO THIS EMAIL, BUT PLEASE POST YOUR BUG 
RELATED COMMENTS THROUGH THE WEB INTERFACE AVAILABLE AT
<http://issues.apache.org/bugzilla/show_bug.cgi?id=27470>.
ANY REPLY MADE TO THIS MESSAGE WILL NOT BE COLLECTED AND 
INSERTED IN THE BUG DATABASE.

http://issues.apache.org/bugzilla/show_bug.cgi?id=27470

Upgrade all source files to ASF 2.0 license

gregor@apache.org changed:

           What    |Removed                     |Added
----------------------------------------------------------------------------
   Target Milestone|1.2                         |1.4



------- Additional Comments From gregor@apache.org  2004-04-16 17:09 -------
we believe this is done. i will leave the bug open for reference.

---------------------------------------------------------------------
To unsubscribe, e-mail: lenya-dev-unsubscribe@cocoon.apache.org
For additional commands, e-mail: lenya-dev-help@cocoon.apache.org


",f
26899,54,JIRA.13113367.1509471930000.113892.1509474182340@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13113367.1509471930000@Atlassian.JIRA,,,2017-10-31 11:23:02-07,"[jira] [Updated] (AIRFLOW-1767) Airflow Scheduler no longer
 schedules DAGs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1767?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1767:
------------------------------------
    Affects Version/s:     (was: 1.10.0)

> Airflow Scheduler no longer schedules DAGs
> ------------------------------------------
>
>                 Key: AIRFLOW-1767
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1767
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0
>         Environment: CeleryExecutor, Docker, 3 Workers
>            Reporter: David Klosowski
>            Priority: Blocker
>             Fix For: 1.9.0
>
>
> The Airflow Scheduler no longer schedules DAGs after this commit on master:
> https://github.com/apache/incubator-airflow/commit/73549763eac74142b7c4018422bb2f8c897b45a8
> Workers never receive any tasks and the scheduler never adjusts DAG state.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26900,54,JIRA.13112850.1509220003000.114018.1509474781051@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112850.1509220003000@Atlassian.JIRA,,,2017-10-31 11:33:01-07,"[jira] [Commented] (AIRFLOW-1763) S3 Task Handler unit tests not
 running","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1763?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16227284#comment-16227284 ] 

ASF subversion and git services commented on AIRFLOW-1763:
----------------------------------------------------------

Commit 44710d7e9ef8b8955ff44da2f55cfb0e0774629e in incubator-airflow''s branch refs/heads/master from [~andyxhadji]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=44710d7 ]

[AIRFLOW-1763] Fix S3TaskHandler unit tests

Fix breaking S3TaskHandler unit tests, and create
a package so that
tests are identified by CI.

Closes #2732 from andyxhadji/AIRFLOW-1763


> S3 Task Handler unit tests not running
> --------------------------------------
>
>                 Key: AIRFLOW-1763
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1763
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>
> S3 Task Handler tests are not being run, because the folder isn''t being discovered due to it not being a python package. Furthermore, these tests are broken (lots of updates to s3_task_handler since tests were run), so they need to be fixed. (different initialization, changed method names and slightly altered return values.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26901,54,JIRA.13112850.1509220003000.114020.1509474781076@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112850.1509220003000@Atlassian.JIRA,,,2017-10-31 11:33:01-07,"[jira] [Commented] (AIRFLOW-1763) S3 Task Handler unit tests not
 running","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1763?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16227285#comment-16227285 ] 

ASF subversion and git services commented on AIRFLOW-1763:
----------------------------------------------------------

Commit 44710d7e9ef8b8955ff44da2f55cfb0e0774629e in incubator-airflow''s branch refs/heads/master from [~andyxhadji]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=44710d7 ]

[AIRFLOW-1763] Fix S3TaskHandler unit tests

Fix breaking S3TaskHandler unit tests, and create
a package so that
tests are identified by CI.

Closes #2732 from andyxhadji/AIRFLOW-1763


> S3 Task Handler unit tests not running
> --------------------------------------
>
>                 Key: AIRFLOW-1763
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1763
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>
> S3 Task Handler tests are not being run, because the folder isn''t being discovered due to it not being a python package. Furthermore, these tests are broken (lots of updates to s3_task_handler since tests were run), so they need to be fixed. (different initialization, changed method names and slightly altered return values.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26902,54,JIRA.13112850.1509220003000.114032.1509474840278@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13112850.1509220003000@Atlassian.JIRA,,,2017-10-31 11:34:00-07,"[jira] [Resolved] (AIRFLOW-1763) S3 Task Handler unit tests not
 running","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1763?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1763.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2732
[https://github.com/apache/incubator-airflow/pull/2732]

> S3 Task Handler unit tests not running
> --------------------------------------
>
>                 Key: AIRFLOW-1763
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1763
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>             Fix For: 1.10.0
>
>
> S3 Task Handler tests are not being run, because the folder isn''t being discovered due to it not being a python package. Furthermore, these tests are broken (lots of updates to s3_task_handler since tests were run), so they need to be fixed. (different initialization, changed method names and slightly altered return values.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26903,54,JIRA.13036876.1485085321000.114101.1509475260065@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13036876.1485085321000@Atlassian.JIRA,,,2017-10-31 11:41:00-07,"[jira] [Commented] (AIRFLOW-788) Context unexpectedly added to hive
 conf","
    [ https://issues.apache.org/jira/browse/AIRFLOW-788?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16227301#comment-16227301 ] 

Ace Haidrey commented on AIRFLOW-788:
-------------------------------------

Should we add an option in the HiveOperator to pass this or not?

> Context unexpectedly added to hive conf
> ---------------------------------------
>
>                 Key: AIRFLOW-788
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-788
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hive_hooks
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>
> If specifying hive_conf to run_cli extra variables are added from the context, e.g. airflow.ctx.dag.dag_id . 
> In secured environments this can raise the need for a configuration change as these variables might not be whitelisted.
> Secondly one could regard it as information leakage, as its is added without the user''s consent.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
1103828,215,20040605011936.2DE2884577@bugzilla.spamassassin.org,36968,bugzilla-daemon,NULL,,,2004-06-04 18:19:36-07,[Bug 3334] journal_live_path returning an invalid path for bayes _journal,"http://bugzilla.spamassassin.org/show_bug.cgi?id=3334





------- Additional Comments From jm@jmason.org  2004-06-04 18:19 -------
Mike -- maintain a copy of what the $conf member was, at the last time the
sed_path() lookup was performed.  if that changes, then the cached value is
invalidated.



------- You are receiving this mail because: -------
You are on the CC list for the bug, or are watching someone who is.

",f
26904,54,JIRA.13036876.1485085321000.114102.1509475260074@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13036876.1485085321000@Atlassian.JIRA,,,2017-10-31 11:41:00-07,"[jira] [Comment Edited] (AIRFLOW-788) Context unexpectedly added to
 hive conf","
    [ https://issues.apache.org/jira/browse/AIRFLOW-788?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16227301#comment-16227301 ] 

Ace Haidrey edited comment on AIRFLOW-788 at 10/31/17 6:40 PM:
---------------------------------------------------------------

Should we add an option in the HiveOperator to pass this or not? By default have it not pass it


was (Author: ahaidrey):
Should we add an option in the HiveOperator to pass this or not?

> Context unexpectedly added to hive conf
> ---------------------------------------
>
>                 Key: AIRFLOW-788
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-788
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hive_hooks
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>
> If specifying hive_conf to run_cli extra variables are added from the context, e.g. airflow.ctx.dag.dag_id . 
> In secured environments this can raise the need for a configuration change as these variables might not be whitelisted.
> Secondly one could regard it as information leakage, as its is added without the user''s consent.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26905,54,JIRA.13113398.1509475897000.114233.1509475920510@Atlassian.JIRA,2251,Alison Stanton (JIRA),JIRA.13113398.1509475897000@Atlassian.JIRA,,,2017-10-31 11:52:00-07,"[jira] [Created] (AIRFLOW-1768) DAG list allows triggering a paused
 DAG","Alison Stanton created AIRFLOW-1768:
---------------------------------------

             Summary: DAG list allows triggering a paused DAG
                 Key: AIRFLOW-1768
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1768
             Project: Apache Airflow
          Issue Type: Bug
          Components: ui
    Affects Versions: 1.8.0
            Reporter: Alison Stanton
            Assignee: Alison Stanton
            Priority: Minor


Right now the admin DAG list (/admin/) will allow you to click the run icon for a DAG that is paused. It then shows up in the DAG Runs page (/admin/dagrun/) as running but never progresses because the DAG is paused. This is confusing to the user. You shouldn''t be able to manually trigger a paused DAG. 

I am working on a PR to add the simple if statement to change this behavior.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26906,54,JIRA.13113402.1509476392000.114331.1509476402741@Atlassian.JIRA,1516,Alex Guziel (JIRA),JIRA.13113402.1509476392000@Atlassian.JIRA,,,2017-10-31 12:00:02-07,"[jira] [Created] (AIRFLOW-1769) Accept templates_dict in
 PythonVirtualenvOperator","Alex Guziel created AIRFLOW-1769:
------------------------------------

             Summary: Accept templates_dict in PythonVirtualenvOperator
                 Key: AIRFLOW-1769
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1769
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Alex Guziel
            Assignee: Alex Guziel






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26907,54,JIRA.13113398.1509475897000.114333.1509476460237@Atlassian.JIRA,2251,Alison Stanton (JIRA),JIRA.13113398.1509475897000@Atlassian.JIRA,,,2017-10-31 12:01:00-07,"[jira] [Updated] (AIRFLOW-1768) DAG list allows triggering a paused
 DAG","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1768?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alison Stanton updated AIRFLOW-1768:
------------------------------------
    External issue URL: https://github.com/apache/incubator-airflow/pull/2740

> DAG list allows triggering a paused DAG
> ---------------------------------------
>
>                 Key: AIRFLOW-1768
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1768
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.0
>            Reporter: Alison Stanton
>            Assignee: Alison Stanton
>            Priority: Minor
>              Labels: easyfix
>
> Right now the admin DAG list (/admin/) will allow you to click the run icon for a DAG that is paused. It then shows up in the DAG Runs page (/admin/dagrun/) as running but never progresses because the DAG is paused. This is confusing to the user. You shouldn''t be able to manually trigger a paused DAG. 
> I am working on a PR to add the simple if statement to change this behavior.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26908,54,JIRA.13113408.1509476795000.114377.1509476828523@Atlassian.JIRA,2138,Ace Haidrey (JIRA),JIRA.13113408.1509476795000@Atlassian.JIRA,,,2017-10-31 12:07:08-07,"[jira] [Created] (AIRFLOW-1770) Add option to file and hiveconfs in
 HiveOperator","Ace Haidrey created AIRFLOW-1770:
------------------------------------

             Summary: Add option to file and hiveconfs in HiveOperator
                 Key: AIRFLOW-1770
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1770
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Ace Haidrey
            Assignee: Ace Haidrey


The HiveOperator as it currently stands is not flexible enough to accept a hive file and operate on that. You need to read in the contents and pass it and if you do that you need to change the way hiveconfs are in your file to jinja templating.
Many teams already have their existing sql/hql files and don''t want to convert them to make them as portable and decoupled as possible.
To accomplish this all we need to do is add the option to pass a hql_file and hiveconfs to the HiveOperator. We change the code in the execute to throw an error if both a hql_file and an hql statement are passed. If just hql_file the simplest way without changing the code of the hive hook is to just read the content of the hql_file and set it to be the self.hql. The hiveconfs get passed directly to the run_cli method and we can combine them with the already passed in hiveconfs.

If we want to make it optional to pass in the context as hiveconfs we can add that too as related to AIRFLOW-788.

I''ve included some simple tests to show it all works how we expect.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26909,54,JIRA.13113430.1509485081000.116641.1509485100188@Atlassian.JIRA,1516,Alex Guziel (JIRA),JIRA.13113430.1509485081000@Atlassian.JIRA,,,2017-10-31 14:25:00-07,"[jira] [Created] (AIRFLOW-1771) Change heartbeat text from boom to
 heartbeat","Alex Guziel created AIRFLOW-1771:
------------------------------------

             Summary: Change heartbeat text from boom to heartbeat
                 Key: AIRFLOW-1771
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1771
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Alex Guziel
            Assignee: Alex Guziel


This sounds like something went wrong to users.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
1103829,215,20040605012028.3FEF984575@bugzilla.spamassassin.org,36968,bugzilla-daemon,NULL,,,2004-06-04 18:20:28-07,[Bug 3446] spamd locks up randomly,"http://bugzilla.spamassassin.org/show_bug.cgi?id=3446





------- Additional Comments From jm@jmason.org  2004-06-04 18:20 -------
+1



------- You are receiving this mail because: -------
You are the assignee for the bug, or are watching the assignee.

",f
26910,54,JIRA.13113402.1509476392000.116843.1509487140170@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13113402.1509476392000@Atlassian.JIRA,,,2017-10-31 14:59:00-07,"[jira] [Commented] (AIRFLOW-1769) Accept templates_dict in
 PythonVirtualenvOperator","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1769?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16227673#comment-16227673 ] 

ASF subversion and git services commented on AIRFLOW-1769:
----------------------------------------------------------

Commit 52f8d7da9da177b86d036289218377220f6610c2 in incubator-airflow''s branch refs/heads/master from [~saguziel]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=52f8d7d ]

[AIRFLOW-1769] Add support for templates in VirtualenvOperator

Closes #2741 from saguziel/aguziel-virtualenv-
templates


> Accept templates_dict in PythonVirtualenvOperator
> -------------------------------------------------
>
>                 Key: AIRFLOW-1769
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1769
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Alex Guziel
>            Assignee: Alex Guziel
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26911,54,JIRA.13113402.1509476392000.116845.1509487140284@Atlassian.JIRA,1516,Alex Guziel (JIRA),JIRA.13113402.1509476392000@Atlassian.JIRA,,,2017-10-31 14:59:00-07,"[jira] [Resolved] (AIRFLOW-1769) Accept templates_dict in
 PythonVirtualenvOperator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1769?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alex Guziel resolved AIRFLOW-1769.
----------------------------------
    Resolution: Fixed

> Accept templates_dict in PythonVirtualenvOperator
> -------------------------------------------------
>
>                 Key: AIRFLOW-1769
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1769
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Alex Guziel
>            Assignee: Alex Guziel
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26912,54,JIRA.13113430.1509485081000.116848.1509487200462@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13113430.1509485081000@Atlassian.JIRA,,,2017-10-31 15:00:00-07,"[jira] [Commented] (AIRFLOW-1771) Change heartbeat text from boom
 to heartbeat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1771?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16227675#comment-16227675 ] 

ASF subversion and git services commented on AIRFLOW-1771:
----------------------------------------------------------

Commit 6b8fe9857429d509ea95c426e6d4271269e8c29f in incubator-airflow''s branch refs/heads/master from [~saguziel]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6b8fe98 ]

[AIRFLOW-1771] Rename heartbeat to avoid confusion

Closes #2743 from saguziel/aguziel-heartbeat


> Change heartbeat text from boom to heartbeat
> --------------------------------------------
>
>                 Key: AIRFLOW-1771
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1771
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Alex Guziel
>            Assignee: Alex Guziel
>             Fix For: 1.9.0
>
>
> This sounds like something went wrong to users.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26913,54,JIRA.13113430.1509485081000.116850.1509487201038@Atlassian.JIRA,1516,Alex Guziel (JIRA),JIRA.13113430.1509485081000@Atlassian.JIRA,,,2017-10-31 15:00:01-07,"[jira] [Resolved] (AIRFLOW-1771) Change heartbeat text from boom to
 heartbeat","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1771?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alex Guziel resolved AIRFLOW-1771.
----------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2743
[https://github.com/apache/incubator-airflow/pull/2743]

> Change heartbeat text from boom to heartbeat
> --------------------------------------------
>
>                 Key: AIRFLOW-1771
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1771
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Alex Guziel
>            Assignee: Alex Guziel
>             Fix For: 1.9.0
>
>
> This sounds like something went wrong to users.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)
",t
43847,54,154329767558.31417.10952856427723935171.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 21:47:55-08,"[GitHub] dlamblin opened a new pull request #4241: [AIRFLOW-XXX] Correct
 typos in UPDATING.md","dlamblin opened a new pull request #4241: [AIRFLOW-XXX] Correct typos in UPDATING.md
URL: https://github.com/apache/incubator-airflow/pull/4241
 
 
   Make sure you have checked _all_ steps below.
   
   ### Jira
   
   - [x] My PR addresses fixing a typo in the documentation and so prepends commit with \[AIRFLOW-XXX\].
   
   ### Description
   
   Applies to v1-10-stable; another pull will apply to master.
   
   - Started with ""habe"", ""serever"" and ""certificiate"" needing to be:  
     ""have"", ""server"", and ""certificate"".
   - Ran a check, ignoring British and US accepted spellings.
   - Kept jargon. EG admin, aync, auth, backend, config, dag, s3, utils, etc.
   - Took exception to:
     - ""num of dag run"" meaning ""number of dag runs"",
     - ""upness"" is normally for quarks,
     - ""url"" being lower-case, and
     - sftp example having an excess file ending.
   - Python documentation writes ""builtin"" hyphenated, cases ""PYTHONPATH"".
   - Gave up on mixed use of ""dag"" and ""DAG"" as well as long line lengths.
   
   ### Tests
   
   - [x] My PR does not need testing for this extremely good reason: It is scoped to documentation.
   
   ### Commits
   
   - [x] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from ""[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)"":
     1. Subject is separated from body by a blank line
     1. Subject is limited to 50 characters (not including Jira issue reference)
     1. Subject does not end with a period
     1. Subject uses the imperative mood (""add"", not ""adding"")
     1. Body wraps at 72 characters
     1. Body explains ""what"" and ""why"", not ""how""
   
   ### Documentation
   
   - [x] Updates only existing documentation.
   
   ### Code Quality
   
   - [x] Passes `flake8`
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
43848,54,154329776793.32654.414955578735894929.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 21:49:27-08,"[GitHub] feng-tao commented on issue #4235: [AIRFLOW-3392] Add index on
 dag_id in sla_miss table","feng-tao commented on issue #4235: [AIRFLOW-3392] Add index on dag_id in sla_miss table
URL: https://github.com/apache/incubator-airflow/pull/4235#issuecomment-441930254
 
 
   lgtm. thanks @KevinYang21 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
43849,54,154329777396.32702.17383448363590554982.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 21:49:33-08,"[GitHub] feng-tao closed pull request #4235: [AIRFLOW-3392] Add index on
 dag_id in sla_miss table","feng-tao closed pull request #4235: [AIRFLOW-3392] Add index on dag_id in sla_miss table
URL: https://github.com/apache/incubator-airflow/pull/4235
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won''t show otherwise due to GitHub magic):

diff --git a/airflow/migrations/versions/03bc53e68815_add_sm_dag_index.py b/airflow/migrations/versions/03bc53e68815_add_sm_dag_index.py
new file mode 100644
index 0000000000..fc8468155c
--- /dev/null
+++ b/airflow/migrations/versions/03bc53e68815_add_sm_dag_index.py
@@ -0,0 +1,40 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+""""""merge_heads_2
+
+Revision ID: 03bc53e68815
+Revises: 0a2a5b66e19d, bf00311e1990
+Create Date: 2018-11-24 20:21:46.605414
+
+""""""
+
+from alembic import op
+
+# revision identifiers, used by Alembic.
+revision = ''03bc53e68815''
+down_revision = (''0a2a5b66e19d'', ''bf00311e1990'')
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    op.create_index(''sm_dag'', ''sla_miss'', [''dag_id''], unique=False)
+
+
+def downgrade():
+    op.drop_index(''sm_dag'', table_name=''sla_miss'')
diff --git a/airflow/models.py b/airflow/models.py
index 9ab2348cc2..4b8b7dc5c9 100755
--- a/airflow/models.py
+++ b/airflow/models.py
@@ -5481,6 +5481,10 @@ class SlaMiss(Base):
     description = Column(Text)
     notification_sent = Column(Boolean, default=False)
 
+    __table_args__ = (
+        Index(''sm_dag'', dag_id, unique=False),
+    )
+
     def __repr__(self):
         return str((
             self.dag_id, self.task_id, self.execution_date.isoformat()))


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
43850,54,JIRA.13200399.1543120717000.35397.1543297800087@Atlassian.JIRA,3026,ASF GitHub Bot (JIRA),JIRA.13200399.1543120717000@Atlassian.JIRA,,,2018-11-26 21:50:00-08,"[jira] [Commented] (AIRFLOW-3392) Add index on dag_id in sla_miss
 table","
    [ https://issues.apache.org/jira/browse/AIRFLOW-3392?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16699947#comment-16699947 ] 

ASF GitHub Bot commented on AIRFLOW-3392:
-----------------------------------------

feng-tao closed pull request #4235: [AIRFLOW-3392] Add index on dag_id in sla_miss table
URL: https://github.com/apache/incubator-airflow/pull/4235
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won''t show otherwise due to GitHub magic):

diff --git a/airflow/migrations/versions/03bc53e68815_add_sm_dag_index.py b/airflow/migrations/versions/03bc53e68815_add_sm_dag_index.py
new file mode 100644
index 0000000000..fc8468155c
--- /dev/null
+++ b/airflow/migrations/versions/03bc53e68815_add_sm_dag_index.py
@@ -0,0 +1,40 @@
+# Licensed to the Apache Software Foundation (ASF) under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  The ASF licenses this file
+# to you under the Apache License, Version 2.0 (the
+# ""License""); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#   http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+""""""merge_heads_2
+
+Revision ID: 03bc53e68815
+Revises: 0a2a5b66e19d, bf00311e1990
+Create Date: 2018-11-24 20:21:46.605414
+
+""""""
+
+from alembic import op
+
+# revision identifiers, used by Alembic.
+revision = ''03bc53e68815''
+down_revision = (''0a2a5b66e19d'', ''bf00311e1990'')
+branch_labels = None
+depends_on = None
+
+
+def upgrade():
+    op.create_index(''sm_dag'', ''sla_miss'', [''dag_id''], unique=False)
+
+
+def downgrade():
+    op.drop_index(''sm_dag'', table_name=''sla_miss'')
diff --git a/airflow/models.py b/airflow/models.py
index 9ab2348cc2..4b8b7dc5c9 100755
--- a/airflow/models.py
+++ b/airflow/models.py
@@ -5481,6 +5481,10 @@ class SlaMiss(Base):
     description = Column(Text)
     notification_sent = Column(Boolean, default=False)
 
+    __table_args__ = (
+        Index(''sm_dag'', dag_id, unique=False),
+    )
+
     def __repr__(self):
         return str((
             self.dag_id, self.task_id, self.execution_date.isoformat()))


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


> Add index on dag_id in sla_miss table
> -------------------------------------
>
>                 Key: AIRFLOW-3392
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-3392
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>            Priority: Major
>
> The select queries on sla_miss table produce a great %%%% of DB traffic and thus made the DB CPU usage unnecessarily high. It would be a low hanging fruit to add an index and reduce the load.



--
This message was sent by Atlassian JIRA
(v7.6.3#76005)

",t
26914,54,JIRA.13086399.1499811601000.118120.1509504182609@Atlassian.JIRA,2176,Megan Patten (JIRA),JIRA.13086399.1499811601000@Atlassian.JIRA,,,2017-10-31 19:43:02-07,"[jira] [Commented] (AIRFLOW-1403) UI did not prevent inserting
 empty key/value","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1403?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16233583#comment-16233583 ] 

Megan Patten commented on AIRFLOW-1403:
---------------------------------------

This issue was resolved by https://github.com/apache/incubator-airflow/pull/2299 and is available in 1.8.2

> UI did not prevent inserting empty key/value
> --------------------------------------------
>
>                 Key: AIRFLOW-1403
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1403
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: Airflow 1.8
>            Reporter: Chengzhi Zhao
>            Priority: Major
>
> How to reproduce: 
> # User goes to Airflow UI and select admin-->variables-->create. 
> # Without typing anything, just save directly. 
> # variables page throw an error and cannot update/delete variables via UI anymore, have to go to DB and delete the null value record.
> Thought on fix: add validation before inserting data to DB



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26915,54,JIRA.13106155.1506722912000.118551.1509512160033@Atlassian.JIRA,2293,Vasanth Kumar (JIRA),JIRA.13106155.1506722912000@Atlassian.JIRA,,,2017-10-31 21:56:00-07,"[jira] [Commented] (AIRFLOW-1665) Airflow webserver/scheduler don''t
 handle database disconnects (mysql)","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1665?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16233659#comment-16233659 ] 

Vasanth Kumar commented on AIRFLOW-1665:
----------------------------------------

Submitted proposed fix: https://github.com/apache/incubator-airflow/pull/2744

> Airflow webserver/scheduler don''t handle database disconnects (mysql)
> ---------------------------------------------------------------------
>
>                 Key: AIRFLOW-1665
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1665
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: Vasanth Kumar
>            Priority: Major
>              Labels: database, reconnect
>
> Airflow webserver & scheduler don''t handle database disconnects.  The process appear to error out and either exit or are left in an off state.  This was observed when using mysql.
> I don''t see any database reconnect configuration or code.
> Stack tace for scheduler:
>   File ""...../MySQLdb/connections.py"", line 204, in __init__
>     super(Connection, self).__init__(*args, **kwargs2)
> sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (2002, ""Can''t connect to local MySQL server through socket ''/tmp/mysql.sock'' (2)"")



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26916,54,JIRA.13115319.1509529826000.119581.1509529860062@Atlassian.JIRA,2189,sam elamin (JIRA),JIRA.13115319.1509529826000@Atlassian.JIRA,,,2017-11-01 02:51:00-07,"[jira] [Created] (AIRFLOW-1772) Google Updated Sensor doesnt work
 with CRON expressions","sam elamin created AIRFLOW-1772:
-----------------------------------

             Summary: Google Updated Sensor doesnt work with CRON expressions 
                 Key: AIRFLOW-1772
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1772
             Project: Apache Airflow
          Issue Type: Bug
          Components: gcp
            Reporter: sam elamin
            Priority: Normal


Hi

I noticed that the (GoogleCloudStorageObjectUpdatedSensor)[https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval

According to the airflow docs a scheduler interval can be a timedelta or a string

When its a cron expression the schedule is a string and hence this [https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70](line) breaks 


the error is below

''''str'' object is not callable''

Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26917,54,JIRA.13115319.1509529826000.119582.1509529920433@Atlassian.JIRA,2189,sam elamin (JIRA),JIRA.13115319.1509529826000@Atlassian.JIRA,,,2017-11-01 02:52:00-07,"[jira] [Updated] (AIRFLOW-1772) Google Updated Sensor doesnt work
 with CRON expressions","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1772?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

sam elamin updated AIRFLOW-1772:
--------------------------------
    Description: 
Hi

I noticed that the [Update Sensor|https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval

According to the airflow docs a scheduler interval can be a timedelta or a string

When its a cron expression the schedule is a string and hence this [line | https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70]( breaks 


the error is below

''''str'' object is not callable''

Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 



  was:
Hi

I noticed that the (GoogleCloudStorageObjectUpdatedSensor)[https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval

According to the airflow docs a scheduler interval can be a timedelta or a string

When its a cron expression the schedule is a string and hence this [https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70](line) breaks 


the error is below

''''str'' object is not callable''

Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 




> Google Updated Sensor doesnt work with CRON expressions 
> --------------------------------------------------------
>
>                 Key: AIRFLOW-1772
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1772
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>            Reporter: sam elamin
>            Priority: Normal
>
> Hi
> I noticed that the [Update Sensor|https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval
> According to the airflow docs a scheduler interval can be a timedelta or a string
> When its a cron expression the schedule is a string and hence this [line | https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70]( breaks 
> the error is below
> ''''str'' object is not callable''
> Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26918,54,JIRA.13115319.1509529826000.119583.1509529920447@Atlassian.JIRA,2189,sam elamin (JIRA),JIRA.13115319.1509529826000@Atlassian.JIRA,,,2017-11-01 02:52:00-07,"[jira] [Updated] (AIRFLOW-1772) Google Updated Sensor doesnt work
 with CRON expressions","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1772?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

sam elamin updated AIRFLOW-1772:
--------------------------------
    Description: 
Hi

I noticed that the [Update Sensor|https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval

According to the airflow docs a scheduler interval can be a timedelta or a string

When its a cron expression the schedule is a string and hence this [line | https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70] breaks 


the error is below

''''str'' object is not callable''

Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 



  was:
Hi

I noticed that the [Update Sensor|https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval

According to the airflow docs a scheduler interval can be a timedelta or a string

When its a cron expression the schedule is a string and hence this [line | https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70]( breaks 


the error is below

''''str'' object is not callable''

Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 




> Google Updated Sensor doesnt work with CRON expressions 
> --------------------------------------------------------
>
>                 Key: AIRFLOW-1772
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1772
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>            Reporter: sam elamin
>            Priority: Normal
>
> Hi
> I noticed that the [Update Sensor|https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval
> According to the airflow docs a scheduler interval can be a timedelta or a string
> When its a cron expression the schedule is a string and hence this [line | https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70] breaks 
> the error is below
> ''''str'' object is not callable''
> Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
631168,147,20040416171612.17816.qmail@nagoya.betaversion.org,23834,bugzilla,NULL,,,2004-04-16 10:16:12-07,"DO NOT REPLY [Bug 28185]  - 
    If saving in Kupu the edited file should not be unlocked in RC","DO NOT REPLY TO THIS EMAIL, BUT PLEASE POST YOUR BUG 
RELATED COMMENTS THROUGH THE WEB INTERFACE AVAILABLE AT
<http://issues.apache.org/bugzilla/show_bug.cgi?id=28185>.
ANY REPLY MADE TO THIS MESSAGE WILL NOT BE COLLECTED AND 
INSERTED IN THE BUG DATABASE.

http://issues.apache.org/bugzilla/show_bug.cgi?id=28185

If saving in Kupu the edited file should not be unlocked in RC





------- Additional Comments From gregor@apache.org  2004-04-16 17:16 -------
any news on this? i''ll move to 1.4 unless you check it in in the next 24h :)

---------------------------------------------------------------------
To unsubscribe, e-mail: lenya-dev-unsubscribe@cocoon.apache.org
For additional commands, e-mail: lenya-dev-help@cocoon.apache.org


",f
26919,54,JIRA.13115319.1509529826000.120009.1509534542896@Atlassian.JIRA,2189,sam elamin (JIRA),JIRA.13115319.1509529826000@Atlassian.JIRA,,,2017-11-01 04:09:02-07,"[jira] [Updated] (AIRFLOW-1772) Google Updated Sensor doesnt work
 with CRON expressions","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1772?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

sam elamin updated AIRFLOW-1772:
--------------------------------
    Description: 
Hi

I noticed that the [Update Sensor|https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval

According to the airflow docs a scheduler interval can be a timedelta or a string

When its a cron expression the schedule is a string and hence this [line | https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70] breaks 


the error is below

''''str'' object is not callable''

Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 


Also as a side note i noticed that the update sensor doesnt match regex and has to be exact, surly adding regex support here would make sense since the gsutil cli does this as well



  was:
Hi

I noticed that the [Update Sensor|https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval

According to the airflow docs a scheduler interval can be a timedelta or a string

When its a cron expression the schedule is a string and hence this [line | https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70] breaks 


the error is below

''''str'' object is not callable''

Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 




> Google Updated Sensor doesnt work with CRON expressions 
> --------------------------------------------------------
>
>                 Key: AIRFLOW-1772
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1772
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: gcp
>            Reporter: sam elamin
>            Priority: Normal
>
> Hi
> I noticed that the [Update Sensor|https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L73] does not work with a cron expression set as the schedule interval
> According to the airflow docs a scheduler interval can be a timedelta or a string
> When its a cron expression the schedule is a string and hence this [line | https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/sensors/gcs_sensor.py#L70] breaks 
> the error is below
> ''''str'' object is not callable''
> Ideally that line should check if the interval is a cron expression and if it is convert it to a time delta 
> Also as a side note i noticed that the update sensor doesnt match regex and has to be exact, surly adding regex support here would make sense since the gsutil cli does this as well



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26920,54,JIRA.13115339.1509539330000.120355.1509539340036@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:29:00-07,[jira] [Created] (AIRFLOW-1773) [Mark Success] not work in UI,"Winty created AIRFLOW-1773:
------------------------------

             Summary: [Mark Success] not work in UI
                 Key: AIRFLOW-1773
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
             Project: Apache Airflow
          Issue Type: Bug
          Components: api
    Affects Versions: 1.8.2
            Reporter: Winty
            Priority: Major
         Attachments: 22.PNG, 33.PNG

- When I click [Mark Success], there is no task instances 

!33.PNG|thumbnail!

so I try to find what is problem. and i can find somthing wrong.

airflow / api / common / experimental / mark_tasks.py

{code:python}
def set_state(task, execution_date, upstream=False, downstream=False,
              future=False, past=False, state=State.SUCCESS, commit=False):
...
    if dag.schedule_interval == ''@once'':
        dates = [start_date]
    else:
        dates = dag.date_range(start_date=start_date, end_date=end_date)
...
    # verify the integrity of the dag runs in case a task was added or removed
    # set the confirmed execution dates as they might be different
    # from what was provided
    confirmed_dates = []
    drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
    for dr in drs:
        dr.dag = dag
        dr.verify_integrity()
        confirmed_dates.append(dr.execution_date)
{code}

the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 

[...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]

!22.PNG|thumbnail!

so DagRun.find() can not find dags.





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26921,54,JIRA.13115339.1509539330000.120359.1509539460270@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:31:00-07,[jira] [Updated] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Winty updated AIRFLOW-1773:
---------------------------
    Attachment:     (was: 22.PNG)

> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.PNG, 33.PNG
>
>
> - When I click [Mark Success], there is no task instances 
> !33.PNG|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.PNG|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26922,54,JIRA.13115339.1509539330000.120360.1509539460278@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:31:00-07,[jira] [Updated] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Winty updated AIRFLOW-1773:
---------------------------
    Attachment:     (was: 33.PNG)

> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.PNG, 33.PNG
>
>
> - When I click [Mark Success], there is no task instances 
> !33.PNG|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.PNG|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26923,54,JIRA.13115339.1509539330000.120358.1509539460249@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:31:00-07,[jira] [Updated] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Winty updated AIRFLOW-1773:
---------------------------
    Attachment: 22.PNG
                33.PNG

> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.PNG, 33.PNG
>
>
> - When I click [Mark Success], there is no task instances 
> !33.PNG|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.PNG|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26924,54,JIRA.13115339.1509539330000.120365.1509539520119@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:32:00-07,[jira] [Work stopped] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1773 stopped by Winty.
--------------------------------------
> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.PNG, 33.PNG
>
>
> - When I click [Mark Success], there is no task instances 
> !33.PNG|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.PNG|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26925,54,JIRA.13115339.1509539330000.120364.1509539520106@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:32:00-07,[jira] [Work started] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1773 started by Winty.
--------------------------------------
> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.PNG, 33.PNG
>
>
> - When I click [Mark Success], there is no task instances 
> !33.PNG|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.PNG|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26926,54,JIRA.13115339.1509539330000.120434.1509539881057@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:38:01-07,[jira] [Updated] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Winty updated AIRFLOW-1773:
---------------------------
    Attachment:     (was: 22.PNG)

> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.jpg, 33.PNG, 33.jpg
>
>
> - When I click [Mark Success], there is no task instances 
> !33.jpg|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.jpg|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26927,54,JIRA.13115339.1509539330000.120433.1509539881045@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:38:01-07,[jira] [Updated] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Winty updated AIRFLOW-1773:
---------------------------
     Attachment: 22.jpg
                 33.jpg
    Description: 
- When I click [Mark Success], there is no task instances 

!33.jpg|thumbnail!

so I try to find what is problem. and i can find somthing wrong.

airflow / api / common / experimental / mark_tasks.py

{code:python}
def set_state(task, execution_date, upstream=False, downstream=False,
              future=False, past=False, state=State.SUCCESS, commit=False):
...
    if dag.schedule_interval == ''@once'':
        dates = [start_date]
    else:
        dates = dag.date_range(start_date=start_date, end_date=end_date)
...
    # verify the integrity of the dag runs in case a task was added or removed
    # set the confirmed execution dates as they might be different
    # from what was provided
    confirmed_dates = []
    drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
    for dr in drs:
        dr.dag = dag
        dr.verify_integrity()
        confirmed_dates.append(dr.execution_date)
{code}

the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 

[...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]

!22.jpg|thumbnail!

so DagRun.find() can not find dags.



  was:
- When I click [Mark Success], there is no task instances 

!33.PNG|thumbnail!

so I try to find what is problem. and i can find somthing wrong.

airflow / api / common / experimental / mark_tasks.py

{code:python}
def set_state(task, execution_date, upstream=False, downstream=False,
              future=False, past=False, state=State.SUCCESS, commit=False):
...
    if dag.schedule_interval == ''@once'':
        dates = [start_date]
    else:
        dates = dag.date_range(start_date=start_date, end_date=end_date)
...
    # verify the integrity of the dag runs in case a task was added or removed
    # set the confirmed execution dates as they might be different
    # from what was provided
    confirmed_dates = []
    drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
    for dr in drs:
        dr.dag = dag
        dr.verify_integrity()
        confirmed_dates.append(dr.execution_date)
{code}

the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 

[...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]

!22.PNG|thumbnail!

so DagRun.find() can not find dags.




> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.jpg, 33.PNG, 33.jpg
>
>
> - When I click [Mark Success], there is no task instances 
> !33.jpg|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.jpg|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26928,54,JIRA.13115339.1509539330000.120437.1509539941154@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-01 05:39:01-07,[jira] [Updated] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Winty updated AIRFLOW-1773:
---------------------------
    Attachment:     (was: 33.PNG)

> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.jpg, 33.jpg
>
>
> - When I click [Mark Success], there is no task instances 
> !33.jpg|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.jpg|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26929,54,JIRA.13115357.1509543196000.120734.1509543240667@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-01 06:34:00-07,"[jira] [Created] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction operator","Guillermo Rodr=C3=ADguez Cano created AIRFLOW-1774:
-------------------------------------------------

             Summary: Better handling of templated parameters in Google ML =
batch prediction operator
                 Key: AIRFLOW-1774
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
             Project: Apache Airflow
          Issue Type: Improvement
          Components: operators
    Affects Versions: Airflow 2.0, 1.9.0
            Reporter: Guillermo Rodr=C3=ADguez Cano
            Priority: Normal


The MLEngineBatchPredictionOperator does not support well the templated par=
ameter job_id due to a helper function.

Google ML requieres a unique job id, therefore it is critical to have the p=
ossibility to customise the job''s name easily, and preferably with some dat=
a related to the DAG, for example, appending the day to the job''s name.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26930,54,JIRA.13115357.1509543196000.120742.1509543240754@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-01 06:34:00-07,"[jira] [Updated] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction operator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1774?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Guillermo Rodr=C3=ADguez Cano updated AIRFLOW-1774:
----------------------------------------------
    Component/s: contrib

> Better handling of templated parameters in Google ML batch prediction ope=
rator
> -------------------------------------------------------------------------=
-----
>
>                 Key: AIRFLOW-1774
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>    Affects Versions: Airflow 2.0, 1.9.0
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Normal
>
> The MLEngineBatchPredictionOperator does not support well the templated p=
arameter job_id due to a helper function.
> Google ML requieres a unique job id, therefore it is critical to have the=
 possibility to customise the job''s name easily, and preferably with some d=
ata related to the DAG, for example, appending the day to the job''s name.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26931,54,JIRA.13115357.1509543196000.120790.1509543361082@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-01 06:36:01-07,"[jira] [Updated] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction operator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1774?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Guillermo Rodr=C3=ADguez Cano updated AIRFLOW-1774:
----------------------------------------------
    Description:=20
The MLEngineBatchPredictionOperator does not support well the templated par=
ameter job_id due to a helper function used to detect and cleanup bad job_i=
d''s inhibiting the templating engine to work.=20

Google ML requieres a unique job id, therefore it is critical to have the p=
ossibility to customise the job''s name easily, and preferably with some dat=
a related to the DAG, for example, appending the day to the job''s name.

  was:
The MLEngineBatchPredictionOperator does not support well the templated par=
ameter job_id due to a helper function.

Google ML requieres a unique job id, therefore it is critical to have the p=
ossibility to customise the job''s name easily, and preferably with some dat=
a related to the DAG, for example, appending the day to the job''s name.


> Better handling of templated parameters in Google ML batch prediction ope=
rator
> -------------------------------------------------------------------------=
-----
>
>                 Key: AIRFLOW-1774
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>    Affects Versions: Airflow 2.0, 1.9.0
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Normal
>
> The MLEngineBatchPredictionOperator does not support well the templated p=
arameter job_id due to a helper function used to detect and cleanup bad job=
_id''s inhibiting the templating engine to work.=20
> Google ML requieres a unique job id, therefore it is critical to have the=
 possibility to customise the job''s name easily, and preferably with some d=
ata related to the DAG, for example, appending the day to the job''s name.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26932,54,JIRA.13115357.1509543196000.120813.1509543420368@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-01 06:37:00-07,"[jira] [Updated] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction operator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1774?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Guillermo Rodr=C3=ADguez Cano updated AIRFLOW-1774:
----------------------------------------------
    Description:=20
The {{MLEngineBatchPredictionOperator}} does not support well the templated=
 parameter {{job_id}} due to a helper function used to detect and cleanup b=
ad job names inhibiting the templating engine to work.=20

Google ML requieres a unique job id, therefore it is critical to have the p=
ossibility to customise the job''s name easily, and preferably with some dat=
a related to the DAG, for example, appending the day to the job''s name via =
a macro like {{ds}}

  was:
The MLEngineBatchPredictionOperator does not support well the templated par=
ameter job_id due to a helper function used to detect and cleanup bad job_i=
d''s inhibiting the templating engine to work.=20

Google ML requieres a unique job id, therefore it is critical to have the p=
ossibility to customise the job''s name easily, and preferably with some dat=
a related to the DAG, for example, appending the day to the job''s name.


> Better handling of templated parameters in Google ML batch prediction ope=
rator
> -------------------------------------------------------------------------=
-----
>
>                 Key: AIRFLOW-1774
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>    Affects Versions: Airflow 2.0, 1.9.0
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Normal
>
> The {{MLEngineBatchPredictionOperator}} does not support well the templat=
ed parameter {{job_id}} due to a helper function used to detect and cleanup=
 bad job names inhibiting the templating engine to work.=20
> Google ML requieres a unique job id, therefore it is critical to have the=
 possibility to customise the job''s name easily, and preferably with some d=
ata related to the DAG, for example, appending the day to the job''s name vi=
a a macro like {{ds}}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26933,54,JIRA.13115361.1509544234000.120953.1509544260774@Atlassian.JIRA,1940,Niels Zeilemaker (JIRA),JIRA.13115361.1509544234000@Atlassian.JIRA,,,2017-11-01 06:51:00-07,[jira] [Created] (AIRFLOW-1775) Remote file handler for logging,"Niels Zeilemaker created AIRFLOW-1775:
-----------------------------------------

             Summary: Remote file handler for logging
                 Key: AIRFLOW-1775
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1775
             Project: Apache Airflow
          Issue Type: Bug
          Components: logging
            Reporter: Niels Zeilemaker
            Priority: Normal


We''re using a mounted Azure file share to store our log files. Currently, Airflow is writing it''s logs to that fileshare. However, this is making the solution quite expensive, as you pay per action on Azure file shares.

If we would have a remote_file_task_logging handler, we could mimic the s3_task_logging, and copy the file to the fileshare in the close method. Reducing the cost, while still providing persistent storage.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549388,277,JIRA.12986655.1467653918000.29149.1467747011055@Atlassian.JIRA,3026,ASF GitHub Bot (JIRA),JIRA.12986655.1467653918000@Atlassian.JIRA,,,2016-07-05 12:30:11-07,"[jira] [Commented] (IOTA-21) Refactor Built of performer to manage
 multi module dependency more efficient and scala style plugin for code
 quality","
    [ https://issues.apache.org/jira/browse/IOTA-21?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15363081#comment-15363081 ] 

ASF GitHub Bot commented on IOTA-21:
------------------------------------

Github user barbaragomes commented on the issue:

    https://github.com/apache/incubator-iota/pull/5
  
    @phalodi Thank you! We will be merging it soon.


> Refactor Built of performer to manage multi module dependency more efficient and scala style plugin for code quality 
> ---------------------------------------------------------------------------------------------------------------------
>
>                 Key: IOTA-21
>                 URL: https://issues.apache.org/jira/browse/IOTA-21
>             Project: Iota
>          Issue Type: Improvement
>            Reporter: sandeep purohit
>            Assignee: sandeep purohit
>   Original Estimate: 3h
>  Remaining Estimate: 3h
>
> In this improvement refactoring the code to manage multi module dependencies more efficient way and add scala style plugin for code quality .



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26934,54,JIRA.13115357.1509543196000.120962.1509544380278@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-01 06:53:00-07,"[jira] [Updated] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction/training operators","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1774?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Guillermo Rodr=C3=ADguez Cano updated AIRFLOW-1774:
----------------------------------------------
    Summary: Better handling of templated parameters in Google ML batch pre=
diction/training operators  (was: Better handling of templated parameters i=
n Google ML batch prediction operator)

> Better handling of templated parameters in Google ML batch prediction/tra=
ining operators
> -------------------------------------------------------------------------=
---------------
>
>                 Key: AIRFLOW-1774
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>    Affects Versions: Airflow 2.0, 1.9.0
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Normal
>
> The {{MLEngineBatchPredictionOperator}} does not support well the templat=
ed parameter {{job_id}} due to a helper function used to detect and cleanup=
 bad job names inhibiting the templating engine to work.=20
> Google ML requieres a unique job id, therefore it is critical to have the=
 possibility to customise the job''s name easily, and preferably with some d=
ata related to the DAG, for example, appending the day to the job''s name vi=
a a macro like {{ds}}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26935,54,JIRA.13115357.1509543196000.120972.1509544440696@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-01 06:54:00-07,"[jira] [Updated] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction/training operators","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1774?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Guillermo Rodr=C3=ADguez Cano updated AIRFLOW-1774:
----------------------------------------------
    Description:=20
The {{MLEngineBatchPredictionOperator}} does not support well the templated=
 parameter {{job_id}} due to a helper function used to detect and cleanup b=
ad job names inhibiting the templating engine to work. I suspect the same m=
ay happen with {{MLEngineTrainingOperator}} as well.

Google ML requieres a unique job id, therefore it is critical to have the p=
ossibility to customise the job''s name easily, and preferably with some dat=
a related to the DAG, for example, appending the day to the job''s name via =
a macro like {{ds}}

  was:
The {{MLEngineBatchPredictionOperator}} does not support well the templated=
 parameter {{job_id}} due to a helper function used to detect and cleanup b=
ad job names inhibiting the templating engine to work.=20

Google ML requieres a unique job id, therefore it is critical to have the p=
ossibility to customise the job''s name easily, and preferably with some dat=
a related to the DAG, for example, appending the day to the job''s name via =
a macro like {{ds}}


> Better handling of templated parameters in Google ML batch prediction/tra=
ining operators
> -------------------------------------------------------------------------=
---------------
>
>                 Key: AIRFLOW-1774
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>    Affects Versions: Airflow 2.0, 1.9.0
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Normal
>
> The {{MLEngineBatchPredictionOperator}} does not support well the templat=
ed parameter {{job_id}} due to a helper function used to detect and cleanup=
 bad job names inhibiting the templating engine to work. I suspect the same=
 may happen with {{MLEngineTrainingOperator}} as well.
> Google ML requieres a unique job id, therefore it is critical to have the=
 possibility to customise the job''s name easily, and preferably with some d=
ata related to the DAG, for example, appending the day to the job''s name vi=
a a macro like {{ds}}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26936,54,JIRA.13115367.1509545715000.121104.1509545760239@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13115367.1509545715000@Atlassian.JIRA,,,2017-11-01 07:16:00-07,[jira] [Created] (AIRFLOW-1776) stdout/stderr logging not captured,"Bolke de Bruin created AIRFLOW-1776:
---------------------------------------

             Summary: stdout/stderr logging not captured
                 Key: AIRFLOW-1776
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1776
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Bolke de Bruin
            Priority: Critical






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26937,54,JIRA.13115367.1509545715000.121105.1509545760260@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13115367.1509545715000@Atlassian.JIRA,,,2017-11-01 07:16:00-07,[jira] [Updated] (AIRFLOW-1776) stdout/stderr logging not captured,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1776?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1776:
------------------------------------
    Fix Version/s: 1.9.0

> stdout/stderr logging not captured
> ----------------------------------
>
>                 Key: AIRFLOW-1776
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1776
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Critical
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26938,54,JIRA.13106155.1506722912000.121180.1509546180333@Atlassian.JIRA,2293,Vasanth Kumar (JIRA),JIRA.13106155.1506722912000@Atlassian.JIRA,,,2017-11-01 07:23:00-07,"[jira] [Work started] (AIRFLOW-1665) Airflow webserver/scheduler
 don''t handle database disconnects (mysql)","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1665?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1665 started by Vasanth Kumar.
----------------------------------------------
> Airflow webserver/scheduler don''t handle database disconnects (mysql)
> ---------------------------------------------------------------------
>
>                 Key: AIRFLOW-1665
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1665
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: Vasanth Kumar
>            Assignee: Vasanth Kumar
>            Priority: Major
>              Labels: database, reconnect
>
> Airflow webserver & scheduler don''t handle database disconnects.  The process appear to error out and either exit or are left in an off state.  This was observed when using mysql.
> I don''t see any database reconnect configuration or code.
> Stack tace for scheduler:
>   File ""...../MySQLdb/connections.py"", line 204, in __init__
>     super(Connection, self).__init__(*args, **kwargs2)
> sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (2002, ""Can''t connect to local MySQL server through socket ''/tmp/mysql.sock'' (2)"")



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549389,277,JIRA.12986655.1467653918000.29705.1467750671006@Atlassian.JIRA,3026,ASF GitHub Bot (JIRA),JIRA.12986655.1467653918000@Atlassian.JIRA,,,2016-07-05 13:31:11-07,"[jira] [Commented] (IOTA-21) Refactor Built of performer to manage
 multi module dependency more efficient and scala style plugin for code
 quality","
    [ https://issues.apache.org/jira/browse/IOTA-21?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15363165#comment-15363165 ] 

ASF GitHub Bot commented on IOTA-21:
------------------------------------

Github user asfgit closed the pull request at:

    https://github.com/apache/incubator-iota/pull/5


> Refactor Built of performer to manage multi module dependency more efficient and scala style plugin for code quality 
> ---------------------------------------------------------------------------------------------------------------------
>
>                 Key: IOTA-21
>                 URL: https://issues.apache.org/jira/browse/IOTA-21
>             Project: Iota
>          Issue Type: Improvement
>            Reporter: sandeep purohit
>            Assignee: sandeep purohit
>   Original Estimate: 3h
>  Remaining Estimate: 3h
>
> In this improvement refactoring the code to manage multi module dependencies more efficient way and add scala style plugin for code quality .



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26939,54,JIRA.13112995.1509361230000.121195.1509546421870@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112995.1509361230000@Atlassian.JIRA,,,2017-11-01 07:27:01-07,"[jira] [Commented] (AIRFLOW-1764) Web Interface should not use
 experimental api","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1764?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16234137#comment-16234137 ] 

ASF subversion and git services commented on AIRFLOW-1764:
----------------------------------------------------------

Commit 0bf7adb209ce969243ffaf4fc5213ff3957cbbc9 in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=0bf7adb ]

[AIRFLOW-1764] The web interface should not use the experimental API

The web interface should not use the experimental
api as the
authentication options differ between the two.
Additionally, rather than
having an API call to get the last run data we can
easily include it in
the generated HMTL response. One less round-trip,
less endpoints, and
less time before the page has fully rendered.

This is based original off @NielsZeilemaker''s PR
for the same Jira
issue (#2734)

Closes #2738 from ashb/no-exp-api-from-web-
interface


> Web Interface should not use experimental api
> ---------------------------------------------
>
>                 Key: AIRFLOW-1764
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1764
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>            Priority: Major
>             Fix For: 1.9.0
>
>
> The web interface should not use the experimental api as the authentication options differ between the two. This means that the latest_runs call should be moved into the web interface.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26940,54,JIRA.13112995.1509361230000.121197.1509546421901@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13112995.1509361230000@Atlassian.JIRA,,,2017-11-01 07:27:01-07,"[jira] [Commented] (AIRFLOW-1764) Web Interface should not use
 experimental api","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1764?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16234138#comment-16234138 ] 

ASF subversion and git services commented on AIRFLOW-1764:
----------------------------------------------------------

Commit 6bed1dc696c924044bf11b3eecce803a56d3b484 in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6bed1dc ]

[AIRFLOW-1764] The web interface should not use the experimental API

The web interface should not use the experimental
api as the
authentication options differ between the two.
Additionally, rather than
having an API call to get the last run data we can
easily include it in
the generated HMTL response. One less round-trip,
less endpoints, and
less time before the page has fully rendered.

This is based original off @NielsZeilemaker''s PR
for the same Jira
issue (#2734)

Closes #2738 from ashb/no-exp-api-from-web-
interface

(cherry picked from commit 0bf7adb209ce969243ffaf4fc5213ff3957cbbc9)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Web Interface should not use experimental api
> ---------------------------------------------
>
>                 Key: AIRFLOW-1764
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1764
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>            Priority: Major
>             Fix For: 1.9.0
>
>
> The web interface should not use the experimental api as the authentication options differ between the two. This means that the latest_runs call should be moved into the web interface.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26941,54,JIRA.13112995.1509361230000.121203.1509546480321@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13112995.1509361230000@Atlassian.JIRA,,,2017-11-01 07:28:00-07,"[jira] [Resolved] (AIRFLOW-1764) Web Interface should not use
 experimental api","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1764?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1764.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2738
[https://github.com/apache/incubator-airflow/pull/2738]

> Web Interface should not use experimental api
> ---------------------------------------------
>
>                 Key: AIRFLOW-1764
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1764
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>            Priority: Major
>             Fix For: 1.9.0
>
>
> The web interface should not use the experimental api as the authentication options differ between the two. This means that the latest_runs call should be moved into the web interface.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26942,54,JIRA.13113100.1509386655000.121257.1509547140139@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-11-01 07:39:00-07,"[jira] [Commented] (AIRFLOW-1765) Default API auth backed should
 deny all.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16234151#comment-16234151 ] 

ASF subversion and git services commented on AIRFLOW-1765:
----------------------------------------------------------

Commit 0e27e1b209e77f22e79e00c2f2e3ab542195405c in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=0e27e1b ]

[AIRFLOW-1765] Make experimental API securable without needing Kerberos.

Previously the experimental API was either wide-
open only (allow any
request) or secured behind Kerberos. This adds a
third option of
deny-all.

Closes #2737 from ashb/exp-api-securable


> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>            Priority: Major
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549390,277,JIRA.12986655.1467653918000.34344.1467787270966@Atlassian.JIRA,8143,sandeep purohit (JIRA),JIRA.12986655.1467653918000@Atlassian.JIRA,,,2016-07-05 23:41:10-07,"[jira] [Resolved] (IOTA-21) Refactor Built of performer to manage
 multi module dependency more efficient and scala style plugin for code
 quality","
     [ https://issues.apache.org/jira/browse/IOTA-21?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

sandeep purohit resolved IOTA-21.
---------------------------------
    Resolution: Fixed

> Refactor Built of performer to manage multi module dependency more efficient and scala style plugin for code quality 
> ---------------------------------------------------------------------------------------------------------------------
>
>                 Key: IOTA-21
>                 URL: https://issues.apache.org/jira/browse/IOTA-21
>             Project: Iota
>          Issue Type: Improvement
>            Reporter: sandeep purohit
>            Assignee: sandeep purohit
>   Original Estimate: 3h
>  Remaining Estimate: 3h
>
> In this improvement refactoring the code to manage multi module dependencies more efficient way and add scala style plugin for code quality .



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26943,54,JIRA.13113100.1509386655000.121267.1509547200573@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-11-01 07:40:00-07,"[jira] [Resolved] (AIRFLOW-1765) Default API auth backed should
 deny all.","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1765.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2737
[https://github.com/apache/incubator-airflow/pull/2737]

> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>            Priority: Major
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26944,54,JIRA.13113100.1509386655000.121261.1509547200520@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13113100.1509386655000@Atlassian.JIRA,,,2017-11-01 07:40:00-07,"[jira] [Commented] (AIRFLOW-1765) Default API auth backed should
 deny all.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1765?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16234152#comment-16234152 ] 

ASF subversion and git services commented on AIRFLOW-1765:
----------------------------------------------------------

Commit 6ecdac701c336da69cccfee2016e9a4f6e90558c in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6ecdac7 ]

[AIRFLOW-1765] Make experimental API securable without needing Kerberos.

Previously the experimental API was either wide-
open only (allow any
request) or secured behind Kerberos. This adds a
third option of
deny-all.

Closes #2737 from ashb/exp-api-securable

(cherry picked from commit 0e27e1b209e77f22e79e00c2f2e3ab542195405c)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Default API auth backed should deny all.
> ----------------------------------------
>
>                 Key: AIRFLOW-1765
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1765
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: api, authentication
>    Affects Versions: 1.8.2
>            Reporter: Ash Berlin-Taylor
>            Priority: Major
>              Labels: security
>             Fix For: 1.9.0
>
>
> It has been discovered that the experimental API in the default configuration is not protected behind any authentication.
> This means that out of the box the Airflow webserver''s /api/experimental/ can be requested by anyone, meaning pools can be updated/deleted and task instance variables can be read.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26945,54,JIRA.13115357.1509543196000.123778.1509564000160@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-01 12:20:00-07,"[jira] [Commented] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction/training operators","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1774?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
234602#comment-16234602 ]=20

Guillermo Rodr=C3=ADguez Cano commented on AIRFLOW-1774:
---------------------------------------------------

And the corresponding PR: https://github.com/apache/incubator-airflow/pull/=
2746

> Better handling of templated parameters in Google ML batch prediction/tra=
ining operators
> -------------------------------------------------------------------------=
---------------
>
>                 Key: AIRFLOW-1774
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>    Affects Versions: Airflow 2.0, 1.9.0
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Normal
>
> The {{MLEngineBatchPredictionOperator}} does not support well the templat=
ed parameter {{job_id}} due to a helper function used to detect and cleanup=
 bad job names inhibiting the templating engine to work. I suspect the same=
 may happen with {{MLEngineTrainingOperator}} as well.
> Google ML requieres a unique job id, therefore it is critical to have the=
 possibility to customise the job''s name easily, and preferably with some d=
ata related to the DAG, for example, appending the day to the job''s name vi=
a a macro like {{ds}}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26946,54,JIRA.13115357.1509543196000.123780.1509564000180@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-01 12:20:00-07,"[jira] [Comment Edited] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction/training operators","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1774?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
234602#comment-16234602 ]=20

Guillermo Rodr=C3=ADguez Cano edited comment on AIRFLOW-1774 at 11/1/17 7:1=
9 PM:
---------------------------------------------------------------------------=
-

And the corresponding PR: https://github.com/apache/incubator-airflow/pull/=
2746 addressing the issue


was (Author: wileeam):
And the corresponding PR: https://github.com/apache/incubator-airflow/pull/=
2746

> Better handling of templated parameters in Google ML batch prediction/tra=
ining operators
> -------------------------------------------------------------------------=
---------------
>
>                 Key: AIRFLOW-1774
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>    Affects Versions: Airflow 2.0, 1.9.0
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Normal
>
> The {{MLEngineBatchPredictionOperator}} does not support well the templat=
ed parameter {{job_id}} due to a helper function used to detect and cleanup=
 bad job names inhibiting the templating engine to work. I suspect the same=
 may happen with {{MLEngineTrainingOperator}} as well.
> Google ML requieres a unique job id, therefore it is critical to have the=
 possibility to customise the job''s name easily, and preferably with some d=
ata related to the DAG, for example, appending the day to the job''s name vi=
a a macro like {{ds}}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26947,54,JIRA.13085941.1499693235000.125324.1509573421268@Atlassian.JIRA,2088,David De La Harpe Golden (JIRA),JIRA.13085941.1499693235000@Atlassian.JIRA,,,2017-11-01 14:57:01-07,[jira] [Updated] (AIRFLOW-1390) Airflow requiring alembic <0.9,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1390?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

David De La Harpe Golden updated AIRFLOW-1390:
----------------------------------------------
    Affects Version/s:     (was: 1.8.1)

> Airflow requiring alembic <0.9
> ------------------------------
>
>                 Key: AIRFLOW-1390
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1390
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: David De La Harpe Golden
>            Priority: Minor
>
> Airflow setup.py currently appears to require alembic <0.9.  However, alembic release is  up to 0.9.3.  Not sure whether this is an oversight or deliberate, but it is potentially inconvenient if you need newer alembic features in the same env as airflow. (In my case, alembic #412 / postgresql exclusion constraint support)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26948,54,JIRA.13085941.1499693235000.125396.1509573421908@Atlassian.JIRA,2088,David De La Harpe Golden (JIRA),JIRA.13085941.1499693235000@Atlassian.JIRA,,,2017-11-01 14:57:01-07,[jira] [Updated] (AIRFLOW-1390) Airflow requiring alembic <0.9,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1390?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

David De La Harpe Golden updated AIRFLOW-1390:
----------------------------------------------
    Description: Airflow setup.py currently appears to require alembic <0.9 \(as at [1.9.0alpha1](https://github.com/apache/incubator-airflow/blob/1.9.0alpha1/setup.py#L211) \)  However, alembic release is  up to 0.9.3.  Not sure whether this is an oversight or deliberate, but it is potentially inconvenient if you need newer alembic features in the same env as airflow. (In my case, alembic #412 / postgresql exclusion constraint support)  (was: Airflow setup.py currently appears to require alembic <0.9.  However, alembic release is  up to 0.9.3.  Not sure whether this is an oversight or deliberate, but it is potentially inconvenient if you need newer alembic features in the same env as airflow. (In my case, alembic #412 / postgresql exclusion constraint support))

> Airflow requiring alembic <0.9
> ------------------------------
>
>                 Key: AIRFLOW-1390
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1390
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: David De La Harpe Golden
>            Priority: Minor
>
> Airflow setup.py currently appears to require alembic <0.9 \(as at [1.9.0alpha1](https://github.com/apache/incubator-airflow/blob/1.9.0alpha1/setup.py#L211) \)  However, alembic release is  up to 0.9.3.  Not sure whether this is an oversight or deliberate, but it is potentially inconvenient if you need newer alembic features in the same env as airflow. (In my case, alembic #412 / postgresql exclusion constraint support)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26949,54,JIRA.13085941.1499693235000.125401.1509573480544@Atlassian.JIRA,2088,David De La Harpe Golden (JIRA),JIRA.13085941.1499693235000@Atlassian.JIRA,,,2017-11-01 14:58:00-07,[jira] [Updated] (AIRFLOW-1390) Airflow requiring alembic <0.9,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1390?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

David De La Harpe Golden updated AIRFLOW-1390:
----------------------------------------------
    Description: Airflow setup.py currently appears to require alembic <0.9 (as at [1.9.0alpha1|https://github.com/apache/incubator-airflow/blob/1.9.0alpha1/setup.py#L211] )  However, alembic release is  up to 0.9.3.  Not sure whether this is an oversight or deliberate, but it is potentially inconvenient if you need newer alembic features in the same env as airflow. (In my case, alembic #412 / postgresql exclusion constraint support)  (was: Airflow setup.py currently appears to require alembic <0.9 \(as at [1.9.0alpha1](https://github.com/apache/incubator-airflow/blob/1.9.0alpha1/setup.py#L211) \)  However, alembic release is  up to 0.9.3.  Not sure whether this is an oversight or deliberate, but it is potentially inconvenient if you need newer alembic features in the same env as airflow. (In my case, alembic #412 / postgresql exclusion constraint support))

> Airflow requiring alembic <0.9
> ------------------------------
>
>                 Key: AIRFLOW-1390
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1390
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: David De La Harpe Golden
>            Priority: Minor
>
> Airflow setup.py currently appears to require alembic <0.9 (as at [1.9.0alpha1|https://github.com/apache/incubator-airflow/blob/1.9.0alpha1/setup.py#L211] )  However, alembic release is  up to 0.9.3.  Not sure whether this is an oversight or deliberate, but it is potentially inconvenient if you need newer alembic features in the same env as airflow. (In my case, alembic #412 / postgresql exclusion constraint support)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26950,54,JIRA.13085941.1499693235000.125404.1509573480572@Atlassian.JIRA,2088,David De La Harpe Golden (JIRA),JIRA.13085941.1499693235000@Atlassian.JIRA,,,2017-11-01 14:58:00-07,[jira] [Updated] (AIRFLOW-1390) Airflow requiring alembic <0.9,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1390?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

David De La Harpe Golden updated AIRFLOW-1390:
----------------------------------------------
    Affects Version/s: 1.9.0

> Airflow requiring alembic <0.9
> ------------------------------
>
>                 Key: AIRFLOW-1390
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1390
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: David De La Harpe Golden
>            Priority: Minor
>
> Airflow setup.py currently appears to require alembic <0.9 (as at [1.9.0alpha1|https://github.com/apache/incubator-airflow/blob/1.9.0alpha1/setup.py#L211] )  However, alembic release is  up to 0.9.3.  Not sure whether this is an oversight or deliberate, but it is potentially inconvenient if you need newer alembic features in the same env as airflow. (In my case, alembic #412 / postgresql exclusion constraint support)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26951,54,JIRA.13115367.1509545715000.125956.1509576060323@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13115367.1509545715000@Atlassian.JIRA,,,2017-11-01 15:41:00-07,"[jira] [Commented] (AIRFLOW-1776) stdout/stderr logging not
 captured","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1776?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16234886#comment-16234886 ] 

ASF subversion and git services commented on AIRFLOW-1776:
----------------------------------------------------------

Commit 590d9fef706d9d7a734e3b748c39ae1fee52ced2 in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=590d9fe ]

[AIRFLOW-1776] Capture stdout and stderr for logging

The new logging framework was not properly
capturing stdout/stderr
output. Redirection the the correct logging
facility is required.

Closes #2745 from bolkedebruin/redirect_std

(cherry picked from commit 5b06b6666296abc8ade524ff5d438ab1210d2938)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> stdout/stderr logging not captured
> ----------------------------------
>
>                 Key: AIRFLOW-1776
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1776
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Critical
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26952,54,JIRA.13115367.1509545715000.125962.1509576060419@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13115367.1509545715000@Atlassian.JIRA,,,2017-11-01 15:41:00-07,[jira] [Resolved] (AIRFLOW-1776) stdout/stderr logging not captured,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1776?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1776.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2745
[https://github.com/apache/incubator-airflow/pull/2745]

> stdout/stderr logging not captured
> ----------------------------------
>
>                 Key: AIRFLOW-1776
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1776
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Critical
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26953,54,JIRA.13115367.1509545715000.125952.1509576060292@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13115367.1509545715000@Atlassian.JIRA,,,2017-11-01 15:41:00-07,"[jira] [Commented] (AIRFLOW-1776) stdout/stderr logging not
 captured","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1776?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16234884#comment-16234884 ] 

ASF subversion and git services commented on AIRFLOW-1776:
----------------------------------------------------------

Commit 5b06b6666296abc8ade524ff5d438ab1210d2938 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=5b06b66 ]

[AIRFLOW-1776] Capture stdout and stderr for logging

The new logging framework was not properly
capturing stdout/stderr
output. Redirection the the correct logging
facility is required.

Closes #2745 from bolkedebruin/redirect_std


> stdout/stderr logging not captured
> ----------------------------------
>
>                 Key: AIRFLOW-1776
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1776
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>            Priority: Critical
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26954,54,JIRA.13115497.1509577042000.126400.1509577080518@Atlassian.JIRA,2120,Edgar Rodriguez (JIRA),JIRA.13115497.1509577042000@Atlassian.JIRA,,,2017-11-01 15:58:00-07,"[jira] [Created] (AIRFLOW-1777) Bad dag with incorrect owner/dag_id
 could break main page autocomplete search","Edgar Rodriguez created AIRFLOW-1777:
----------------------------------------

             Summary: Bad dag with incorrect owner/dag_id could break main page autocomplete search
                 Key: AIRFLOW-1777
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1777
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Edgar Rodriguez


Bad dag with incorrect owner/dag_id could break main page autocomplete search

*Placeholder *



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26955,54,JIRA.13115497.1509577042000.126872.1509579540039@Atlassian.JIRA,2120,Edgar Rodriguez (JIRA),JIRA.13115497.1509577042000@Atlassian.JIRA,,,2017-11-01 16:39:00-07,"[jira] [Assigned] (AIRFLOW-1777) Bad dag with incorrect
 owner/dag_id could break main page autocomplete search","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1777?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Edgar Rodriguez reassigned AIRFLOW-1777:
----------------------------------------

    Assignee: Edgar Rodriguez

> Bad dag with incorrect owner/dag_id could break main page autocomplete search
> -----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1777
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1777
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Edgar Rodriguez
>            Assignee: Edgar Rodriguez
>
> Bad dag with incorrect owner/dag_id could break main page autocomplete search
> *Placeholder *



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26956,54,JIRA.13034876.1484355568000.129406.1509611700716@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13034876.1484355568000@Atlassian.JIRA,,,2017-11-02 01:35:00-07,[jira] [Commented] (AIRFLOW-756) Refactor ssh_hook and ssh_operator,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235390#comment-16235390 ] 

ASF subversion and git services commented on AIRFLOW-756:
---------------------------------------------------------

Commit 0ba6ab6e5c5de0257dfc6f542c7396bda5c382b1 in incubator-airflow''s branch refs/heads/master from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=0ba6ab6 ]

[AIRFLOW-1712][AIRFLOW-756][AIRFLOW-751] Log SSHOperator output

SSHOperator does now write stdout to log, just
like SSHExecutorOperator
did in the past

Closes #2686 from OpringaoDoTurno/bring-ssh-logs-
back


> Refactor ssh_hook and ssh_operator
> ----------------------------------
>
>                 Key: AIRFLOW-756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-756
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: hooks, operators
>            Reporter: Michael Gonzalez
>            Assignee: Jayesh
>            Priority: Minor
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26957,54,JIRA.13034667.1484317624000.129410.1509611700763@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13034667.1484317624000@Atlassian.JIRA,,,2017-11-02 01:35:00-07,[jira] [Commented] (AIRFLOW-751) SFTP file transfer functionality,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-751?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235391#comment-16235391 ] 

ASF subversion and git services commented on AIRFLOW-751:
---------------------------------------------------------

Commit 0ba6ab6e5c5de0257dfc6f542c7396bda5c382b1 in incubator-airflow''s branch refs/heads/master from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=0ba6ab6 ]

[AIRFLOW-1712][AIRFLOW-756][AIRFLOW-751] Log SSHOperator output

SSHOperator does now write stdout to log, just
like SSHExecutorOperator
did in the past

Closes #2686 from OpringaoDoTurno/bring-ssh-logs-
back


> SFTP file transfer functionality
> --------------------------------
>
>                 Key: AIRFLOW-751
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-751
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Michael Gonzalez
>            Assignee: Jayesh
>            Priority: Minor
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26958,54,JIRA.13109170.1507880653000.129402.1509611700636@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-02 01:35:00-07,[jira] [Commented] (AIRFLOW-1712) Log SSHOperator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
235389#comment-16235389 ]=20

ASF subversion and git services commented on AIRFLOW-1712:
----------------------------------------------------------

Commit 0ba6ab6e5c5de0257dfc6f542c7396bda5c382b1 in incubator-airflow''s bran=
ch refs/heads/master from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D0ba=
6ab6 ]

[AIRFLOW-1712][AIRFLOW-756][AIRFLOW-751] Log SSHOperator output

SSHOperator does now write stdout to log, just
like SSHExecutorOperator
did in the past

Closes #2686 from OpringaoDoTurno/bring-ssh-logs-
back


> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26959,54,JIRA.13034876.1484355568000.129414.1509611760778@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13034876.1484355568000@Atlassian.JIRA,,,2017-11-02 01:36:00-07,[jira] [Commented] (AIRFLOW-756) Refactor ssh_hook and ssh_operator,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235393#comment-16235393 ] 

ASF subversion and git services commented on AIRFLOW-756:
---------------------------------------------------------

Commit 2a6c441968fce339ccc9d9e23689a9c21beee724 in incubator-airflow''s branch refs/heads/v1-9-test from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2a6c441 ]

[AIRFLOW-1712][AIRFLOW-756][AIRFLOW-751] Log SSHOperator output

SSHOperator does now write stdout to log, just
like SSHExecutorOperator
did in the past

Closes #2686 from OpringaoDoTurno/bring-ssh-logs-
back

(cherry picked from commit 0ba6ab6e5c5de0257dfc6f542c7396bda5c382b1)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Refactor ssh_hook and ssh_operator
> ----------------------------------
>
>                 Key: AIRFLOW-756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-756
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: hooks, operators
>            Reporter: Michael Gonzalez
>            Assignee: Jayesh
>            Priority: Minor
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26960,54,JIRA.13109170.1507880653000.129413.1509611760768@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-02 01:36:00-07,[jira] [Commented] (AIRFLOW-1712) Log SSHOperator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
235392#comment-16235392 ]=20

ASF subversion and git services commented on AIRFLOW-1712:
----------------------------------------------------------

Commit 2a6c441968fce339ccc9d9e23689a9c21beee724 in incubator-airflow''s bran=
ch refs/heads/v1-9-test from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D2a6=
c441 ]

[AIRFLOW-1712][AIRFLOW-756][AIRFLOW-751] Log SSHOperator output

SSHOperator does now write stdout to log, just
like SSHExecutorOperator
did in the past

Closes #2686 from OpringaoDoTurno/bring-ssh-logs-
back

(cherry picked from commit 0ba6ab6e5c5de0257dfc6f542c7396bda5c382b1)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26961,54,JIRA.13109170.1507880653000.129425.1509611761024@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-02 01:36:01-07,[jira] [Resolved] (AIRFLOW-1712) Log SSHOperator output,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1712.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2686
[https://github.com/apache/incubator-airflow/pull/2686]

> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26962,54,JIRA.13034667.1484317624000.129421.1509611760893@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13034667.1484317624000@Atlassian.JIRA,,,2017-11-02 01:36:00-07,[jira] [Commented] (AIRFLOW-751) SFTP file transfer functionality,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-751?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235394#comment-16235394 ] 

ASF subversion and git services commented on AIRFLOW-751:
---------------------------------------------------------

Commit 2a6c441968fce339ccc9d9e23689a9c21beee724 in incubator-airflow''s branch refs/heads/v1-9-test from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2a6c441 ]

[AIRFLOW-1712][AIRFLOW-756][AIRFLOW-751] Log SSHOperator output

SSHOperator does now write stdout to log, just
like SSHExecutorOperator
did in the past

Closes #2686 from OpringaoDoTurno/bring-ssh-logs-
back

(cherry picked from commit 0ba6ab6e5c5de0257dfc6f542c7396bda5c382b1)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> SFTP file transfer functionality
> --------------------------------
>
>                 Key: AIRFLOW-751
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-751
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Michael Gonzalez
>            Assignee: Jayesh
>            Priority: Minor
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26963,54,JIRA.13081281.1498007085000.130021.1509617580249@Atlassian.JIRA,2155,Cedrik Neumann (JIRA),JIRA.13081281.1498007085000@Atlassian.JIRA,,,2017-11-02 03:13:00-07,"[jira] [Commented] (AIRFLOW-1330) Connection.parse_from_uri doesn''t
 work for google_cloud_platform and so on","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1330?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235496#comment-16235496 ] 

Cedrik Neumann commented on AIRFLOW-1330:
-----------------------------------------

I just wanted to add that option (2) also provides the flexibility to add more exotic connections (jdbc, ...) without making assumptions on the form of the connection string. 

> Connection.parse_from_uri doesn''t work for google_cloud_platform and so on
> --------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1330
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1330
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli
>            Reporter: Yu Ishikawa
>            Assignee: Shintaro Murakami
>            Priority: Major
>             Fix For: 1.9.0
>
>
> h2. Overview
> {{Connection.parse_from_uri}} doesn''t work for some types like {{google_cloud_platform}} whose type name includes under scores. Since `urllib.parse.urlparse()` which is used in {{Connection.parse_from_url}} doesn''t support a schema name which include under scores.
> So, airflow''s CLI doesn''t work when a given connection URI includes under scores like {{google_cloud_platform://XXXXX}}.
> h3. Workaround
> https://medium.com/@yuu.ishikawa/apache-airflow-how-to-add-a-connection-to-google-cloud-with-cli-af2cc8df138d



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26964,54,JIRA.13106740.1507049578000.130530.1509623522072@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13106740.1507049578000@Atlassian.JIRA,,,2017-11-02 04:52:02-07,"[jira] [Commented] (AIRFLOW-1675) Fix API docstrings to be properly
 rendered","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1675?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235604#comment-16235604 ] 

ASF subversion and git services commented on AIRFLOW-1675:
----------------------------------------------------------

Commit a5f51cc4a8dd87284d114ebab15401b2988d6599 in incubator-airflow''s branch refs/heads/master from [~cjonesy]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=a5f51cc ]

[AIRFLOW-1675] Fix docstrings for API docs

Some docstrings were missing spaces, causing them
to render strangely
in documentation. This corrects the issue by
adding in the spaces.

Closes #2667 from cjonesy/master


> Fix API docstrings to be properly rendered
> ------------------------------------------
>
>                 Key: AIRFLOW-1675
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1675
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: docs, Documentation
>            Reporter: Kengo Seki
>            Assignee: Charlie Jones
>            Priority: Minor
>              Labels: easyfix
>             Fix For: 1.9.0
>
>         Attachments: Screen Shot 2017-10-04 at 1.44.59.png
>
>
> Some docstrings lack a line between description and parameters, so they aren''t properly rendered as the attached screenshot.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26965,54,JIRA.13106740.1507049578000.130538.1509623522107@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13106740.1507049578000@Atlassian.JIRA,,,2017-11-02 04:52:02-07,"[jira] [Resolved] (AIRFLOW-1675) Fix API docstrings to be properly
 rendered","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1675?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1675.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2667
[https://github.com/apache/incubator-airflow/pull/2667]

> Fix API docstrings to be properly rendered
> ------------------------------------------
>
>                 Key: AIRFLOW-1675
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1675
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: docs, Documentation
>            Reporter: Kengo Seki
>            Assignee: Charlie Jones
>            Priority: Minor
>              Labels: easyfix
>             Fix For: 1.9.0
>
>         Attachments: Screen Shot 2017-10-04 at 1.44.59.png
>
>
> Some docstrings lack a line between description and parameters, so they aren''t properly rendered as the attached screenshot.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26966,54,JIRA.13106740.1507049578000.130533.1509623522087@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13106740.1507049578000@Atlassian.JIRA,,,2017-11-02 04:52:02-07,"[jira] [Commented] (AIRFLOW-1675) Fix API docstrings to be properly
 rendered","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1675?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235605#comment-16235605 ] 

ASF subversion and git services commented on AIRFLOW-1675:
----------------------------------------------------------

Commit f1238123e88a00f04d9985f9ab4caf8e0304bb81 in incubator-airflow''s branch refs/heads/v1-9-test from [~cjonesy]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f123812 ]

[AIRFLOW-1675] Fix docstrings for API docs

Some docstrings were missing spaces, causing them
to render strangely
in documentation. This corrects the issue by
adding in the spaces.

Closes #2667 from cjonesy/master

(cherry picked from commit a5f51cc4a8dd87284d114ebab15401b2988d6599)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Fix API docstrings to be properly rendered
> ------------------------------------------
>
>                 Key: AIRFLOW-1675
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1675
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: docs, Documentation
>            Reporter: Kengo Seki
>            Assignee: Charlie Jones
>            Priority: Minor
>              Labels: easyfix
>             Fix For: 1.9.0
>
>         Attachments: Screen Shot 2017-10-04 at 1.44.59.png
>
>
> Some docstrings lack a line between description and parameters, so they aren''t properly rendered as the attached screenshot.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26967,54,JIRA.13100171.1504709924000.130550.1509623640163@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13100171.1504709924000@Atlassian.JIRA,,,2017-11-02 04:54:00-07,"[jira] [Commented] (AIRFLOW-1571) Add AWS Lambda Hook for invoking
 Lambda Function","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1571?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235609#comment-16235609 ] 

ASF subversion and git services commented on AIRFLOW-1571:
----------------------------------------------------------

Commit cc01b7df3d3d5d2784b5e72aa282e957c9cf63ad in incubator-airflow''s branch refs/heads/master from sid.gupta
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=cc01b7d ]

[AIRFLOW-1571] Add AWS Lambda Hook

Closes #2718 from sid88in/feature/aws_lambda_hook2


> Add AWS Lambda Hook for invoking Lambda Function
> ------------------------------------------------
>
>                 Key: AIRFLOW-1571
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1571
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Siddharth
>            Assignee: Siddharth
>            Priority: Major
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26968,54,JIRA.13100171.1504709924000.130554.1509623700422@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13100171.1504709924000@Atlassian.JIRA,,,2017-11-02 04:55:00-07,"[jira] [Commented] (AIRFLOW-1571) Add AWS Lambda Hook for invoking
 Lambda Function","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1571?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16235610#comment-16235610 ] 

ASF subversion and git services commented on AIRFLOW-1571:
----------------------------------------------------------

Commit 017f18b9c98f89e17b2be7ba87bb389b3c99c4a4 in incubator-airflow''s branch refs/heads/v1-9-test from sid.gupta
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=017f18b ]

[AIRFLOW-1571] Add AWS Lambda Hook

Closes #2718 from sid88in/feature/aws_lambda_hook2

(cherry picked from commit cc01b7df3d3d5d2784b5e72aa282e957c9cf63ad)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Add AWS Lambda Hook for invoking Lambda Function
> ------------------------------------------------
>
>                 Key: AIRFLOW-1571
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1571
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Siddharth
>            Assignee: Siddharth
>            Priority: Major
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26969,54,JIRA.13100171.1504709924000.130559.1509623700467@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13100171.1504709924000@Atlassian.JIRA,,,2017-11-02 04:55:00-07,"[jira] [Resolved] (AIRFLOW-1571) Add AWS Lambda Hook for invoking
 Lambda Function","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1571?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1571.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2718
[https://github.com/apache/incubator-airflow/pull/2718]

> Add AWS Lambda Hook for invoking Lambda Function
> ------------------------------------------------
>
>                 Key: AIRFLOW-1571
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1571
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Siddharth
>            Assignee: Siddharth
>            Priority: Major
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
631169,147,20040416183305.20867.qmail@nagoya.betaversion.org,23834,bugzilla,NULL,,,2004-04-16 11:33:05-07,"DO NOT REPLY [Bug 28369]  - 
    Lenya GUI: french translation","DO NOT REPLY TO THIS EMAIL, BUT PLEASE POST YOUR BUG 
RELATED COMMENTS THROUGH THE WEB INTERFACE AVAILABLE AT
<http://issues.apache.org/bugzilla/show_bug.cgi?id=28369>.
ANY REPLY MADE TO THIS MESSAGE WILL NOT BE COLLECTED AND 
INSERTED IN THE BUG DATABASE.

http://issues.apache.org/bugzilla/show_bug.cgi?id=28369

Lenya GUI: french translation





------- Additional Comments From olange@petit-atelier.ch  2004-04-16 18:33 -------
Created an attachment (id=11263)
Updated french message catalogs (fixed typo, reverted to character references as we are sure it works and I found no way to get the characters back)

---------------------------------------------------------------------
To unsubscribe, e-mail: lenya-dev-unsubscribe@cocoon.apache.org
For additional commands, e-mail: lenya-dev-help@cocoon.apache.org


",f
26970,54,JIRA.13115339.1509539330000.130571.1509623820049@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-02 04:57:00-07,[jira] [Commented] (AIRFLOW-1773) [Mark Success] not work in UI,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
235614#comment-16235614 ]=20

Winty commented on AIRFLOW-1773:
--------------------------------

I solve problem. jut

airflow / api / common / experimental / mark_tasks.py
{code:python}

def set_state(task, execution_date, upstream=3DFalse, downstream=3DFalse,=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 future=3DFalse, past=3DFalse, state=3DState.SUCCESS, commit=3DFalse):
...
    # microseconds are supported by the database, but is not handled
=C2=A0=C2=A0=C2=A0 # correctly by airflow on e.g. the filesystem and in oth=
er places=C2=A0=C2=A0=C2=A0=20
    # execution_date =3D execution_date.replace(microsecond=3D0) # I remove=
 this code
...
{code}


> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.jpg, 33.jpg
>
>
> - When I click [Mark Success], there is no task instances=20
> !33.jpg|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=3DFalse, downstream=3DFalse,
>               future=3DFalse, past=3DFalse, state=3DState.SUCCESS, commit=
=3DFalse):
> ...
>     if dag.schedule_interval =3D=3D ''@once'':
>         dates =3D [start_date]
>     else:
>         dates =3D dag.date_range(start_date=3Dstart_date, end_date=3Dend_=
date)
> ...
>     # verify the integrity of the dag runs in case a task was added or re=
moved
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates =3D []
>     drs =3D DagRun.find(dag_id=3Ddag.dag_id, execution_date=3Ddates)
>     for dr in drs:
>         dr.dag =3D dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find sam=
e in DagRuns exactly. but, dag.date_range() make times upper second like=20
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1=
, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), =
datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.jpg|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26971,54,JIRA.13115339.1509539330000.130577.1509623880353@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-02 04:58:00-07,"[jira] [Comment Edited] (AIRFLOW-1773) [Mark Success] not work in
 UI","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
235614#comment-16235614 ]=20

Winty edited comment on AIRFLOW-1773 at 11/2/17 11:57 AM:
----------------------------------------------------------

I solve problem. just remove a line

airflow / api / common / experimental / mark_tasks.py
{code:python}

def set_state(task, execution_date, upstream=3DFalse, downstream=3DFalse,=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 future=3DFalse, past=3DFalse, state=3DState.SUCCESS, commit=3DFalse):
...
    # microseconds are supported by the database, but is not handled
=C2=A0=C2=A0=C2=A0 # correctly by airflow on e.g. the filesystem and in oth=
er places=C2=A0=C2=A0=C2=A0=20
    # execution_date =3D execution_date.replace(microsecond=3D0) # I remove=
 this code
...
{code}



was (Author: winty):
I solve problem. jut

airflow / api / common / experimental / mark_tasks.py
{code:python}

def set_state(task, execution_date, upstream=3DFalse, downstream=3DFalse,=
=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=A0=C2=
=A0 future=3DFalse, past=3DFalse, state=3DState.SUCCESS, commit=3DFalse):
...
    # microseconds are supported by the database, but is not handled
=C2=A0=C2=A0=C2=A0 # correctly by airflow on e.g. the filesystem and in oth=
er places=C2=A0=C2=A0=C2=A0=20
    # execution_date =3D execution_date.replace(microsecond=3D0) # I remove=
 this code
...
{code}


> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.jpg, 33.jpg
>
>
> - When I click [Mark Success], there is no task instances=20
> !33.jpg|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=3DFalse, downstream=3DFalse,
>               future=3DFalse, past=3DFalse, state=3DState.SUCCESS, commit=
=3DFalse):
> ...
>     if dag.schedule_interval =3D=3D ''@once'':
>         dates =3D [start_date]
>     else:
>         dates =3D dag.date_range(start_date=3Dstart_date, end_date=3Dend_=
date)
> ...
>     # verify the integrity of the dag runs in case a task was added or re=
moved
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates =3D []
>     drs =3D DagRun.find(dag_id=3Ddag.dag_id, execution_date=3Ddates)
>     for dr in drs:
>         dr.dag =3D dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find sam=
e in DagRuns exactly. but, dag.date_range() make times upper second like=20
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1=
, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), =
datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.jpg|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26972,54,JIRA.13115339.1509539330000.130578.1509623940148@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-02 04:59:00-07,[jira] [Work started] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1773 started by Winty.
--------------------------------------
> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.jpg, 33.jpg
>
>
> - When I click [Mark Success], there is no task instances 
> !33.jpg|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.jpg|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26973,54,JIRA.13115339.1509539330000.130580.1509623940179@Atlassian.JIRA,2365,Winty (JIRA),JIRA.13115339.1509539330000@Atlassian.JIRA,,,2017-11-02 04:59:00-07,[jira] [Work stopped] (AIRFLOW-1773) [Mark Success] not work in UI,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1773?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1773 stopped by Winty.
--------------------------------------
> [Mark Success] not work in UI
> -----------------------------
>
>                 Key: AIRFLOW-1773
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1773
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: api
>    Affects Versions: 1.8.2
>            Reporter: Winty
>            Assignee: Winty
>            Priority: Major
>              Labels: newbie
>         Attachments: 22.jpg, 33.jpg
>
>
> - When I click [Mark Success], there is no task instances 
> !33.jpg|thumbnail!
> so I try to find what is problem. and i can find somthing wrong.
> airflow / api / common / experimental / mark_tasks.py
> {code:python}
> def set_state(task, execution_date, upstream=False, downstream=False,
>               future=False, past=False, state=State.SUCCESS, commit=False):
> ...
>     if dag.schedule_interval == ''@once'':
>         dates = [start_date]
>     else:
>         dates = dag.date_range(start_date=start_date, end_date=end_date)
> ...
>     # verify the integrity of the dag runs in case a task was added or removed
>     # set the confirmed execution dates as they might be different
>     # from what was provided
>     confirmed_dates = []
>     drs = DagRun.find(dag_id=dag.dag_id, execution_date=dates)
>     for dr in drs:
>         dr.dag = dag
>         dr.verify_integrity()
>         confirmed_dates.append(dr.execution_date)
> {code}
> the drs is empty. because DagRun.find() with ''execution_date'' is find same in DagRuns exactly. but, dag.date_range() make times upper second like 
> [...datetime.datetime(2017, 11, 1, 11, 30), datetime.datetime(2017, 11, 1, 12, 0), dateti
> me.datetime(2017, 11, 1, 12, 30), datetime.datetime(2017, 11, 1, 13, 0), datetime.datetime(2
> 017, 11, 1, 13, 30), datetime.datetime(2017, 11, 1, 14, 0)...]
> !22.jpg|thumbnail!
> so DagRun.find() can not find dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
631170,147,1082144558.2239.86.camel@gideon.rkunet.org,21601,Rolf Kulemann,4080129F.70308@apache.org,21653,Gregor J. Rothfuss,2004-04-16 12:42:39-07,Re: Form editors and encoding problem,"On Fri, 2004-04-16 at 19:06, Gregor J. Rothfuss wrote:
> Michael Wechner wrote:
>=20
> > Something seems to have changed. The form editors have once again an=20
> > encoding
> > problem, e.g. ""S=C3=BCsser H=C3=BCgel"" doesn''t work anymore.
> >=20
> > Rolf, didn''t you mention in a recent email that you have switched=20
> > something?
>=20
> there are 2 choices: revert the web.xml change or fix the form editor to=20
> use utf-8. since i want to release 1.2 final this weekend, i am leaning=20
> towards the former.

Ok, I will check it tomorrow.

Have a nice weekend.

--=20
Regards,

    Rolf Kulemann


---------------------------------------------------------------------
To unsubscribe, e-mail: lenya-dev-unsubscribe@cocoon.apache.org
For additional commands, e-mail: lenya-dev-help@cocoon.apache.org


",f
26974,54,JIRA.13115640.1509632973000.131675.1509633000214@Atlassian.JIRA,1869,Alessio Palma (JIRA),JIRA.13115640.1509632973000@Atlassian.JIRA,,,2017-11-02 07:30:00-07,[jira] [Created] (AIRFLOW-1778) Task_instance and Dag_run recycle,"Alessio Palma created AIRFLOW-1778:
--------------------------------------

             Summary: Task_instance and Dag_run recycle
                 Key: AIRFLOW-1778
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1778
             Project: Apache Airflow
          Issue Type: Wish
          Components: DagRun
    Affects Versions: 1.8.1
            Reporter: Alessio Palma


Hello, I''m not sure this is a real issue but something strange happened on our airflow installation.

We started a dag which did not complete due to some issues on Hadoop, so we delete all the dag_run affected and left untouched the task_instances  waited for it to complete but... 

1. Every dag restarted with the correct execution date
2. No processor executed for real
3. airflow used the old task_instance record.

So the question is... it was so bad just to delete the dag_run without deleting the task_instances?  Also... If it is bad to delete the dag_run, why it is possible to execute this action from the admin menu? 

Should the delete dag_run option to be removed? 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26975,54,JIRA.13115650.1509637174000.132440.1509637200335@Atlassian.JIRA,2368,Rob Keevil (JIRA),JIRA.13115650.1509637174000@Atlassian.JIRA,,,2017-11-02 08:40:00-07,[jira] [Created] (AIRFLOW-1779) Add keepalive packets to ssh hook,"Rob Keevil created AIRFLOW-1779:
-----------------------------------

             Summary: Add keepalive packets to ssh hook
                 Key: AIRFLOW-1779
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1779
             Project: Apache Airflow
          Issue Type: Improvement
          Components: contrib
    Affects Versions: Airflow 1.8
            Reporter: Rob Keevil
            Assignee: Rob Keevil


The existing SSH hook does not make use of keepalive packets, and connections may thus be dropped for long running commands that do not have frequent terminal output.  Paramiko has a set_keepalive method available on Transport objects, and we should optionally set this in the SSHHook.

I''m working on adding this, will link PR here when ready.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26976,54,JIRA.13115650.1509637174000.134142.1509645480476@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13115650.1509637174000@Atlassian.JIRA,,,2017-11-02 10:58:00-07,[jira] [Commented] (AIRFLOW-1779) Add keepalive packets to ssh hook,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1779?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16236265#comment-16236265 ] 

ASF subversion and git services commented on AIRFLOW-1779:
----------------------------------------------------------

Commit 1bde7833854210676dcd6e8da8b6d9567e12031a in incubator-airflow''s branch refs/heads/master from [~RJKeevil]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1bde783 ]

[AIRFLOW-1779] Add keepalive packets to ssh hook

Make use of paramiko''s set_keepalive method to
send keepalive packets every
keepalive_interval seconds.  This will prevent
long running queries with no terminal
output from being termanated as idle, for example
by an intermediate NAT.

Set on by default with a 30 second interval.

Closes #2749 from RJKeevil/add-sshhook-keepalive


> Add keepalive packets to ssh hook
> ---------------------------------
>
>                 Key: AIRFLOW-1779
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1779
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib
>    Affects Versions: Airflow 1.8
>            Reporter: Rob Keevil
>            Assignee: Rob Keevil
>             Fix For: 1.9.0
>
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> The existing SSH hook does not make use of keepalive packets, and connections may thus be dropped for long running commands that do not have frequent terminal output.  Paramiko has a set_keepalive method available on Transport objects, and we should optionally set this in the SSHHook.
> I''m working on adding this, will link PR here when ready.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26977,54,JIRA.13115650.1509637174000.134149.1509645480537@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13115650.1509637174000@Atlassian.JIRA,,,2017-11-02 10:58:00-07,[jira] [Resolved] (AIRFLOW-1779) Add keepalive packets to ssh hook,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1779?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1779.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2749
[https://github.com/apache/incubator-airflow/pull/2749]

> Add keepalive packets to ssh hook
> ---------------------------------
>
>                 Key: AIRFLOW-1779
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1779
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib
>    Affects Versions: Airflow 1.8
>            Reporter: Rob Keevil
>            Assignee: Rob Keevil
>             Fix For: 1.9.0
>
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> The existing SSH hook does not make use of keepalive packets, and connections may thus be dropped for long running commands that do not have frequent terminal output.  Paramiko has a set_keepalive method available on Transport objects, and we should optionally set this in the SSHHook.
> I''m working on adding this, will link PR here when ready.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26978,54,JIRA.13115650.1509637174000.134145.1509645480504@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13115650.1509637174000@Atlassian.JIRA,,,2017-11-02 10:58:00-07,[jira] [Commented] (AIRFLOW-1779) Add keepalive packets to ssh hook,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1779?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16236267#comment-16236267 ] 

ASF subversion and git services commented on AIRFLOW-1779:
----------------------------------------------------------

Commit d2f9d189c6c4244467434de600aae95e2a76558e in incubator-airflow''s branch refs/heads/v1-9-test from [~RJKeevil]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d2f9d18 ]

[AIRFLOW-1779] Add keepalive packets to ssh hook

Make use of paramiko''s set_keepalive method to
send keepalive packets every
keepalive_interval seconds.  This will prevent
long running queries with no terminal
output from being termanated as idle, for example
by an intermediate NAT.

Set on by default with a 30 second interval.

Closes #2749 from RJKeevil/add-sshhook-keepalive

(cherry picked from commit 1bde7833854210676dcd6e8da8b6d9567e12031a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Add keepalive packets to ssh hook
> ---------------------------------
>
>                 Key: AIRFLOW-1779
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1779
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib
>    Affects Versions: Airflow 1.8
>            Reporter: Rob Keevil
>            Assignee: Rob Keevil
>             Fix For: 1.9.0
>
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> The existing SSH hook does not make use of keepalive packets, and connections may thus be dropped for long running commands that do not have frequent terminal output.  Paramiko has a set_keepalive method available on Transport objects, and we should optionally set this in the SSHHook.
> I''m working on adding this, will link PR here when ready.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26979,54,JIRA.12994308.1470151190000.134382.1509645613614@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.12994308.1470151190000@Atlassian.JIRA,,,2017-11-02 11:00:13-07,[jira] [Commented] (AIRFLOW-387) Dropped postgres connections,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-387?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16236274#comment-16236274 ] 

ASF subversion and git services commented on AIRFLOW-387:
---------------------------------------------------------

Commit 7765278479b81a73244ff44e47973bbc5bf9bcef in incubator-airflow''s branch refs/heads/master from [~StephanErb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=7765278 ]

[AIRFLOW-387] Close SQLAlchemy sessions properly

This commit adopts the `provide_session` helper in
almost the entire
codebase. This ensures session are handled and
closed consistently.
In particular, this ensures we don''t forget to
close and thus leak
database connections.

As an additional change, the `provide_session`
helper has been extended
to also rollback and close created connections
under error conditions.

As an additional helper, this commit also
introduces a contextmanager
that provides the same functionality as the
`provide_session`
decorator. This is helpful in cases where the
scope of a session should
be smaller than the entire method where it is
being used.

Closes #2739 from StephanErb/session_close


> Dropped postgres connections
> ----------------------------
>
>                 Key: AIRFLOW-387
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-387
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: Airflow 1.7.1.3
>         Environment: Running aiflow with postgresql+psycopg2, Postgres 9.1.19 backend.
>            Reporter: Andreas Merkel
>            Priority: Major
>
> With the {{postgresql+psycopg2}} driver, we observe dropped connections in the database logs on the postgres server:
> {code}
> 2016-08-02 14:49:26 UTC LOG:  could not receive data from client: Connection reset by peer
> 2016-08-02 14:49:26 UTC LOG:  unexpected EOF on client connection
> {code}
> Three of these messages appear even if Airflow is started without any DAGs. If there are DAGs, the messages appear whenever the DAG is executed (even for simple DAGs like a Python operator that does nothing).
> On the Airflow side, everything works fine; it''s just the messages in the log indicating that Airflow is somehow dropping the connections to Postgres without closing them properly.
> h2. Environment
> {{airflow.cfg}}:
> {code}
> [core]
> airflow_home = $PWD
> dags_folder = $PWD/dags
> base_log_folder = $PWD/airflow_logs
> plugins_folder = $PWD/plugins
> executor = LocalExecutor
> sql_alchemy_conn = postgresql+psycopg2://postgres:postgres@127.0.0.1/airflow
> parallelism = 32
> dag_concurrency = 16
> max_active_runs_per_dag = 16
> load_examples = False
> donot_pickle = False
> fernet_key =  ; provided via environment
> dags_are_paused_at_creation = False
> [webserver]
> expose_config = true
> authenticate = False
> filter_by_owner = False
> workers = 4
> worker_class = sync
> [scheduler]
> job_heartbeat_sec = 5
> scheduler_heartbeat_sec = 5
> [celery]
> celeryd_concurrency = 1  ; Oddly enough, this is needed though we don''t use celery.
> {code}
> Installed packages:
> {code}
> airflow==1.7.1.3
> alembic==0.8.6
> Babel==1.3
> cffi==1.6.0
> chartkick==0.4.2
> croniter==0.3.12
> cryptography==1.3.2
> dill==0.2.5
> docutils==0.12
> enum34==1.1.6
> Flask==0.10.1
> Flask-Admin==1.4.0
> Flask-Cache==0.13.1
> Flask-Login==0.2.11
> Flask-WTF==0.12
> funcsigs==0.4
> future==0.15.2
> gunicorn==19.3.0
> idna==2.1
> ipaddress==1.0.16
> itsdangerous==0.24
> Jinja2==2.8
> lockfile==0.12.2
> Mako==1.0.4
> Markdown==2.6.6
> MarkupSafe==0.23
> numpy==1.11.0
> pandas==0.18.1
> psycopg2==2.6.1
> pyasn1==0.1.9
> pycparser==2.14
> Pygments==2.1.3
> python-daemon==2.1.1
> python-dateutil==2.5.3
> python-editor==1.0.1
> pytz==2016.4
> requests==2.10.0
> setproctitle==1.1.10
> six==1.10.0
> smoketest-dag==0.7
> SQLAlchemy==1.0.13
> thrift==0.9.3
> Werkzeug==0.11.10
> WTForms==2.1
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26980,54,JIRA.12994308.1470151190000.134405.1509645660795@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.12994308.1470151190000@Atlassian.JIRA,,,2017-11-02 11:01:00-07,[jira] [Resolved] (AIRFLOW-387) Dropped postgres connections,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-387?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-387.
------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

> Dropped postgres connections
> ----------------------------
>
>                 Key: AIRFLOW-387
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-387
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: Airflow 1.7.1.3
>         Environment: Running aiflow with postgresql+psycopg2, Postgres 9.1.19 backend.
>            Reporter: Andreas Merkel
>            Priority: Major
>             Fix For: 1.9.1
>
>
> With the {{postgresql+psycopg2}} driver, we observe dropped connections in the database logs on the postgres server:
> {code}
> 2016-08-02 14:49:26 UTC LOG:  could not receive data from client: Connection reset by peer
> 2016-08-02 14:49:26 UTC LOG:  unexpected EOF on client connection
> {code}
> Three of these messages appear even if Airflow is started without any DAGs. If there are DAGs, the messages appear whenever the DAG is executed (even for simple DAGs like a Python operator that does nothing).
> On the Airflow side, everything works fine; it''s just the messages in the log indicating that Airflow is somehow dropping the connections to Postgres without closing them properly.
> h2. Environment
> {{airflow.cfg}}:
> {code}
> [core]
> airflow_home = $PWD
> dags_folder = $PWD/dags
> base_log_folder = $PWD/airflow_logs
> plugins_folder = $PWD/plugins
> executor = LocalExecutor
> sql_alchemy_conn = postgresql+psycopg2://postgres:postgres@127.0.0.1/airflow
> parallelism = 32
> dag_concurrency = 16
> max_active_runs_per_dag = 16
> load_examples = False
> donot_pickle = False
> fernet_key =  ; provided via environment
> dags_are_paused_at_creation = False
> [webserver]
> expose_config = true
> authenticate = False
> filter_by_owner = False
> workers = 4
> worker_class = sync
> [scheduler]
> job_heartbeat_sec = 5
> scheduler_heartbeat_sec = 5
> [celery]
> celeryd_concurrency = 1  ; Oddly enough, this is needed though we don''t use celery.
> {code}
> Installed packages:
> {code}
> airflow==1.7.1.3
> alembic==0.8.6
> Babel==1.3
> cffi==1.6.0
> chartkick==0.4.2
> croniter==0.3.12
> cryptography==1.3.2
> dill==0.2.5
> docutils==0.12
> enum34==1.1.6
> Flask==0.10.1
> Flask-Admin==1.4.0
> Flask-Cache==0.13.1
> Flask-Login==0.2.11
> Flask-WTF==0.12
> funcsigs==0.4
> future==0.15.2
> gunicorn==19.3.0
> idna==2.1
> ipaddress==1.0.16
> itsdangerous==0.24
> Jinja2==2.8
> lockfile==0.12.2
> Mako==1.0.4
> Markdown==2.6.6
> MarkupSafe==0.23
> numpy==1.11.0
> pandas==0.18.1
> psycopg2==2.6.1
> pyasn1==0.1.9
> pycparser==2.14
> Pygments==2.1.3
> python-daemon==2.1.1
> python-dateutil==2.5.3
> python-editor==1.0.1
> pytz==2016.4
> requests==2.10.0
> setproctitle==1.1.10
> six==1.10.0
> smoketest-dag==0.7
> SQLAlchemy==1.0.13
> thrift==0.9.3
> Werkzeug==0.11.10
> WTForms==2.1
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26981,54,JIRA.12965885.1462694460000.134496.1509646200634@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.12965885.1462694460000@Atlassian.JIRA,,,2017-11-02 11:10:00-07,"[jira] [Commented] (AIRFLOW-71) docker_operator - pulling from
 private repositories","
    [ https://issues.apache.org/jira/browse/AIRFLOW-71?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16236293#comment-16236293 ] 

ASF subversion and git services commented on AIRFLOW-71:
--------------------------------------------------------

Commit f101ff0063ad5b22b6efc8d89b1b31b4c6abfa0d in incubator-airflow''s branch refs/heads/master from Stefanie Grunwald
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f101ff0 ]

[AIRFLOW-71] Add support for private Docker images

Pulling images from private Docker registries requires authentication,
so additional parameters are added in order to perform the login step.


> docker_operator - pulling from private repositories 
> ----------------------------------------------------
>
>                 Key: AIRFLOW-71
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-71
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: docker, operators
>            Reporter: Amikam Snir
>            Priority: Minor
>
> Login to docker registry. This feature allows pulling images from private repositories. 
> Click [here|https://docs.docker.com/engine/reference/commandline/login/] for more info



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26982,54,JIRA.13106390.1506948161000.134504.1509646200707@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13106390.1506948161000@Atlassian.JIRA,,,2017-11-02 11:10:00-07,[jira] [Commented] (AIRFLOW-1669) Fix Docker import in Master,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1669?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16236295#comment-16236295 ] 

ASF subversion and git services commented on AIRFLOW-1669:
----------------------------------------------------------

Commit a61d9444cdb2780f70187459c072c962d0fd6660 in incubator-airflow''s branch refs/heads/master from Stefanie Grunwald
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=a61d944 ]

[AIRFLOW-1669] Fix Docker and pin Moto to 1.1.19

https://github.com/spulec/moto/pull/1048 introduced `docker` as a
dependency in Moto, causing a conflict as Airflow uses `docker-py`. As
both packages don''t work together, Moto is pinned to the version
prior to that change.


> Fix Docker import in Master
> ---------------------------
>
>                 Key: AIRFLOW-1669
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1669
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Priority: Major
>             Fix For: 1.9.0, 1.8.3
>
>
> Hi all,
> Currently master is failing due a wrong dependency. I would like to revert this and move back to the docker dependency.
> Cheers, Fokko



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26983,54,JIRA.12965885.1462694460000.134557.1509646442080@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.12965885.1462694460000@Atlassian.JIRA,,,2017-11-02 11:14:02-07,"[jira] [Commented] (AIRFLOW-71) docker_operator - pulling from
 private repositories","
    [ https://issues.apache.org/jira/browse/AIRFLOW-71?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16236302#comment-16236302 ] 

ASF subversion and git services commented on AIRFLOW-71:
--------------------------------------------------------

Commit d4406c0cefddf67ad0d811750813e64fe22c5f3a in incubator-airflow''s branch refs/heads/v1-9-test from Stefanie Grunwald
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d4406c0 ]

[AIRFLOW-71] Add support for private Docker images

Pulling images from private Docker registries requires authentication,
so additional parameters are added in order to perform the login step.

(cherry picked from commit f101ff0063ad5b22b6efc8d89b1b31b4c6abfa0d)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> docker_operator - pulling from private repositories 
> ----------------------------------------------------
>
>                 Key: AIRFLOW-71
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-71
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: docker, operators
>            Reporter: Amikam Snir
>            Priority: Minor
>
> Login to docker registry. This feature allows pulling images from private repositories. 
> Click [here|https://docs.docker.com/engine/reference/commandline/login/] for more info



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26984,54,JIRA.13106390.1506948161000.134561.1509646442120@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13106390.1506948161000@Atlassian.JIRA,,,2017-11-02 11:14:02-07,[jira] [Commented] (AIRFLOW-1669) Fix Docker import in Master,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1669?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16236303#comment-16236303 ] 

ASF subversion and git services commented on AIRFLOW-1669:
----------------------------------------------------------

Commit f7f2a8cdd87cbb54a4feeca0e379e954917dafbf in incubator-airflow''s branch refs/heads/v1-9-test from Stefanie Grunwald
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f7f2a8c ]

[AIRFLOW-1669] Fix Docker and pin Moto to 1.1.19

https://github.com/spulec/moto/pull/1048 introduced `docker` as a
dependency in Moto, causing a conflict as Airflow uses `docker-py`. As
both packages don''t work together, Moto is pinned to the version
prior to that change.

(cherry picked from commit a61d9444cdb2780f70187459c072c962d0fd6660)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Fix Docker import in Master
> ---------------------------
>
>                 Key: AIRFLOW-1669
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1669
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Priority: Major
>             Fix For: 1.9.0, 1.8.3
>
>
> Hi all,
> Currently master is failing due a wrong dependency. I would like to revert this and move back to the docker dependency.
> Cheers, Fokko



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26985,54,JIRA.12965885.1462694460000.134614.1509646620782@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.12965885.1462694460000@Atlassian.JIRA,,,2017-11-02 11:17:00-07,"[jira] [Resolved] (AIRFLOW-71) docker_operator - pulling from
 private repositories","
     [ https://issues.apache.org/jira/browse/AIRFLOW-71?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-71.
-----------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

> docker_operator - pulling from private repositories 
> ----------------------------------------------------
>
>                 Key: AIRFLOW-71
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-71
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: docker, operators
>            Reporter: Amikam Snir
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> Login to docker registry. This feature allows pulling images from private repositories. 
> Click [here|https://docs.docker.com/engine/reference/commandline/login/] for more info



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26986,54,JIRA.13106390.1506948161000.134615.1509646680182@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13106390.1506948161000@Atlassian.JIRA,,,2017-11-02 11:18:00-07,[jira] [Updated] (AIRFLOW-1669) Fix Docker import in Master,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1669?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1669:
------------------------------------
    Fix Version/s:     (was: 1.8.3)

> Fix Docker import in Master
> ---------------------------
>
>                 Key: AIRFLOW-1669
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1669
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Priority: Major
>             Fix For: 1.9.0
>
>
> Hi all,
> Currently master is failing due a wrong dependency. I would like to revert this and move back to the docker dependency.
> Cheers, Fokko



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26987,54,JIRA.13038318.1485472816000.136725.1509656404379@Atlassian.JIRA,1690,Jayesh (JIRA),JIRA.13038318.1485472816000@Atlassian.JIRA,,,2017-11-02 14:00:04-07,"[jira] [Commented] (AIRFLOW-811) Bash_operator dont read multiline
 output","
    [ https://issues.apache.org/jira/browse/AIRFLOW-811?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16236594#comment-16236594 ] 

Jayesh commented on AIRFLOW-811:
--------------------------------

this turn out to be more changes than simply taking care of bug in for loop.
https://github.com/apache/incubator-airflow/pull/2026

> Bash_operator dont read multiline output
> ----------------------------------------
>
>                 Key: AIRFLOW-811
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-811
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Jayesh
>            Assignee: Jayesh
>            Priority: Minor
>
> following piece of code is the root cause of it.  
> {code}
> line = ''''
>                 for line in iter(sp.stdout.readline, b''''):
>                     line = line.decode(self.output_encoding).strip()
>                     logging.info(line)
> {code}
> I plan to fix it using string buffer instead of just 1 line string variable here.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26988,54,JIRA.13115789.1509670762000.138509.1509670809106@Atlassian.JIRA,1301,Dan Davydov (JIRA),JIRA.13115789.1509670762000@Atlassian.JIRA,,,2017-11-02 18:00:09-07,"[jira] [Created] (AIRFLOW-1780) Long Unicode Characters Cause
 Logging","Dan Davydov created AIRFLOW-1780:
------------------------------------

             Summary: Long Unicode Characters Cause Logging
                 Key: AIRFLOW-1780
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1780
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Dan Davydov
            Assignee: Dan Davydov
            Priority: Major


Conditions to replicate:
Create a DAG with a single BashOperator that logs ~10 long lines of text with at least one unicode character (e.g. cat a file with these contents)
Use BashTaskRunner
Log to a file
Run a worker that picks up the task or an airflow run --local command

Behavior:
The BashOperator/cat command hangs

Most likely this is due to a pipe issue, where the unicode characters are filling up the pipe and .format() is not able to process partial unicode characters. Interestingly removing one of the conditions above (e.g. logging to stdout instead of a file or having short lines doesn''t cause the issue).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26989,54,JIRA.13115789.1509670762000.138512.1509670820886@Atlassian.JIRA,1301,Dan Davydov (JIRA),JIRA.13115789.1509670762000@Atlassian.JIRA,,,2017-11-02 18:00:20-07,"[jira] [Updated] (AIRFLOW-1780) Long Unicode Characters In
 BashOperator Output Cause Task Hang","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1780?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dan Davydov updated AIRFLOW-1780:
---------------------------------
    Summary: Long Unicode Characters In BashOperator Output Cause Task Hang  (was: Long Unicode Characters Cause Logging)

> Long Unicode Characters In BashOperator Output Cause Task Hang
> --------------------------------------------------------------
>
>                 Key: AIRFLOW-1780
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1780
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Dan Davydov
>            Assignee: Dan Davydov
>            Priority: Major
>
> Conditions to replicate:
> Create a DAG with a single BashOperator that logs ~10 long lines of text with at least one unicode character (e.g. cat a file with these contents)
> Use BashTaskRunner
> Log to a file
> Run a worker that picks up the task or an airflow run --local command
> Behavior:
> The BashOperator/cat command hangs
> Most likely this is due to a pipe issue, where the unicode characters are filling up the pipe and .format() is not able to process partial unicode characters. Interestingly removing one of the conditions above (e.g. logging to stdout instead of a file or having short lines doesn''t cause the issue).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26990,54,JIRA.13115864.1509700594000.141154.1509700620042@Atlassian.JIRA,2316,Konstantin Privezentsev (JIRA),JIRA.13115864.1509700594000@Atlassian.JIRA,,,2017-11-03 02:17:00-07,"[jira] [Created] (AIRFLOW-1781) Incorrect search user in group in
 LDAP authentication","Konstantin Privezentsev created AIRFLOW-1781:
------------------------------------------------

             Summary: Incorrect search user in group in LDAP authentication
                 Key: AIRFLOW-1781
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1781
             Project: Apache Airflow
          Issue Type: Bug
          Components: authentication
            Reporter: Konstantin Privezentsev
            Priority: Minor


LDAP DN is case insensitive. But search inside group is case sensitive.
As a result user doesn''t have correct permissions after login in case when he logins with name ""user"" and  in LDAP his DN is ""User"".






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549395,277,JIRA.12987271.1467835604000.40979.1467835631032@Atlassian.JIRA,21272,Barbara Gomes (JIRA),JIRA.12987271.1467835604000@Atlassian.JIRA,,,2016-07-06 13:07:11-07,"[jira] [Created] (IOTA-24) Offer control-aware mailbox to
 Performers","Barbara Gomes created IOTA-24:
---------------------------------

             Summary: Offer control-aware mailbox to Performers
                 Key: IOTA-24
                 URL: https://issues.apache.org/jira/browse/IOTA-24
             Project: Iota
          Issue Type: New Feature
    Affects Versions: 0.1
            Reporter: Barbara Gomes
            Assignee: Barbara Gomes
            Priority: Minor


Fey uses only the Default mailbox for all of the Performers started.

We need to offer the developer the possibility of defining which mailbox should the Performer use.

As the possible mailbox to be used should be configure before the System actor gets created, in the application config, we will start with an small step and offer the developer the possibility to choose if the Performer will use the Default mailbox or a control-aware mailbox.

*Note* - Suggestions on how can we add a new dispatcher configuration to the actor System after it has started would be appreciated.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
26991,54,JIRA.13057844.1490093306000.146114.1509739500270@Atlassian.JIRA,2069,Alan Ma (JIRA),JIRA.13057844.1490093306000@Atlassian.JIRA,,,2017-11-03 13:05:00-07,"[jira] [Assigned] (AIRFLOW-1021) Double logging required for new
 users with LDAP","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1021?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alan Ma reassigned AIRFLOW-1021:
--------------------------------

    Assignee: Alan Ma

> Double logging required for new users with LDAP
> -----------------------------------------------
>
>                 Key: AIRFLOW-1021
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1021
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.8
>            Reporter: Marcelo G. Almiron
>            Assignee: Alan Ma
>            Priority: Major
>
> Every user needs to login twice to access Airflow for the first time with LDAP. 
> In the first trial the user is not persistent, so there is no `id` associated, which leads to `None` value returned by `load_user(userid)`, since `userid` is none `None`.
> A quick fix is to add the new user to the session and commit before merging.  That is, in module `airflow/contrib/auth/backends/ldap_auth.py`,  we can change
> ```
> if not user:
>     user = models.User(
>         username=username,
>         is_superuser=False)
> session.merge(user)
> session.commit()
> flask_login.login_user(LdapUser(user))
> ```
> by
> ```
> if not user:
>     user = models.User(
>         username=username,
>         is_superuser=False)
>     session.add(user)
>             
> session.commit()
> session.merge(user)
> flask_login.login_user(LdapUser(user))
> ```



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26992,54,JIRA.13057844.1490093306000.147945.1509749941639@Atlassian.JIRA,2069,Alan Ma (JIRA),JIRA.13057844.1490093306000@Atlassian.JIRA,,,2017-11-03 15:59:01-07,"[jira] [Commented] (AIRFLOW-1021) Double logging required for new
 users with LDAP","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1021?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16238542#comment-16238542 ] 

Alan Ma commented on AIRFLOW-1021:
----------------------------------

Submitted a PR in your place. https://github.com/apache/incubator-airflow/pull/2751

> Double logging required for new users with LDAP
> -----------------------------------------------
>
>                 Key: AIRFLOW-1021
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1021
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.8
>            Reporter: Marcelo G. Almiron
>            Assignee: Alan Ma
>            Priority: Major
>
> Every user needs to login twice to access Airflow for the first time with LDAP. 
> In the first trial the user is not persistent, so there is no `id` associated, which leads to `None` value returned by `load_user(userid)`, since `userid` is none `None`.
> A quick fix is to add the new user to the session and commit before merging.  That is, in module `airflow/contrib/auth/backends/ldap_auth.py`,  we can change
> ```
> if not user:
>     user = models.User(
>         username=username,
>         is_superuser=False)
> session.merge(user)
> session.commit()
> flask_login.login_user(LdapUser(user))
> ```
> by
> ```
> if not user:
>     user = models.User(
>         username=username,
>         is_superuser=False)
>     session.add(user)
>             
> session.commit()
> session.merge(user)
> flask_login.login_user(LdapUser(user))
> ```



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26993,54,JIRA.13115357.1509543196000.148766.1509758520037@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13115357.1509543196000@Atlassian.JIRA,,,2017-11-03 18:22:00-07,"[jira] [Assigned] (AIRFLOW-1774) Better handling of templated
 parameters in Google ML batch prediction/training operators","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1774?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Guillermo Rodr=C3=ADguez Cano reassigned AIRFLOW-1774:
-------------------------------------------------

    Assignee: Guillermo Rodr=C3=ADguez Cano

> Better handling of templated parameters in Google ML batch prediction/tra=
ining operators
> -------------------------------------------------------------------------=
---------------
>
>                 Key: AIRFLOW-1774
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1774
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, operators
>    Affects Versions: Airflow 2.0, 1.9.0
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Assignee: Guillermo Rodr=C3=ADguez Cano
>            Priority: Normal
>
> The {{MLEngineBatchPredictionOperator}} does not support well the templat=
ed parameter {{job_id}} due to a helper function used to detect and cleanup=
 bad job names inhibiting the templating engine to work. I suspect the same=
 may happen with {{MLEngineTrainingOperator}} as well.
> Google ML requieres a unique job id, therefore it is critical to have the=
 possibility to customise the job''s name easily, and preferably with some d=
ata related to the DAG, for example, appending the day to the job''s name vi=
a a macro like {{ds}}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26994,54,JIRA.13116117.1509762027000.148960.1509762060171@Atlassian.JIRA,2069,Alan Ma (JIRA),JIRA.13116117.1509762027000@Atlassian.JIRA,,,2017-11-03 19:21:00-07,[jira] [Created] (AIRFLOW-1782) Allow HttpSensor to support xcom,"Alan Ma created AIRFLOW-1782:
--------------------------------

             Summary: Allow HttpSensor to support xcom
                 Key: AIRFLOW-1782
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1782
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Alan Ma
            Assignee: Alan Ma
            Priority: Minor


Allow HttpSensor to send data to other tasks through xcom.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26995,54,JIRA.13113408.1509476795000.148963.1509762060209@Atlassian.JIRA,2069,Alan Ma (JIRA),JIRA.13113408.1509476795000@Atlassian.JIRA,,,2017-11-03 19:21:00-07,"[jira] [Assigned] (AIRFLOW-1770) Add option to file and hiveconfs
 in HiveOperator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1770?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alan Ma reassigned AIRFLOW-1770:
--------------------------------

    Assignee: Alan Ma  (was: Ace Haidrey)

> Add option to file and hiveconfs in HiveOperator
> ------------------------------------------------
>
>                 Key: AIRFLOW-1770
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1770
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Ace Haidrey
>            Assignee: Alan Ma
>            Priority: Major
>              Labels: operators
>
> The HiveOperator as it currently stands is not flexible enough to accept a hive file and operate on that. You need to read in the contents and pass it and if you do that you need to change the way hiveconfs are in your file to jinja templating.
> Many teams already have their existing sql/hql files and don''t want to convert them to make them as portable and decoupled as possible.
> To accomplish this all we need to do is add the option to pass a hql_file and hiveconfs to the HiveOperator. We change the code in the execute to throw an error if both a hql_file and an hql statement are passed. If just hql_file the simplest way without changing the code of the hive hook is to just read the content of the hql_file and set it to be the self.hql. The hiveconfs get passed directly to the run_cli method and we can combine them with the already passed in hiveconfs.
> If we want to make it optional to pass in the context as hiveconfs we can add that too as related to AIRFLOW-788.
> I''ve included some simple tests to show it all works how we expect.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26996,54,JIRA.13116184.1509817529000.151063.1509817560060@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:46:00-07,"[jira] [Created] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","Dmytro Kulyk created AIRFLOW-1783:
-------------------------------------

             Summary: DAG created from function become unavailable in UI sporadically
                 Key: AIRFLOW-1783
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
             Project: Apache Airflow
          Issue Type: Bug
          Components: DAG, scheduler, ui, webserver
    Affects Versions: 1.8.1, 1.8.2
         Environment: Ubuntu 16.04.3 LTS
Python 2.7.12
CeleryExecutor: 2-nodes cluster
            Reporter: Dmytro Kulyk
            Priority: Major
         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, cube_update.py, cube_update_sit.py

DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
!Capture_before.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""error""
!Capture_error.JPG|thumbnail!
!Capture_after.JPG|thumbnail!
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code:python}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code:python}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26997,54,JIRA.13116184.1509817529000.151065.1509817620042@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:47:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
!Capture_before.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""error""
!Capture_error.JPG|thumbnail!
!Capture_after.JPG|thumbnail!
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
!Capture_before.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""error""
!Capture_error.JPG|thumbnail!
!Capture_after.JPG|thumbnail!
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code:python}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code:python}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, cube_update.py, cube_update_sit.py
>
>
> DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> !Capture_before.JPG|thumbnail!
> However, in case of any activity upon this object (refresh etc) there is an ""error""
> !Capture_error.JPG|thumbnail!
> !Capture_after.JPG|thumbnail!
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26998,54,JIRA.13116184.1509817529000.151073.1509817803693@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:50:03-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
!Capture_before.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
!Capture_error.JPG|thumbnail!
!Capture_after.JPG|thumbnail!
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
!Capture_before.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""error""
!Capture_error.JPG|thumbnail!
!Capture_after.JPG|thumbnail!
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, cube_update.py, cube_update_sit.py
>
>
> DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> !Capture_before.JPG|thumbnail!
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> !Capture_error.JPG|thumbnail!
> !Capture_after.JPG|thumbnail!
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
26999,54,JIRA.13116184.1509817529000.151079.1509817860137@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:51:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
!Capture_before.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
!Capture_error.JPG|thumbnail!
!Capture_after.JPG|thumbnail!
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
!Capture_before.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
!Capture_error.JPG|thumbnail!
!Capture_after.JPG|thumbnail!
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> !Capture_before.JPG|thumbnail!
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> !Capture_error.JPG|thumbnail!
> !Capture_after.JPG|thumbnail!
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27000,54,JIRA.13116184.1509817529000.151248.1509817929731@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:52:09-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Attachment: Capture_tree.JPG

> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> !Capture_before.JPG|thumbnail!
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> !Capture_error.JPG|thumbnail!
> !Capture_after.JPG|thumbnail!
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27001,54,JIRA.13116184.1509817529000.151263.1509818040361@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:54:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] [^Capture_tree.JPG]
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
!Capture_before.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
!Capture_error.JPG|thumbnail!
!Capture_after.JPG|thumbnail!
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27002,54,JIRA.13116184.1509817529000.151274.1509818100184@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:55:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]
!Capture_tree.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27003,54,JIRA.13116184.1509817529000.151273.1509818100173@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:55:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]
!Capture_tree.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]
!Capture_tree.jpg|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> !Capture_tree.JPG|thumbnail!
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27004,54,JIRA.13116184.1509817529000.151272.1509818100160@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:55:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]
!Capture_tree.jpg|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] [^Capture_tree.JPG]
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> !Capture_tree.jpg|thumbnail!
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27005,54,JIRA.13116184.1509817529000.151276.1509818220060@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:57:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]
!Capture_tree.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> !Capture_tree.JPG|thumbnail!
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27006,54,JIRA.13116184.1509817529000.151277.1509818280577@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:58:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG] !Capture_tree.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]
!Capture_tree.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG] !Capture_tree.JPG|thumbnail!
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27007,54,JIRA.13116184.1509817529000.151287.1509818280681@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 10:58:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG] !Capture_tree.JPG|thumbnail!
However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date, #datetime(2017, 9, 1),
>         schedule_interval = schedule_interval, #''0 8 * * *'',
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27008,54,JIRA.13116184.1509817529000.151303.1509818460370@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 11:01:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date,
        schedule_interval = schedule_interval,
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date, #datetime(2017, 9, 1),
        schedule_interval = schedule_interval, #''0 8 * * *'',
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date,
>         schedule_interval = schedule_interval,
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27009,54,JIRA.13078474.1496967081000.151342.1509819120204@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13078474.1496967081000@Atlassian.JIRA,,,2017-11-04 11:12:00-07,"[jira] [Comment Edited] (AIRFLOW-1296) DAGs using operators
 involving cascading skipped tasks fail prematurely","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1296?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16196272#comment-16196272 ] 

Dmytro Kulyk edited comment on AIRFLOW-1296 at 11/4/17 6:11 PM:
----------------------------------------------------------------

have you tried to play with ""trigger_rule""?
After update we''ve reverse situation when SKIPPED status is being pushed despite of trigger_rule set to ""all_done""
Which is opposite to documented [here|https://airflow.incubator.apache.org/concepts.html#latest-run-only]


was (Author: kotyara):
have you tried to play with ""trigger_rule""?
After update we''ve reverse situation when SKIPPED status is being pushed despite of trigger_rule set to ""all_done""

> DAGs using operators involving cascading skipped tasks fail prematurely
> -----------------------------------------------------------------------
>
>                 Key: AIRFLOW-1296
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1296
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.1
>            Reporter: Daniel Huang
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.8.2
>
>
> So this is basically the same issue as AIRFLOW-872 and AIRFLOW-719. A workaround had fixed this (https://github.com/apache/incubator-airflow/pull/2125), but was later reverted (https://github.com/apache/incubator-airflow/pull/2195). I totally agree with the reason for reverting, but I still think this is an issue. 
> The issue is related to any operators that involves cascading skipped tasks, like ShortCircuitOperator or LatestOnlyOperator. These operators mark only their *direct* downstream task as SKIPPED, but additional downstream tasks from that skipped task is left up to the scheduler to cascade the SKIPPED state (see latest only op docs about this expected behavior https://airflow.incubator.apache.org/concepts.html#latest-run-only). However, instead the scheduler marks the DAG run as FAILED prematurely before the DAG has a chance to skip all downstream tasks.
> This example DAG should reproduce the issue: https://gist.github.com/dhuang/61d38fb001c3a917edf4817bb0c915f9. 
> Expected result: DAG succeeds with tasks - latest_only (success) -> dummy1 (skipped) -> dummy2 (skipped) -> dummy3 (skipped)
> Actual result: DAG fails with tasks - latest_only (success) -> dummy1 (skipped) -> dummy2 (none) -> dummy3 (none)
> I believe the results I''m seeing are because of this deadlock prevention logic, https://github.com/apache/incubator-airflow/blob/1.8.1/airflow/models.py#L4182. While that actual result shown above _could_ mean a deadlock, in this case it shouldn''t be. Since this {{update_state}} logic is reached first in each scheduler run, dummy2/dummy3 don''t get a chance to cascade the SKIPPED state. Commenting out that block gives me the results I expect.
> [~bolke] I know you spent awhile trying to reproduce my issue and weren''t able to, but I''m still hitting this on a fresh environment, default configs, sqlite/mysql dbs, local/sequential/celery executors, and 1.8.1/master.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27010,54,JIRA.13078474.1496967081000.151357.1509819180918@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13078474.1496967081000@Atlassian.JIRA,,,2017-11-04 11:13:00-07,"[jira] [Comment Edited] (AIRFLOW-1296) DAGs using operators
 involving cascading skipped tasks fail prematurely","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1296?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16196272#comment-16196272 ] 

Dmytro Kulyk edited comment on AIRFLOW-1296 at 11/4/17 6:12 PM:
----------------------------------------------------------------

have you tried to play with ""trigger_rule""?
After update to 1.8.3 we facing reverse situation when SKIPPED status is being pushed despite of trigger_rule set to ""all_done""
Which is opposite to documented [here|https://airflow.incubator.apache.org/concepts.html#latest-run-only]


was (Author: kotyara):
have you tried to play with ""trigger_rule""?
After update we''ve reverse situation when SKIPPED status is being pushed despite of trigger_rule set to ""all_done""
Which is opposite to documented [here|https://airflow.incubator.apache.org/concepts.html#latest-run-only]

> DAGs using operators involving cascading skipped tasks fail prematurely
> -----------------------------------------------------------------------
>
>                 Key: AIRFLOW-1296
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1296
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.8.1
>            Reporter: Daniel Huang
>            Assignee: Bolke de Bruin
>            Priority: Blocker
>             Fix For: 1.8.2
>
>
> So this is basically the same issue as AIRFLOW-872 and AIRFLOW-719. A workaround had fixed this (https://github.com/apache/incubator-airflow/pull/2125), but was later reverted (https://github.com/apache/incubator-airflow/pull/2195). I totally agree with the reason for reverting, but I still think this is an issue. 
> The issue is related to any operators that involves cascading skipped tasks, like ShortCircuitOperator or LatestOnlyOperator. These operators mark only their *direct* downstream task as SKIPPED, but additional downstream tasks from that skipped task is left up to the scheduler to cascade the SKIPPED state (see latest only op docs about this expected behavior https://airflow.incubator.apache.org/concepts.html#latest-run-only). However, instead the scheduler marks the DAG run as FAILED prematurely before the DAG has a chance to skip all downstream tasks.
> This example DAG should reproduce the issue: https://gist.github.com/dhuang/61d38fb001c3a917edf4817bb0c915f9. 
> Expected result: DAG succeeds with tasks - latest_only (success) -> dummy1 (skipped) -> dummy2 (skipped) -> dummy3 (skipped)
> Actual result: DAG fails with tasks - latest_only (success) -> dummy1 (skipped) -> dummy2 (none) -> dummy3 (none)
> I believe the results I''m seeing are because of this deadlock prevention logic, https://github.com/apache/incubator-airflow/blob/1.8.1/airflow/models.py#L4182. While that actual result shown above _could_ mean a deadlock, in this case it shouldn''t be. Since this {{update_state}} logic is reached first in each scheduler run, dummy2/dummy3 don''t get a chance to cascade the SKIPPED state. Commenting out that block gives me the results I expect.
> [~bolke] I know you spent awhile trying to reproduce my issue and weren''t able to, but I''m still hitting this on a fresh environment, default configs, sqlite/mysql dbs, local/sequential/celery executors, and 1.8.1/master.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27011,54,JIRA.13116193.1509820706000.151433.1509820741433@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116193.1509820706000@Atlassian.JIRA,,,2017-11-04 11:39:01-07,"[jira] [Created] (AIRFLOW-1784) SKIPPED status is being cascading
 wrongly","Dmytro Kulyk created AIRFLOW-1784:
-------------------------------------

             Summary: SKIPPED status is being cascading wrongly
                 Key: AIRFLOW-1784
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1784
             Project: Apache Airflow
          Issue Type: Bug
          Components: operators
    Affects Versions: 1.8.2
         Environment: Ubuntu 16.04.3 LTS 
Python 2.7.12 
CeleryExecutor: 2-nodes cluster
            Reporter: Dmytro Kulyk
            Priority: Critical
         Attachments: Capture_graph.JPG, Capture_tree2.JPG

After implementation of AIRFLOW-1296 within 1.8.2 there is an wrong behavior of LatestOnlyOperator which is forcing SKIPPED status cascading despite of TriggerRule=''all_done'' set
Which is opposite to documented [here|https://airflow.incubator.apache.org/concepts.html#latest-run-only]

*Expected Behavior:*
dummy task and all downstreams (update_*) should not be skipped

Full listings are attached
1.8.1 did not have such issue



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27012,54,JIRA.13116193.1509820706000.151435.1509820800110@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116193.1509820706000@Atlassian.JIRA,,,2017-11-04 11:40:00-07,"[jira] [Updated] (AIRFLOW-1784) SKIPPED status is being cascading
 wrongly","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1784?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1784:
----------------------------------
    Labels: documentation latestonly operators  (was: documentation operators)

> SKIPPED status is being cascading wrongly
> -----------------------------------------
>
>                 Key: AIRFLOW-1784
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1784
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>    Affects Versions: 1.8.2
>         Environment: Ubuntu 16.04.3 LTS 
> Python 2.7.12 
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Critical
>              Labels: documentation, latestonly, operators
>         Attachments: Capture_graph.JPG, Capture_tree2.JPG
>
>
> After implementation of AIRFLOW-1296 within 1.8.2 there is an wrong behavior of LatestOnlyOperator which is forcing SKIPPED status cascading despite of TriggerRule=''all_done'' set
> Which is opposite to documented [here|https://airflow.incubator.apache.org/concepts.html#latest-run-only]
> *Expected Behavior:*
> dummy task and all downstreams (update_*) should not be skipped
> Full listings are attached
> 1.8.1 did not have such issue



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27013,54,JIRA.13116193.1509820706000.151434.1509820800089@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116193.1509820706000@Atlassian.JIRA,,,2017-11-04 11:40:00-07,"[jira] [Updated] (AIRFLOW-1784) SKIPPED status is being cascading
 wrongly","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1784?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1784:
----------------------------------
    Labels: documentation operators  (was: )

> SKIPPED status is being cascading wrongly
> -----------------------------------------
>
>                 Key: AIRFLOW-1784
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1784
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>    Affects Versions: 1.8.2
>         Environment: Ubuntu 16.04.3 LTS 
> Python 2.7.12 
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Critical
>              Labels: documentation, latestonly, operators
>         Attachments: Capture_graph.JPG, Capture_tree2.JPG
>
>
> After implementation of AIRFLOW-1296 within 1.8.2 there is an wrong behavior of LatestOnlyOperator which is forcing SKIPPED status cascading despite of TriggerRule=''all_done'' set
> Which is opposite to documented [here|https://airflow.incubator.apache.org/concepts.html#latest-run-only]
> *Expected Behavior:*
> dummy task and all downstreams (update_*) should not be skipped
> Full listings are attached
> 1.8.1 did not have such issue



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27014,54,JIRA.13116193.1509820706000.151466.1509820980042@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116193.1509820706000@Atlassian.JIRA,,,2017-11-04 11:43:00-07,"[jira] [Updated] (AIRFLOW-1784) SKIPPED status is being cascading
 wrongly","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1784?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1784:
----------------------------------
    Attachment: cube_update.py

> SKIPPED status is being cascading wrongly
> -----------------------------------------
>
>                 Key: AIRFLOW-1784
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1784
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>    Affects Versions: 1.8.2
>         Environment: Ubuntu 16.04.3 LTS 
> Python 2.7.12 
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Critical
>              Labels: documentation, latestonly, operators
>         Attachments: Capture_graph.JPG, Capture_tree2.JPG, cube_update.py
>
>
> After implementation of AIRFLOW-1296 within 1.8.2 there is an wrong behavior of LatestOnlyOperator which is forcing SKIPPED status cascading despite of TriggerRule=''all_done'' set
> Which is opposite to documented [here|https://airflow.incubator.apache.org/concepts.html#latest-run-only]
> *Expected Behavior:*
> dummy task and all downstreams (update_*) should not be skipped
> Full listings are attached
> 1.8.1 did not have such issue



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27015,54,JIRA.13110221.1508315687000.151522.1509822600066@Atlassian.JIRA,2372,Kamil Sambor (JIRA),JIRA.13110221.1508315687000@Atlassian.JIRA,,,2017-11-04 12:10:00-07,"[jira] [Assigned] (AIRFLOW-1729) Ignore whole directories in
 .airflowignore","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1729?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kamil Sambor reassigned AIRFLOW-1729:
-------------------------------------

    Assignee: Kamil Sambor

> Ignore whole directories in .airflowignore
> ------------------------------------------
>
>                 Key: AIRFLOW-1729
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1729
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core
>    Affects Versions: Airflow 2.0
>            Reporter: Cedric Hourcade
>            Assignee: Kamil Sambor
>            Priority: Minor
>
> The .airflowignore file allows to prevent scanning files for DAG. But even if we blacklist fulldirectory the {{os.walk}} will still go through them no matter how deep they are and skip files one by one, which can be an issue when you keep around big .git or virtualvenv directories.
> I suggest to add something like:
> {code}
> dirs[:] = [d for d in dirs if not any([re.findall(p, os.path.join(root, d)) for p in patterns])]
> {code}
> to prune the directories here: https://github.com/apache/incubator-airflow/blob/cfc2f73c445074e1e09d6ef6a056cd2b33a945da/airflow/utils/dag_processing.py#L208-L209 and in {{list_py_file_paths}}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27016,54,JIRA.13116184.1509817529000.151591.1509824520157@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 12:42:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created from function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

In case when DAG created directly (without function) - everything works as a charm

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date,
        schedule_interval = schedule_interval,
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date,
        schedule_interval = schedule_interval,
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created from function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> In case when DAG created directly (without function) - everything works as a charm
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date,
>         schedule_interval = schedule_interval,
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27017,54,JIRA.13116184.1509817529000.151595.1509824580126@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 12:43:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created using reusable function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

In case when DAG created directly (without function) - everything works as a charm

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date,
        schedule_interval = schedule_interval,
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created from function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

In case when DAG created directly (without function) - everything works as a charm

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date,
        schedule_interval = schedule_interval,
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui, webserver
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created using reusable function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> In case when DAG created directly (without function) - everything works as a charm
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date,
>         schedule_interval = schedule_interval,
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27018,54,JIRA.13116184.1509817529000.151618.1509825120467@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 12:52:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Component/s:     (was: webserver)

> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created using reusable function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> In case when DAG created directly (without function) - everything works as a charm
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date,
>         schedule_interval = schedule_interval,
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27019,54,JIRA.13116184.1509817529000.151941.1509831900209@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 14:45:00-07,"[jira] [Commented] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16239262#comment-16239262 ] 

Dmytro Kulyk commented on AIRFLOW-1783:
---------------------------------------

Everything works as a charm in 2 cases:
# when DAG created directly (without function) 
# function and DAG creation call are in same file

> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created using reusable function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date,
>         schedule_interval = schedule_interval,
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27020,54,JIRA.13116184.1509817529000.151940.1509831900096@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 14:45:00-07,"[jira] [Updated] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Dmytro Kulyk updated AIRFLOW-1783:
----------------------------------
    Description: 
I have a DAG created using reusable function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date,
        schedule_interval = schedule_interval,
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}


  was:
I have a DAG created using reusable function (see codescript and attachment below)
After uploading to server it become visible and can be executed by scheduler or manually
[^Capture_before.JPG] 
[^Capture_tree.JPG]

However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
[^Capture_error.JPG]
[^Capture_after.JPG]
{code}
-------------------------------------------------------------------------------
Node: uat01.airflow.retail
-------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
    base_date = dag.latest_execution_date or datetime.now()
AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
{code}

In case when DAG created directly (without function) - everything works as a charm

cube_update.py
{code}
from datetime import datetime
from lib.cube_update import cube_updater

env = ''sit''
start_date = datetime(2017, 7, 1)
schedule_interval = ''0 8 * * *''
 
dag = cube_updater(env, start_date, schedule_interval)
{code}
lib/__init__.py - empty one
lib/cube_update.py
{code}
from airflow import DAG
...

def cube_updater(env, start_date, schedule_interval):
    ...
    dag = DAG(
        dag_id = dag_id,
        default_args = default_args,
        description = ''OLAP Cube updater'',
        start_date = start_date,
        schedule_interval = schedule_interval,
        max_active_runs = 6,
        catchup=True)
....
    return dag
{code}



> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>            Priority: Major
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created using reusable function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date,
>         schedule_interval = schedule_interval,
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27021,54,JIRA.13109391.1507934213000.151997.1509832920184@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13109391.1507934213000@Atlassian.JIRA,,,2017-11-04 15:02:00-07,"[jira] [Commented] (AIRFLOW-1715) Recognizing non-dag files as
 DAGs, then reporting non-existing errors","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1715?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16239278#comment-16239278 ] 

Dmytro Kulyk commented on AIRFLOW-1715:
---------------------------------------

i beleive it have same roots with AIRFLOW-1783

> Recognizing non-dag files as DAGs, then reporting non-existing errors
> ---------------------------------------------------------------------
>
>                 Key: AIRFLOW-1715
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1715
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>         Environment: Ubuntu 16.04
>            Reporter: Zelko Nikolic
>
> In our DAG folder there''s a few helper python files and tools. Airflow wrongly recognizes some of them as DAGs and then reports errors about them, trying to interpret them as DAGs.
> This is completely wrong. There should be a certain signature ID at the top of the file that distinguishes DAG files from other python files.
> Alternatively, there can be a workaround signature that says: ""THIS IS NOT A DAG"", so that Airflow can skip that file.
> For example a comment that looks like this could be such a directive:
> \#  <AIRFLOW.NOT_DAG>



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27022,54,JIRA.13109391.1507934213000.151998.1509832980159@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13109391.1507934213000@Atlassian.JIRA,,,2017-11-04 15:03:00-07,"[jira] [Comment Edited] (AIRFLOW-1715) Recognizing non-dag files as
 DAGs, then reporting non-existing errors","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1715?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16239278#comment-16239278 ] 

Dmytro Kulyk edited comment on AIRFLOW-1715 at 11/4/17 10:02 PM:
-----------------------------------------------------------------

i believe it have same roots with AIRFLOW-1783


was (Author: kotyara):
i beleive it have same roots with AIRFLOW-1783

> Recognizing non-dag files as DAGs, then reporting non-existing errors
> ---------------------------------------------------------------------
>
>                 Key: AIRFLOW-1715
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1715
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>         Environment: Ubuntu 16.04
>            Reporter: Zelko Nikolic
>
> In our DAG folder there''s a few helper python files and tools. Airflow wrongly recognizes some of them as DAGs and then reports errors about them, trying to interpret them as DAGs.
> This is completely wrong. There should be a certain signature ID at the top of the file that distinguishes DAG files from other python files.
> Alternatively, there can be a workaround signature that says: ""THIS IS NOT A DAG"", so that Airflow can skip that file.
> For example a comment that looks like this could be such a directive:
> \#  <AIRFLOW.NOT_DAG>



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27023,54,JIRA.13116184.1509817529000.152001.1509832980208@Atlassian.JIRA,2222,Dmytro Kulyk (JIRA),JIRA.13116184.1509817529000@Atlassian.JIRA,,,2017-11-04 15:03:00-07,"[jira] [Commented] (AIRFLOW-1783) DAG created from function become
 unavailable in UI sporadically","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1783?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16239279#comment-16239279 ] 

Dmytro Kulyk commented on AIRFLOW-1783:
---------------------------------------

possibly same roots with AIRFLOW-1715

> DAG created from function become unavailable in UI sporadically
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1783
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1783
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG, scheduler, ui
>    Affects Versions: 1.8.1, 1.8.2
>         Environment: Ubuntu 16.04.3 LTS
> Python 2.7.12
> CeleryExecutor: 2-nodes cluster
>            Reporter: Dmytro Kulyk
>         Attachments: Capture_after.JPG, Capture_before.JPG, Capture_error.JPG, Capture_tree.JPG, cube_update.py, cube_update_sit.py
>
>
> I have a DAG created using reusable function (see codescript and attachment below)
> After uploading to server it become visible and can be executed by scheduler or manually
> [^Capture_before.JPG] 
> [^Capture_tree.JPG]
> However, in case of any activity upon this object (refresh etc) there is an ""kaboom error""
> [^Capture_error.JPG]
> [^Capture_after.JPG]
> {code}
> -------------------------------------------------------------------------------
> Node: uat01.airflow.retail
> -------------------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python2.7/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/flask_login.py"", line 755, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 219, in view_func
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/utils.py"", line 125, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/www/views.py"", line 1161, in tree
>     base_date = dag.latest_execution_date or datetime.now()
> AttributeError: ''NoneType'' object has no attribute ''latest_execution_date''
> {code}
> cube_update.py
> {code}
> from datetime import datetime
> from lib.cube_update import cube_updater
> env = ''sit''
> start_date = datetime(2017, 7, 1)
> schedule_interval = ''0 8 * * *''
>  
> dag = cube_updater(env, start_date, schedule_interval)
> {code}
> lib/__init__.py - empty one
> lib/cube_update.py
> {code}
> from airflow import DAG
> ...
> def cube_updater(env, start_date, schedule_interval):
>     ...
>     dag = DAG(
>         dag_id = dag_id,
>         default_args = default_args,
>         description = ''OLAP Cube updater'',
>         start_date = start_date,
>         schedule_interval = schedule_interval,
>         max_active_runs = 6,
>         catchup=True)
> ....
>     return dag
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27024,54,JIRA.13092475.1501852037000.153596.1509886860231@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13092475.1501852037000@Atlassian.JIRA,,,2017-11-05 05:01:00-08,[jira] [Commented] (AIRFLOW-1486) Unexpected S3 writing log error,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1486?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16239517#comment-16239517 ] 

Fokko Driesprong commented on AIRFLOW-1486:
-------------------------------------------

What''s is the status on this?

> Unexpected S3 writing log error
> -------------------------------
>
>                 Key: AIRFLOW-1486
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1486
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: 1.8.0
>            Reporter: Stanislav Kudriashev
>            Assignee: Stanislav Kudriashev
>             Fix For: 1.8.1
>
>
> *Current:*
> {code}
> ERROR:root:Could not read logs from s3://airflow-log/my_dag/task1/2017-08-04T16:03:00/1.log
> INFO:root:The key my_dag/task1/2017-08-04T16:03:00/1.log now contains 4648 bytes
> {code}
> *Expected:*
> {code}
> INFO:root:The key my_dag/task1/2017-08-04T16:03:00/1.log now contains 4648 bytes
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27025,54,JIRA.13116257.1509892666000.153793.1509892680209@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13116257.1509892666000@Atlassian.JIRA,,,2017-11-05 06:38:00-08,[jira] [Created] (AIRFLOW-1785) Enable Python 3 tests,"Fokko Driesprong created AIRFLOW-1785:
-----------------------------------------

             Summary: Enable Python 3 tests
                 Key: AIRFLOW-1785
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1785
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Fokko Driesprong


Some of the tests are being skipped when running Python 3.4. We would like to enable these tests to make sure that everything runs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27026,54,JIRA.13092475.1501852037000.154152.1509900060093@Atlassian.JIRA,1439,Stanislav Kudriashev (JIRA),JIRA.13092475.1501852037000@Atlassian.JIRA,,,2017-11-05 08:41:00-08,[jira] [Commented] (AIRFLOW-1486) Unexpected S3 writing log error,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1486?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16239637#comment-16239637 ] 

Stanislav Kudriashev commented on AIRFLOW-1486:
-----------------------------------------------

[~Fokko], here is PR that was closed: https://github.com/apache/incubator-airflow/pull/2499

> Unexpected S3 writing log error
> -------------------------------
>
>                 Key: AIRFLOW-1486
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1486
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: 1.8.0
>            Reporter: Stanislav Kudriashev
>            Assignee: Stanislav Kudriashev
>             Fix For: 1.8.1
>
>
> *Current:*
> {code}
> ERROR:root:Could not read logs from s3://airflow-log/my_dag/task1/2017-08-04T16:03:00/1.log
> INFO:root:The key my_dag/task1/2017-08-04T16:03:00/1.log now contains 4648 bytes
> {code}
> *Expected:*
> {code}
> INFO:root:The key my_dag/task1/2017-08-04T16:03:00/1.log now contains 4648 bytes
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
1103848,215,20040605235356.28DC484473@bugzilla.spamassassin.org,36968,bugzilla-daemon,NULL,,,2004-06-05 16:53:56-07,[Bug 1849] Rule improvement,"http://bugzilla.spamassassin.org/show_bug.cgi?id=1849





------- Additional Comments From lwilton@earthlink.net  2004-06-05 16:53 -------
If the Devs aren''t interested in incorporating them into the standard rulesets, 
I''m sure that we will be more than happy to take them over at the RulesEmporium 
and either incorporate them into existing rulesets or create a new ruleset.  We 
could probably get them into circulation in 3-4 days, considering the time to 
run a few mass checks on them.



------- You are receiving this mail because: -------
You are the assignee for the bug, or are watching the assignee.

",f
27027,54,JIRA.13092475.1501852037000.154158.1509900120273@Atlassian.JIRA,1439,Stanislav Kudriashev (JIRA),JIRA.13092475.1501852037000@Atlassian.JIRA,,,2017-11-05 08:42:00-08,[jira] [Resolved] (AIRFLOW-1486) Unexpected S3 writing log error,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1486?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Stanislav Kudriashev resolved AIRFLOW-1486.
-------------------------------------------
    Resolution: Fixed

> Unexpected S3 writing log error
> -------------------------------
>
>                 Key: AIRFLOW-1486
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1486
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: 1.8.0
>            Reporter: Stanislav Kudriashev
>            Assignee: Stanislav Kudriashev
>             Fix For: 1.8.1
>
>
> *Current:*
> {code}
> ERROR:root:Could not read logs from s3://airflow-log/my_dag/task1/2017-08-04T16:03:00/1.log
> INFO:root:The key my_dag/task1/2017-08-04T16:03:00/1.log now contains 4648 bytes
> {code}
> *Expected:*
> {code}
> INFO:root:The key my_dag/task1/2017-08-04T16:03:00/1.log now contains 4648 bytes
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27028,54,JIRA.13092475.1501852037000.154155.1509900120241@Atlassian.JIRA,1439,Stanislav Kudriashev (JIRA),JIRA.13092475.1501852037000@Atlassian.JIRA,,,2017-11-05 08:42:00-08,[jira] [Commented] (AIRFLOW-1486) Unexpected S3 writing log error,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1486?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16239638#comment-16239638 ] 

Stanislav Kudriashev commented on AIRFLOW-1486:
-----------------------------------------------

https://github.com/apache/incubator-airflow/commit/d9109d6458d136cd2b76ef7180be498ae09b3ea3

> Unexpected S3 writing log error
> -------------------------------
>
>                 Key: AIRFLOW-1486
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1486
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: 1.8.0
>            Reporter: Stanislav Kudriashev
>            Assignee: Stanislav Kudriashev
>             Fix For: 1.8.1
>
>
> *Current:*
> {code}
> ERROR:root:Could not read logs from s3://airflow-log/my_dag/task1/2017-08-04T16:03:00/1.log
> INFO:root:The key my_dag/task1/2017-08-04T16:03:00/1.log now contains 4648 bytes
> {code}
> *Expected:*
> {code}
> INFO:root:The key my_dag/task1/2017-08-04T16:03:00/1.log now contains 4648 bytes
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27029,54,JIRA.13077452.1496705782000.154161.1509900180084@Atlassian.JIRA,1439,Stanislav Kudriashev (JIRA),JIRA.13077452.1496705782000@Atlassian.JIRA,,,2017-11-05 08:43:00-08,"[jira] [Commented] (AIRFLOW-1280) Gantt Chart Height Isn''t Set
 Properly","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1280?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16239639#comment-16239639 ] 

Stanislav Kudriashev commented on AIRFLOW-1280:
-----------------------------------------------

[~aoen], should we close this one?

> Gantt Chart Height Isn''t Set Properly
> -------------------------------------
>
>                 Key: AIRFLOW-1280
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1280
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Stanislav Kudriashev
>         Attachments: gantt.jpg, gantt2.jpg
>
>
> The Gantt view has a dynamic height for the chart body based on the number of tasks but it''s only a heuristic. Instead we should calculate the height of the chart legend and make the height of the chart equal to the height of the legend + desired height for the chart body.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27030,54,JIRA.12990672.1468912243000.158797.1509977100717@Atlassian.JIRA,1958,sam sen (JIRA),JIRA.12990672.1468912243000@Atlassian.JIRA,,,2017-11-06 06:05:00-08,"[jira] [Commented] (AIRFLOW-342)  exception in ''airflow scheduler''
 : Connection reset by peer","
    [ https://issues.apache.org/jira/browse/AIRFLOW-342?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16240339#comment-16240339 ] 

sam sen commented on AIRFLOW-342:
---------------------------------

Same here, is there a workaround? We tried using Celery 4.x but downgraded to see if that would fix the issue, nope.

>  exception in ''airflow scheduler'' : Connection reset by peer
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-342
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-342
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery, scheduler
>    Affects Versions: Airflow 1.7.1.3
>         Environment: OS: Red Hat Enterprise Linux Server 7.2 (Maipo)
> Python: 2.7.5
> Airflow: 1.7.1.3
>            Reporter: Hila Visan
>            Assignee: Hila Visan
>
> ''airflow scheduler'' command throws an exception when running it. 
> Despite the exception, the workers run the tasks from the queues as expected.
> Error details:
>  
> [2016-06-30 19:00:10,130] {jobs.py:758} ERROR - [Errno 104] Connection reset by peer
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 755, in _execute
>     executor.heartbeat()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/base_executor.py"", line 107, in heartbeat
>     self.sync()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/celery_executor.py"", line 74, in sync
>     state = async.state
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 394, in state
>     return self._get_task_meta()[''status'']
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 339, in _get_task_meta
>     return self._maybe_set_cache(self.backend.get_task_meta(self.id))
>   File ""/usr/lib/python2.7/site-packages/celery/backends/amqp.py"", line 163, in get_task_meta
>     binding.declare()
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 521, in declare
>    self.exchange.declare(nowait)
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 174, in declare
>     nowait=nowait, passive=passive,
>   File ""/usr/lib/python2.7/site-packages/amqp/channel.py"", line 615, in exchange_declare
>     self._send_method((40, 10), args)
>   File ""/usr/lib/python2.7/site-packages/amqp/abstract_channel.py"", line 56, in _send_method
>     self.channel_id, method_sig, args, content,
>   File ""/usr/lib/python2.7/site-packages/amqp/method_framing.py"", line 221, in write_method
>     write_frame(1, channel, payload)
>   File ""/usr/lib/python2.7/site-packages/amqp/transport.py"", line 182, in write_frame
>     frame_type, channel, size, payload, 0xce,
>   File ""/usr/lib64/python2.7/socket.py"", line 224, in meth
>     return getattr(self._sock,name)(*args)
> error: [Errno 104] Connection reset by peer
> [2016-06-30 19:00:10,131] {jobs.py:759} ERROR - Tachycardia!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27031,54,JIRA.12990672.1468912243000.159330.1509981540615@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.12990672.1468912243000@Atlassian.JIRA,,,2017-11-06 07:19:00-08,"[jira] [Commented] (AIRFLOW-342)  exception in ''airflow scheduler''
 : Connection reset by peer","
    [ https://issues.apache.org/jira/browse/AIRFLOW-342?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16240428#comment-16240428 ] 

Bolke de Bruin commented on AIRFLOW-342:
----------------------------------------

Please verify and confirm that your rabbitmq is running and accepting connection during the time that this happens. ''Connection reset by peer'' means the server most likely stopped working without properly closing the connection (e.g. a crash)



>  exception in ''airflow scheduler'' : Connection reset by peer
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-342
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-342
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery, scheduler
>    Affects Versions: Airflow 1.7.1.3
>         Environment: OS: Red Hat Enterprise Linux Server 7.2 (Maipo)
> Python: 2.7.5
> Airflow: 1.7.1.3
>            Reporter: Hila Visan
>            Assignee: Hila Visan
>
> ''airflow scheduler'' command throws an exception when running it. 
> Despite the exception, the workers run the tasks from the queues as expected.
> Error details:
>  
> [2016-06-30 19:00:10,130] {jobs.py:758} ERROR - [Errno 104] Connection reset by peer
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 755, in _execute
>     executor.heartbeat()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/base_executor.py"", line 107, in heartbeat
>     self.sync()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/celery_executor.py"", line 74, in sync
>     state = async.state
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 394, in state
>     return self._get_task_meta()[''status'']
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 339, in _get_task_meta
>     return self._maybe_set_cache(self.backend.get_task_meta(self.id))
>   File ""/usr/lib/python2.7/site-packages/celery/backends/amqp.py"", line 163, in get_task_meta
>     binding.declare()
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 521, in declare
>    self.exchange.declare(nowait)
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 174, in declare
>     nowait=nowait, passive=passive,
>   File ""/usr/lib/python2.7/site-packages/amqp/channel.py"", line 615, in exchange_declare
>     self._send_method((40, 10), args)
>   File ""/usr/lib/python2.7/site-packages/amqp/abstract_channel.py"", line 56, in _send_method
>     self.channel_id, method_sig, args, content,
>   File ""/usr/lib/python2.7/site-packages/amqp/method_framing.py"", line 221, in write_method
>     write_frame(1, channel, payload)
>   File ""/usr/lib/python2.7/site-packages/amqp/transport.py"", line 182, in write_frame
>     frame_type, channel, size, payload, 0xce,
>   File ""/usr/lib64/python2.7/socket.py"", line 224, in meth
>     return getattr(self._sock,name)(*args)
> error: [Errno 104] Connection reset by peer
> [2016-06-30 19:00:10,131] {jobs.py:759} ERROR - Tachycardia!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27032,54,JIRA.12990672.1468912243000.159495.1509982861722@Atlassian.JIRA,1958,sam sen (JIRA),JIRA.12990672.1468912243000@Atlassian.JIRA,,,2017-11-06 07:41:01-08,"[jira] [Commented] (AIRFLOW-342)  exception in ''airflow scheduler''
 : Connection reset by peer","
    [ https://issues.apache.org/jira/browse/AIRFLOW-342?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16240450#comment-16240450 ] 

sam sen commented on AIRFLOW-342:
---------------------------------

FYI, I solved my issue by modifying the config option for `celery_result_backend`. I was following various online tutorials and they differed in terms of the backend datastore. I initially had it set to the same setting as the `broker_url`. Once I pointed it to my MySQL instance, I no longer received the error message.

RabbitMQ - 3.6.1
Airflow - 1.8.2
AMQ - 2.2.2
Celery 4.1

>  exception in ''airflow scheduler'' : Connection reset by peer
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-342
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-342
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery, scheduler
>    Affects Versions: Airflow 1.7.1.3
>         Environment: OS: Red Hat Enterprise Linux Server 7.2 (Maipo)
> Python: 2.7.5
> Airflow: 1.7.1.3
>            Reporter: Hila Visan
>            Assignee: Hila Visan
>
> ''airflow scheduler'' command throws an exception when running it. 
> Despite the exception, the workers run the tasks from the queues as expected.
> Error details:
>  
> [2016-06-30 19:00:10,130] {jobs.py:758} ERROR - [Errno 104] Connection reset by peer
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 755, in _execute
>     executor.heartbeat()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/base_executor.py"", line 107, in heartbeat
>     self.sync()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/celery_executor.py"", line 74, in sync
>     state = async.state
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 394, in state
>     return self._get_task_meta()[''status'']
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 339, in _get_task_meta
>     return self._maybe_set_cache(self.backend.get_task_meta(self.id))
>   File ""/usr/lib/python2.7/site-packages/celery/backends/amqp.py"", line 163, in get_task_meta
>     binding.declare()
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 521, in declare
>    self.exchange.declare(nowait)
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 174, in declare
>     nowait=nowait, passive=passive,
>   File ""/usr/lib/python2.7/site-packages/amqp/channel.py"", line 615, in exchange_declare
>     self._send_method((40, 10), args)
>   File ""/usr/lib/python2.7/site-packages/amqp/abstract_channel.py"", line 56, in _send_method
>     self.channel_id, method_sig, args, content,
>   File ""/usr/lib/python2.7/site-packages/amqp/method_framing.py"", line 221, in write_method
>     write_frame(1, channel, payload)
>   File ""/usr/lib/python2.7/site-packages/amqp/transport.py"", line 182, in write_frame
>     frame_type, channel, size, payload, 0xce,
>   File ""/usr/lib64/python2.7/socket.py"", line 224, in meth
>     return getattr(self._sock,name)(*args)
> error: [Errno 104] Connection reset by peer
> [2016-06-30 19:00:10,131] {jobs.py:759} ERROR - Tachycardia!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27033,54,JIRA.13066565.1493090749000.162660.1510006262084@Atlassian.JIRA,2290,Marc Bollinger (JIRA),JIRA.13066565.1493090749000@Atlassian.JIRA,,,2017-11-06 14:11:02-08,"[jira] [Commented] (AIRFLOW-1143) Tasks rejected by workers get
 stuck in QUEUED","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1143?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16240997#comment-16240997 ] 

Marc Bollinger commented on AIRFLOW-1143:
-----------------------------------------

[~aoen] I''m not super familiar with the scheduler -- do we believe this was fixed in AIRFLOW-1641, which is going out with 1.9?

> Tasks rejected by workers get stuck in QUEUED
> ---------------------------------------------
>
>                 Key: AIRFLOW-1143
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1143
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>            Reporter: Dan Davydov
>            Assignee: Gerard Toonstra
>
> If the scheduler schedules a task that is sent to a worker that then rejects the task (e.g. because one of the dependencies of the tasks became bad, like the pool became full), the task will be stuck in the QUEUED state. We hit this trying to switch from invoking the scheduler ""airflow scheduler -n 5"" to just ""airflow scheduler"".
> Restarting the scheduler fixes this because it cleans up orphans, but we shouldn''t have to restart the scheduler to fix these problems (the missing job heartbeats should make the scheduler requeue the task).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27034,54,JIRA.13066565.1493090749000.162687.1510006800185@Atlassian.JIRA,1301,Dan Davydov (JIRA),JIRA.13066565.1493090749000@Atlassian.JIRA,,,2017-11-06 14:20:00-08,"[jira] [Commented] (AIRFLOW-1143) Tasks rejected by workers get
 stuck in QUEUED","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1143?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16241002#comment-16241002 ] 

Dan Davydov commented on AIRFLOW-1143:
--------------------------------------

It may fix it, I''m not sure we were ever sure of the specific root cause. I think to have full confidence we should add a test that checks for this.

> Tasks rejected by workers get stuck in QUEUED
> ---------------------------------------------
>
>                 Key: AIRFLOW-1143
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1143
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>            Reporter: Dan Davydov
>            Assignee: Gerard Toonstra
>
> If the scheduler schedules a task that is sent to a worker that then rejects the task (e.g. because one of the dependencies of the tasks became bad, like the pool became full), the task will be stuck in the QUEUED state. We hit this trying to switch from invoking the scheduler ""airflow scheduler -n 5"" to just ""airflow scheduler"".
> Restarting the scheduler fixes this because it cleans up orphans, but we shouldn''t have to restart the scheduler to fix these problems (the missing job heartbeats should make the scheduler requeue the task).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27035,54,JIRA.13115789.1509670762000.164329.1510012980122@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13115789.1509670762000@Atlassian.JIRA,,,2017-11-06 16:03:00-08,"[jira] [Commented] (AIRFLOW-1780) Long Unicode Characters In
 BashOperator Output Cause Task Hang","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1780?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16241208#comment-16241208 ] 

ASF subversion and git services commented on AIRFLOW-1780:
----------------------------------------------------------

Commit 1a7b63eb16ffa1cb97cb09f71997dcd39f28e645 in incubator-airflow''s branch refs/heads/master from [~aoen]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1a7b63e ]

[AIRFLOW-1780] Fix long output lines with unicode from hanging parent

Fix long task output lines with unicode from
hanging parent process. Tasks that create output
that gets piped into a file in the parent airflow
process would hang if they had long lines with
unicode characters.

Closes #2758 from aoen/ddavydov--
fix_unicode_output_string


> Long Unicode Characters In BashOperator Output Cause Task Hang
> --------------------------------------------------------------
>
>                 Key: AIRFLOW-1780
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1780
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Dan Davydov
>            Assignee: Dan Davydov
>
> Conditions to replicate:
> Create a DAG with a single BashOperator that logs ~10 long lines of text with at least one unicode character (e.g. cat a file with these contents)
> Use BashTaskRunner
> Log to a file
> Run a worker that picks up the task or an airflow run --local command
> Behavior:
> The BashOperator/cat command hangs
> Most likely this is due to a pipe issue, where the unicode characters are filling up the pipe and .format() is not able to process partial unicode characters. Interestingly removing one of the conditions above (e.g. logging to stdout instead of a file or having short lines doesn''t cause the issue).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27036,54,JIRA.13116563.1510022565000.165456.1510022580491@Atlassian.JIRA,2373,Felix Yuan (JIRA),JIRA.13116563.1510022565000@Atlassian.JIRA,,,2017-11-06 18:43:00-08,"[jira] [Created] (AIRFLOW-1786) Upstream Sensor skipped but whole
 DAG fails","Felix Yuan created AIRFLOW-1786:
-----------------------------------

             Summary: Upstream Sensor skipped but whole DAG fails
                 Key: AIRFLOW-1786
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1786
             Project: Apache Airflow
          Issue Type: Bug
          Components: DAG
    Affects Versions: Airflow 1.8
            Reporter: Felix Yuan
            Priority: Minor


I have created a daily DAG, which first monitor whether a file is ready on FTP server, then execute an ETL operation. If someday the file is missing, this is expected, so I set the ""soft_fail"" as True to simply skipped the FTP sensor.

During execution, I see the task of ""FTP sensor"" is surely be skipped, but the whole DAG is marked as failed.

My expectation is the upstream task (FTP Sensor) is skipped, then all downstream tasks are skipped as well.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27037,54,JIRA.13107023.1507141874000.165477.1510022940076@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13107023.1507141874000@Atlassian.JIRA,,,2017-11-06 18:49:00-08,"[jira] [Assigned] (AIRFLOW-1681) Create way to batch retry task
 instances in the CRUD","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1681?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kevin Yang reassigned AIRFLOW-1681:
-----------------------------------

    Assignee: Kevin Yang

> Create way to batch retry task instances in the CRUD
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1681
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1681
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Dan Davydov
>            Assignee: Kevin Yang
>
> The old way to batch retry tasks was to select them on the Task Instances page on the webserver and do a With Selected -> Delete.
> This no longer works as you will get overlapping task instance logs (e.g. the first retry log will be placed in the same location as the first try log). We need an option in the crud called With Selected -> Retry that does the same thing as With Selected -> Delete but follows the logic for task clearing (sets state to none, increases max_tries). Once this feature is stable With Selected -> Delete should probably be removed as it leaders to bad states with the logs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27038,54,JIRA.13116566.1510023245000.165487.1510023300225@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13116566.1510023245000@Atlassian.JIRA,,,2017-11-06 18:55:00-08,"[jira] [Created] (AIRFLOW-1787) Fix batch clear RUNNING task
 instance and inconsistent timestamp format bugs","Kevin Yang created AIRFLOW-1787:
-----------------------------------

             Summary: Fix batch clear RUNNING task instance and inconsistent timestamp format bugs
                 Key: AIRFLOW-1787
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1787
             Project: Apache Airflow
          Issue Type: Bug
          Components: webserver
            Reporter: Kevin Yang
            Assignee: Kevin Yang


* Batch clear in CRUD is not working for task instances in RUNNING state, need to be fixed
* Batch clear and set status are not working for manually triggered task instances because manually triggered task instances have different execution date format.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27039,54,JIRA.13116576.1510025628000.165650.1510025640061@Atlassian.JIRA,2375,Sam Kitajima-Kimbrel (JIRA),JIRA.13116576.1510025628000@Atlassian.JIRA,,,2017-11-06 19:34:00-08,"[jira] [Created] (AIRFLOW-1788) EMR hook doesn''t accept all boto3
 EMR parameters","Sam Kitajima-Kimbrel created AIRFLOW-1788:
---------------------------------------------

             Summary: EMR hook doesn''t accept all boto3 EMR parameters
                 Key: AIRFLOW-1788
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1788
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Sam Kitajima-Kimbrel
            Assignee: Sam Kitajima-Kimbrel


Expected: `SecurityConfiguration` and other optional arguments to boto3''s run_job_flow() method are passed through via the EMR hook in Airflow.

Actual: only a subset of parameters are passed through because they''re hardcoded in the hook.

Recommended fix: use **kwargs



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27040,54,JIRA.13116576.1510025628000.165746.1510026660032@Atlassian.JIRA,2375,Sam Kitajima-Kimbrel (JIRA),JIRA.13116576.1510025628000@Atlassian.JIRA,,,2017-11-06 19:51:00-08,"[jira] [Commented] (AIRFLOW-1788) EMR hook doesn''t accept all boto3
 EMR parameters","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1788?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16241441#comment-16241441 ] 

Sam Kitajima-Kimbrel commented on AIRFLOW-1788:
-----------------------------------------------

Proposed fix: https://github.com/apache/incubator-airflow/pull/2760

> EMR hook doesn''t accept all boto3 EMR parameters
> ------------------------------------------------
>
>                 Key: AIRFLOW-1788
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1788
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Sam Kitajima-Kimbrel
>            Assignee: Sam Kitajima-Kimbrel
>
> Expected: `SecurityConfiguration` and other optional arguments to boto3''s run_job_flow() method are passed through via the EMR hook in Airflow.
> Actual: only a subset of parameters are passed through because they''re hardcoded in the hook.
> Recommended fix: use **kwargs



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27041,54,JIRA.13116576.1510025628000.165934.1510028701203@Atlassian.JIRA,2375,Sam Kitajima-Kimbrel (JIRA),JIRA.13116576.1510025628000@Atlassian.JIRA,,,2017-11-06 20:25:01-08,"[jira] [Updated] (AIRFLOW-1788) EMR hook doesn''t pass through
 SecurityConfiguration parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1788?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sam Kitajima-Kimbrel updated AIRFLOW-1788:
------------------------------------------
    Summary: EMR hook doesn''t pass through SecurityConfiguration parameter  (was: EMR hook doesn''t accept all boto3 EMR parameters)

> EMR hook doesn''t pass through SecurityConfiguration parameter
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1788
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1788
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Sam Kitajima-Kimbrel
>            Assignee: Sam Kitajima-Kimbrel
>
> Expected: `SecurityConfiguration` and other optional arguments to boto3''s run_job_flow() method are passed through via the EMR hook in Airflow.
> Actual: only a subset of parameters are passed through because they''re hardcoded in the hook.
> Recommended fix: use **kwargs



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27042,54,JIRA.13116576.1510025628000.165935.1510028760139@Atlassian.JIRA,2375,Sam Kitajima-Kimbrel (JIRA),JIRA.13116576.1510025628000@Atlassian.JIRA,,,2017-11-06 20:26:00-08,"[jira] [Updated] (AIRFLOW-1788) EMR hook doesn''t pass through
 SecurityConfiguration parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1788?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sam Kitajima-Kimbrel updated AIRFLOW-1788:
------------------------------------------
    Description: 
Expected: `SecurityConfiguration` argument to boto3''s run_job_flow() method are passed through via the EMR hook in Airflow.

Actual: Not passed through because hard-coded parameter names.

  was:
Expected: `SecurityConfiguration` and other optional arguments to boto3''s run_job_flow() method are passed through via the EMR hook in Airflow.

Actual: only a subset of parameters are passed through because they''re hardcoded in the hook.

Recommended fix: use **kwargs


> EMR hook doesn''t pass through SecurityConfiguration parameter
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1788
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1788
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Sam Kitajima-Kimbrel
>            Assignee: Sam Kitajima-Kimbrel
>
> Expected: `SecurityConfiguration` argument to boto3''s run_job_flow() method are passed through via the EMR hook in Airflow.
> Actual: Not passed through because hard-coded parameter names.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27043,54,JIRA.13116612.1510047458000.167746.1510047480734@Atlassian.JIRA,1960,=?utf-8?Q?Ignasi_Peir=C3=B3_=28JIRA=29?=,JIRA.13116612.1510047458000@Atlassian.JIRA,,,2017-11-07 01:38:00-08,"[jira] [Created] (AIRFLOW-1789) Log SSHOperator stderr to
 log.warning","Ignasi Peir=C3=B3 created AIRFLOW-1789:
-------------------------------------

             Summary: Log SSHOperator stderr to log.warning
                 Key: AIRFLOW-1789
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1789
             Project: Apache Airflow
          Issue Type: Improvement
          Components: logging, operators
            Reporter: Ignasi Peir=C3=B3
            Assignee: Ignasi Peir=C3=B3
            Priority: Minor


Logging of SSHOperator was added in issue AIRFLOW-1712
In github [#2686|https://github.com/apache/incubator-airflow/pull/2686]issu=
e it was asked that stderr should be logged to log.warning. This issue impl=
ements this functionality:
* stdout is logged to log.info
* stderr is logged to log.warning




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27044,54,JIRA.13116612.1510047458000.167823.1510048320429@Atlassian.JIRA,1960,=?utf-8?Q?Ignasi_Peir=C3=B3_=28JIRA=29?=,JIRA.13116612.1510047458000@Atlassian.JIRA,,,2017-11-07 01:52:00-08,"[jira] [Updated] (AIRFLOW-1789) Log SSHOperator stderr to
 log.warning","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1789?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ignasi Peir=C3=B3 updated AIRFLOW-1789:
----------------------------------
    Description:=20
Logging of SSHOperator was added in issue AIRFLOW-1712
In github [#2686|https://github.com/apache/incubator-airflow/pull/2686] iss=
ue it was asked that stderr should be logged to log.warning. This issue imp=
lements this functionality:
* stdout is logged to log.info
* stderr is logged to log.warning


  was:
Logging of SSHOperator was added in issue AIRFLOW-1712
In github [#2686|https://github.com/apache/incubator-airflow/pull/2686]issu=
e it was asked that stderr should be logged to log.warning. This issue impl=
ements this functionality:
* stdout is logged to log.info
* stderr is logged to log.warning



> Log SSHOperator stderr to log.warning
> -------------------------------------
>
>                 Key: AIRFLOW-1789
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1789
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>
> Logging of SSHOperator was added in issue AIRFLOW-1712
> In github [#2686|https://github.com/apache/incubator-airflow/pull/2686] i=
ssue it was asked that stderr should be logged to log.warning. This issue i=
mplements this functionality:
> * stdout is logged to log.info
> * stderr is logged to log.warning



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27045,54,JIRA.13116697.1510066613000.170332.1510066620317@Atlassian.JIRA,2376,Hugo Prudent (JIRA),JIRA.13116697.1510066613000@Atlassian.JIRA,,,2017-11-07 06:57:00-08,[jira] [Created] (AIRFLOW-1790) AWS Batch Operator Suppport,"Hugo Prudent created AIRFLOW-1790:
-------------------------------------

             Summary: AWS Batch Operator Suppport
                 Key: AIRFLOW-1790
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1790
             Project: Apache Airflow
          Issue Type: New Feature
          Components: operators
    Affects Versions: 1.9.0
            Reporter: Hugo Prudent
            Assignee: Hugo Prudent


Add support to AWS Batch operator



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27046,54,JIRA.13116697.1510066613000.170736.1510069260198@Atlassian.JIRA,2377,Hugh Christensen (JIRA),JIRA.13116697.1510066613000@Atlassian.JIRA,,,2017-11-07 07:41:00-08,[jira] [Commented] (AIRFLOW-1790) AWS Batch Operator Suppport,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1790?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16242210#comment-16242210 ] 

Hugh Christensen commented on AIRFLOW-1790:
-------------------------------------------

Thank you [~hprudent]. This will be an important addition to Airflow. I note that this is one of the roadmap items for 2017 https://cwiki.apache.org/confluence/display/AIRFLOW/2017+Roadmap+Items. 

Would be great to get this included asap.

> AWS Batch Operator Suppport
> ---------------------------
>
>                 Key: AIRFLOW-1790
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1790
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: operators
>    Affects Versions: 1.9.0
>            Reporter: Hugo Prudent
>            Assignee: Hugo Prudent
>
> Add support to AWS Batch operator



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27047,54,JIRA.13115789.1509670762000.173061.1510081501424@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13115789.1509670762000@Atlassian.JIRA,,,2017-11-07 11:05:01-08,"[jira] [Updated] (AIRFLOW-1780) Long Unicode Characters In
 BashOperator Output Cause Task Hang","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1780?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1780:
------------------------------------
    Fix Version/s: 1.9.1

> Long Unicode Characters In BashOperator Output Cause Task Hang
> --------------------------------------------------------------
>
>                 Key: AIRFLOW-1780
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1780
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Dan Davydov
>            Assignee: Dan Davydov
>             Fix For: 1.9.1
>
>
> Conditions to replicate:
> Create a DAG with a single BashOperator that logs ~10 long lines of text with at least one unicode character (e.g. cat a file with these contents)
> Use BashTaskRunner
> Log to a file
> Run a worker that picks up the task or an airflow run --local command
> Behavior:
> The BashOperator/cat command hangs
> Most likely this is due to a pipe issue, where the unicode characters are filling up the pipe and .format() is not able to process partial unicode characters. Interestingly removing one of the conditions above (e.g. logging to stdout instead of a file or having short lines doesn''t cause the issue).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27048,54,JIRA.13115789.1509670762000.173081.1510081560458@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13115789.1509670762000@Atlassian.JIRA,,,2017-11-07 11:06:00-08,"[jira] [Resolved] (AIRFLOW-1780) Long Unicode Characters In
 BashOperator Output Cause Task Hang","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1780?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1780.
-------------------------------------
    Resolution: Fixed

> Long Unicode Characters In BashOperator Output Cause Task Hang
> --------------------------------------------------------------
>
>                 Key: AIRFLOW-1780
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1780
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Dan Davydov
>            Assignee: Dan Davydov
>             Fix For: 1.9.1
>
>
> Conditions to replicate:
> Create a DAG with a single BashOperator that logs ~10 long lines of text with at least one unicode character (e.g. cat a file with these contents)
> Use BashTaskRunner
> Log to a file
> Run a worker that picks up the task or an airflow run --local command
> Behavior:
> The BashOperator/cat command hangs
> Most likely this is due to a pipe issue, where the unicode characters are filling up the pipe and .format() is not able to process partial unicode characters. Interestingly removing one of the conditions above (e.g. logging to stdout instead of a file or having short lines doesn''t cause the issue).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706905,24,153193972118.30396.11760981804168671793@gitbox.apache.org,25322,zhasheng,NULL,,,2018-07-18 11:48:41-07,"[incubator-mxnet-site] branch asf-site updated: Bump the publish
 timestamp.","This is an automated email from the ASF dual-hosted git repository.

zhasheng pushed a commit to branch asf-site
in repository https://gitbox.apache.org/repos/asf/incubator-mxnet-site.git


The following commit(s) were added to refs/heads/asf-site by this push:
     new 6e44135  Bump the publish timestamp.
6e44135 is described below

commit 6e441356d7963c66f92f6d42136b8a7962c50c2b
Author: mxnet-ci <mxnet-ci>
AuthorDate: Wed Jul 18 18:48:35 2018 +0000

    Bump the publish timestamp.
---
 date.txt | 1 +
 1 file changed, 1 insertion(+)

diff --git a/date.txt b/date.txt
new file mode 100644
index 0000000..1494e89
--- /dev/null
+++ b/date.txt
@@ -0,0 +1 @@
+Wed Jul 18 18:48:35 UTC 2018


",t
27049,54,JIRA.13115789.1509670762000.173096.1510081620649@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13115789.1509670762000@Atlassian.JIRA,,,2017-11-07 11:07:00-08,"[jira] [Updated] (AIRFLOW-1780) Long Unicode Characters In
 BashOperator Output Cause Task Hang","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1780?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1780:
------------------------------------
    Affects Version/s: 1.8.2
                       1.9.0

> Long Unicode Characters In BashOperator Output Cause Task Hang
> --------------------------------------------------------------
>
>                 Key: AIRFLOW-1780
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1780
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0, 1.8.2
>            Reporter: Dan Davydov
>            Assignee: Dan Davydov
>             Fix For: 1.9.1
>
>
> Conditions to replicate:
> Create a DAG with a single BashOperator that logs ~10 long lines of text with at least one unicode character (e.g. cat a file with these contents)
> Use BashTaskRunner
> Log to a file
> Run a worker that picks up the task or an airflow run --local command
> Behavior:
> The BashOperator/cat command hangs
> Most likely this is due to a pipe issue, where the unicode characters are filling up the pipe and .format() is not able to process partial unicode characters. Interestingly removing one of the conditions above (e.g. logging to stdout instead of a file or having short lines doesn''t cause the issue).



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27050,54,JIRA.13116566.1510023245000.173525.1510082640812@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13116566.1510023245000@Atlassian.JIRA,,,2017-11-07 11:24:00-08,"[jira] [Commented] (AIRFLOW-1787) Fix batch clear RUNNING task
 instance and inconsistent timestamp format bugs","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1787?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16242698#comment-16242698 ] 

ASF subversion and git services commented on AIRFLOW-1787:
----------------------------------------------------------

Commit 313f5bac4a3f804094bcd583e0e5fbc3b5f405bb in incubator-airflow''s branch refs/heads/master from [~kevinyang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=313f5ba ]

[AIRFLOW-1787] Fix task instance batch clear and set state bugs

Fixes Batch clear in Task Instances view is not working
for task instances in RUNNING state and all batch
operations in Task instances view cannot work when
manually triggered task instances are selected
because they have a different execution date
format.

Closes #2759 from yrqls21/fix-ti-batch-clear-n
-set-state-bugs


> Fix batch clear RUNNING task instance and inconsistent timestamp format bugs
> ----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1787
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1787
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>
> * Batch clear in CRUD is not working for task instances in RUNNING state, need to be fixed
> * Batch clear and set status are not working for manually triggered task instances because manually triggered task instances have different execution date format.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27051,54,JIRA.13116890.1510125098000.178818.1510125120044@Atlassian.JIRA,2378,Shawn Wang (JIRA),JIRA.13116890.1510125098000@Atlassian.JIRA,,,2017-11-07 23:12:00-08,"[jira] [Created] (AIRFLOW-1791) Unexpected ""AttributeError:
 ''unicode'' object has no attribute ''val''"" from Variable.setdefault","Shawn Wang created AIRFLOW-1791:
-----------------------------------

             Summary: Unexpected ""AttributeError: ''unicode'' object has no attribute ''val''"" from Variable.setdefault
                 Key: AIRFLOW-1791
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1791
             Project: Apache Airflow
          Issue Type: Bug
          Components: core
    Affects Versions: Airflow 1.8
         Environment: Python 2.7, Airflow 1.8.2
            Reporter: Shawn Wang


In Variable.setdefault method,

{code:python}
        obj = Variable.get(key, default_var=default_sentinel, deserialize_json=False)
        if obj is default_sentinel:
            // ...
        else:
            if deserialize_json:
                return json.loads(obj.val)
            else:
                return obj.val
{code}

While obj is retrieved by ""get"" method which has already fetched the val attribute from obj, so this ""obj.val"" throws the AttributeError.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27052,54,JIRA.13117003.1510154838000.182563.1510154880420@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13117003.1510154838000@Atlassian.JIRA,,,2017-11-08 07:28:00-08,[jira] [Created] (AIRFLOW-1792) Missing attribute DruidOperator,"Fokko Driesprong created AIRFLOW-1792:
-----------------------------------------

             Summary: Missing attribute DruidOperator
                 Key: AIRFLOW-1792
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1792
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Fokko Driesprong


The intervals templating attribute is missing for the DruidOperator and throws a warning.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27053,54,JIRA.13117028.1510159199000.183235.1510159260233@Atlassian.JIRA,2155,Cedrik Neumann (JIRA),JIRA.13117028.1510159199000@Atlassian.JIRA,,,2017-11-08 08:41:00-08,"[jira] [Created] (AIRFLOW-1793) DockerOperator doesn''t work with
 docker_conn_id","Cedrik Neumann created AIRFLOW-1793:
---------------------------------------

             Summary: DockerOperator doesn''t work with docker_conn_id
                 Key: AIRFLOW-1793
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1793
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.9.0
            Reporter: Cedrik Neumann


The implementation of DockerOperator uses `self.base_url` when loading the DockerHook instead of `self.docker_url`:
https://github.com/apache/incubator-airflow/blob/v1-9-stable/airflow/operators/docker_operator.py#L150

{noformat}
[2017-11-08 16:10:13,082] {base_task_runner.py:98} INFO - Subtask:   File ""/src/apache-airflow/airflow/operators/docker_operator.py"", line 161, in execute
[2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:     self.cli = self.get_hook().get_conn()
[2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:   File ""/src/apache-airflow/airflow/operators/docker_operator.py"", line 150, in get_hook
[2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:     base_url=self.base_url,
[2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask: AttributeError: ''DockerOperator'' object has no attribute ''base_url''
{noformat}




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27054,54,JIRA.13022646.1479852557000.183537.1510160820338@Atlassian.JIRA,2305,William Pursell (JIRA),JIRA.13022646.1479852557000@Atlassian.JIRA,,,2017-11-08 09:07:00-08,[jira] [Assigned] (AIRFLOW-646) setup.py install fails,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-646?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

William Pursell reassigned AIRFLOW-646:
---------------------------------------

    Assignee: William Pursell

> setup.py install fails
> ----------------------
>
>                 Key: AIRFLOW-646
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-646
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.7.1
>         Environment: OS X
> Ubuntu 16.04
> Python 2.7
> Python 3.5
>            Reporter: Nick Allen
>            Assignee: William Pursell
>
> Running `python setup.py install` or listing airflow as a dependency in another setup.py install_requires section results in the following error:
> {quote}
> Running python-daemon-2.1.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-fibs2crb/python-daemon-2.1.2/egg-dist-tmp-hbg5xgc1
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 154, in save_modules
>     yield saved
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 243, in run_setup
>     DirectorySandbox(setup_dir).run(runner)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 273, in run
>     return func()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 242, in runner
>     _execfile(setup_script, ns)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 46, in _execfile
>     exec(code, globals, locals)
>   File ""/tmp/easy_install-fibs2crb/python-daemon-2.1.2/setup.py"", line 43, in <module>
>     sys.exit(errno)
> AttributeError: module ''version'' has no attribute ''ChangelogAwareDistribution''
> During handling of the above exception, another exception occurred:
> Traceback (most recent call last):
>   File ""setup.py"", line 281, in <module>
>     do_setup()
>   File ""setup.py"", line 275, in do_setup
>     ''extra_clean'': CleanCommand,
>   File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
>     dist.run_commands()
>   File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
>     self.run_command(cmd)
>   File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
>     cmd_obj.run()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/install.py"", line 67, in run
>     self.do_egg_install()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/install.py"", line 117, in do_egg_install
>     cmd.run()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 391, in run
>     self.easy_install(spec, not self.no_deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 621, in easy_install
>     return self.install_item(None, spec, tmpdir, deps, True)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 672, in install_item
>     self.process_distribution(spec, dist, deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 717, in process_distribution
>     [requirement], self.local_index, self.easy_install
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 826, in resolve
>     dist = best[req.key] = env.best_match(req, ws, installer)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1092, in best_match
>     return self.obtain(req, installer)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1104, in obtain
>     return installer(requirement)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 640, in easy_install
>     return self.install_item(spec, dist.location, tmpdir, deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 670, in install_item
>     dists = self.install_eggs(spec, download, tmpdir)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 850, in install_eggs
>     return self.build_and_install(setup_script, setup_base)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1078, in build_and_install
>     self.run_setup(setup_script, setup_base, args)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1064, in run_setup
>     run_setup(setup_script, args)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 246, in run_setup
>     raise
>   File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
>     self.gen.throw(type, value, traceback)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
>     self.gen.throw(type, value, traceback)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 166, in save_modules
>     saved_exc.resume()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 141, in resume
>     six.reraise(type, exc, self._tb)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/_vendor/six.py"", line 685, in reraise
>     raise value.with_traceback(tb)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 154, in save_modules
>     yield saved
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 243, in run_setup
>     DirectorySandbox(setup_dir).run(runner)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 273, in run
>     return func()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 242, in runner
>     _execfile(setup_script, ns)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 46, in _execfile
>     exec(code, globals, locals)
>   File ""/tmp/easy_install-fibs2crb/python-daemon-2.1.2/setup.py"", line 43, in <module>
>     sys.exit(errno)
> AttributeError: module ''version'' has no attribute ''ChangelogAwareDistribution''
> {quote}
> Issue appears to be due to setup.py structure of {{python-daemon}} package, working on opening an issue on project''s source page. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27055,54,JIRA.13116612.1510047458000.184460.1510166341008@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13116612.1510047458000@Atlassian.JIRA,,,2017-11-08 10:39:01-08,"[jira] [Commented] (AIRFLOW-1789) Log SSHOperator stderr to
 log.warning","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1789?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244499#comment-16244499 ]=20

ASF subversion and git services commented on AIRFLOW-1789:
----------------------------------------------------------

Commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8 in incubator-airflow''s bran=
ch refs/heads/master from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D194=
3a96 ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh


> Log SSHOperator stderr to log.warning
> -------------------------------------
>
>                 Key: AIRFLOW-1789
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1789
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>
> Logging of SSHOperator was added in issue AIRFLOW-1712
> In github [#2686|https://github.com/apache/incubator-airflow/pull/2686] i=
ssue it was asked that stderr should be logged to log.warning. This issue i=
mplements this functionality:
> * stdout is logged to log.info
> * stderr is logged to log.warning



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27056,54,JIRA.13109170.1507880653000.184465.1510166341072@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-08 10:39:01-08,[jira] [Commented] (AIRFLOW-1712) Log SSHOperator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244501#comment-16244501 ]=20

ASF subversion and git services commented on AIRFLOW-1712:
----------------------------------------------------------

Commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8 in incubator-airflow''s bran=
ch refs/heads/master from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D194=
3a96 ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh


> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27057,54,JIRA.13109170.1507880653000.184464.1510166341059@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-08 10:39:01-08,[jira] [Commented] (AIRFLOW-1712) Log SSHOperator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244500#comment-16244500 ]=20

ASF subversion and git services commented on AIRFLOW-1712:
----------------------------------------------------------

Commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8 in incubator-airflow''s bran=
ch refs/heads/master from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D194=
3a96 ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh


> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27058,54,JIRA.13109170.1507880653000.184476.1510166401481@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-08 10:40:01-08,[jira] [Commented] (AIRFLOW-1712) Log SSHOperator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244504#comment-16244504 ]=20

ASF subversion and git services commented on AIRFLOW-1712:
----------------------------------------------------------

Commit 54ab5032aab9a466bc2157b828a35c591dd07e2e in incubator-airflow''s bran=
ch refs/heads/v1-9-test from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D54a=
b503 ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh

(cherry picked from commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549396,277,JIRA.12987271.1467835604000.41001.1467835751143@Atlassian.JIRA,21272,Barbara Gomes (JIRA),JIRA.12987271.1467835604000@Atlassian.JIRA,,,2016-07-06 13:09:11-07,"[jira] [Updated] (IOTA-24) Offer control-aware mailbox to
 Performers","
     [ https://issues.apache.org/jira/browse/IOTA-24?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Barbara Gomes updated IOTA-24:
------------------------------
    Description: 
Fey uses only the Default mailbox for all of the Performers started.

We need to offer the developer the possibility of defining which mailbox should the Performer use.

As the possible mailbox to be used should be configure before the ActorSystem gets created, in the application config, we will start with an small step and offer the developer the possibility to choose if the Performer will use the Default mailbox or a control-aware mailbox.

*Note* - Suggestions on how can we add a new dispatcher configuration to the ActorSystem after it has started would be appreciated.

  was:
Fey uses only the Default mailbox for all of the Performers started.

We need to offer the developer the possibility of defining which mailbox should the Performer use.

As the possible mailbox to be used should be configure before the System actor gets created, in the application config, we will start with an small step and offer the developer the possibility to choose if the Performer will use the Default mailbox or a control-aware mailbox.

*Note* - Suggestions on how can we add a new dispatcher configuration to the actor System after it has started would be appreciated.


> Offer control-aware mailbox to Performers
> -----------------------------------------
>
>                 Key: IOTA-24
>                 URL: https://issues.apache.org/jira/browse/IOTA-24
>             Project: Iota
>          Issue Type: New Feature
>    Affects Versions: 0.1
>            Reporter: Barbara Gomes
>            Assignee: Barbara Gomes
>            Priority: Minor
>
> Fey uses only the Default mailbox for all of the Performers started.
> We need to offer the developer the possibility of defining which mailbox should the Performer use.
> As the possible mailbox to be used should be configure before the ActorSystem gets created, in the application config, we will start with an small step and offer the developer the possibility to choose if the Performer will use the Default mailbox or a control-aware mailbox.
> *Note* - Suggestions on how can we add a new dispatcher configuration to the ActorSystem after it has started would be appreciated.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
27059,54,JIRA.13109170.1507880653000.184490.1510166401771@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-08 10:40:01-08,[jira] [Commented] (AIRFLOW-1712) Log SSHOperator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244507#comment-16244507 ]=20

ASF subversion and git services commented on AIRFLOW-1712:
----------------------------------------------------------

Commit 38593dbd7a663ebb5d192845426386c5dc984f9d in incubator-airflow''s bran=
ch refs/heads/v1-9-stable from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D385=
93db ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh

(cherry picked from commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27060,54,JIRA.13109170.1507880653000.184494.1510166401811@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-08 10:40:01-08,[jira] [Commented] (AIRFLOW-1712) Log SSHOperator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244508#comment-16244508 ]=20

ASF subversion and git services commented on AIRFLOW-1712:
----------------------------------------------------------

Commit 38593dbd7a663ebb5d192845426386c5dc984f9d in incubator-airflow''s bran=
ch refs/heads/v1-9-stable from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D385=
93db ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh

(cherry picked from commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27061,54,JIRA.13109170.1507880653000.184483.1510166401548@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13109170.1507880653000@Atlassian.JIRA,,,2017-11-08 10:40:01-08,[jira] [Commented] (AIRFLOW-1712) Log SSHOperator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1712?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244505#comment-16244505 ]=20

ASF subversion and git services commented on AIRFLOW-1712:
----------------------------------------------------------

Commit 54ab5032aab9a466bc2157b828a35c591dd07e2e in incubator-airflow''s bran=
ch refs/heads/v1-9-test from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D54a=
b503 ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh

(cherry picked from commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Log SSHOperator output
> ----------------------
>
>                 Key: AIRFLOW-1712
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1712
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> After commiting [AIRFLOW-756] and [AIRFLOW-751], SSHOperator does not log=
 stdout like SSHExecutor did.
> Edit ssh_operator to log stdout again.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27062,54,JIRA.13116612.1510047458000.184497.1510166401836@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13116612.1510047458000@Atlassian.JIRA,,,2017-11-08 10:40:01-08,"[jira] [Resolved] (AIRFLOW-1789) Log SSHOperator stderr to
 log.warning","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1789?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1789.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2761
[https://github.com/apache/incubator-airflow/pull/2761]

> Log SSHOperator stderr to log.warning
> -------------------------------------
>
>                 Key: AIRFLOW-1789
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1789
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> Logging of SSHOperator was added in issue AIRFLOW-1712
> In github [#2686|https://github.com/apache/incubator-airflow/pull/2686] i=
ssue it was asked that stderr should be logged to log.warning. This issue i=
mplements this functionality:
> * stdout is logged to log.info
> * stderr is logged to log.warning



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
621362,146,JIRA.12821500.1429182573000.135672.1434981360642@Atlassian.JIRA,23736,Rajat Khandelwal (JIRA),JIRA.12821500.1429182573000@Atlassian.JIRA,,,2015-06-22 06:56:00-07,"[jira] [Commented] (LENS-513) add jar should be able to take regex
 path and should be able to add multiple jars","
    [ https://issues.apache.org/jira/browse/LENS-513?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14595945#comment-14595945 ] 

Rajat Khandelwal commented on LENS-513:
---------------------------------------

Committed. Thanks [~yash360@gmail.com]

> add jar should be able to take regex path and should be able to add multiple jars
> ---------------------------------------------------------------------------------
>
>                 Key: LENS-513
>                 URL: https://issues.apache.org/jira/browse/LENS-513
>             Project: Apache Lens
>          Issue Type: Improvement
>          Components: client
>    Affects Versions: 2.2
>            Reporter: Rajat Khandelwal
>            Assignee: Yash Sharma
>              Labels: newbie
>             Fix For: 2.2
>
>         Attachments: LENS-513.16.patch, LENS-513.28.patch
>
>




--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
27063,54,JIRA.13116612.1510047458000.184474.1510166401411@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13116612.1510047458000@Atlassian.JIRA,,,2017-11-08 10:40:01-08,"[jira] [Commented] (AIRFLOW-1789) Log SSHOperator stderr to
 log.warning","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1789?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244503#comment-16244503 ]=20

ASF subversion and git services commented on AIRFLOW-1789:
----------------------------------------------------------

Commit 54ab5032aab9a466bc2157b828a35c591dd07e2e in incubator-airflow''s bran=
ch refs/heads/v1-9-test from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D54a=
b503 ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh

(cherry picked from commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Log SSHOperator stderr to log.warning
> -------------------------------------
>
>                 Key: AIRFLOW-1789
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1789
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> Logging of SSHOperator was added in issue AIRFLOW-1712
> In github [#2686|https://github.com/apache/incubator-airflow/pull/2686] i=
ssue it was asked that stderr should be logged to log.warning. This issue i=
mplements this functionality:
> * stdout is logged to log.info
> * stderr is logged to log.warning



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27064,54,JIRA.13117003.1510154838000.184549.1510166461187@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117003.1510154838000@Atlassian.JIRA,,,2017-11-08 10:41:01-08,[jira] [Commented] (AIRFLOW-1792) Missing attribute DruidOperator,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1792?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16244515#comment-16244515 ] 

ASF subversion and git services commented on AIRFLOW-1792:
----------------------------------------------------------

Commit 84623da77be81c4f8d517891fe70db3d882c471a in incubator-airflow''s branch refs/heads/v1-9-stable from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=84623da ]

[AIRFLOW-1792] Missing intervals DruidOperator

The DruidOperator allows you to template the
intervals field which is
important when you are doing backfills with Druid.
This field was
missing in the constructor and Airflow threw a
warning

Closes #2764 from Fokko/patch-1

(cherry picked from commit 3eb2dd86b9cdb5d83767d5969011a83c6521370d)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Missing attribute DruidOperator
> -------------------------------
>
>                 Key: AIRFLOW-1792
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1792
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>
> The intervals templating attribute is missing for the DruidOperator and throws a warning.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27065,54,JIRA.13117003.1510154838000.184548.1510166461177@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117003.1510154838000@Atlassian.JIRA,,,2017-11-08 10:41:01-08,[jira] [Commented] (AIRFLOW-1792) Missing attribute DruidOperator,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1792?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16244514#comment-16244514 ] 

ASF subversion and git services commented on AIRFLOW-1792:
----------------------------------------------------------

Commit 8dea4a64ee34e2aa235b45a1b4a711fc5cfe7ea6 in incubator-airflow''s branch refs/heads/v1-9-test from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=8dea4a6 ]

[AIRFLOW-1792] Missing intervals DruidOperator

The DruidOperator allows you to template the
intervals field which is
important when you are doing backfills with Druid.
This field was
missing in the constructor and Airflow threw a
warning

Closes #2764 from Fokko/patch-1

(cherry picked from commit 3eb2dd86b9cdb5d83767d5969011a83c6521370d)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Missing attribute DruidOperator
> -------------------------------
>
>                 Key: AIRFLOW-1792
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1792
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>
> The intervals templating attribute is missing for the DruidOperator and throws a warning.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27066,54,JIRA.13116612.1510047458000.184486.1510166401699@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13116612.1510047458000@Atlassian.JIRA,,,2017-11-08 10:40:01-08,"[jira] [Commented] (AIRFLOW-1789) Log SSHOperator stderr to
 log.warning","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1789?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
244506#comment-16244506 ]=20

ASF subversion and git services commented on AIRFLOW-1789:
----------------------------------------------------------

Commit 38593dbd7a663ebb5d192845426386c5dc984f9d in incubator-airflow''s bran=
ch refs/heads/v1-9-stable from [~OpringaoDoTurno]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D385=
93db ]

[AIRFLOW-1789][AIRFLOW-1712] Log SSHOperator stderr to log.warning

Logging functionality for SSHOperator was added in
[AIRFLOW-1712] but it
only logged stdout.
This commit also logs stderr to log.warning

Closes #2761 from OpringaoDoTurno/stderr_in_ssh

(cherry picked from commit 1943a96e708dd68a6990b022ffbbe3729a8c27b8)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Log SSHOperator stderr to log.warning
> -------------------------------------
>
>                 Key: AIRFLOW-1789
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1789
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: logging, operators
>            Reporter: Ignasi Peir=C3=B3
>            Assignee: Ignasi Peir=C3=B3
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> Logging of SSHOperator was added in issue AIRFLOW-1712
> In github [#2686|https://github.com/apache/incubator-airflow/pull/2686] i=
ssue it was asked that stderr should be logged to log.warning. This issue i=
mplements this functionality:
> * stdout is logged to log.info
> * stderr is logged to log.warning



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
730826,24,154579596547.12673.10844619844867714147.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-12-25 19:46:05-08,"[GitHub] pengzhao-intel commented on issue #13088: make ROIAlign support
 position-sensitive pooling","pengzhao-intel commented on issue #13088: make ROIAlign support position-sensitive pooling
URL: https://github.com/apache/incubator-mxnet/pull/13088#issuecomment-449897970
 
 
   Seems the problem of connection. Please try to retrigger the CI by adding a space/blankline in the code.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27067,54,JIRA.13117063.1510166784000.184644.1510166820145@Atlassian.JIRA,1751,Daniel Huang (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-08 10:47:00-08,[jira] [Created] (AIRFLOW-1794) No Exception.message in Python 3,"Daniel Huang created AIRFLOW-1794:
-------------------------------------

             Summary: No Exception.message in Python 3
                 Key: AIRFLOW-1794
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Daniel Huang
            Assignee: Daniel Huang
            Priority: Minor


[~ashb] ran into this

{code}
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
    raise value
  File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
    return self._run_view(f, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
    return fn(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
    .format(task_log_reader, e.message)]
AttributeError: ''AttributeError'' object has no attribute ''message''
{code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27068,54,JIRA.13116566.1510023245000.184662.1510167000440@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13116566.1510023245000@Atlassian.JIRA,,,2017-11-08 10:50:00-08,"[jira] [Updated] (AIRFLOW-1787) Fix batch clear RUNNING task
 instance and inconsistent timestamp format bugs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1787?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1787:
------------------------------------
    Fix Version/s: 1.9.0

> Fix batch clear RUNNING task instance and inconsistent timestamp format bugs
> ----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1787
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1787
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>             Fix For: 1.9.0
>
>
> * Batch clear in CRUD is not working for task instances in RUNNING state, need to be fixed
> * Batch clear and set status are not working for manually triggered task instances because manually triggered task instances have different execution date format.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27069,54,JIRA.13117003.1510154838000.184543.1510166461123@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117003.1510154838000@Atlassian.JIRA,,,2017-11-08 10:41:01-08,[jira] [Commented] (AIRFLOW-1792) Missing attribute DruidOperator,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1792?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16244512#comment-16244512 ] 

ASF subversion and git services commented on AIRFLOW-1792:
----------------------------------------------------------

Commit 3eb2dd86b9cdb5d83767d5969011a83c6521370d in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3eb2dd8 ]

[AIRFLOW-1792] Missing intervals DruidOperator

The DruidOperator allows you to template the
intervals field which is
important when you are doing backfills with Druid.
This field was
missing in the constructor and Airflow threw a
warning

Closes #2764 from Fokko/patch-1


> Missing attribute DruidOperator
> -------------------------------
>
>                 Key: AIRFLOW-1792
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1792
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>
> The intervals templating attribute is missing for the DruidOperator and throws a warning.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27070,54,JIRA.13117003.1510154838000.184553.1510166520319@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117003.1510154838000@Atlassian.JIRA,,,2017-11-08 10:42:00-08,[jira] [Resolved] (AIRFLOW-1792) Missing attribute DruidOperator,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1792?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1792.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2764
[https://github.com/apache/incubator-airflow/pull/2764]

> Missing attribute DruidOperator
> -------------------------------
>
>                 Key: AIRFLOW-1792
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1792
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>             Fix For: 1.9.0
>
>
> The intervals templating attribute is missing for the DruidOperator and throws a warning.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27071,54,JIRA.13117063.1510166784000.184683.1510167300337@Atlassian.JIRA,1751,Daniel Huang (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-08 10:55:00-08,"[jira] [Work started] (AIRFLOW-1794) No Exception.message in Python
 3","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1794?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1794 started by Daniel Huang.
---------------------------------------------
> No Exception.message in Python 3
> --------------------------------
>
>                 Key: AIRFLOW-1794
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> [~ashb] ran into this
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
>     raise value
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
>     .format(task_log_reader, e.message)]
> AttributeError: ''AttributeError'' object has no attribute ''message''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27072,54,JIRA.13099337.1504306043000.186211.1510178700689@Atlassian.JIRA,1751,Daniel Huang (JIRA),JIRA.13099337.1504306043000@Atlassian.JIRA,,,2017-11-08 14:05:00-08,"[jira] [Assigned] (AIRFLOW-1559) MySQL warnings about aborted
 connections, missing engine disposal","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1559?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Daniel Huang reassigned AIRFLOW-1559:
-------------------------------------

    Assignee: Daniel Huang

> MySQL warnings about aborted connections, missing engine disposal
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1559
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1559
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: db
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> We''re seeing a flood of warnings about aborted connections in our MySQL logs. 
> {code}
> Aborted connection 56720 to db: ''airflow'' user: ''foo'' host: ''x.x.x.x'' (Got an error reading communication packets)
> {code}
> It appears this is because we''re not performing [engine disposal|http://docs.sqlalchemy.org/en/latest/core/connections.html#engine-disposal]. The most common source of this warning is from the scheduler, when it kicks off new processes to process the DAG files. Calling dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L403 greatly reduced these messages. However, the worker is still causing some of these, I assume from when we spin up processes to run tasks. We do call dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L1394-L1396, but I think it''s a bit early. Not sure if there''s a place we can put this cleanup to ensure it''s done everywhere.
> Quick script to reproduce this warning message:
> {code}
> from airflow import settings
> from airflow.models import Connection
> session = settings.Session()
> session.query(Connection).count()
> session.close()
> # not calling settings.engine.dispose()
> {code}
> Reproduced with Airflow 1.8.1, MySQL 5.7, and SQLAlchemy 1.1.13. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27073,54,JIRA.13099337.1504306043000.186213.1510178700712@Atlassian.JIRA,1751,Daniel Huang (JIRA),JIRA.13099337.1504306043000@Atlassian.JIRA,,,2017-11-08 14:05:00-08,"[jira] [Work started] (AIRFLOW-1559) MySQL warnings about aborted
 connections, missing engine disposal","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1559?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1559 started by Daniel Huang.
---------------------------------------------
> MySQL warnings about aborted connections, missing engine disposal
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1559
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1559
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: db
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> We''re seeing a flood of warnings about aborted connections in our MySQL logs. 
> {code}
> Aborted connection 56720 to db: ''airflow'' user: ''foo'' host: ''x.x.x.x'' (Got an error reading communication packets)
> {code}
> It appears this is because we''re not performing [engine disposal|http://docs.sqlalchemy.org/en/latest/core/connections.html#engine-disposal]. The most common source of this warning is from the scheduler, when it kicks off new processes to process the DAG files. Calling dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L403 greatly reduced these messages. However, the worker is still causing some of these, I assume from when we spin up processes to run tasks. We do call dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L1394-L1396, but I think it''s a bit early. Not sure if there''s a place we can put this cleanup to ensure it''s done everywhere.
> Quick script to reproduce this warning message:
> {code}
> from airflow import settings
> from airflow.models import Connection
> session = settings.Session()
> session.query(Connection).count()
> session.close()
> # not calling settings.engine.dispose()
> {code}
> Reproduced with Airflow 1.8.1, MySQL 5.7, and SQLAlchemy 1.1.13. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27074,54,JIRA.13099337.1504306043000.186228.1510178760215@Atlassian.JIRA,1751,Daniel Huang (JIRA),JIRA.13099337.1504306043000@Atlassian.JIRA,,,2017-11-08 14:06:00-08,"[jira] [Commented] (AIRFLOW-1559) MySQL warnings about aborted
 connections, missing engine disposal","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1559?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16244805#comment-16244805 ] 

Daniel Huang commented on AIRFLOW-1559:
---------------------------------------

[~jeffreybian] I added engine dispose calls in a few spots, including the ones you mentioned. Please take a look! https://github.com/apache/incubator-airflow/pull/2767

> MySQL warnings about aborted connections, missing engine disposal
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1559
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1559
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: db
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> We''re seeing a flood of warnings about aborted connections in our MySQL logs. 
> {code}
> Aborted connection 56720 to db: ''airflow'' user: ''foo'' host: ''x.x.x.x'' (Got an error reading communication packets)
> {code}
> It appears this is because we''re not performing [engine disposal|http://docs.sqlalchemy.org/en/latest/core/connections.html#engine-disposal]. The most common source of this warning is from the scheduler, when it kicks off new processes to process the DAG files. Calling dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L403 greatly reduced these messages. However, the worker is still causing some of these, I assume from when we spin up processes to run tasks. We do call dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L1394-L1396, but I think it''s a bit early. Not sure if there''s a place we can put this cleanup to ensure it''s done everywhere.
> Quick script to reproduce this warning message:
> {code}
> from airflow import settings
> from airflow.models import Connection
> session = settings.Session()
> session.query(Connection).count()
> session.close()
> # not calling settings.engine.dispose()
> {code}
> Reproduced with Airflow 1.8.1, MySQL 5.7, and SQLAlchemy 1.1.13. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27075,54,JIRA.13117222.1510223865000.194613.1510223880027@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-09 02:38:00-08,"[jira] [Created] (AIRFLOW-1795) S3Hook no longer accepts s3_conn_id
 breaking build in ops/sensors and back-compat","Ash Berlin-Taylor created AIRFLOW-1795:
------------------------------------------

             Summary: S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
                 Key: AIRFLOW-1795
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.9.0
            Reporter: Ash Berlin-Taylor
             Fix For: 1.9.0


Found whilst testing Airflow 1.9.0rc1

Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.

This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:

{code}
    def poke(self, context):
        from airflow.hooks.S3_hook import S3Hook
        hook = S3Hook(s3_conn_id=self.s3_conn_id)
{code}

There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.

My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?

- Rename all instances with deprecation warnings.
- S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
- Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)

{noformat}
airflow/operators/redshift_to_s3_operator.py
33:    :param s3_conn_id: reference to a specific S3 connection
34:    :type s3_conn_id: string
51:            s3_conn_id=''s3_default'',
62:        self.s3_conn_id = s3_conn_id
69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)

airflow/operators/s3_file_transform_operator.py
40:    :param source_s3_conn_id: source s3 connection
41:    :type source_s3_conn_id: str
44:    :param dest_s3_conn_id: destination s3 connection
45:    :type dest_s3_conn_id: str
62:            source_s3_conn_id=''s3_default'',
63:            dest_s3_conn_id=''s3_default'',
68:        self.source_s3_conn_id = source_s3_conn_id
70:        self.dest_s3_conn_id = dest_s3_conn_id
75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)

airflow/operators/s3_to_hive_operator.py
74:    :param s3_conn_id: source s3 connection
75:    :type s3_conn_id: str
102:            s3_conn_id=''s3_default'',
119:        self.s3_conn_id = s3_conn_id
130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)

airflow/operators/sensors.py
504:    :param s3_conn_id: a reference to the s3 connection
505:    :type s3_conn_id: str
514:            s3_conn_id=''s3_default'',
531:        self.s3_conn_id = s3_conn_id
535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
568:            s3_conn_id=''s3_default'',
576:        self.s3_conn_id = s3_conn_id
582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
{noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27076,54,JIRA.13117225.1510224552000.194652.1510224600064@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13117225.1510224552000@Atlassian.JIRA,,,2017-11-09 02:50:00-08,"[jira] [Created] (AIRFLOW-1796) Badly configured  logging config
 can thwo exception in WWW logs view","Ash Berlin-Taylor created AIRFLOW-1796:
------------------------------------------

             Summary: Badly configured  logging config can thwo exception in WWW logs view
                 Key: AIRFLOW-1796
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1796
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Ash Berlin-Taylor


It is possible to specify a custom logging config that changes the {{file.task}} handler to {{s3.task}} but forget to update the {{core.task_log_reader}} config section.

This should be validated at start time, and mentioned in the comments of the default logging config that the config setting needs updating to.

This ends up as the following stack trace/error if you don''t set it properly when trying to view task logs:

{noformat}
File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 712, in log
   logs = handler.read(ti)
AttributeError: ''NoneType'' object has no attribute ''read''
{noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27077,54,JIRA.13117227.1510224990000.194705.1510225020184@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 02:57:00-08,"[jira] [Created] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","Ash Berlin-Taylor created AIRFLOW-1797:
------------------------------------------

             Summary: Cannot write task logs to S3 with Python3
                 Key: AIRFLOW-1797
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Ash Berlin-Taylor


{noformat}
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
    encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
  File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
    client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
  File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
    return future.result()
  File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
    return self._coordinator.result()
  File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
    raise self._exception
  File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
    return self._execute_main(kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
    return_value = self._main(**kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
  File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
    return self._make_api_call(operation_name, kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
    request_signer=self._request_signer, context=request_context)
  File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
    responses = self._emit(event_name, kwargs, stop_on_response=True)
  File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
    response = handler(**kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
    calculate_md5(params, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
    binary_md5 = _calculate_md5_from_file(body)
  File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
    md5.update(chunk)
TypeError: Unicode-objects must be encoded before hashing
{noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27078,54,JIRA.13116697.1510066613000.195296.1510232100093@Atlassian.JIRA,2380,Hugo Prudente (JIRA),JIRA.13116697.1510066613000@Atlassian.JIRA,,,2017-11-09 04:55:00-08,[jira] [Work started] (AIRFLOW-1790) AWS Batch Operator Suppport,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1790?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1790 started by Hugo Prudente.
----------------------------------------------
> AWS Batch Operator Suppport
> ---------------------------
>
>                 Key: AIRFLOW-1790
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1790
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: operators
>    Affects Versions: 1.9.0
>            Reporter: Hugo Prudente
>            Assignee: Hugo Prudente
>
> Add support to AWS Batch operator



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27079,54,JIRA.13116697.1510066613000.195305.1510232220267@Atlassian.JIRA,2380,Hugo Prudente (JIRA),JIRA.13116697.1510066613000@Atlassian.JIRA,,,2017-11-09 04:57:00-08,[jira] [Resolved] (AIRFLOW-1790) AWS Batch Operator Suppport,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1790?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Hugo Prudente resolved AIRFLOW-1790.
------------------------------------
    Resolution: Implemented

Pull request submitted: https://github.com/apache/incubator-airflow/pull/2762

Waiting for review.

> AWS Batch Operator Suppport
> ---------------------------
>
>                 Key: AIRFLOW-1790
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1790
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: operators
>    Affects Versions: 1.9.0
>            Reporter: Hugo Prudente
>            Assignee: Hugo Prudente
>
> Add support to AWS Batch operator



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27080,54,JIRA.13117227.1510224990000.195470.1510233303520@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 05:15:03-08,"[jira] [Updated] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor updated AIRFLOW-1797:
---------------------------------------
    External issue URL: https://github.com/apache/incubator-airflow/pull/2771

> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27081,54,JIRA.13111829.1508886107000.196763.1510242180594@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-11-09 07:43:00-08,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16245860#comment-16245860 ] 

Ash Berlin-Taylor commented on AIRFLOW-1756:
--------------------------------------------

PR to fix underlying issue and expand tests https://github.com/apache/incubator-airflow/pull/2773

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27082,54,JIRA.13022646.1479852557000.197942.1510247760201@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13022646.1479852557000@Atlassian.JIRA,,,2017-11-09 09:16:00-08,[jira] [Commented] (AIRFLOW-646) setup.py install fails,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-646?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246082#comment-16246082 ] 

ASF subversion and git services commented on AIRFLOW-646:
---------------------------------------------------------

Commit 9425d359b0e140095b9a0afca0238e8d1d7590de in incubator-airflow''s branch refs/heads/master from [~wrp]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=9425d35 ]

[AIRFLOW-646] Add docutils to setup_requires

python-daemon declares its docutils dependency in a setup_requires
clause, and ''python setup.py install'' fails since it misses
that dependency.

Closes #2765 from wrp/docutils


> setup.py install fails
> ----------------------
>
>                 Key: AIRFLOW-646
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-646
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.7.1
>         Environment: OS X
> Ubuntu 16.04
> Python 2.7
> Python 3.5
>            Reporter: Nick Allen
>            Assignee: William Pursell
>
> Running `python setup.py install` or listing airflow as a dependency in another setup.py install_requires section results in the following error:
> {quote}
> Running python-daemon-2.1.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-fibs2crb/python-daemon-2.1.2/egg-dist-tmp-hbg5xgc1
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 154, in save_modules
>     yield saved
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 243, in run_setup
>     DirectorySandbox(setup_dir).run(runner)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 273, in run
>     return func()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 242, in runner
>     _execfile(setup_script, ns)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 46, in _execfile
>     exec(code, globals, locals)
>   File ""/tmp/easy_install-fibs2crb/python-daemon-2.1.2/setup.py"", line 43, in <module>
>     sys.exit(errno)
> AttributeError: module ''version'' has no attribute ''ChangelogAwareDistribution''
> During handling of the above exception, another exception occurred:
> Traceback (most recent call last):
>   File ""setup.py"", line 281, in <module>
>     do_setup()
>   File ""setup.py"", line 275, in do_setup
>     ''extra_clean'': CleanCommand,
>   File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
>     dist.run_commands()
>   File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
>     self.run_command(cmd)
>   File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
>     cmd_obj.run()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/install.py"", line 67, in run
>     self.do_egg_install()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/install.py"", line 117, in do_egg_install
>     cmd.run()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 391, in run
>     self.easy_install(spec, not self.no_deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 621, in easy_install
>     return self.install_item(None, spec, tmpdir, deps, True)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 672, in install_item
>     self.process_distribution(spec, dist, deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 717, in process_distribution
>     [requirement], self.local_index, self.easy_install
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 826, in resolve
>     dist = best[req.key] = env.best_match(req, ws, installer)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1092, in best_match
>     return self.obtain(req, installer)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1104, in obtain
>     return installer(requirement)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 640, in easy_install
>     return self.install_item(spec, dist.location, tmpdir, deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 670, in install_item
>     dists = self.install_eggs(spec, download, tmpdir)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 850, in install_eggs
>     return self.build_and_install(setup_script, setup_base)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1078, in build_and_install
>     self.run_setup(setup_script, setup_base, args)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1064, in run_setup
>     run_setup(setup_script, args)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 246, in run_setup
>     raise
>   File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
>     self.gen.throw(type, value, traceback)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
>     self.gen.throw(type, value, traceback)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 166, in save_modules
>     saved_exc.resume()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 141, in resume
>     six.reraise(type, exc, self._tb)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/_vendor/six.py"", line 685, in reraise
>     raise value.with_traceback(tb)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 154, in save_modules
>     yield saved
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 243, in run_setup
>     DirectorySandbox(setup_dir).run(runner)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 273, in run
>     return func()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 242, in runner
>     _execfile(setup_script, ns)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 46, in _execfile
>     exec(code, globals, locals)
>   File ""/tmp/easy_install-fibs2crb/python-daemon-2.1.2/setup.py"", line 43, in <module>
>     sys.exit(errno)
> AttributeError: module ''version'' has no attribute ''ChangelogAwareDistribution''
> {quote}
> Issue appears to be due to setup.py structure of {{python-daemon}} package, working on opening an issue on project''s source page. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27083,54,JIRA.13022646.1479852557000.198085.1510248480430@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13022646.1479852557000@Atlassian.JIRA,,,2017-11-09 09:28:00-08,[jira] [Resolved] (AIRFLOW-646) setup.py install fails,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-646?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-646.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> setup.py install fails
> ----------------------
>
>                 Key: AIRFLOW-646
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-646
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.7.1
>         Environment: OS X
> Ubuntu 16.04
> Python 2.7
> Python 3.5
>            Reporter: Nick Allen
>            Assignee: William Pursell
>             Fix For: 1.10.0
>
>
> Running `python setup.py install` or listing airflow as a dependency in another setup.py install_requires section results in the following error:
> {quote}
> Running python-daemon-2.1.2/setup.py -q bdist_egg --dist-dir /tmp/easy_install-fibs2crb/python-daemon-2.1.2/egg-dist-tmp-hbg5xgc1
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 154, in save_modules
>     yield saved
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 243, in run_setup
>     DirectorySandbox(setup_dir).run(runner)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 273, in run
>     return func()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 242, in runner
>     _execfile(setup_script, ns)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 46, in _execfile
>     exec(code, globals, locals)
>   File ""/tmp/easy_install-fibs2crb/python-daemon-2.1.2/setup.py"", line 43, in <module>
>     sys.exit(errno)
> AttributeError: module ''version'' has no attribute ''ChangelogAwareDistribution''
> During handling of the above exception, another exception occurred:
> Traceback (most recent call last):
>   File ""setup.py"", line 281, in <module>
>     do_setup()
>   File ""setup.py"", line 275, in do_setup
>     ''extra_clean'': CleanCommand,
>   File ""/usr/local/lib/python3.5/distutils/core.py"", line 148, in setup
>     dist.run_commands()
>   File ""/usr/local/lib/python3.5/distutils/dist.py"", line 955, in run_commands
>     self.run_command(cmd)
>   File ""/usr/local/lib/python3.5/distutils/dist.py"", line 974, in run_command
>     cmd_obj.run()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/install.py"", line 67, in run
>     self.do_egg_install()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/install.py"", line 117, in do_egg_install
>     cmd.run()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 391, in run
>     self.easy_install(spec, not self.no_deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 621, in easy_install
>     return self.install_item(None, spec, tmpdir, deps, True)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 672, in install_item
>     self.process_distribution(spec, dist, deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 717, in process_distribution
>     [requirement], self.local_index, self.easy_install
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 826, in resolve
>     dist = best[req.key] = env.best_match(req, ws, installer)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1092, in best_match
>     return self.obtain(req, installer)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/__init__.py"", line 1104, in obtain
>     return installer(requirement)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 640, in easy_install
>     return self.install_item(spec, dist.location, tmpdir, deps)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 670, in install_item
>     dists = self.install_eggs(spec, download, tmpdir)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 850, in install_eggs
>     return self.build_and_install(setup_script, setup_base)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1078, in build_and_install
>     self.run_setup(setup_script, setup_base, args)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/command/easy_install.py"", line 1064, in run_setup
>     run_setup(setup_script, args)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 246, in run_setup
>     raise
>   File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
>     self.gen.throw(type, value, traceback)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/contextlib.py"", line 77, in __exit__
>     self.gen.throw(type, value, traceback)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 166, in save_modules
>     saved_exc.resume()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 141, in resume
>     six.reraise(type, exc, self._tb)
>   File ""/usr/local/lib/python3.5/site-packages/pkg_resources/_vendor/six.py"", line 685, in reraise
>     raise value.with_traceback(tb)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 154, in save_modules
>     yield saved
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 195, in setup_context
>     yield
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 243, in run_setup
>     DirectorySandbox(setup_dir).run(runner)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 273, in run
>     return func()
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 242, in runner
>     _execfile(setup_script, ns)
>   File ""/usr/local/lib/python3.5/site-packages/setuptools/sandbox.py"", line 46, in _execfile
>     exec(code, globals, locals)
>   File ""/tmp/easy_install-fibs2crb/python-daemon-2.1.2/setup.py"", line 43, in <module>
>     sys.exit(errno)
> AttributeError: module ''version'' has no attribute ''ChangelogAwareDistribution''
> {quote}
> Issue appears to be due to setup.py structure of {{python-daemon}} package, working on opening an issue on project''s source page. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27084,54,JIRA.13117378.1510255035000.199690.1510255080238@Atlassian.JIRA,1751,Daniel Huang (JIRA),JIRA.13117378.1510255035000@Atlassian.JIRA,,,2017-11-09 11:18:00-08,"[jira] [Created] (AIRFLOW-1798) Include celery ssl configs in
 default template","Daniel Huang created AIRFLOW-1798:
-------------------------------------

             Summary: Include celery ssl configs in default template
                 Key: AIRFLOW-1798
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1798
             Project: Apache Airflow
          Issue Type: Improvement
          Components: configuration
            Reporter: Daniel Huang
            Assignee: Daniel Huang
            Priority: Trivial






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27085,54,JIRA.13117378.1510255035000.199727.1510255260118@Atlassian.JIRA,1751,Daniel Huang (JIRA),JIRA.13117378.1510255035000@Atlassian.JIRA,,,2017-11-09 11:21:00-08,"[jira] [Work started] (AIRFLOW-1798) Include celery ssl configs in
 default template","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1798?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1798 started by Daniel Huang.
---------------------------------------------
> Include celery ssl configs in default template
> ----------------------------------------------
>
>                 Key: AIRFLOW-1798
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1798
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: configuration
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Trivial
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27086,54,JIRA.13117227.1510224990000.200034.1510256880504@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 11:48:00-08,"[jira] [Commented] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246388#comment-16246388 ] 

ASF subversion and git services commented on AIRFLOW-1797:
----------------------------------------------------------

Commit d592f891e58650472c8fba89bace3cce54a7972b in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d592f89 ]

[AIRFLOW-1797] S3Hook.load_string didn''t work on Python3

With the switch to Boto3 we now need the content
to be bytes, not a
string. On Python2 there is no difference, but for
Python3 this matters.

And since there were no real tests covering the
S3Hook I''ve added some
basic ones.

Closes #2771 from ashb/AIRFLOW-1797

(cherry picked from commit 28411b1e7eddb3338a329db3e52ee09de3676784)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27087,54,JIRA.13117227.1510224990000.200030.1510256880455@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 11:48:00-08,"[jira] [Commented] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246386#comment-16246386 ] 

ASF subversion and git services commented on AIRFLOW-1797:
----------------------------------------------------------

Commit 28411b1e7eddb3338a329db3e52ee09de3676784 in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=28411b1 ]

[AIRFLOW-1797] S3Hook.load_string didn''t work on Python3

With the switch to Boto3 we now need the content
to be bytes, not a
string. On Python2 there is no difference, but for
Python3 this matters.

And since there were no real tests covering the
S3Hook I''ve added some
basic ones.

Closes #2771 from ashb/AIRFLOW-1797


> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27088,54,JIRA.13117227.1510224990000.200044.1510256940459@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 11:49:00-08,"[jira] [Commented] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246392#comment-16246392 ] 

ASF subversion and git services commented on AIRFLOW-1797:
----------------------------------------------------------

Commit 6b7c17d17b664c74d507dc006eb12cd023feb837 in incubator-airflow''s branch refs/heads/v1-9-stable from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6b7c17d ]

[AIRFLOW-1797] S3Hook.load_string didn''t work on Python3

With the switch to Boto3 we now need the content
to be bytes, not a
string. On Python2 there is no difference, but for
Python3 this matters.

And since there were no real tests covering the
S3Hook I''ve added some
basic ones.

Closes #2771 from ashb/AIRFLOW-1797

(cherry picked from commit 28411b1e7eddb3338a329db3e52ee09de3676784)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.1
>
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27089,54,JIRA.13117227.1510224990000.200049.1510257000240@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 11:50:00-08,"[jira] [Updated] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1797:
------------------------------------
    Fix Version/s:     (was: 1.9.1)
                   1.9.0

> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27090,54,JIRA.13117227.1510224990000.200035.1510256880513@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 11:48:00-08,"[jira] [Commented] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246389#comment-16246389 ] 

ASF subversion and git services commented on AIRFLOW-1797:
----------------------------------------------------------

Commit d592f891e58650472c8fba89bace3cce54a7972b in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d592f89 ]

[AIRFLOW-1797] S3Hook.load_string didn''t work on Python3

With the switch to Boto3 we now need the content
to be bytes, not a
string. On Python2 there is no difference, but for
Python3 this matters.

And since there were no real tests covering the
S3Hook I''ve added some
basic ones.

Closes #2771 from ashb/AIRFLOW-1797

(cherry picked from commit 28411b1e7eddb3338a329db3e52ee09de3676784)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27091,54,JIRA.13117227.1510224990000.200048.1510256940494@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 11:49:00-08,"[jira] [Resolved] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1797.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2771
[https://github.com/apache/incubator-airflow/pull/2771]

> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.1
>
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27092,54,JIRA.13117227.1510224990000.200043.1510256940451@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 11:49:00-08,"[jira] [Commented] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246391#comment-16246391 ] 

ASF subversion and git services commented on AIRFLOW-1797:
----------------------------------------------------------

Commit 6b7c17d17b664c74d507dc006eb12cd023feb837 in incubator-airflow''s branch refs/heads/v1-9-stable from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6b7c17d ]

[AIRFLOW-1797] S3Hook.load_string didn''t work on Python3

With the switch to Boto3 we now need the content
to be bytes, not a
string. On Python2 there is no difference, but for
Python3 this matters.

And since there were no real tests covering the
S3Hook I''ve added some
basic ones.

Closes #2771 from ashb/AIRFLOW-1797

(cherry picked from commit 28411b1e7eddb3338a329db3e52ee09de3676784)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.1
>
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27093,54,JIRA.13117227.1510224990000.200028.1510256880434@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117227.1510224990000@Atlassian.JIRA,,,2017-11-09 11:48:00-08,"[jira] [Commented] (AIRFLOW-1797) Cannot write task logs to S3 with
 Python3","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1797?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246385#comment-16246385 ] 

ASF subversion and git services commented on AIRFLOW-1797:
----------------------------------------------------------

Commit 28411b1e7eddb3338a329db3e52ee09de3676784 in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=28411b1 ]

[AIRFLOW-1797] S3Hook.load_string didn''t work on Python3

With the switch to Boto3 we now need the content
to be bytes, not a
string. On Python2 there is no difference, but for
Python3 this matters.

And since there were no real tests covering the
S3Hook I''ve added some
basic ones.

Closes #2771 from ashb/AIRFLOW-1797


> Cannot write task logs to S3 with Python3
> -----------------------------------------
>
>                 Key: AIRFLOW-1797
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1797
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/utils/log/s3_task_handler.py"", line 161, in s3_write
>     encrypt=configuration.getboolean(''core'', ''ENCRYPT_S3_LOGS''),
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/hooks/S3_hook.py"", line 253, in load_string
>     client.upload_fileobj(filelike_buffer, bucket_name, key, ExtraArgs=extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py"", line 431, in upload_fileobj
>     return future.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 73, in result
>     return self._coordinator.result()
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py"", line 233, in result
>     raise self._exception
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 126, in __call__
>     return self._execute_main(kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py"", line 150, in _execute_main
>     return_value = self._main(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/s3transfer/upload.py"", line 679, in _main
>     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 312, in _api_call
>     return self._make_api_call(operation_name, kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/client.py"", line 586, in _make_api_call
>     request_signer=self._request_signer, context=request_context)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 242, in emit_until_response
>     responses = self._emit(event_name, kwargs, stop_on_response=True)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/hooks.py"", line 210, in _emit
>     response = handler(**kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 201, in conditionally_calculate_md5
>     calculate_md5(params, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 179, in calculate_md5
>     binary_md5 = _calculate_md5_from_file(body)
>   File ""/usr/local/lib/python3.5/dist-packages/botocore/handlers.py"", line 193, in _calculate_md5_from_file
>     md5.update(chunk)
> TypeError: Unicode-objects must be encoded before hashing
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27094,54,JIRA.13111829.1508886107000.200679.1510261081517@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-11-09 12:58:01-08,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246498#comment-16246498 ] 

ASF subversion and git services commented on AIRFLOW-1756:
----------------------------------------------------------

Commit 715602ce6a78d773ca85397cf8a0fa85afe42b74 in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=715602c ]

[AIRFLOW-1756] Fix S3TaskHandler to work with Boto3-based S3Hook

The change from boto2 to boto3 in S3Hook caused
this to break (the
return type of `hook.get_key()` changed. There''s a
better method
designed for that we should use anyway.

This wasn''t caught by the tests as the mocks
weren''t updated. Rather
than mocking the return of the hook I have changed
it to use ""moto""
(already in use elsewhere in the tests) to mock at
the S3 layer, not
our hook.

Closes #2773 from ashb/AIRFLOW-1756-s3-logging-
boto3-fix


> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549401,277,JIRA.12986662.1467659713000.53068.1467934270947@Atlassian.JIRA,3026,ASF GitHub Bot (JIRA),JIRA.12986662.1467659713000@Atlassian.JIRA,,,2016-07-07 16:31:10-07,"[jira] [Commented] (IOTA-22) Refactor the Build.scala of the
 Performers so that the jar name of stream and zmq should not contain
 version number","
    [ https://issues.apache.org/jira/browse/IOTA-22?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15366968#comment-15366968 ] 

ASF GitHub Bot commented on IOTA-22:
------------------------------------

Github user barbaragomes commented on the issue:

    https://github.com/apache/incubator-iota/pull/6
  
    Closing #6 
    
    New PR was created to reflect the Build.scala refactoring #8 


> Refactor the Build.scala of the Performers so that the jar name of stream and zmq should not contain version number
> -------------------------------------------------------------------------------------------------------------------
>
>                 Key: IOTA-22
>                 URL: https://issues.apache.org/jira/browse/IOTA-22
>             Project: Iota
>          Issue Type: Improvement
>            Reporter: Shivansh
>            Assignee: Shivansh
>            Priority: Minor
>
> The jar names are being made like fey_stream-assembly-1.0.jar which should be like fey_stream.jar as mentioned in the configuration and the README.md file 



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
27095,54,JIRA.13111829.1508886107000.200734.1510261260664@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-11-09 13:01:00-08,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246509#comment-16246509 ] 

ASF subversion and git services commented on AIRFLOW-1756:
----------------------------------------------------------

Commit 1f417efc8c88ac68f343e6a30ec11b98ea15eced in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1f417ef ]

[AIRFLOW-1756] Fix S3TaskHandler to work with Boto3-based S3Hook

The change from boto2 to boto3 in S3Hook caused
this to break (the
return type of `hook.get_key()` changed. There''s a
better method
designed for that we should use anyway.

This wasn''t caught by the tests as the mocks
weren''t updated. Rather
than mocking the return of the hook I have changed
it to use ""moto""
(already in use elsewhere in the tests) to mock at
the S3 layer, not
our hook.

Closes #2773 from ashb/AIRFLOW-1756-s3-logging-
boto3-fix

(cherry picked from commit 715602ce6a78d773ca85397cf8a0fa85afe42b74)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27096,54,JIRA.13111829.1508886107000.200760.1510261320339@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-11-09 13:02:00-08,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246513#comment-16246513 ] 

ASF subversion and git services commented on AIRFLOW-1756:
----------------------------------------------------------

Commit f7afd5a9858218b839fec10774dfadfd1a478969 in incubator-airflow''s branch refs/heads/v1-9-stable from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f7afd5a ]

[AIRFLOW-1756] Fix S3TaskHandler to work with Boto3-based S3Hook

The change from boto2 to boto3 in S3Hook caused
this to break (the
return type of `hook.get_key()` changed. There''s a
better method
designed for that we should use anyway.

This wasn''t caught by the tests as the mocks
weren''t updated. Rather
than mocking the return of the hook I have changed
it to use ""moto""
(already in use elsewhere in the tests) to mock at
the S3 layer, not
our hook.

Closes #2773 from ashb/AIRFLOW-1756-s3-logging-
boto3-fix

(cherry picked from commit 715602ce6a78d773ca85397cf8a0fa85afe42b74)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27097,54,JIRA.13111829.1508886107000.200733.1510261260656@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-11-09 13:01:00-08,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246508#comment-16246508 ] 

ASF subversion and git services commented on AIRFLOW-1756:
----------------------------------------------------------

Commit 1f417efc8c88ac68f343e6a30ec11b98ea15eced in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1f417ef ]

[AIRFLOW-1756] Fix S3TaskHandler to work with Boto3-based S3Hook

The change from boto2 to boto3 in S3Hook caused
this to break (the
return type of `hook.get_key()` changed. There''s a
better method
designed for that we should use anyway.

This wasn''t caught by the tests as the mocks
weren''t updated. Rather
than mocking the return of the hook I have changed
it to use ""moto""
(already in use elsewhere in the tests) to mock at
the S3 layer, not
our hook.

Closes #2773 from ashb/AIRFLOW-1756-s3-logging-
boto3-fix

(cherry picked from commit 715602ce6a78d773ca85397cf8a0fa85afe42b74)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27098,54,JIRA.13111829.1508886107000.200759.1510261320328@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-11-09 13:02:00-08,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246512#comment-16246512 ] 

ASF subversion and git services commented on AIRFLOW-1756:
----------------------------------------------------------

Commit f7afd5a9858218b839fec10774dfadfd1a478969 in incubator-airflow''s branch refs/heads/v1-9-stable from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f7afd5a ]

[AIRFLOW-1756] Fix S3TaskHandler to work with Boto3-based S3Hook

The change from boto2 to boto3 in S3Hook caused
this to break (the
return type of `hook.get_key()` changed. There''s a
better method
designed for that we should use anyway.

This wasn''t caught by the tests as the mocks
weren''t updated. Rather
than mocking the return of the hook I have changed
it to use ""moto""
(already in use elsewhere in the tests) to mock at
the S3 layer, not
our hook.

Closes #2773 from ashb/AIRFLOW-1756-s3-logging-
boto3-fix

(cherry picked from commit 715602ce6a78d773ca85397cf8a0fa85afe42b74)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706922,24,153194279076.16049.18430682341405014270.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 12:39:50-07,"[GitHub] ifeherva commented on a change in pull request #11643: Added the
 diag() operator","ifeherva commented on a change in pull request #11643: Added the diag() operator
URL: https://github.com/apache/incubator-mxnet/pull/11643#discussion_r203504277
 
 

 ##########
 File path: python/mxnet/ndarray/ndarray.py
 ##########
 @@ -1302,6 +1302,14 @@ def flip(self, *args, **kwargs):
         """"""
         return op.flip(self, *args, **kwargs)
 
+    def diag(self, k=0, **kwargs):
 
 Review comment:
   Good point, added it.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27099,54,JIRA.13111829.1508886107000.200770.1510261320395@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-11-09 13:02:00-08,"[jira] [Resolved] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1756.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2773
[https://github.com/apache/incubator-airflow/pull/2773]

> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27100,54,JIRA.13117028.1510159199000.200847.1510261860432@Atlassian.JIRA,2155,Cedrik Neumann (JIRA),JIRA.13117028.1510159199000@Atlassian.JIRA,,,2017-11-09 13:11:00-08,"[jira] [Assigned] (AIRFLOW-1793) DockerOperator doesn''t work with
 docker_conn_id","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1793?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Cedrik Neumann reassigned AIRFLOW-1793:
---------------------------------------

    Assignee: Cedrik Neumann

> DockerOperator doesn''t work with docker_conn_id
> -----------------------------------------------
>
>                 Key: AIRFLOW-1793
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1793
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Cedrik Neumann
>            Assignee: Cedrik Neumann
>
> The implementation of DockerOperator uses `self.base_url` when loading the DockerHook instead of `self.docker_url`:
> https://github.com/apache/incubator-airflow/blob/v1-9-stable/airflow/operators/docker_operator.py#L150
> {noformat}
> [2017-11-08 16:10:13,082] {base_task_runner.py:98} INFO - Subtask:   File ""/src/apache-airflow/airflow/operators/docker_operator.py"", line 161, in execute
> [2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:     self.cli = self.get_hook().get_conn()
> [2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:   File ""/src/apache-airflow/airflow/operators/docker_operator.py"", line 150, in get_hook
> [2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:     base_url=self.base_url,
> [2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask: AttributeError: ''DockerOperator'' object has no attribute ''base_url''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27101,54,JIRA.13117028.1510159199000.200859.1510261980052@Atlassian.JIRA,2155,Cedrik Neumann (JIRA),JIRA.13117028.1510159199000@Atlassian.JIRA,,,2017-11-09 13:13:00-08,"[jira] [Updated] (AIRFLOW-1793) DockerOperator doesn''t work with
 docker_conn_id","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1793?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Cedrik Neumann updated AIRFLOW-1793:
------------------------------------
    Affects Version/s:     (was: 1.9.0)

> DockerOperator doesn''t work with docker_conn_id
> -----------------------------------------------
>
>                 Key: AIRFLOW-1793
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1793
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Cedrik Neumann
>            Assignee: Cedrik Neumann
>
> The implementation of DockerOperator uses `self.base_url` when loading the DockerHook instead of `self.docker_url`:
> https://github.com/apache/incubator-airflow/blob/v1-9-stable/airflow/operators/docker_operator.py#L150
> {noformat}
> [2017-11-08 16:10:13,082] {base_task_runner.py:98} INFO - Subtask:   File ""/src/apache-airflow/airflow/operators/docker_operator.py"", line 161, in execute
> [2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:     self.cli = self.get_hook().get_conn()
> [2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:   File ""/src/apache-airflow/airflow/operators/docker_operator.py"", line 150, in get_hook
> [2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask:     base_url=self.base_url,
> [2017-11-08 16:10:13,083] {base_task_runner.py:98} INFO - Subtask: AttributeError: ''DockerOperator'' object has no attribute ''base_url''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27102,54,JIRA.13111829.1508886107000.200670.1510261081409@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13111829.1508886107000@Atlassian.JIRA,,,2017-11-09 12:58:01-08,"[jira] [Commented] (AIRFLOW-1756) S3 Task Handler Cannot Read Logs
 With New S3Hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1756?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16246497#comment-16246497 ] 

ASF subversion and git services commented on AIRFLOW-1756:
----------------------------------------------------------

Commit 715602ce6a78d773ca85397cf8a0fa85afe42b74 in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=715602c ]

[AIRFLOW-1756] Fix S3TaskHandler to work with Boto3-based S3Hook

The change from boto2 to boto3 in S3Hook caused
this to break (the
return type of `hook.get_key()` changed. There''s a
better method
designed for that we should use anyway.

This wasn''t caught by the tests as the mocks
weren''t updated. Rather
than mocking the return of the hook I have changed
it to use ""moto""
(already in use elsewhere in the tests) to mock at
the S3 layer, not
our hook.

Closes #2773 from ashb/AIRFLOW-1756-s3-logging-
boto3-fix


> S3 Task Handler Cannot Read Logs With New S3Hook
> ------------------------------------------------
>
>                 Key: AIRFLOW-1756
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1756
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Colin Son
>            Priority: Critical
>             Fix For: 1.9.0
>
>
> With the changes to the S3Hook, it seems like it cannot read the S3 task logs.
> In the `s3_read` in the S3TaskHandler.py:
> {code}
> s3_key = self.hook.get_key(remote_log_location)
> if s3_key:
>     return s3_key.get_contents_as_string().decode()
> {code}
> Since the s3_key object is now a dict, you cannot call `get_contents_as_string()` on a dict object. You have to use the S3Hook''s `read_key()` method to read the contents of the task logs now. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27103,54,JIRA.13117406.1510264735000.201472.1510264800993@Atlassian.JIRA,2381,Ryan (JIRA),JIRA.13117406.1510264735000@Atlassian.JIRA,,,2017-11-09 14:00:00-08,"[jira] [Updated] (AIRFLOW-1799) models.py raising exception with
 TypeError: not all arguments converted during string formatting","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1799?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ryan updated AIRFLOW-1799:
--------------------------
    Description: 
My airflow install has been logging the message
{code:java}
Logged from file models.py, line 2159
Traceback (most recent call last): File ""/usr/lib/python2.7/logging/__init__.py"", line 859, in emit msg = self.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 732, in format return fmt.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 471, in format record.message = record.getMessage() File ""/usr/lib/python2.7/logging/__init__.py"", line 335, in getMessage msg = msg %%%% self.args TypeError: not all arguments converted during string formatting
{code}

This is due to a misuse of formatting syntax

  was:
Logged from file models.py, line 2159
Traceback (most recent call last): File ""/usr/lib/python2.7/logging/__init__.py"", line 859, in emit msg = self.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 732, in format return fmt.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 471, in format record.message = record.getMessage() File ""/usr/lib/python2.7/logging/__init__.py"", line 335, in getMessage msg = msg %%%% self.args TypeError: not all arguments converted during string formatting

This is due to a misuse of formatting syntax


> models.py raising exception with TypeError: not all arguments converted during string formatting
> ------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1799
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1799
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>            Reporter: Ryan
>            Assignee: Ryan
>            Priority: Minor
>
> My airflow install has been logging the message
> {code:java}
> Logged from file models.py, line 2159
> Traceback (most recent call last): File ""/usr/lib/python2.7/logging/__init__.py"", line 859, in emit msg = self.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 732, in format return fmt.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 471, in format record.message = record.getMessage() File ""/usr/lib/python2.7/logging/__init__.py"", line 335, in getMessage msg = msg %%%% self.args TypeError: not all arguments converted during string formatting
> {code}
> This is due to a misuse of formatting syntax



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27104,54,JIRA.13117406.1510264735000.201469.1510264740958@Atlassian.JIRA,2381,Ryan (JIRA),JIRA.13117406.1510264735000@Atlassian.JIRA,,,2017-11-09 13:59:00-08,"[jira] [Created] (AIRFLOW-1799) models.py raising exception with
 TypeError: not all arguments converted during string formatting","Ryan created AIRFLOW-1799:
-----------------------------

             Summary: models.py raising exception with TypeError: not all arguments converted during string formatting
                 Key: AIRFLOW-1799
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1799
             Project: Apache Airflow
          Issue Type: Bug
          Components: operators
            Reporter: Ryan
            Assignee: Ryan
            Priority: Minor


Logged from file models.py, line 2159
Traceback (most recent call last): File ""/usr/lib/python2.7/logging/__init__.py"", line 859, in emit msg = self.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 732, in format return fmt.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 471, in format record.message = record.getMessage() File ""/usr/lib/python2.7/logging/__init__.py"", line 335, in getMessage msg = msg %%%% self.args TypeError: not all arguments converted during string formatting

This is due to a misuse of formatting syntax



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27105,54,JIRA.13111827.1508885615000.202247.1510269780068@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13111827.1508885615000@Atlassian.JIRA,,,2017-11-09 15:23:00-08,"[jira] [Updated] (AIRFLOW-1755) URL Prefix for both Flower and Web
 admin","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1755?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1755:
---------------------------
    Description: 
Similar to AIRFLOW-339.
Should also fix AIRFLOW-964.

I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
{code}
# Root URL to use for the web server
web_server_url_prefix = /airflow
...
# The root URL for Flower
flower_url_prefix = /flower
{code}

This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.

  was:
Similar to AIRFLOW-339.
Should also fix AIRFLOW-964.

I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
{code}
# Root URL to use for the web server
web_server_url_prefix: /flower
...
# The root URL for Flower
flower_url_prefix = /flower
{code}

This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.


> URL Prefix for both Flower and Web admin
> ----------------------------------------
>
>                 Key: AIRFLOW-1755
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1755
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: cli, core
>    Affects Versions: Airflow 2.0
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>
> Similar to AIRFLOW-339.
> Should also fix AIRFLOW-964.
> I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
> {code}
> # Root URL to use for the web server
> web_server_url_prefix = /airflow
> ...
> # The root URL for Flower
> flower_url_prefix = /flower
> {code}
> This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27106,54,JIRA.13057844.1490093306000.202549.1510272240425@Atlassian.JIRA,2069,Alan Ma (JIRA),JIRA.13057844.1490093306000@Atlassian.JIRA,,,2017-11-09 16:04:00-08,"[jira] [Comment Edited] (AIRFLOW-1021) Double logging required for
 new users with LDAP","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1021?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16238542#comment-16238542 ] 

Alan Ma edited comment on AIRFLOW-1021 at 11/10/17 12:03 AM:
-------------------------------------------------------------

Submitted a PR in your place. https://github.com/apache/incubator-airflow/pull/2778


was (Author: ama):
Submitted a PR in your place. https://github.com/apache/incubator-airflow/pull/2751

> Double logging required for new users with LDAP
> -----------------------------------------------
>
>                 Key: AIRFLOW-1021
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1021
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>    Affects Versions: Airflow 1.8
>            Reporter: Marcelo G. Almiron
>            Assignee: Alan Ma
>
> Every user needs to login twice to access Airflow for the first time with LDAP. 
> In the first trial the user is not persistent, so there is no `id` associated, which leads to `None` value returned by `load_user(userid)`, since `userid` is none `None`.
> A quick fix is to add the new user to the session and commit before merging.  That is, in module `airflow/contrib/auth/backends/ldap_auth.py`,  we can change
> ```
> if not user:
>     user = models.User(
>         username=username,
>         is_superuser=False)
> session.merge(user)
> session.commit()
> flask_login.login_user(LdapUser(user))
> ```
> by
> ```
> if not user:
>     user = models.User(
>         username=username,
>         is_superuser=False)
>     session.add(user)
>             
> session.commit()
> session.merge(user)
> flask_login.login_user(LdapUser(user))
> ```



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27107,54,JIRA.13117498.1510298910000.204631.1510298940152@Atlassian.JIRA,2382,Rajshekhar K (JIRA),JIRA.13117498.1510298910000@Atlassian.JIRA,,,2017-11-09 23:29:00-08,"[jira] [Created] (AIRFLOW-1800) no airflow logs visible for upstart
 init - based services","Rajshekhar K created AIRFLOW-1800:
-------------------------------------

             Summary: no airflow logs visible for upstart init - based services
                 Key: AIRFLOW-1800
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1800
             Project: Apache Airflow
          Issue Type: Improvement
          Components: scheduler, webserver, worker
         Environment: the issue affects logs on ubuntu-trusty/RHEL 6.x families - which use upstart init system
            Reporter: Rajshekhar K
            Priority: Minor


When user start the service - airflow-worker/webserver/scheduler it''s expected to see the logs - which are generally printed when airflow components are started manually on the console. The logs are not visible in syslog/messages or in /var/log/upstart/*.log files - no files are generated for them.




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27108,54,JIRA.13063358.1491960900000.204721.1510300200595@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13063358.1491960900000@Atlassian.JIRA,,,2017-11-09 23:50:00-08,"[jira] [Commented] (AIRFLOW-1102) ''airflow webserver'' doesn''t work
 with gunicorn 19.4+","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1102?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247130#comment-16247130 ] 

ASF subversion and git services commented on AIRFLOW-1102:
----------------------------------------------------------

Commit cbb00d4055839d8b0bb3473a9ab3942480a373f5 in incubator-airflow''s branch refs/heads/master from [~bcharous]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=cbb00d4 ]

[AIRFLOW-1102] Upgrade Gunicorn >=19.4.0

Closes #2775 from briancharous/upgrade-gunicorn


> ''airflow webserver'' doesn''t work with gunicorn 19.4+
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1102
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1102
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: dependencies
>            Reporter: Yuu Yamashita
>
> {{airflow webserver}} will show an error of ""Error: ''airflow.www.gunicorn_config'' doesn''t exist"" if it invoked with gunicorn 19.4+, just like the following example.
> {noformat}
> %%%% airflow webserver
> [2017-04-11 05:34:28,945] {__init__.py:57} INFO - Using executor LocalExecutor
>   ____________       _____________
>  ____    |__( )_________  __/__  /________      __
> ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
> ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
>  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
> [2017-04-11 05:34:29,968] [32339] {models.py:167} INFO - Filling up the DagBag from ./dags
> [2017-04-11 05:34:30,427] [32339] {connectionpool.py:735} INFO - Starting new HTTPS connection (1): sts.amazonaws.com
> Running the Gunicorn Server with:
> Workers: 4 sync
> Host: 0.0.0.0:8081
> Timeout: 120
> Logfiles: - -
> =================================================================
> Error: ''airflow.www.gunicorn_config'' doesn''t exist
> {noformat}
> At present, it seems that airflow is configured to require gunicorn prior to 19.4. It should be fixed intrinsically for future upgrade of gunicorn anyways.
> https://github.com/benoitc/gunicorn/blob/19.4/gunicorn/app/base.py#L111-L119
> Starting from {{gunicorn}} 9.4, it seems it started requiring prefix ""python:"" for module name passed to {{-c}}. Because {{airflow webserver}} [is specifying airflow.www.gunicorn_config|https://github.com/apache/incubator-airflow/blob/6b1c327ee886488eedbe8a8721708b89f37d5560/airflow/bin/cli.py#L779] as {{-c}} for {{gunicorn}}, it won''t work with gunicorn 9.4+
> * gunicorn 9.3: https://github.com/benoitc/gunicorn/blob/19.3/gunicorn/app/base.py#L111-L114
> * gunicorn 9.4: https://github.com/benoitc/gunicorn/blob/19.4/gunicorn/app/base.py#L111-L119
> I''ve opened [a PR to gunicorn|https://github.com/benoitc/gunicorn/pull/1499] to restore original behavior prior to 19.3. However, it''d be also better to be fixed in {{airflow webserver}} itself.
> h5. See also
> * https://github.com/benoitc/gunicorn/pull/1068



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27109,54,JIRA.13063358.1491960900000.204733.1510300260786@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13063358.1491960900000@Atlassian.JIRA,,,2017-11-09 23:51:00-08,"[jira] [Commented] (AIRFLOW-1102) ''airflow webserver'' doesn''t work
 with gunicorn 19.4+","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1102?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247131#comment-16247131 ] 

ASF subversion and git services commented on AIRFLOW-1102:
----------------------------------------------------------

Commit 9fd765dc0e547e00039eb2b0fbb1ed418c6abb0a in incubator-airflow''s branch refs/heads/v1-9-test from [~bcharous]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=9fd765d ]

[AIRFLOW-1102] Upgrade Gunicorn >=19.4.0

Closes #2775 from briancharous/upgrade-gunicorn

(cherry picked from commit cbb00d4055839d8b0bb3473a9ab3942480a373f5)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> ''airflow webserver'' doesn''t work with gunicorn 19.4+
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1102
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1102
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: dependencies
>            Reporter: Yuu Yamashita
>             Fix For: 1.9.0
>
>
> {{airflow webserver}} will show an error of ""Error: ''airflow.www.gunicorn_config'' doesn''t exist"" if it invoked with gunicorn 19.4+, just like the following example.
> {noformat}
> %%%% airflow webserver
> [2017-04-11 05:34:28,945] {__init__.py:57} INFO - Using executor LocalExecutor
>   ____________       _____________
>  ____    |__( )_________  __/__  /________      __
> ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
> ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
>  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
> [2017-04-11 05:34:29,968] [32339] {models.py:167} INFO - Filling up the DagBag from ./dags
> [2017-04-11 05:34:30,427] [32339] {connectionpool.py:735} INFO - Starting new HTTPS connection (1): sts.amazonaws.com
> Running the Gunicorn Server with:
> Workers: 4 sync
> Host: 0.0.0.0:8081
> Timeout: 120
> Logfiles: - -
> =================================================================
> Error: ''airflow.www.gunicorn_config'' doesn''t exist
> {noformat}
> At present, it seems that airflow is configured to require gunicorn prior to 19.4. It should be fixed intrinsically for future upgrade of gunicorn anyways.
> https://github.com/benoitc/gunicorn/blob/19.4/gunicorn/app/base.py#L111-L119
> Starting from {{gunicorn}} 9.4, it seems it started requiring prefix ""python:"" for module name passed to {{-c}}. Because {{airflow webserver}} [is specifying airflow.www.gunicorn_config|https://github.com/apache/incubator-airflow/blob/6b1c327ee886488eedbe8a8721708b89f37d5560/airflow/bin/cli.py#L779] as {{-c}} for {{gunicorn}}, it won''t work with gunicorn 9.4+
> * gunicorn 9.3: https://github.com/benoitc/gunicorn/blob/19.3/gunicorn/app/base.py#L111-L114
> * gunicorn 9.4: https://github.com/benoitc/gunicorn/blob/19.4/gunicorn/app/base.py#L111-L119
> I''ve opened [a PR to gunicorn|https://github.com/benoitc/gunicorn/pull/1499] to restore original behavior prior to 19.3. However, it''d be also better to be fixed in {{airflow webserver}} itself.
> h5. See also
> * https://github.com/benoitc/gunicorn/pull/1068



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27110,54,JIRA.13063358.1491960900000.204741.1510300260890@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13063358.1491960900000@Atlassian.JIRA,,,2017-11-09 23:51:00-08,"[jira] [Commented] (AIRFLOW-1102) ''airflow webserver'' doesn''t work
 with gunicorn 19.4+","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1102?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247132#comment-16247132 ] 

ASF subversion and git services commented on AIRFLOW-1102:
----------------------------------------------------------

Commit 40c9c6584058c1fdf78305853a24dc7cf6baaa9c in incubator-airflow''s branch refs/heads/v1-9-stable from [~bcharous]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=40c9c65 ]

[AIRFLOW-1102] Upgrade Gunicorn >=19.4.0

Closes #2775 from briancharous/upgrade-gunicorn

(cherry picked from commit cbb00d4055839d8b0bb3473a9ab3942480a373f5)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> ''airflow webserver'' doesn''t work with gunicorn 19.4+
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1102
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1102
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: dependencies
>            Reporter: Yuu Yamashita
>             Fix For: 1.9.0
>
>
> {{airflow webserver}} will show an error of ""Error: ''airflow.www.gunicorn_config'' doesn''t exist"" if it invoked with gunicorn 19.4+, just like the following example.
> {noformat}
> %%%% airflow webserver
> [2017-04-11 05:34:28,945] {__init__.py:57} INFO - Using executor LocalExecutor
>   ____________       _____________
>  ____    |__( )_________  __/__  /________      __
> ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
> ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
>  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
> [2017-04-11 05:34:29,968] [32339] {models.py:167} INFO - Filling up the DagBag from ./dags
> [2017-04-11 05:34:30,427] [32339] {connectionpool.py:735} INFO - Starting new HTTPS connection (1): sts.amazonaws.com
> Running the Gunicorn Server with:
> Workers: 4 sync
> Host: 0.0.0.0:8081
> Timeout: 120
> Logfiles: - -
> =================================================================
> Error: ''airflow.www.gunicorn_config'' doesn''t exist
> {noformat}
> At present, it seems that airflow is configured to require gunicorn prior to 19.4. It should be fixed intrinsically for future upgrade of gunicorn anyways.
> https://github.com/benoitc/gunicorn/blob/19.4/gunicorn/app/base.py#L111-L119
> Starting from {{gunicorn}} 9.4, it seems it started requiring prefix ""python:"" for module name passed to {{-c}}. Because {{airflow webserver}} [is specifying airflow.www.gunicorn_config|https://github.com/apache/incubator-airflow/blob/6b1c327ee886488eedbe8a8721708b89f37d5560/airflow/bin/cli.py#L779] as {{-c}} for {{gunicorn}}, it won''t work with gunicorn 9.4+
> * gunicorn 9.3: https://github.com/benoitc/gunicorn/blob/19.3/gunicorn/app/base.py#L111-L114
> * gunicorn 9.4: https://github.com/benoitc/gunicorn/blob/19.4/gunicorn/app/base.py#L111-L119
> I''ve opened [a PR to gunicorn|https://github.com/benoitc/gunicorn/pull/1499] to restore original behavior prior to 19.3. However, it''d be also better to be fixed in {{airflow webserver}} itself.
> h5. See also
> * https://github.com/benoitc/gunicorn/pull/1068



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27111,54,JIRA.13063358.1491960900000.204750.1510300261020@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13063358.1491960900000@Atlassian.JIRA,,,2017-11-09 23:51:01-08,"[jira] [Resolved] (AIRFLOW-1102) ''airflow webserver'' doesn''t work
 with gunicorn 19.4+","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1102?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1102.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2775
[https://github.com/apache/incubator-airflow/pull/2775]

> ''airflow webserver'' doesn''t work with gunicorn 19.4+
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1102
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1102
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: dependencies
>            Reporter: Yuu Yamashita
>             Fix For: 1.9.0
>
>
> {{airflow webserver}} will show an error of ""Error: ''airflow.www.gunicorn_config'' doesn''t exist"" if it invoked with gunicorn 19.4+, just like the following example.
> {noformat}
> %%%% airflow webserver
> [2017-04-11 05:34:28,945] {__init__.py:57} INFO - Using executor LocalExecutor
>   ____________       _____________
>  ____    |__( )_________  __/__  /________      __
> ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
> ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
>  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
> [2017-04-11 05:34:29,968] [32339] {models.py:167} INFO - Filling up the DagBag from ./dags
> [2017-04-11 05:34:30,427] [32339] {connectionpool.py:735} INFO - Starting new HTTPS connection (1): sts.amazonaws.com
> Running the Gunicorn Server with:
> Workers: 4 sync
> Host: 0.0.0.0:8081
> Timeout: 120
> Logfiles: - -
> =================================================================
> Error: ''airflow.www.gunicorn_config'' doesn''t exist
> {noformat}
> At present, it seems that airflow is configured to require gunicorn prior to 19.4. It should be fixed intrinsically for future upgrade of gunicorn anyways.
> https://github.com/benoitc/gunicorn/blob/19.4/gunicorn/app/base.py#L111-L119
> Starting from {{gunicorn}} 9.4, it seems it started requiring prefix ""python:"" for module name passed to {{-c}}. Because {{airflow webserver}} [is specifying airflow.www.gunicorn_config|https://github.com/apache/incubator-airflow/blob/6b1c327ee886488eedbe8a8721708b89f37d5560/airflow/bin/cli.py#L779] as {{-c}} for {{gunicorn}}, it won''t work with gunicorn 9.4+
> * gunicorn 9.3: https://github.com/benoitc/gunicorn/blob/19.3/gunicorn/app/base.py#L111-L114
> * gunicorn 9.4: https://github.com/benoitc/gunicorn/blob/19.4/gunicorn/app/base.py#L111-L119
> I''ve opened [a PR to gunicorn|https://github.com/benoitc/gunicorn/pull/1499] to restore original behavior prior to 19.3. However, it''d be also better to be fixed in {{airflow webserver}} itself.
> h5. See also
> * https://github.com/benoitc/gunicorn/pull/1068



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27112,54,JIRA.13117406.1510264735000.204766.1510300380206@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117406.1510264735000@Atlassian.JIRA,,,2017-11-09 23:53:00-08,"[jira] [Commented] (AIRFLOW-1799) models.py raising exception with
 TypeError: not all arguments converted during string formatting","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1799?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247137#comment-16247137 ] 

ASF subversion and git services commented on AIRFLOW-1799:
----------------------------------------------------------

Commit f14f8558be08b8175c0f88f8cf02be3e7583151f in incubator-airflow''s branch refs/heads/master from [~rbuckl]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f14f855 ]

[AIRFLOW-1799] Fix logging line which raises errors

Closes #2777 from ryancbuckley/log-syntax-fix


> models.py raising exception with TypeError: not all arguments converted during string formatting
> ------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1799
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1799
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>            Reporter: Ryan
>            Assignee: Ryan
>            Priority: Minor
>
> My airflow install has been logging the message
> {code:java}
> Logged from file models.py, line 2159
> Traceback (most recent call last): File ""/usr/lib/python2.7/logging/__init__.py"", line 859, in emit msg = self.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 732, in format return fmt.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 471, in format record.message = record.getMessage() File ""/usr/lib/python2.7/logging/__init__.py"", line 335, in getMessage msg = msg %%%% self.args TypeError: not all arguments converted during string formatting
> {code}
> This is due to a misuse of formatting syntax



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27113,54,JIRA.13117406.1510264735000.204768.1510300380264@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117406.1510264735000@Atlassian.JIRA,,,2017-11-09 23:53:00-08,"[jira] [Commented] (AIRFLOW-1799) models.py raising exception with
 TypeError: not all arguments converted during string formatting","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1799?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247138#comment-16247138 ] 

ASF subversion and git services commented on AIRFLOW-1799:
----------------------------------------------------------

Commit a8f3efa9929f17c913c13c4a0e1b02f7c45c4a64 in incubator-airflow''s branch refs/heads/v1-9-test from [~rbuckl]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=a8f3efa ]

[AIRFLOW-1799] Fix logging line which raises errors

Closes #2777 from ryancbuckley/log-syntax-fix

(cherry picked from commit f14f8558be08b8175c0f88f8cf02be3e7583151f)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> models.py raising exception with TypeError: not all arguments converted during string formatting
> ------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1799
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1799
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>            Reporter: Ryan
>            Assignee: Ryan
>            Priority: Minor
>
> My airflow install has been logging the message
> {code:java}
> Logged from file models.py, line 2159
> Traceback (most recent call last): File ""/usr/lib/python2.7/logging/__init__.py"", line 859, in emit msg = self.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 732, in format return fmt.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 471, in format record.message = record.getMessage() File ""/usr/lib/python2.7/logging/__init__.py"", line 335, in getMessage msg = msg %%%% self.args TypeError: not all arguments converted during string formatting
> {code}
> This is due to a misuse of formatting syntax



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27114,54,JIRA.13117406.1510264735000.204773.1510300440696@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117406.1510264735000@Atlassian.JIRA,,,2017-11-09 23:54:00-08,"[jira] [Resolved] (AIRFLOW-1799) models.py raising exception with
 TypeError: not all arguments converted during string formatting","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1799?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1799.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2777
[https://github.com/apache/incubator-airflow/pull/2777]

> models.py raising exception with TypeError: not all arguments converted during string formatting
> ------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1799
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1799
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>            Reporter: Ryan
>            Assignee: Ryan
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> My airflow install has been logging the message
> {code:java}
> Logged from file models.py, line 2159
> Traceback (most recent call last): File ""/usr/lib/python2.7/logging/__init__.py"", line 859, in emit msg = self.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 732, in format return fmt.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 471, in format record.message = record.getMessage() File ""/usr/lib/python2.7/logging/__init__.py"", line 335, in getMessage msg = msg %%%% self.args TypeError: not all arguments converted during string formatting
> {code}
> This is due to a misuse of formatting syntax



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27115,54,JIRA.13117406.1510264735000.204770.1510300440629@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117406.1510264735000@Atlassian.JIRA,,,2017-11-09 23:54:00-08,"[jira] [Commented] (AIRFLOW-1799) models.py raising exception with
 TypeError: not all arguments converted during string formatting","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1799?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247139#comment-16247139 ] 

ASF subversion and git services commented on AIRFLOW-1799:
----------------------------------------------------------

Commit 1f8898e4e958092cee49dee4d1d18fe3e5a11d0c in incubator-airflow''s branch refs/heads/v1-9-stable from [~rbuckl]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1f8898e ]

[AIRFLOW-1799] Fix logging line which raises errors

Closes #2777 from ryancbuckley/log-syntax-fix

(cherry picked from commit f14f8558be08b8175c0f88f8cf02be3e7583151f)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> models.py raising exception with TypeError: not all arguments converted during string formatting
> ------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1799
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1799
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>            Reporter: Ryan
>            Assignee: Ryan
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> My airflow install has been logging the message
> {code:java}
> Logged from file models.py, line 2159
> Traceback (most recent call last): File ""/usr/lib/python2.7/logging/__init__.py"", line 859, in emit msg = self.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 732, in format return fmt.format(record) File ""/usr/lib/python2.7/logging/__init__.py"", line 471, in format record.message = record.getMessage() File ""/usr/lib/python2.7/logging/__init__.py"", line 335, in getMessage msg = msg %%%% self.args TypeError: not all arguments converted during string formatting
> {code}
> This is due to a misuse of formatting syntax



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27116,54,JIRA.13117063.1510166784000.204792.1510300680530@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-09 23:58:00-08,[jira] [Commented] (AIRFLOW-1794) No Exception.message in Python 3,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1794?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247145#comment-16247145 ] 

ASF subversion and git services commented on AIRFLOW-1794:
----------------------------------------------------------

Commit f5f7701258f858698562579cbfb4e1934056e009 in incubator-airflow''s branch refs/heads/master from [~dxhuang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f5f7701 ]

[AIRFLOW-1794] Remove uses of Exception.message for Python 3

Closes #2766 from dhuang/AIRFLOW-1794


> No Exception.message in Python 3
> --------------------------------
>
>                 Key: AIRFLOW-1794
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> [~ashb] ran into this
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
>     raise value
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
>     .format(task_log_reader, e.message)]
> AttributeError: ''AttributeError'' object has no attribute ''message''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27117,54,JIRA.13117063.1510166784000.204801.1510300680955@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-09 23:58:00-08,[jira] [Commented] (AIRFLOW-1794) No Exception.message in Python 3,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1794?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247148#comment-16247148 ] 

ASF subversion and git services commented on AIRFLOW-1794:
----------------------------------------------------------

Commit 3bc0b7d33b92758108c90c2afc411e4ea4358afe in incubator-airflow''s branch refs/heads/v1-9-test from [~dxhuang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3bc0b7d ]

[AIRFLOW-1794] Remove uses of Exception.message for Python 3

Closes #2766 from dhuang/AIRFLOW-1794

(cherry picked from commit f5f7701258f858698562579cbfb4e1934056e009)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> No Exception.message in Python 3
> --------------------------------
>
>                 Key: AIRFLOW-1794
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> [~ashb] ran into this
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
>     raise value
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
>     .format(task_log_reader, e.message)]
> AttributeError: ''AttributeError'' object has no attribute ''message''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27118,54,JIRA.13117063.1510166784000.204799.1510300680927@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-09 23:58:00-08,[jira] [Commented] (AIRFLOW-1794) No Exception.message in Python 3,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1794?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247147#comment-16247147 ] 

ASF subversion and git services commented on AIRFLOW-1794:
----------------------------------------------------------

Commit 3bc0b7d33b92758108c90c2afc411e4ea4358afe in incubator-airflow''s branch refs/heads/v1-9-test from [~dxhuang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3bc0b7d ]

[AIRFLOW-1794] Remove uses of Exception.message for Python 3

Closes #2766 from dhuang/AIRFLOW-1794

(cherry picked from commit f5f7701258f858698562579cbfb4e1934056e009)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> No Exception.message in Python 3
> --------------------------------
>
>                 Key: AIRFLOW-1794
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> [~ashb] ran into this
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
>     raise value
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
>     .format(task_log_reader, e.message)]
> AttributeError: ''AttributeError'' object has no attribute ''message''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27119,54,JIRA.13117063.1510166784000.204795.1510300680836@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-09 23:58:00-08,[jira] [Commented] (AIRFLOW-1794) No Exception.message in Python 3,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1794?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247146#comment-16247146 ] 

ASF subversion and git services commented on AIRFLOW-1794:
----------------------------------------------------------

Commit f5f7701258f858698562579cbfb4e1934056e009 in incubator-airflow''s branch refs/heads/master from [~dxhuang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f5f7701 ]

[AIRFLOW-1794] Remove uses of Exception.message for Python 3

Closes #2766 from dhuang/AIRFLOW-1794


> No Exception.message in Python 3
> --------------------------------
>
>                 Key: AIRFLOW-1794
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> [~ashb] ran into this
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
>     raise value
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
>     .format(task_log_reader, e.message)]
> AttributeError: ''AttributeError'' object has no attribute ''message''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27120,54,JIRA.13117063.1510166784000.204803.1510300740194@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-09 23:59:00-08,[jira] [Commented] (AIRFLOW-1794) No Exception.message in Python 3,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1794?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247149#comment-16247149 ] 

ASF subversion and git services commented on AIRFLOW-1794:
----------------------------------------------------------

Commit e2d49f3c82a55069fb1b9cac49cbc1702a3f260c in incubator-airflow''s branch refs/heads/v1-9-stable from [~dxhuang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=e2d49f3 ]

[AIRFLOW-1794] Remove uses of Exception.message for Python 3

Closes #2766 from dhuang/AIRFLOW-1794

(cherry picked from commit f5f7701258f858698562579cbfb4e1934056e009)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> No Exception.message in Python 3
> --------------------------------
>
>                 Key: AIRFLOW-1794
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> [~ashb] ran into this
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
>     raise value
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
>     .format(task_log_reader, e.message)]
> AttributeError: ''AttributeError'' object has no attribute ''message''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549417,277,JIRA.12988918.1468366670000.19789.1468439360499@Atlassian.JIRA,21272,Barbara Gomes (JIRA),JIRA.12988918.1468366670000@Atlassian.JIRA,,,2016-07-13 12:49:20-07,"[jira] [Comment Edited] (IOTA-25) Support HTTP protocol for dynamic
 population of Fey jar repo","
    [ https://issues.apache.org/jira/browse/IOTA-25?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15375613#comment-15375613 ] 

Barbara Gomes edited comment on IOTA-25 at 7/13/16 7:48 PM:
------------------------------------------------------------

New Configuration:

{code}
// Configuration for downloading performers jar at runtime
// using the JSON Location property
dynamic-jar-population{
  // Directory where Fey will download the jars that the location is
  // specified in the JSON
  downloaded-repository = ${HOME}""/.fey/jars""

  // If enabled, Fey will clean up the jar-downloaded-repository every time
  // it starts, forcing the jars to be downloaded again
  // If false, Fey will only download jars that are not in jar-downloaded-repository
  force-pull = false
}
{code}


was (Author: bmgomes):
New Configuration:

{code}
// Configuration for downloading performers jar at runtime
// using the JSON Location property
dynamic-jar-population{
  // Directory where Fey will download the jars that the location is
  // specified in the JSON
  jar-downloaded-repository = ${HOME}""/.fey/jars""

  // If enabled, Fey will clean up the jar-downloaded-repository every time
  // it starts, forcing the jars to be downloaded again
  // If false, Fey will only download jars that are not in jar-downloaded-repository
  force-pull-jar = false
}
{code}

> Support HTTP protocol for dynamic population of Fey jar repo
> ------------------------------------------------------------
>
>                 Key: IOTA-25
>                 URL: https://issues.apache.org/jira/browse/IOTA-25
>             Project: Iota
>          Issue Type: New Feature
>    Affects Versions: 0.1
>            Reporter: Rutvij Clerk
>            Assignee: Barbara Gomes
>            Priority: Minor
>             Fix For: 0.1
>
>   Original Estimate: 96h
>  Remaining Estimate: 96h
>
> Support HTTP(S) protocol in Fey core to dynamically populate jar repo at runtime. As a further refinement, we should also support HTTP Basic Auth for protected resources.
> We should also have a boolean parameter for e.g. forcePullJar in the JSON to enable the user to pull the jar every time a fey JSON is deployed.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
27121,54,JIRA.13117063.1510166784000.204805.1510300740218@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-09 23:59:00-08,[jira] [Commented] (AIRFLOW-1794) No Exception.message in Python 3,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1794?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247150#comment-16247150 ] 

ASF subversion and git services commented on AIRFLOW-1794:
----------------------------------------------------------

Commit e2d49f3c82a55069fb1b9cac49cbc1702a3f260c in incubator-airflow''s branch refs/heads/v1-9-stable from [~dxhuang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=e2d49f3 ]

[AIRFLOW-1794] Remove uses of Exception.message for Python 3

Closes #2766 from dhuang/AIRFLOW-1794

(cherry picked from commit f5f7701258f858698562579cbfb4e1934056e009)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> No Exception.message in Python 3
> --------------------------------
>
>                 Key: AIRFLOW-1794
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>
> [~ashb] ran into this
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
>     raise value
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
>     .format(task_log_reader, e.message)]
> AttributeError: ''AttributeError'' object has no attribute ''message''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27122,54,JIRA.13117063.1510166784000.204810.1510300803923@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117063.1510166784000@Atlassian.JIRA,,,2017-11-10 00:00:03-08,[jira] [Resolved] (AIRFLOW-1794) No Exception.message in Python 3,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1794?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1794.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2766
[https://github.com/apache/incubator-airflow/pull/2766]

> No Exception.message in Python 3
> --------------------------------
>
>                 Key: AIRFLOW-1794
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1794
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>             Fix For: 1.9.0
>
>
> [~ashb] ran into this
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1988, in wsgi_app
>     response = self.full_dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1641, in full_dispatch_request
>     rv = self.handle_user_exception(e)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1544, in handle_user_exception
>     reraise(exc_type, exc_value, tb)
>   File ""/usr/local/lib/python3.5/dist-packages/flask/_compat.py"", line 33, in reraise
>     raise value
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1639, in full_dispatch_request
>     rv = self.dispatch_request()
>   File ""/usr/local/lib/python3.5/dist-packages/flask/app.py"", line 1625, in dispatch_request
>     return self.view_functions[rule.endpoint](**req.view_args)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 69, in inner
>     return self._run_view(f, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_admin/base.py"", line 368, in _run_view
>     return fn(self, *args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/flask_login.py"", line 758, in decorated_view
>     return func(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/utils.py"", line 262, in wrapper
>     return f(*args, **kwargs)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/www/views.py"", line 715, in log
>     .format(task_log_reader, e.message)]
> AttributeError: ''AttributeError'' object has no attribute ''message''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27123,54,JIRA.13099790.1504605491000.204951.1510301520735@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13099790.1504605491000@Atlassian.JIRA,,,2017-11-10 00:12:00-08,"[jira] [Commented] (AIRFLOW-1563) OSError while attempting to
 symlink latest log folder","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1563?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247168#comment-16247168 ] 

ASF subversion and git services commented on AIRFLOW-1563:
----------------------------------------------------------

Commit faa9a5266c0b2e68693dd106b5cb46d30770dadc in incubator-airflow''s branch refs/heads/master from [~nzeilemaker]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=faa9a52 ]

[AIRFLOW-1563] Catch OSError while symlinking the latest log directory

Closes #2564 from NielsZeilemaker/AIRFLOW-1563


> OSError while attempting to symlink latest log folder
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1563
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1563
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli
>    Affects Versions: Airflow 1.8
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>
> We''re getting an OSError due a azure fileshare volume mount being used for the logs
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 882, in scheduler
>     job.run()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 200, in run
>     self._execute()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 1312, in _execute
>     self._execute_helper(processor_manager)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 1409, in _execute_helper
>     simple_dags = processor_manager.heartbeat()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/dag_processing.py"", line 631, in heartbeat
>     self.symlink_latest_log_directory()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/dag_processing.py"", line 525, in symlink_latest_log_directory
>     os.symlink(log_directory, latest_log_directory_path)
> OSError: [Errno 95] Operation not supported



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27124,54,JIRA.13099790.1504605491000.204956.1510301520762@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13099790.1504605491000@Atlassian.JIRA,,,2017-11-10 00:12:00-08,"[jira] [Commented] (AIRFLOW-1563) OSError while attempting to
 symlink latest log folder","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1563?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247170#comment-16247170 ] 

ASF subversion and git services commented on AIRFLOW-1563:
----------------------------------------------------------

Commit faa9a5266c0b2e68693dd106b5cb46d30770dadc in incubator-airflow''s branch refs/heads/master from [~nzeilemaker]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=faa9a52 ]

[AIRFLOW-1563] Catch OSError while symlinking the latest log directory

Closes #2564 from NielsZeilemaker/AIRFLOW-1563


> OSError while attempting to symlink latest log folder
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1563
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1563
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli
>    Affects Versions: Airflow 1.8
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>
> We''re getting an OSError due a azure fileshare volume mount being used for the logs
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 882, in scheduler
>     job.run()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 200, in run
>     self._execute()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 1312, in _execute
>     self._execute_helper(processor_manager)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 1409, in _execute_helper
>     simple_dags = processor_manager.heartbeat()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/dag_processing.py"", line 631, in heartbeat
>     self.symlink_latest_log_directory()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/dag_processing.py"", line 525, in symlink_latest_log_directory
>     os.symlink(log_directory, latest_log_directory_path)
> OSError: [Errno 95] Operation not supported



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27125,54,JIRA.13117498.1510298910000.205233.1510303620907@Atlassian.JIRA,2382,Rajshekhar K (JIRA),JIRA.13117498.1510298910000@Atlassian.JIRA,,,2017-11-10 00:47:00-08,"[jira] [Work started] (AIRFLOW-1800) no airflow logs visible for
 upstart init - based services","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1800?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1800 started by Rajshekhar K.
---------------------------------------------
> no airflow logs visible for upstart init - based services
> ---------------------------------------------------------
>
>                 Key: AIRFLOW-1800
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1800
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: scheduler, webserver, worker
>         Environment: the issue affects logs on ubuntu-trusty/RHEL 6.x families - which use upstart init system
>            Reporter: Rajshekhar K
>            Assignee: Rajshekhar K
>            Priority: Minor
>              Labels: easyfix
>
> When user start the service - airflow-worker/webserver/scheduler it''s expected to see the logs - which are generally printed when airflow components are started manually on the console. The logs are not visible in syslog/messages or in /var/log/upstart/*.log files - no files are generated for them.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549418,277,JIRA.12988918.1468366670000.33278.1468525641056@Atlassian.JIRA,3026,ASF GitHub Bot (JIRA),JIRA.12988918.1468366670000@Atlassian.JIRA,,,2016-07-14 12:47:21-07,"[jira] [Commented] (IOTA-25) Support HTTP protocol for dynamic
 population of Fey jar repo","
    [ https://issues.apache.org/jira/browse/IOTA-25?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15378234#comment-15378234 ] 

ASF GitHub Bot commented on IOTA-25:
------------------------------------

GitHub user barbaragomes opened a pull request:

    https://github.com/apache/incubator-iota/pull/13

    [IOTA-25] Support HTTP protocol for dynamic population of Fey jar repo

    https://issues.apache.org/jira/browse/IOTA-25

You can merge this pull request into a Git repository by running:

    $ git pull https://github.com/barbaragomes/incubator-iota IOTA-25

Alternatively you can review and apply these changes as the patch at:

    https://github.com/apache/incubator-iota/pull/13.patch

To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:

    This closes #13
    
----
commit 606fbd2890765017c64f33fb08f025a8b9081eeb
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-13T17:43:05Z

    Aggregate in Build.scala should point to the Projects not the IDs

commit a9698b34f2b3ddf30da538bfb99dfeffbb231c16
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-13T19:52:09Z

    Adding configuration for supporting dynamic jar download

commit a2cc5b601967328e3118fd8190af4b6e9bbadd45
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-13T19:52:50Z

    Adding new JSON property to specify Jar location

commit 6fbba6351663c3a65d69eaae1251226cbcbc7b2f
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-14T01:06:27Z

    Dynamically download jars
    
    - Restructure how to receive a JSON
    - Json analysis will be done when a new JSON is received, before Fey actually process it
    
    Affected:
    
    - Fey checkpoint
    - Fey Json receiver
    - Fey Json process

commit 47dac9df4928fcefbef69311bb171798be21cc08
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-14T16:43:11Z

    Location should not include jarName

commit cca9d8b35c09dcc9c3085fb4ba1403123bacdf1b
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-14T16:59:30Z

    processInitialFiles should not be called inside the run() loop

commit 93c19a34bb8cdbaada28c2eafc6acc8174141103
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-14T18:53:04Z

    Fixing Basic Auth
    
    - Added Credentials property to JSON Spec
    - Trying to resolve credentials first looking to the environment vars

commit a12d654ffefdddb5fbad50c052e9002ecf46aeb4
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-14T19:42:13Z

    Updating README to point out the changes

commit ae1e9461fd15519f1c434ce0e9d68e985ed65a96
Author: Barbara Gomes <barbaramaltagomes@gmail.com>
Date:   2016-07-14T19:45:47Z

    Fixing README.md section anchors

----


> Support HTTP protocol for dynamic population of Fey jar repo
> ------------------------------------------------------------
>
>                 Key: IOTA-25
>                 URL: https://issues.apache.org/jira/browse/IOTA-25
>             Project: Iota
>          Issue Type: New Feature
>    Affects Versions: 0.1
>            Reporter: Rutvij Clerk
>            Assignee: Barbara Gomes
>            Priority: Minor
>             Fix For: 0.1
>
>   Original Estimate: 96h
>  Remaining Estimate: 96h
>
> Support HTTP(S) protocol in Fey core to dynamically populate jar repo at runtime. As a further refinement, we should also support HTTP Basic Auth for protected resources.
> We should also have a boolean parameter for e.g. forcePullJar in the JSON to enable the user to pull the jar every time a fey JSON is deployed.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
27126,54,JIRA.13117498.1510298910000.205238.1510303680174@Atlassian.JIRA,2382,Rajshekhar K (JIRA),JIRA.13117498.1510298910000@Atlassian.JIRA,,,2017-11-10 00:48:00-08,"[jira] [Updated] (AIRFLOW-1800) no airflow logs visible for upstart
 init - based services","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1800?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Rajshekhar K updated AIRFLOW-1800:
----------------------------------
    Affects Version/s: Airflow 2.0

> no airflow logs visible for upstart init - based services
> ---------------------------------------------------------
>
>                 Key: AIRFLOW-1800
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1800
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: scheduler, webserver, worker
>    Affects Versions: Airflow 2.0
>         Environment: the issue affects logs on ubuntu-trusty/RHEL 6.x families - which use upstart init system
>            Reporter: Rajshekhar K
>            Assignee: Rajshekhar K
>            Priority: Minor
>              Labels: easyfix
>
> When user start the service - airflow-worker/webserver/scheduler it''s expected to see the logs - which are generally printed when airflow components are started manually on the console. The logs are not visible in syslog/messages or in /var/log/upstart/*.log files - no files are generated for them.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27127,54,JIRA.13117498.1510298910000.205243.1510303800056@Atlassian.JIRA,2382,Rajshekhar K (JIRA),JIRA.13117498.1510298910000@Atlassian.JIRA,,,2017-11-10 00:50:00-08,"[jira] [Work stopped] (AIRFLOW-1800) no airflow logs visible for
 upstart init - based services","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1800?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1800 stopped by Rajshekhar K.
---------------------------------------------
> no airflow logs visible for upstart init - based services
> ---------------------------------------------------------
>
>                 Key: AIRFLOW-1800
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1800
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: scheduler, webserver, worker
>    Affects Versions: Airflow 2.0
>         Environment: the issue affects logs on ubuntu-trusty/RHEL 6.x families - which use upstart init system
>            Reporter: Rajshekhar K
>            Assignee: Rajshekhar K
>            Priority: Minor
>              Labels: easyfix
>
> When user start the service - airflow-worker/webserver/scheduler it''s expected to see the logs - which are generally printed when airflow components are started manually on the console. The logs are not visible in syslog/messages or in /var/log/upstart/*.log files - no files are generated for them.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27128,54,JIRA.13117498.1510298910000.205225.1510303500847@Atlassian.JIRA,2382,Rajshekhar K (JIRA),JIRA.13117498.1510298910000@Atlassian.JIRA,,,2017-11-10 00:45:00-08,"[jira] [Assigned] (AIRFLOW-1800) no airflow logs visible for
 upstart init - based services","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1800?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Rajshekhar K reassigned AIRFLOW-1800:
-------------------------------------

    Assignee: Rajshekhar K

> no airflow logs visible for upstart init - based services
> ---------------------------------------------------------
>
>                 Key: AIRFLOW-1800
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1800
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: scheduler, webserver, worker
>         Environment: the issue affects logs on ubuntu-trusty/RHEL 6.x families - which use upstart init system
>            Reporter: Rajshekhar K
>            Assignee: Rajshekhar K
>            Priority: Minor
>              Labels: easyfix
>
> When user start the service - airflow-worker/webserver/scheduler it''s expected to see the logs - which are generally printed when airflow components are started manually on the console. The logs are not visible in syslog/messages or in /var/log/upstart/*.log files - no files are generated for them.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27129,54,JIRA.13117498.1510298910000.205232.1510303620167@Atlassian.JIRA,2382,Rajshekhar K (JIRA),JIRA.13117498.1510298910000@Atlassian.JIRA,,,2017-11-10 00:47:00-08,"[jira] [Commented] (AIRFLOW-1800) no airflow logs visible for
 upstart init - based services","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1800?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16247209#comment-16247209 ] 

Rajshekhar K commented on AIRFLOW-1800:
---------------------------------------

adding 
{code:java}
console output
{code}
in the 
{noformat}
/etc/init/airflow-*.conf
{noformat}
 starts showing logs in 
{noformat}
/var/log/upstart/airflow-*.log
{noformat}
 files

> no airflow logs visible for upstart init - based services
> ---------------------------------------------------------
>
>                 Key: AIRFLOW-1800
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1800
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: scheduler, webserver, worker
>         Environment: the issue affects logs on ubuntu-trusty/RHEL 6.x families - which use upstart init system
>            Reporter: Rajshekhar K
>            Assignee: Rajshekhar K
>            Priority: Minor
>              Labels: easyfix
>
> When user start the service - airflow-worker/webserver/scheduler it''s expected to see the logs - which are generally printed when airflow components are started manually on the console. The logs are not visible in syslog/messages or in /var/log/upstart/*.log files - no files are generated for them.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27130,54,JIRA.13109009.1507832929000.209493.1510342980655@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13109009.1507832929000@Atlassian.JIRA,,,2017-11-10 11:43:00-08,"[jira] [Assigned] (AIRFLOW-1710) Add timezone setting to Airflow
 DAG","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1710?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin reassigned AIRFLOW-1710:
---------------------------------------

      Assignee: Bolke de Bruin
    Issue Type: Task  (was: Bug)

> Add timezone setting to Airflow DAG
> -----------------------------------
>
>                 Key: AIRFLOW-1710
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1710
>             Project: Apache Airflow
>          Issue Type: Task
>          Components: DAG
>    Affects Versions: 1.9.0
>            Reporter: Chris Riccomini
>            Assignee: Bolke de Bruin
>
> We have some use cases where we''d like to run DAGs pegged to a specific timezone.
> Customers want things to happen in their time zone. If they have DST, it''s not a constant offset from UTC. If they aren''t in the US, their DST isn''t a constant offset from California''s DST (where we run). GB, for example.
> One way to solve this would be to have the DAG start with PythonOperator task that calculates the difference between UTC and the expected timezone, and sleeps for that amount of time.
> Another (cleaner?) way would be to add a field to the DAG model that allows the DAG author to specify the timezone that the DAG should be scheduled in. For example, we could have our Airflow box continue to run on UTC, but schedule a specific DAG for Pacific/US, which would adjust according to daylight savings time. We could schedule other DAGs to run on GB DST, etc.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27131,54,JIRA.13109009.1507832929000.209496.1510343040097@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13109009.1507832929000@Atlassian.JIRA,,,2017-11-10 11:44:00-08,[jira] [Updated] (AIRFLOW-1710) Add timezone setting to Airflow DAG,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1710?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin updated AIRFLOW-1710:
------------------------------------
    Issue Type: Sub-task  (was: Task)
        Parent: AIRFLOW-288

> Add timezone setting to Airflow DAG
> -----------------------------------
>
>                 Key: AIRFLOW-1710
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1710
>             Project: Apache Airflow
>          Issue Type: Sub-task
>          Components: DAG
>    Affects Versions: 1.9.0
>            Reporter: Chris Riccomini
>            Assignee: Bolke de Bruin
>
> We have some use cases where we''d like to run DAGs pegged to a specific timezone.
> Customers want things to happen in their time zone. If they have DST, it''s not a constant offset from UTC. If they aren''t in the US, their DST isn''t a constant offset from California''s DST (where we run). GB, for example.
> One way to solve this would be to have the DAG start with PythonOperator task that calculates the difference between UTC and the expected timezone, and sleeps for that amount of time.
> Another (cleaner?) way would be to add a field to the DAG model that allows the DAG author to specify the timezone that the DAG should be scheduled in. For example, we could have our Airflow box continue to run on UTC, but schedule a specific DAG for Pacific/US, which would adjust according to daylight savings time. We could schedule other DAGs to run on GB DST, etc.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27132,54,JIRA.13117658.1510343120000.209509.1510343160115@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117658.1510343120000@Atlassian.JIRA,,,2017-11-10 11:46:00-08,"[jira] [Created] (AIRFLOW-1801) Url encode all execution dates in
 the UI","Bolke de Bruin created AIRFLOW-1801:
---------------------------------------

             Summary: Url encode all execution dates in the UI
                 Key: AIRFLOW-1801
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1801
             Project: Apache Airflow
          Issue Type: Sub-task
          Components: ui
    Affects Versions: 1.9.0
            Reporter: Bolke de Bruin


Execution dates are not properly url encoded therefore losing timezone information



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27133,54,JIRA.13117659.1510343499000.209549.1510343520212@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117659.1510343499000@Atlassian.JIRA,,,2017-11-10 11:52:00-08,"[jira] [Created] (AIRFLOW-1802) Convert database datetime fields to
 time zone aware fields","Bolke de Bruin created AIRFLOW-1802:
---------------------------------------

             Summary: Convert database datetime fields to time zone aware fields
                 Key: AIRFLOW-1802
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1802
             Project: Apache Airflow
          Issue Type: Sub-task
    Affects Versions: 1.9.0
            Reporter: Bolke de Bruin
            Assignee: Bolke de Bruin


DateTime fields are currently naive in the databases. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27134,54,JIRA.13117662.1510343683000.209598.1510343700429@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117662.1510343683000@Atlassian.JIRA,,,2017-11-10 11:55:00-08,"[jira] [Created] (AIRFLOW-1803) Add documentation on time zone
 usage","Bolke de Bruin created AIRFLOW-1803:
---------------------------------------

             Summary: Add documentation on time zone usage
                 Key: AIRFLOW-1803
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1803
             Project: Apache Airflow
          Issue Type: Sub-task
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27135,54,JIRA.13117663.1510343855000.209609.1510343880063@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117663.1510343855000@Atlassian.JIRA,,,2017-11-10 11:58:00-08,"[jira] [Created] (AIRFLOW-1804) Add airflow.cfg option for time
 zone configuration","Bolke de Bruin created AIRFLOW-1804:
---------------------------------------

             Summary: Add airflow.cfg option for time zone configuration
                 Key: AIRFLOW-1804
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1804
             Project: Apache Airflow
          Issue Type: Sub-task
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27136,54,JIRA.13117704.1510361841000.210973.1510361880076@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13117704.1510361841000@Atlassian.JIRA,,,2017-11-10 16:58:00-08,"[jira] [Created] (AIRFLOW-1805) Allow to supply Slack token through
 connection","Kevin Yang created AIRFLOW-1805:
-----------------------------------

             Summary: Allow to supply Slack token through connection
                 Key: AIRFLOW-1805
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1805
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Kevin Yang


To prevent passing in Slack token directly in plain text, it is safer to pass in the token as ''password'' through connection.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27137,54,JIRA.13117723.1510396492000.212289.1510396500095@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117723.1510396492000@Atlassian.JIRA,,,2017-11-11 02:35:00-08,[jira] [Created] (AIRFLOW-1806) Make scheduler aware of timezones,"Bolke de Bruin created AIRFLOW-1806:
---------------------------------------

             Summary: Make scheduler aware of timezones
                 Key: AIRFLOW-1806
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1806
             Project: Apache Airflow
          Issue Type: Sub-task
            Reporter: Bolke de Bruin


When supplied with time zone aware date times 

dag.following_schedule and dag.previous_schedule should do arithmetic in local time in order to take care of day light savings time



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27138,54,JIRA.13117724.1510396623000.212292.1510396680027@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117724.1510396623000@Atlassian.JIRA,,,2017-11-11 02:38:00-08,"[jira] [Created] (AIRFLOW-1807) Make sure all date time fields
 store in UTC and are always time zone aware","Bolke de Bruin created AIRFLOW-1807:
---------------------------------------

             Summary: Make sure all date time fields store in UTC and are always time zone aware
                 Key: AIRFLOW-1807
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1807
             Project: Apache Airflow
          Issue Type: Sub-task
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27139,54,JIRA.13117729.1510401682000.212718.1510401720074@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117729.1510401682000@Atlassian.JIRA,,,2017-11-11 04:02:00-08,"[jira] [Created] (AIRFLOW-1808) Convert all utcnow() to timezone
 aware utcnow()","Bolke de Bruin created AIRFLOW-1808:
---------------------------------------

             Summary: Convert all utcnow() to timezone aware utcnow()
                 Key: AIRFLOW-1808
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1808
             Project: Apache Airflow
          Issue Type: Sub-task
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27140,54,JIRA.13117730.1510401757000.212721.1510401780179@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117730.1510401757000@Atlassian.JIRA,,,2017-11-11 04:03:00-08,[jira] [Created] (AIRFLOW-1809) Fix all tests that use dates,"Bolke de Bruin created AIRFLOW-1809:
---------------------------------------

             Summary: Fix all tests that use dates
                 Key: AIRFLOW-1809
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1809
             Project: Apache Airflow
          Issue Type: Sub-task
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27141,54,JIRA.13117734.1510403686000.212783.1510403700366@Atlassian.JIRA,2323,Sanjay Pillai (JIRA),JIRA.13117734.1510403686000@Atlassian.JIRA,,,2017-11-11 04:35:00-08,"[jira] [Created] (AIRFLOW-1810) remove unused mysql import in
 migrations","Sanjay Pillai created AIRFLOW-1810:
--------------------------------------

             Summary: remove unused mysql import in migrations
                 Key: AIRFLOW-1810
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1810
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Sanjay Pillai
            Assignee: Sanjay Pillai
            Priority: Minor






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27142,54,JIRA.13117734.1510403686000.212800.1510403880929@Atlassian.JIRA,2323,Sanjay Pillai (JIRA),JIRA.13117734.1510403686000@Atlassian.JIRA,,,2017-11-11 04:38:00-08,"[jira] [Updated] (AIRFLOW-1810) remove unused mysql import in
 migrations","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1810?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sanjay Pillai updated AIRFLOW-1810:
-----------------------------------
    Description: sqlalachemy.dialects.mysql is imported in few migraitons, but not used. 

> remove unused mysql import in migrations
> ----------------------------------------
>
>                 Key: AIRFLOW-1810
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1810
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Sanjay Pillai
>            Assignee: Sanjay Pillai
>            Priority: Minor
>
> sqlalachemy.dialects.mysql is imported in few migraitons, but not used. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27143,54,JIRA.13117734.1510403686000.212871.1510405200656@Atlassian.JIRA,2323,Sanjay Pillai (JIRA),JIRA.13117734.1510403686000@Atlassian.JIRA,,,2017-11-11 05:00:00-08,"[jira] [Updated] (AIRFLOW-1810) Remove unused mysql import in
 migrations","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1810?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sanjay Pillai updated AIRFLOW-1810:
-----------------------------------
    Summary: Remove unused mysql import in migrations  (was: remove unused mysql import in migrations)

> Remove unused mysql import in migrations
> ----------------------------------------
>
>                 Key: AIRFLOW-1810
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1810
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Sanjay Pillai
>            Assignee: Sanjay Pillai
>            Priority: Minor
>
> sqlalachemy.dialects.mysql is imported in few migraitons, but not used. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27144,54,JIRA.13109553.1508100330000.212872.1510405202278@Atlassian.JIRA,2323,Sanjay Pillai (JIRA),JIRA.13109553.1508100330000@Atlassian.JIRA,,,2017-11-11 05:00:02-08,"[jira] [Closed] (AIRFLOW-1716) Fix multiple __init__ definition in
 SimpleDag","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1716?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sanjay Pillai closed AIRFLOW-1716.
----------------------------------
    Resolution: Fixed

> Fix multiple __init__ definition in SimpleDag
> ---------------------------------------------
>
>                 Key: AIRFLOW-1716
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1716
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG
>    Affects Versions: 1.9.0
>            Reporter: Sanjay Pillai
>            Assignee: Sanjay Pillai
>
> In airflow.utitls.dag_processing.SimpleDag  __init__ definition is repeated. 
> probably a result of incorrect merge conflict resolution. 
> removed the previous __init__ def and updated doc string. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27145,54,JIRA.13117757.1510437189000.214121.1510437240064@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13117757.1510437189000@Atlassian.JIRA,,,2017-11-11 13:54:00-08,[jira] [Created] (AIRFLOW-1811) Fix rendered Druid Operator,"Fokko Driesprong created AIRFLOW-1811:
-----------------------------------------

             Summary: Fix rendered Druid Operator
                 Key: AIRFLOW-1811
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1811
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Fokko Driesprong


Hi all,

I''ve learned how the rendered field works in Airflow and how it can be used to visualise the job. I''ve did some small adjustments to the Druid operator to enable this.

Cheers, Fokko






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27146,54,JIRA.13117777.1510477812000.214887.1510477860040@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13117777.1510477812000@Atlassian.JIRA,,,2017-11-12 01:11:00-08,"[jira] [Created] (AIRFLOW-1812) Update Logging config example in
 Updating.md","Fokko Driesprong created AIRFLOW-1812:
-----------------------------------------

             Summary: Update Logging config example in Updating.md
                 Key: AIRFLOW-1812
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1812
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Fokko Driesprong






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27147,54,JIRA.12984446.1467137330000.215388.1510498920553@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.12984446.1467137330000@Atlassian.JIRA,,,2017-11-12 07:02:00-08,[jira] [Commented] (AIRFLOW-288) Make system timezone configurable,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-288?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16248887#comment-16248887 ] 

ASF subversion and git services commented on AIRFLOW-288:
---------------------------------------------------------

Commit 9ec7f0f04bcec67fa62fbd85085acaa5e3b298b7 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=9ec7f0f ]

[AIRFLOW-1801][AIRFLOW-288] Url encode execution dates

Execution dates can contain special characters
that
need to be url encoded. In case of timezone
information
this information is lost if not url encoded.

Closes #2779 from bolkedebruin/AIRFLOW-1801


> Make system timezone configurable
> ---------------------------------
>
>                 Key: AIRFLOW-288
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-288
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Vineet Goel
>            Assignee: Vineet Goel
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706923,24,153194412341.7243.4865527962722632337.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 13:02:03-07,"[GitHub] szha closed pull request #11803: Use feature detection instead of
 version detection","szha closed pull request #11803: Use feature detection instead of version detection
URL: https://github.com/apache/incubator-mxnet/pull/11803
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won''t show otherwise due to GitHub magic):

diff --git a/python/mxnet/base.py b/python/mxnet/base.py
index 0fb73b3c7dd..4df794bdfe3 100644
--- a/python/mxnet/base.py
+++ b/python/mxnet/base.py
@@ -20,44 +20,59 @@
 """"""ctypes library of mxnet and helper functions.""""""
 from __future__ import absolute_import
 
+import atexit
+import ctypes
+import inspect
 import os
 import sys
-import ctypes
-import atexit
 import warnings
-import inspect
+
 import numpy as np
+
 from . import libinfo
+
 warnings.filterwarnings(''default'', category=DeprecationWarning)
 
 __all__ = [''MXNetError'']
 #----------------------------
 # library loading
 #----------------------------
-if sys.version_info[0] == 3:
-    string_types = str,
-    numeric_types = (float, int, np.generic)
-    integer_types = (int, np.int32, np.int64)
+
+# pylint: disable=pointless-statement
+try:
+    basestring
+    long
+except NameError:
+    basestring = str
+    long = int
+# pylint: enable=pointless-statement
+
+integer_types = (int, long, np.int32, np.int64)
+numeric_types = (float, int, long, np.generic)
+string_types = basestring,
+
+if sys.version_info[0] > 2:
     # this function is needed for python3
     # to convert ctypes.char_p .value back to python str
     py_str = lambda x: x.decode(''utf-8'')
 else:
-    string_types = basestring,
-    numeric_types = (float, int, long, np.generic)
-    integer_types = (int, long, np.int32, np.int64)
     py_str = lambda x: x
 
+
 class _NullType(object):
     """"""Placeholder for arguments""""""
     def __repr__(self):
         return ''_Null''
 
+
 _Null = _NullType()
 
+
 class MXNetError(Exception):
     """"""Error that will be throwed by all mxnet functions.""""""
     pass
 
+
 class NotImplementedForSymbol(MXNetError):
     """"""Error: Not implemented for symbol""""""
     def __init__(self, function, alias, *args):
@@ -65,6 +80,7 @@ def __init__(self, function, alias, *args):
         self.function = function.__name__
         self.alias = alias
         self.args = [str(type(a)) for a in args]
+
     def __str__(self):
         msg = ''Function {}''.format(self.function)
         if self.alias:
@@ -74,6 +90,7 @@ def __str__(self):
         msg += '' is not implemented for Symbol and only available in NDArray.''
         return msg
 
+
 class NotSupportedForSparseNDArray(MXNetError):
     """"""Error: Not supported for SparseNDArray""""""
     def __init__(self, function, alias, *args):
@@ -81,6 +98,7 @@ def __init__(self, function, alias, *args):
         self.function = function.__name__
         self.alias = alias
         self.args = [str(type(a)) for a in args]
+
     def __str__(self):
         msg = ''Function {}''.format(self.function)
         if self.alias:
@@ -90,6 +108,7 @@ def __str__(self):
         msg += '' is not supported for SparseNDArray and only available in NDArray.''
         return msg
 
+
 class MXCallbackList(ctypes.Structure):
     """"""Structure that holds Callback information. Passed to CustomOpProp.""""""
     _fields_ = [
@@ -98,6 +117,7 @@ class MXCallbackList(ctypes.Structure):
         (''contexts'', ctypes.POINTER(ctypes.c_void_p))
         ]
 
+
 # Please see: https://stackoverflow.com/questions/5189699/how-to-make-a-class-property
 class _MXClassPropertyDescriptor(object):
     def __init__(self, fget, fset=None):
@@ -125,6 +145,7 @@ def setter(self, func):
         self.fset = func
         return self
 
+
 class _MXClassPropertyMetaClass(type):
     def __setattr__(cls, key, value):
         if key in cls.__dict__:
@@ -134,8 +155,9 @@ def __setattr__(cls, key, value):
 
         return super(_MXClassPropertyMetaClass, cls).__setattr__(key, value)
 
+
 # with_metaclass function obtained from: https://github.com/benjaminp/six/blob/master/six.py
-#pylint: disable=unused-argument
+# pylint: disable=unused-argument
 def with_metaclass(meta, *bases):
     """"""Create a base class with a metaclass.""""""
     # This requires a bit of explanation: the basic idea is to make a dummy
@@ -150,7 +172,8 @@ def __new__(cls, name, this_bases, d):
         def __prepare__(cls, name, this_bases):
             return meta.__prepare__(name, bases)
     return type.__new__(metaclass, ''temporary_class'', (), {})
-#pylint: enable=unused-argument
+# pylint: enable=unused-argument
+
 
 def classproperty(func):
     if not isinstance(func, (classmethod, staticmethod)):
@@ -159,7 +182,6 @@ def classproperty(func):
     return _MXClassPropertyDescriptor(func)
 
 
-
 def _load_lib():
     """"""Load library by searching possible path.""""""
     lib_path = libinfo.find_lib_path()
@@ -168,6 +190,7 @@ def _load_lib():
     lib.MXGetLastError.restype = ctypes.c_char_p
     return lib
 
+
 # version number
 __version__ = libinfo.__version__
 # library instance of mxnet
@@ -192,6 +215,8 @@ def _load_lib():
 CudaModuleHandle = ctypes.c_void_p
 CudaKernelHandle = ctypes.c_void_p
 ProfileHandle = ctypes.c_void_p
+
+
 #----------------------------
 # helper function definition
 #----------------------------
@@ -346,6 +371,7 @@ def c_array_buf(ctype, buf):
     """"""
     return (ctype * len(buf)).from_buffer(buf)
 
+
 def c_handle_array(objs):
     """"""Create ctypes const void ** from a list of MXNet objects with handles.
 
@@ -363,6 +389,7 @@ def c_handle_array(objs):
     arr[:] = [o.handle for o in objs]
     return arr
 
+
 def ctypes2buffer(cptr, length):
     """"""Convert ctypes pointer to buffer type.
 
@@ -386,6 +413,7 @@ def ctypes2buffer(cptr, length):
         raise RuntimeError(''memmove failed'')
     return res
 
+
 def ctypes2numpy_shared(cptr, shape):
     """"""Convert a ctypes pointer to a numpy array.
 
@@ -456,6 +484,7 @@ def _notify_shutdown():
     """"""Notify MXNet about a shutdown.""""""
     check_call(_LIB.MXNotifyShutdown())
 
+
 atexit.register(_notify_shutdown)
 
 
@@ -585,7 +614,6 @@ def _init_op_module(root_namespace, module_name, make_op_func):
         setattr(cur_module, function.__name__, function)
         cur_module.__all__.append(function.__name__)
 
-
         if op_name_prefix == ''_contrib_'':
             hdl = OpHandle()
             check_call(_LIB.NNGetOpHandle(c_str(name), ctypes.byref(hdl)))
@@ -616,17 +644,18 @@ def get_module_file(module_name):
         """"""Return the generated module file based on module name.""""""
         path = os.path.dirname(__file__)
         module_path = module_name.split(''.'')
-        module_path[-1] = ''gen_''+module_path[-1]
+        module_path[-1] = ''gen_'' + module_path[-1]
         file_name = os.path.join(path, ''..'', *module_path) + ''.py''
         module_file = open(file_name, ''w'')
         dependencies = {''symbol'': [''from ._internal import SymbolBase'',
                                    ''from ..base import _Null''],
                         ''ndarray'': [''from ._internal import NDArrayBase'',
                                     ''from ..base import _Null'']}
-        module_file.write(''# File content is auto-generated. Do not modify.''+os.linesep)
-        module_file.write(''# pylint: skip-file''+os.linesep)
+        module_file.write(''# File content is auto-generated. Do not modify.'' + os.linesep)
+        module_file.write(''# pylint: skip-file'' + os.linesep)
         module_file.write(os.linesep.join(dependencies[module_name.split(''.'')[1]]))
         return module_file
+
     def write_all_str(module_file, module_all_list):
         """"""Write the proper __all__ based on available operators.""""""
         module_file.write(os.linesep)


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27148,54,JIRA.13117658.1510343120000.215387.1510498920540@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117658.1510343120000@Atlassian.JIRA,,,2017-11-12 07:02:00-08,"[jira] [Commented] (AIRFLOW-1801) Url encode all execution dates in
 the UI","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1801?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16248886#comment-16248886 ] 

ASF subversion and git services commented on AIRFLOW-1801:
----------------------------------------------------------

Commit 9ec7f0f04bcec67fa62fbd85085acaa5e3b298b7 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=9ec7f0f ]

[AIRFLOW-1801][AIRFLOW-288] Url encode execution dates

Execution dates can contain special characters
that
need to be url encoded. In case of timezone
information
this information is lost if not url encoded.

Closes #2779 from bolkedebruin/AIRFLOW-1801


> Url encode all execution dates in the UI
> ----------------------------------------
>
>                 Key: AIRFLOW-1801
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1801
>             Project: Apache Airflow
>          Issue Type: Sub-task
>          Components: ui
>    Affects Versions: 1.9.0
>            Reporter: Bolke de Bruin
>
> Execution dates are not properly url encoded therefore losing timezone information



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27149,54,JIRA.13117658.1510343120000.215397.1510498920718@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117658.1510343120000@Atlassian.JIRA,,,2017-11-12 07:02:00-08,"[jira] [Commented] (AIRFLOW-1801) Url encode all execution dates in
 the UI","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1801?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16248888#comment-16248888 ] 

ASF subversion and git services commented on AIRFLOW-1801:
----------------------------------------------------------

Commit 9ec7f0f04bcec67fa62fbd85085acaa5e3b298b7 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=9ec7f0f ]

[AIRFLOW-1801][AIRFLOW-288] Url encode execution dates

Execution dates can contain special characters
that
need to be url encoded. In case of timezone
information
this information is lost if not url encoded.

Closes #2779 from bolkedebruin/AIRFLOW-1801


> Url encode all execution dates in the UI
> ----------------------------------------
>
>                 Key: AIRFLOW-1801
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1801
>             Project: Apache Airflow
>          Issue Type: Sub-task
>          Components: ui
>    Affects Versions: 1.9.0
>            Reporter: Bolke de Bruin
>
> Execution dates are not properly url encoded therefore losing timezone information



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27150,54,JIRA.13086271.1499782721000.219921.1510567680176@Atlassian.JIRA,2385,Arjun Arippa (JIRA),JIRA.13086271.1499782721000@Atlassian.JIRA,,,2017-11-13 02:08:00-08,[jira] [Commented] (AIRFLOW-1400) catchup=False caused exception,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1400?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249331#comment-16249331 ] 

Arjun Arippa commented on AIRFLOW-1400:
---------------------------------------

I''m also getting the same error when trying with the pattern:

default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, schedule_interval=''@once'',concurrency=1)

I''m using airflow version 1.8.0.

Thnx,
Arjun

> catchup=False caused exception
> ------------------------------
>
>                 Key: AIRFLOW-1400
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1400
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: Airflow 1.8
>            Reporter: Xi Wang
>
> When I set of the task with catchup=False, it threw a error as follow(logs/scheduler/my_dag_name):
> [2017-07-10 15:13:12,534] {jobs.py:354} DagFileProcessor373 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1171, in _process_dags
>     dag_run = self.create_dag_run(dag)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 776, in create_dag_run
>     if next_start <= now:
> TypeError: can''t compare datetime.datetime to NoneType
> It seems next_start was not defined properly, (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L777)
> Any help is appreciated.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27151,54,JIRA.13086271.1499782721000.219925.1510567740296@Atlassian.JIRA,2385,Arjun Arippa (JIRA),JIRA.13086271.1499782721000@Atlassian.JIRA,,,2017-11-13 02:09:00-08,"[jira] [Comment Edited] (AIRFLOW-1400) catchup=False caused
 exception","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1400?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249331#comment-16249331 ] 

Arjun Arippa edited comment on AIRFLOW-1400 at 11/13/17 10:08 AM:
------------------------------------------------------------------

I''m also getting the same error when trying with the pattern:


{code:
default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, 
{code}
schedule_interval=''@once'',concurrency=1)
}
I''m using airflow version 1.8.0.

Thnx,
Arjun


was (Author: arjun.arippa):
I''m also getting the same error when trying with the pattern:

default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, schedule_interval=''@once'',concurrency=1)

I''m using airflow version 1.8.0.

Thnx,
Arjun

> catchup=False caused exception
> ------------------------------
>
>                 Key: AIRFLOW-1400
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1400
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: Airflow 1.8
>            Reporter: Xi Wang
>
> When I set of the task with catchup=False, it threw a error as follow(logs/scheduler/my_dag_name):
> [2017-07-10 15:13:12,534] {jobs.py:354} DagFileProcessor373 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1171, in _process_dags
>     dag_run = self.create_dag_run(dag)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 776, in create_dag_run
>     if next_start <= now:
> TypeError: can''t compare datetime.datetime to NoneType
> It seems next_start was not defined properly, (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L777)
> Any help is appreciated.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
549419,277,JIRA.12988918.1468366670000.33326.1468525820625@Atlassian.JIRA,21272,Barbara Gomes (JIRA),JIRA.12988918.1468366670000@Atlassian.JIRA,,,2016-07-14 12:50:20-07,"[jira] [Comment Edited] (IOTA-25) Support HTTP protocol for dynamic
 population of Fey jar repo","
    [ https://issues.apache.org/jira/browse/IOTA-25?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=15375448#comment-15375448 ] 

Barbara Gomes edited comment on IOTA-25 at 7/14/16 7:50 PM:
------------------------------------------------------------

Fey JSON will have a new optional property under the property *source* of a *performer*. Specification:

{code}
""location"":{
  ""type"": ""object"",
  ""url"": {
    ""type"": ""string"",
    ""pattern"": ""(?i)(^(http|https|file)):\/\/""
  },
  ""credentials"":{
    ""user"":{
      ""type"": ""string""
    },
    ""password"": {
      ""type"": ""string""
    },
  ""required"":[""user"",""password""]
  }
}
{code}


was (Author: bmgomes):
Fey JSON will have a new optional property under the property *source* of a *performer*. Specification:

{code}
 ""location"":
{
   ""type"":""string"",
   ""pattern"":""(?i)(^(http|https|file)):\/\/""
}
{code}

> Support HTTP protocol for dynamic population of Fey jar repo
> ------------------------------------------------------------
>
>                 Key: IOTA-25
>                 URL: https://issues.apache.org/jira/browse/IOTA-25
>             Project: Iota
>          Issue Type: New Feature
>    Affects Versions: 0.1
>            Reporter: Rutvij Clerk
>            Assignee: Barbara Gomes
>            Priority: Minor
>             Fix For: 0.1
>
>   Original Estimate: 96h
>  Remaining Estimate: 96h
>
> Support HTTP(S) protocol in Fey core to dynamically populate jar repo at runtime. As a further refinement, we should also support HTTP Basic Auth for protected resources.
> We should also have a boolean parameter for e.g. forcePullJar in the JSON to enable the user to pull the jar every time a fey JSON is deployed.



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)

",f
27152,54,JIRA.13086271.1499782721000.219931.1510567740376@Atlassian.JIRA,2385,Arjun Arippa (JIRA),JIRA.13086271.1499782721000@Atlassian.JIRA,,,2017-11-13 02:09:00-08,"[jira] [Comment Edited] (AIRFLOW-1400) catchup=False caused
 exception","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1400?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249331#comment-16249331 ] 

Arjun Arippa edited comment on AIRFLOW-1400 at 11/13/17 10:08 AM:
------------------------------------------------------------------

I''m also getting the same error when trying with the pattern:


{code:default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, 
{code}
schedule_interval=''@once'',concurrency=1)
}
I''m using airflow version 1.8.0.

Thnx,
Arjun


was (Author: arjun.arippa):
I''m also getting the same error when trying with the pattern:


{code:
default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, 
{code}
schedule_interval=''@once'',concurrency=1)
}
I''m using airflow version 1.8.0.

Thnx,
Arjun

> catchup=False caused exception
> ------------------------------
>
>                 Key: AIRFLOW-1400
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1400
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: Airflow 1.8
>            Reporter: Xi Wang
>
> When I set of the task with catchup=False, it threw a error as follow(logs/scheduler/my_dag_name):
> [2017-07-10 15:13:12,534] {jobs.py:354} DagFileProcessor373 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1171, in _process_dags
>     dag_run = self.create_dag_run(dag)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 776, in create_dag_run
>     if next_start <= now:
> TypeError: can''t compare datetime.datetime to NoneType
> It seems next_start was not defined properly, (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L777)
> Any help is appreciated.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27153,54,JIRA.13086271.1499782721000.219940.1510567800487@Atlassian.JIRA,2385,Arjun Arippa (JIRA),JIRA.13086271.1499782721000@Atlassian.JIRA,,,2017-11-13 02:10:00-08,"[jira] [Comment Edited] (AIRFLOW-1400) catchup=False caused
 exception","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1400?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249331#comment-16249331 ] 

Arjun Arippa edited comment on AIRFLOW-1400 at 11/13/17 10:09 AM:
------------------------------------------------------------------

I''m also getting the same error when trying with the pattern:

{code:}
default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, 

schedule_interval=''@once'',concurrency=1)
{code}
I''m using airflow version 1.8.0.

Thnx,
Arjun


was (Author: arjun.arippa):
I''m also getting the same error when trying with the pattern:


{code:default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, 
{code}
schedule_interval=''@once'',concurrency=1)
}
I''m using airflow version 1.8.0.

Thnx,
Arjun

> catchup=False caused exception
> ------------------------------
>
>                 Key: AIRFLOW-1400
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1400
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: Airflow 1.8
>            Reporter: Xi Wang
>
> When I set of the task with catchup=False, it threw a error as follow(logs/scheduler/my_dag_name):
> [2017-07-10 15:13:12,534] {jobs.py:354} DagFileProcessor373 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1171, in _process_dags
>     dag_run = self.create_dag_run(dag)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 776, in create_dag_run
>     if next_start <= now:
> TypeError: can''t compare datetime.datetime to NoneType
> It seems next_start was not defined properly, (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L777)
> Any help is appreciated.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27154,54,JIRA.13086271.1499782721000.219955.1510567860575@Atlassian.JIRA,2385,Arjun Arippa (JIRA),JIRA.13086271.1499782721000@Atlassian.JIRA,,,2017-11-13 02:11:00-08,"[jira] [Comment Edited] (AIRFLOW-1400) catchup=False caused
 exception","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1400?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249331#comment-16249331 ] 

Arjun Arippa edited comment on AIRFLOW-1400 at 11/13/17 10:10 AM:
------------------------------------------------------------------

I''m also getting the same error when trying with the pattern:

{code:}
default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, schedule_interval=''@once'',concurrency=1)
{code}
I''m using airflow version 1.8.0.

Thnx,
Arjun


was (Author: arjun.arippa):
I''m also getting the same error when trying with the pattern:

{code:}
default_args = {
    ''owner'': ''airflow'',
    ''depends_on_past'': False,
    ''start_date'': datetime(2017,11,13),
    ''email'': [''airflow@airflow.com''],
    ''email_on_failure'': False,
    ''email_on_retry'': False,
    ''retries'': 1,
    ''retry_delay'': timedelta(seconds=5)
}

dag = DAG(
        dag_id=''my_dag_id'', default_args=default_args, 

schedule_interval=''@once'',concurrency=1)
{code}
I''m using airflow version 1.8.0.

Thnx,
Arjun

> catchup=False caused exception
> ------------------------------
>
>                 Key: AIRFLOW-1400
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1400
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: Airflow 1.8
>            Reporter: Xi Wang
>
> When I set of the task with catchup=False, it threw a error as follow(logs/scheduler/my_dag_name):
> [2017-07-10 15:13:12,534] {jobs.py:354} DagFileProcessor373 ERROR - Got an exception! Propagating...
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 346, in helper
>     pickle_dags)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1581, in process_file
>     self._process_dags(dagbag, dags, ti_keys_to_schedule)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 1171, in _process_dags
>     dag_run = self.create_dag_run(dag)
>   File ""/usr/lib/python2.7/site-packages/airflow/utils/db.py"", line 53, in wrapper
>     result = func(*args, **kwargs)
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 776, in create_dag_run
>     if next_start <= now:
> TypeError: can''t compare datetime.datetime to NoneType
> It seems next_start was not defined properly, (https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L777)
> Any help is appreciated.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27155,54,JIRA.13117919.1510576163000.220862.1510576200057@Atlassian.JIRA,2368,Rob Keevil (JIRA),JIRA.13117919.1510576163000@Atlassian.JIRA,,,2017-11-13 04:30:00-08,"[jira] [Created] (AIRFLOW-1813) SSH Operator errors on commands
 with no output","Rob Keevil created AIRFLOW-1813:
-----------------------------------

             Summary: SSH Operator errors on commands with no output
                 Key: AIRFLOW-1813
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1813
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Rob Keevil


The SSH Operator will throw an empty ""SSH operator error"" when running commands that do not immediately log something to the terminal.  This is due to a call to stdout.channel.recv when the channel currently has a 0-size buffer, either because the command has not yet logged anything, or never will (e.g. sleep 5).  A simple check of the buffer size before reading will fix this issue, will link a PR shortly



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27156,54,JIRA.13117919.1510576163000.221763.1510585320119@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117919.1510576163000@Atlassian.JIRA,,,2017-11-13 07:02:00-08,"[jira] [Commented] (AIRFLOW-1813) SSH Operator errors on commands
 with no output","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1813?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249686#comment-16249686 ] 

ASF subversion and git services commented on AIRFLOW-1813:
----------------------------------------------------------

Commit d4d8eb932657f4d1eccfaa8bb1d12933535fae94 in incubator-airflow''s branch refs/heads/master from [~RJKeevil]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d4d8eb9 ]

[AIRFLOW-1813] Bug SSH Operator empty buffer

The SSH Operator will throw an empty ""SSH operator
error"" when running
commands that do not immediately log something to
the terminal. This is
due to a call to stdout.channel.recv when the
channel currently has a
0-size buffer, either because the command has not
yet logged anything,
or never will (e.g. sleep 5)

Make code PEP8 compliant

Closes #2785 from RJKeevil/fix-ssh-operator-no-
terminal-output


> SSH Operator errors on commands with no output
> ----------------------------------------------
>
>                 Key: AIRFLOW-1813
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1813
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rob Keevil
>   Original Estimate: 1h
>  Remaining Estimate: 1h
>
> The SSH Operator will throw an empty ""SSH operator error"" when running commands that do not immediately log something to the terminal.  This is due to a call to stdout.channel.recv when the channel currently has a 0-size buffer, either because the command has not yet logged anything, or never will (e.g. sleep 5).  A simple check of the buffer size before reading will fix this issue, will link a PR shortly



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706924,24,153194412921.7332.13744583743448756359@gitbox.apache.org,25322,zhasheng,NULL,,,2018-07-18 13:02:10-07,"[incubator-mxnet] branch master updated: Use feature detection in
 base.py (#11803)","This is an automated email from the ASF dual-hosted git repository.

zhasheng pushed a commit to branch master
in repository https://gitbox.apache.org/repos/asf/incubator-mxnet.git


The following commit(s) were added to refs/heads/master by this push:
     new 1240577  Use feature detection in base.py (#11803)
1240577 is described below

commit 1240577dd629e97a740d65742a5008a6fc48a380
Author: cclauss <cclauss@bluewin.ch>
AuthorDate: Wed Jul 18 22:02:00 2018 +0200

    Use feature detection in base.py (#11803)
---
 python/mxnet/base.py | 63 ++++++++++++++++++++++++++++++++++++++--------------
 1 file changed, 46 insertions(+), 17 deletions(-)

diff --git a/python/mxnet/base.py b/python/mxnet/base.py
index 0fb73b3..4df794b 100644
--- a/python/mxnet/base.py
+++ b/python/mxnet/base.py
@@ -20,44 +20,59 @@
 """"""ctypes library of mxnet and helper functions.""""""
 from __future__ import absolute_import
 
+import atexit
+import ctypes
+import inspect
 import os
 import sys
-import ctypes
-import atexit
 import warnings
-import inspect
+
 import numpy as np
+
 from . import libinfo
+
 warnings.filterwarnings(''default'', category=DeprecationWarning)
 
 __all__ = [''MXNetError'']
 #----------------------------
 # library loading
 #----------------------------
-if sys.version_info[0] == 3:
-    string_types = str,
-    numeric_types = (float, int, np.generic)
-    integer_types = (int, np.int32, np.int64)
+
+# pylint: disable=pointless-statement
+try:
+    basestring
+    long
+except NameError:
+    basestring = str
+    long = int
+# pylint: enable=pointless-statement
+
+integer_types = (int, long, np.int32, np.int64)
+numeric_types = (float, int, long, np.generic)
+string_types = basestring,
+
+if sys.version_info[0] > 2:
     # this function is needed for python3
     # to convert ctypes.char_p .value back to python str
     py_str = lambda x: x.decode(''utf-8'')
 else:
-    string_types = basestring,
-    numeric_types = (float, int, long, np.generic)
-    integer_types = (int, long, np.int32, np.int64)
     py_str = lambda x: x
 
+
 class _NullType(object):
     """"""Placeholder for arguments""""""
     def __repr__(self):
         return ''_Null''
 
+
 _Null = _NullType()
 
+
 class MXNetError(Exception):
     """"""Error that will be throwed by all mxnet functions.""""""
     pass
 
+
 class NotImplementedForSymbol(MXNetError):
     """"""Error: Not implemented for symbol""""""
     def __init__(self, function, alias, *args):
@@ -65,6 +80,7 @@ class NotImplementedForSymbol(MXNetError):
         self.function = function.__name__
         self.alias = alias
         self.args = [str(type(a)) for a in args]
+
     def __str__(self):
         msg = ''Function {}''.format(self.function)
         if self.alias:
@@ -74,6 +90,7 @@ class NotImplementedForSymbol(MXNetError):
         msg += '' is not implemented for Symbol and only available in NDArray.''
         return msg
 
+
 class NotSupportedForSparseNDArray(MXNetError):
     """"""Error: Not supported for SparseNDArray""""""
     def __init__(self, function, alias, *args):
@@ -81,6 +98,7 @@ class NotSupportedForSparseNDArray(MXNetError):
         self.function = function.__name__
         self.alias = alias
         self.args = [str(type(a)) for a in args]
+
     def __str__(self):
         msg = ''Function {}''.format(self.function)
         if self.alias:
@@ -90,6 +108,7 @@ class NotSupportedForSparseNDArray(MXNetError):
         msg += '' is not supported for SparseNDArray and only available in NDArray.''
         return msg
 
+
 class MXCallbackList(ctypes.Structure):
     """"""Structure that holds Callback information. Passed to CustomOpProp.""""""
     _fields_ = [
@@ -98,6 +117,7 @@ class MXCallbackList(ctypes.Structure):
         (''contexts'', ctypes.POINTER(ctypes.c_void_p))
         ]
 
+
 # Please see: https://stackoverflow.com/questions/5189699/how-to-make-a-class-property
 class _MXClassPropertyDescriptor(object):
     def __init__(self, fget, fset=None):
@@ -125,6 +145,7 @@ class _MXClassPropertyDescriptor(object):
         self.fset = func
         return self
 
+
 class _MXClassPropertyMetaClass(type):
     def __setattr__(cls, key, value):
         if key in cls.__dict__:
@@ -134,8 +155,9 @@ class _MXClassPropertyMetaClass(type):
 
         return super(_MXClassPropertyMetaClass, cls).__setattr__(key, value)
 
+
 # with_metaclass function obtained from: https://github.com/benjaminp/six/blob/master/six.py
-#pylint: disable=unused-argument
+# pylint: disable=unused-argument
 def with_metaclass(meta, *bases):
     """"""Create a base class with a metaclass.""""""
     # This requires a bit of explanation: the basic idea is to make a dummy
@@ -150,7 +172,8 @@ def with_metaclass(meta, *bases):
         def __prepare__(cls, name, this_bases):
             return meta.__prepare__(name, bases)
     return type.__new__(metaclass, ''temporary_class'', (), {})
-#pylint: enable=unused-argument
+# pylint: enable=unused-argument
+
 
 def classproperty(func):
     if not isinstance(func, (classmethod, staticmethod)):
@@ -159,7 +182,6 @@ def classproperty(func):
     return _MXClassPropertyDescriptor(func)
 
 
-
 def _load_lib():
     """"""Load library by searching possible path.""""""
     lib_path = libinfo.find_lib_path()
@@ -168,6 +190,7 @@ def _load_lib():
     lib.MXGetLastError.restype = ctypes.c_char_p
     return lib
 
+
 # version number
 __version__ = libinfo.__version__
 # library instance of mxnet
@@ -192,6 +215,8 @@ RtcHandle = ctypes.c_void_p
 CudaModuleHandle = ctypes.c_void_p
 CudaKernelHandle = ctypes.c_void_p
 ProfileHandle = ctypes.c_void_p
+
+
 #----------------------------
 # helper function definition
 #----------------------------
@@ -346,6 +371,7 @@ def c_array_buf(ctype, buf):
     """"""
     return (ctype * len(buf)).from_buffer(buf)
 
+
 def c_handle_array(objs):
     """"""Create ctypes const void ** from a list of MXNet objects with handles.
 
@@ -363,6 +389,7 @@ def c_handle_array(objs):
     arr[:] = [o.handle for o in objs]
     return arr
 
+
 def ctypes2buffer(cptr, length):
     """"""Convert ctypes pointer to buffer type.
 
@@ -386,6 +413,7 @@ def ctypes2buffer(cptr, length):
         raise RuntimeError(''memmove failed'')
     return res
 
+
 def ctypes2numpy_shared(cptr, shape):
     """"""Convert a ctypes pointer to a numpy array.
 
@@ -456,6 +484,7 @@ def _notify_shutdown():
     """"""Notify MXNet about a shutdown.""""""
     check_call(_LIB.MXNotifyShutdown())
 
+
 atexit.register(_notify_shutdown)
 
 
@@ -585,7 +614,6 @@ def _init_op_module(root_namespace, module_name, make_op_func):
         setattr(cur_module, function.__name__, function)
         cur_module.__all__.append(function.__name__)
 
-
         if op_name_prefix == ''_contrib_'':
             hdl = OpHandle()
             check_call(_LIB.NNGetOpHandle(c_str(name), ctypes.byref(hdl)))
@@ -616,17 +644,18 @@ def _generate_op_module_signature(root_namespace, module_name, op_code_gen_func)
         """"""Return the generated module file based on module name.""""""
         path = os.path.dirname(__file__)
         module_path = module_name.split(''.'')
-        module_path[-1] = ''gen_''+module_path[-1]
+        module_path[-1] = ''gen_'' + module_path[-1]
         file_name = os.path.join(path, ''..'', *module_path) + ''.py''
         module_file = open(file_name, ''w'')
         dependencies = {''symbol'': [''from ._internal import SymbolBase'',
                                    ''from ..base import _Null''],
                         ''ndarray'': [''from ._internal import NDArrayBase'',
                                     ''from ..base import _Null'']}
-        module_file.write(''# File content is auto-generated. Do not modify.''+os.linesep)
-        module_file.write(''# pylint: skip-file''+os.linesep)
+        module_file.write(''# File content is auto-generated. Do not modify.'' + os.linesep)
+        module_file.write(''# pylint: skip-file'' + os.linesep)
         module_file.write(os.linesep.join(dependencies[module_name.split(''.'')[1]]))
         return module_file
+
     def write_all_str(module_file, module_all_list):
         """"""Write the proper __all__ based on available operators.""""""
         module_file.write(os.linesep)


",t
27157,54,JIRA.13117919.1510576163000.222257.1510588560082@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117919.1510576163000@Atlassian.JIRA,,,2017-11-13 07:56:00-08,"[jira] [Commented] (AIRFLOW-1813) SSH Operator errors on commands
 with no output","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1813?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249761#comment-16249761 ] 

ASF subversion and git services commented on AIRFLOW-1813:
----------------------------------------------------------

Commit fab727d34edfbd9b96c088ae5f8f538f4f74a114 in incubator-airflow''s branch refs/heads/v1-9-test from [~RJKeevil]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=fab727d ]

[AIRFLOW-1813] Bug SSH Operator empty buffer

The SSH Operator will throw an empty ""SSH operator
error"" when running
commands that do not immediately log something to
the terminal. This is
due to a call to stdout.channel.recv when the
channel currently has a
0-size buffer, either because the command has not
yet logged anything,
or never will (e.g. sleep 5)

Make code PEP8 compliant

Closes #2785 from RJKeevil/fix-ssh-operator-no-
terminal-output

(cherry picked from commit d4d8eb932657f4d1eccfaa8bb1d12933535fae94)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> SSH Operator errors on commands with no output
> ----------------------------------------------
>
>                 Key: AIRFLOW-1813
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1813
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rob Keevil
>   Original Estimate: 1h
>  Remaining Estimate: 1h
>
> The SSH Operator will throw an empty ""SSH operator error"" when running commands that do not immediately log something to the terminal.  This is due to a call to stdout.channel.recv when the channel currently has a 0-size buffer, either because the command has not yet logged anything, or never will (e.g. sleep 5).  A simple check of the buffer size before reading will fix this issue, will link a PR shortly



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27158,54,JIRA.13117988.1510588618000.222270.1510588620830@Atlassian.JIRA,2386,Galak (JIRA),JIRA.13117988.1510588618000@Atlassian.JIRA,,,2017-11-13 07:57:00-08,"[jira] [Created] (AIRFLOW-1814) Add op_args and op_kwargs in
 PythonOperator templated fields","Galak created AIRFLOW-1814:
------------------------------

             Summary: Add op_args and op_kwargs in PythonOperator templated fields
                 Key: AIRFLOW-1814
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1814
             Project: Apache Airflow
          Issue Type: Wish
          Components: operators
    Affects Versions: Airflow 1.8, 1.8.0
            Reporter: Galak
            Priority: Minor


*I''m wondering if ""_op_args_"" and ""_op_kwargs_"" PythonOperator parameters could be templated.*

I have 2 different use cases where this change could help a lot:

+1/ Provide some job execution information as a python callable argument:+
let''s explain it through a simple example:
{code}
simple_task = PythonOperator(
    task_id=''simple_task'',
    provide_context=True,
    python_callable=extract_data,
    op_args=[
	""my_db_connection_id""
	""select * from my_table""
	""/data/{dag.dag_id}/{ts}/my_export.csv""
    ],
    dag=dag
)
{code}
""extract_data"" python function seems to be simple here, but it could be anything re-usable in multiple dags...


+2/ Provide some XCom value as a python callable argument:+
Let''s say I a have a task which is retrieving or calculating a value, and then storing it in an XCom for further use by other tasks:
{code}
value_producer_task = PythonOperator(
    task_id=''value_producer_task'',
    provide_context=True,
    python_callable=produce_value,
    op_args=[
	""my_db_connection_id"",
	""some_other_static_parameter"",
	""my_xcom_key""
    ],
    dag=dag
)
{code}

Then I can just configure a PythonCallable task to use the produced value:
{code}
value_consumer_task = PythonOperator(
    task_id=''value_consumer_task'',
    provide_context=True,
    python_callable=consume_value,
    op_args=[
	""{{ task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'') }}""
    ],
    dag=dag
)
{code}


I quickly tried the following class:

{code}
from airflow.operators.python_operator import PythonOperator
class MyPythonOperator(PythonOperator):
    template_fields = PythonOperator.template_fields + (''op_args'', ''op_kwargs'')
{code}

and it worked like a charm.

So could these 2 arguments be added to templated_fields? Or did I miss some major drawback to this change?




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27159,54,JIRA.13117919.1510576163000.222300.1510588860352@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117919.1510576163000@Atlassian.JIRA,,,2017-11-13 08:01:00-08,"[jira] [Commented] (AIRFLOW-1813) SSH Operator errors on commands
 with no output","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1813?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249764#comment-16249764 ] 

ASF subversion and git services commented on AIRFLOW-1813:
----------------------------------------------------------

Commit d2372d458d574b14cae73853d820d9b007f0c179 in incubator-airflow''s branch refs/heads/v1-9-stable from [~RJKeevil]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d2372d4 ]

[AIRFLOW-1813] Bug SSH Operator empty buffer

The SSH Operator will throw an empty ""SSH operator
error"" when running
commands that do not immediately log something to
the terminal. This is
due to a call to stdout.channel.recv when the
channel currently has a
0-size buffer, either because the command has not
yet logged anything,
or never will (e.g. sleep 5)

Make code PEP8 compliant

Closes #2785 from RJKeevil/fix-ssh-operator-no-
terminal-output

(cherry picked from commit d4d8eb932657f4d1eccfaa8bb1d12933535fae94)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> SSH Operator errors on commands with no output
> ----------------------------------------------
>
>                 Key: AIRFLOW-1813
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1813
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rob Keevil
>   Original Estimate: 1h
>  Remaining Estimate: 1h
>
> The SSH Operator will throw an empty ""SSH operator error"" when running commands that do not immediately log something to the terminal.  This is due to a call to stdout.channel.recv when the channel currently has a 0-size buffer, either because the command has not yet logged anything, or never will (e.g. sleep 5).  A simple check of the buffer size before reading will fix this issue, will link a PR shortly



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27160,54,JIRA.13116566.1510023245000.222857.1510594800918@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13116566.1510023245000@Atlassian.JIRA,,,2017-11-13 09:40:00-08,"[jira] [Updated] (AIRFLOW-1787) Fix batch clear RUNNING task
 instance and inconsistent timestamp format bugs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1787?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1787:
-------------------------------------
    Fix Version/s:     (was: 1.9.0)

> Fix batch clear RUNNING task instance and inconsistent timestamp format bugs
> ----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1787
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1787
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>             Fix For: 1.10.0
>
>
> * Batch clear in CRUD is not working for task instances in RUNNING state, need to be fixed
> * Batch clear and set status are not working for manually triggered task instances because manually triggered task instances have different execution date format.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27161,54,JIRA.13116566.1510023245000.222854.1510594800902@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13116566.1510023245000@Atlassian.JIRA,,,2017-11-13 09:40:00-08,"[jira] [Updated] (AIRFLOW-1787) Fix batch clear RUNNING task
 instance and inconsistent timestamp format bugs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1787?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1787:
-------------------------------------
    Fix Version/s: 1.10.0

> Fix batch clear RUNNING task instance and inconsistent timestamp format bugs
> ----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1787
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1787
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>             Fix For: 1.10.0
>
>
> * Batch clear in CRUD is not working for task instances in RUNNING state, need to be fixed
> * Batch clear and set status are not working for manually triggered task instances because manually triggered task instances have different execution date format.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27162,54,JIRA.13117988.1510588618000.223534.1510598520534@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13117988.1510588618000@Atlassian.JIRA,,,2017-11-13 10:42:00-08,"[jira] [Commented] (AIRFLOW-1814) Add op_args and op_kwargs in
 PythonOperator templated fields","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1814?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16249969#comment-16249969 ] 

Ash Berlin-Taylor commented on AIRFLOW-1814:
--------------------------------------------

They could be added, but there is an way already by using {{provide_context=True}}. When you set that to True then everything that is accessible from a Jinja template is accessible as a named parameter.:

{code}
def consume_value(task_instance, **kwargs):
    my_xcom_value = task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'')

value_consumer_task = PythonOperator(
    task_id=''value_consumer_task'',
    provide_context=True,
    python_callable=consume_value,
    dag=dag,
)
{code}

I can see when having it be templated directly might make some things nicer though.

> Add op_args and op_kwargs in PythonOperator templated fields
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1814
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1814
>             Project: Apache Airflow
>          Issue Type: Wish
>          Components: operators
>    Affects Versions: Airflow 1.8, 1.8.0
>            Reporter: Galak
>            Priority: Minor
>
> *I''m wondering if ""_op_args_"" and ""_op_kwargs_"" PythonOperator parameters could be templated.*
> I have 2 different use cases where this change could help a lot:
> +1/ Provide some job execution information as a python callable argument:+
> let''s explain it through a simple example:
> {code}
> simple_task = PythonOperator(
>     task_id=''simple_task'',
>     provide_context=True,
>     python_callable=extract_data,
>     op_args=[
> 	""my_db_connection_id""
> 	""select * from my_table""
> 	""/data/{dag.dag_id}/{ts}/my_export.csv""
>     ],
>     dag=dag
> )
> {code}
> ""extract_data"" python function seems to be simple here, but it could be anything re-usable in multiple dags...
> +2/ Provide some XCom value as a python callable argument:+
> Let''s say I a have a task which is retrieving or calculating a value, and then storing it in an XCom for further use by other tasks:
> {code}
> value_producer_task = PythonOperator(
>     task_id=''value_producer_task'',
>     provide_context=True,
>     python_callable=produce_value,
>     op_args=[
> 	""my_db_connection_id"",
> 	""some_other_static_parameter"",
> 	""my_xcom_key""
>     ],
>     dag=dag
> )
> {code}
> Then I can just configure a PythonCallable task to use the produced value:
> {code}
> value_consumer_task = PythonOperator(
>     task_id=''value_consumer_task'',
>     provide_context=True,
>     python_callable=consume_value,
>     op_args=[
> 	""{{ task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'') }}""
>     ],
>     dag=dag
> )
> {code}
> I quickly tried the following class:
> {code}
> from airflow.operators.python_operator import PythonOperator
> class MyPythonOperator(PythonOperator):
>     template_fields = PythonOperator.template_fields + (''op_args'', ''op_kwargs'')
> {code}
> and it worked like a charm.
> So could these 2 arguments be added to templated_fields? Or did I miss some major drawback to this change?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27163,54,JIRA.13117988.1510588618000.223862.1510600620273@Atlassian.JIRA,2386,Galak (JIRA),JIRA.13117988.1510588618000@Atlassian.JIRA,,,2017-11-13 11:17:00-08,"[jira] [Commented] (AIRFLOW-1814) Add op_args and op_kwargs in
 PythonOperator templated fields","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1814?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16250028#comment-16250028 ] 

Galak commented on AIRFLOW-1814:
--------------------------------

Thank you for your answer.

I agree, I can do it programmatically in the consume_value callable function, but then I would add a dependency upon airflow objects.

It''s a question of separation of concerns:
All reusable / generic functions should not know about how to retrieve an xcom value; Moreover, they could be called with different values, depending on the context: one could call it with a static value, another with a job execution property, another one with a value coming from a previous task instance... Then I would have to create a python wrapper function for every new call context.
IMHO, ""consume_value"" should only receive a value to consume, and apply its business logic to it.

> Add op_args and op_kwargs in PythonOperator templated fields
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1814
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1814
>             Project: Apache Airflow
>          Issue Type: Wish
>          Components: operators
>    Affects Versions: Airflow 1.8, 1.8.0
>            Reporter: Galak
>            Priority: Minor
>
> *I''m wondering if ""_op_args_"" and ""_op_kwargs_"" PythonOperator parameters could be templated.*
> I have 2 different use cases where this change could help a lot:
> +1/ Provide some job execution information as a python callable argument:+
> let''s explain it through a simple example:
> {code}
> simple_task = PythonOperator(
>     task_id=''simple_task'',
>     provide_context=True,
>     python_callable=extract_data,
>     op_args=[
> 	""my_db_connection_id""
> 	""select * from my_table""
> 	""/data/{dag.dag_id}/{ts}/my_export.csv""
>     ],
>     dag=dag
> )
> {code}
> ""extract_data"" python function seems to be simple here, but it could be anything re-usable in multiple dags...
> +2/ Provide some XCom value as a python callable argument:+
> Let''s say I a have a task which is retrieving or calculating a value, and then storing it in an XCom for further use by other tasks:
> {code}
> value_producer_task = PythonOperator(
>     task_id=''value_producer_task'',
>     provide_context=True,
>     python_callable=produce_value,
>     op_args=[
> 	""my_db_connection_id"",
> 	""some_other_static_parameter"",
> 	""my_xcom_key""
>     ],
>     dag=dag
> )
> {code}
> Then I can just configure a PythonCallable task to use the produced value:
> {code}
> value_consumer_task = PythonOperator(
>     task_id=''value_consumer_task'',
>     provide_context=True,
>     python_callable=consume_value,
>     op_args=[
> 	""{{ task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'') }}""
>     ],
>     dag=dag
> )
> {code}
> I quickly tried the following class:
> {code}
> from airflow.operators.python_operator import PythonOperator
> class MyPythonOperator(PythonOperator):
>     template_fields = PythonOperator.template_fields + (''op_args'', ''op_kwargs'')
> {code}
> and it worked like a charm.
> So could these 2 arguments be added to templated_fields? Or did I miss some major drawback to this change?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27164,54,JIRA.13118256.1510668379000.232956.1510668420156@Atlassian.JIRA,1449,Daniel van der Ende (JIRA),JIRA.13118256.1510668379000@Atlassian.JIRA,,,2017-11-14 06:07:00-08,"[jira] [Created] (AIRFLOW-1815) Add jinja support for Druid index
 spec","Daniel van der Ende created AIRFLOW-1815:
--------------------------------------------

             Summary: Add jinja support for Druid index spec
                 Key: AIRFLOW-1815
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1815
             Project: Apache Airflow
          Issue Type: Bug
          Components: operators
            Reporter: Daniel van der Ende
            Assignee: Daniel van der Ende


At the moment, the druid operator only supports jinja templating for the interval parameter. It would be nice to support jinja templating in the json indexing spec (file) passed to it as well.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27165,54,JIRA.12967098.1462902754000.233425.1510670520560@Atlassian.JIRA,1332,Timo (JIRA),JIRA.12967098.1462902754000@Atlassian.JIRA,,,2017-11-14 06:42:00-08,"[jira] [Commented] (AIRFLOW-98) Using Flask extensions from a
 plugin","
    [ https://issues.apache.org/jira/browse/AIRFLOW-98?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16251470#comment-16251470 ] 

Timo commented on AIRFLOW-98:
-----------------------------

This functionality is not only useful for enabling more extensions, but also so we can register extra jinja filters and use the Flask app decorators.

If the order of execution (plugins first, then the Flask app) isn''t easy to change, would it possible to have an optional `init_app(app)` function in the plugin file or plugin class? Something like being done for [`login_manager`|https://github.com/apache/incubator-airflow/blob/d4d8eb932657f4d1eccfaa8bb1d12933535fae94/airflow/www/app.py#L46] or [`api_auth`|https://github.com/apache/incubator-airflow/blob/d4d8eb932657f4d1eccfaa8bb1d12933535fae94/airflow/www/app.py#L50]

> Using Flask extensions from a plugin
> ------------------------------------
>
>                 Key: AIRFLOW-98
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-98
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Timo
>
> I am creating a plugin which should be able to use some Flask extensions. Is there any way to do this?
> Basically I need to import the airflow.www.app.app object in the plugin to initialise the extension with.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27166,54,JIRA.12967098.1462902754000.233444.1510670640482@Atlassian.JIRA,1332,Timo (JIRA),JIRA.12967098.1462902754000@Atlassian.JIRA,,,2017-11-14 06:44:00-08,"[jira] [Comment Edited] (AIRFLOW-98) Using Flask extensions from a
 plugin","
    [ https://issues.apache.org/jira/browse/AIRFLOW-98?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16251470#comment-16251470 ] 

Timo edited comment on AIRFLOW-98 at 11/14/17 2:43 PM:
-------------------------------------------------------

This functionality is not only useful for enabling more extensions, but also so we can register extra jinja filters and use the Flask app decorators.

If the order of execution (plugins first, then the Flask app) isn''t easy to change, would it possible to have an optional {{init_app(app)}} function in the plugin file or plugin class? Something like being done for [{{login_manager}}|https://github.com/apache/incubator-airflow/blob/d4d8eb932657f4d1eccfaa8bb1d12933535fae94/airflow/www/app.py#L46] or [{{api_auth}}|https://github.com/apache/incubator-airflow/blob/d4d8eb932657f4d1eccfaa8bb1d12933535fae94/airflow/www/app.py#L50]


was (Author: timovwb):
This functionality is not only useful for enabling more extensions, but also so we can register extra jinja filters and use the Flask app decorators.

If the order of execution (plugins first, then the Flask app) isn''t easy to change, would it possible to have an optional `init_app(app)` function in the plugin file or plugin class? Something like being done for [`login_manager`|https://github.com/apache/incubator-airflow/blob/d4d8eb932657f4d1eccfaa8bb1d12933535fae94/airflow/www/app.py#L46] or [`api_auth`|https://github.com/apache/incubator-airflow/blob/d4d8eb932657f4d1eccfaa8bb1d12933535fae94/airflow/www/app.py#L50]

> Using Flask extensions from a plugin
> ------------------------------------
>
>                 Key: AIRFLOW-98
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-98
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Timo
>
> I am creating a plugin which should be able to use some Flask extensions. Is there any way to do this?
> Basically I need to import the airflow.www.app.app object in the plugin to initialise the extension with.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27167,54,JIRA.13117988.1510588618000.234000.1510673160315@Atlassian.JIRA,2386,Galak (JIRA),JIRA.13117988.1510588618000@Atlassian.JIRA,,,2017-11-14 07:26:00-08,"[jira] [Updated] (AIRFLOW-1814) Add op_args and op_kwargs in
 PythonOperator templated fields","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1814?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Galak updated AIRFLOW-1814:
---------------------------
    Description: 
*I''m wondering if ""_op_args_"" and ""_op_kwargs_"" PythonOperator parameters could be templated.*

I have 2 different use cases where this change could help a lot:

+1/ Provide some job execution information as a python callable argument:+
let''s explain it through a simple example:
{code}
simple_task = PythonOperator(
    task_id=''simple_task'',
    provide_context=True,
    python_callable=extract_data,
    op_args=[
	""my_db_connection_id""
	""select * from my_table""
	""/data/{{ dag.dag_id }}/{{ ts }}/my_export.csv""
    ],
    dag=dag
)
{code}
""extract_data"" python function seems to be simple here, but it could be anything re-usable in multiple dags...


+2/ Provide some XCom value as a python callable argument:+
Let''s say I a have a task which is retrieving or calculating a value, and then storing it in an XCom for further use by other tasks:
{code}
value_producer_task = PythonOperator(
    task_id=''value_producer_task'',
    provide_context=True,
    python_callable=produce_value,
    op_args=[
	""my_db_connection_id"",
	""some_other_static_parameter"",
	""my_xcom_key""
    ],
    dag=dag
)
{code}

Then I can just configure a PythonCallable task to use the produced value:
{code}
value_consumer_task = PythonOperator(
    task_id=''value_consumer_task'',
    provide_context=True,
    python_callable=consume_value,
    op_args=[
	""{{ task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'') }}""
    ],
    dag=dag
)
{code}


I quickly tried the following class:

{code}
from airflow.operators.python_operator import PythonOperator
class MyPythonOperator(PythonOperator):
    template_fields = PythonOperator.template_fields + (''op_args'', ''op_kwargs'')
{code}

and it worked like a charm.

So could these 2 arguments be added to templated_fields? Or did I miss some major drawback to this change?


  was:
*I''m wondering if ""_op_args_"" and ""_op_kwargs_"" PythonOperator parameters could be templated.*

I have 2 different use cases where this change could help a lot:

+1/ Provide some job execution information as a python callable argument:+
let''s explain it through a simple example:
{code}
simple_task = PythonOperator(
    task_id=''simple_task'',
    provide_context=True,
    python_callable=extract_data,
    op_args=[
	""my_db_connection_id""
	""select * from my_table""
	""/data/{dag.dag_id}/{ts}/my_export.csv""
    ],
    dag=dag
)
{code}
""extract_data"" python function seems to be simple here, but it could be anything re-usable in multiple dags...


+2/ Provide some XCom value as a python callable argument:+
Let''s say I a have a task which is retrieving or calculating a value, and then storing it in an XCom for further use by other tasks:
{code}
value_producer_task = PythonOperator(
    task_id=''value_producer_task'',
    provide_context=True,
    python_callable=produce_value,
    op_args=[
	""my_db_connection_id"",
	""some_other_static_parameter"",
	""my_xcom_key""
    ],
    dag=dag
)
{code}

Then I can just configure a PythonCallable task to use the produced value:
{code}
value_consumer_task = PythonOperator(
    task_id=''value_consumer_task'',
    provide_context=True,
    python_callable=consume_value,
    op_args=[
	""{{ task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'') }}""
    ],
    dag=dag
)
{code}


I quickly tried the following class:

{code}
from airflow.operators.python_operator import PythonOperator
class MyPythonOperator(PythonOperator):
    template_fields = PythonOperator.template_fields + (''op_args'', ''op_kwargs'')
{code}

and it worked like a charm.

So could these 2 arguments be added to templated_fields? Or did I miss some major drawback to this change?



> Add op_args and op_kwargs in PythonOperator templated fields
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1814
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1814
>             Project: Apache Airflow
>          Issue Type: Wish
>          Components: operators
>    Affects Versions: Airflow 1.8, 1.8.0
>            Reporter: Galak
>            Priority: Minor
>
> *I''m wondering if ""_op_args_"" and ""_op_kwargs_"" PythonOperator parameters could be templated.*
> I have 2 different use cases where this change could help a lot:
> +1/ Provide some job execution information as a python callable argument:+
> let''s explain it through a simple example:
> {code}
> simple_task = PythonOperator(
>     task_id=''simple_task'',
>     provide_context=True,
>     python_callable=extract_data,
>     op_args=[
> 	""my_db_connection_id""
> 	""select * from my_table""
> 	""/data/{{ dag.dag_id }}/{{ ts }}/my_export.csv""
>     ],
>     dag=dag
> )
> {code}
> ""extract_data"" python function seems to be simple here, but it could be anything re-usable in multiple dags...
> +2/ Provide some XCom value as a python callable argument:+
> Let''s say I a have a task which is retrieving or calculating a value, and then storing it in an XCom for further use by other tasks:
> {code}
> value_producer_task = PythonOperator(
>     task_id=''value_producer_task'',
>     provide_context=True,
>     python_callable=produce_value,
>     op_args=[
> 	""my_db_connection_id"",
> 	""some_other_static_parameter"",
> 	""my_xcom_key""
>     ],
>     dag=dag
> )
> {code}
> Then I can just configure a PythonCallable task to use the produced value:
> {code}
> value_consumer_task = PythonOperator(
>     task_id=''value_consumer_task'',
>     provide_context=True,
>     python_callable=consume_value,
>     op_args=[
> 	""{{ task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'') }}""
>     ],
>     dag=dag
> )
> {code}
> I quickly tried the following class:
> {code}
> from airflow.operators.python_operator import PythonOperator
> class MyPythonOperator(PythonOperator):
>     template_fields = PythonOperator.template_fields + (''op_args'', ''op_kwargs'')
> {code}
> and it worked like a charm.
> So could these 2 arguments be added to templated_fields? Or did I miss some major drawback to this change?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27168,54,JIRA.13118343.1510687944000.237186.1510687980829@Atlassian.JIRA,2387,Dan Sedov (JIRA),JIRA.13118343.1510687944000@Atlassian.JIRA,,,2017-11-14 11:33:00-08,"[jira] [Created] (AIRFLOW-1816) Add missing region param to
 DataProc{Pig,Hive,SparkSql}Operators","Dan Sedov created AIRFLOW-1816:
----------------------------------

             Summary: Add missing region param to DataProc{Pig,Hive,SparkSql}Operators
                 Key: AIRFLOW-1816
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1816
             Project: Apache Airflow
          Issue Type: Bug
          Components: contrib
            Reporter: Dan Sedov
            Assignee: Dan Sedov
            Priority: Minor


Add region field to the remainder of Dataproc Jobs that were missed under AIRFLOW-1576.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27169,54,JIRA.13117988.1510588618000.238876.1510698300213@Atlassian.JIRA,2386,Galak (JIRA),JIRA.13117988.1510588618000@Atlassian.JIRA,,,2017-11-14 14:25:00-08,"[jira] [Commented] (AIRFLOW-1814) Add op_args and op_kwargs in
 PythonOperator templated fields","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1814?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16252547#comment-16252547 ] 

Galak commented on AIRFLOW-1814:
--------------------------------

I discovered a blocker issue for this change while using the work-around class:
{code}
class MyPythonOperator(PythonOperator):
    template_fields = PythonOperator.template_fields + (''op_args'', ''op_kwargs'')
{code}

all non string arguments passed to this operator will make the task fail with an {{AirflowException}} raised by {{BaseOperator.render_template_from_field}}...

example:
{code}
value_consumer_task = MyPythonOperator(
        task_id=''value_consumer_task'',
        provide_context=True,
        python_callable=consume_value,
        op_args=[
            ""{{ task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'') }}"",
            3  # // << this argument makes the task fail with message: airflow.exceptions.AirflowException: Type ''<class ''int''>'' used for parameter ''op_args'' is not supported for templating
        ],
    dag=dag
)
{code}

Would it be possible for {{BaseOperator.render_template_from_field}} to log a warning and simply return the value itself when it is not a string (nor a collection, nor a dictionary)?

At this point, I should probably work on the change and send a pull request, isn''t it? But I don''t have a lot of free time, so I would like to be sure I''m not going in a wrong direction...



> Add op_args and op_kwargs in PythonOperator templated fields
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1814
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1814
>             Project: Apache Airflow
>          Issue Type: Wish
>          Components: operators
>    Affects Versions: Airflow 1.8, 1.8.0
>            Reporter: Galak
>            Priority: Minor
>
> *I''m wondering if ""_op_args_"" and ""_op_kwargs_"" PythonOperator parameters could be templated.*
> I have 2 different use cases where this change could help a lot:
> +1/ Provide some job execution information as a python callable argument:+
> let''s explain it through a simple example:
> {code}
> simple_task = PythonOperator(
>     task_id=''simple_task'',
>     provide_context=True,
>     python_callable=extract_data,
>     op_args=[
> 	""my_db_connection_id""
> 	""select * from my_table""
> 	""/data/{{ dag.dag_id }}/{{ ts }}/my_export.csv""
>     ],
>     dag=dag
> )
> {code}
> ""extract_data"" python function seems to be simple here, but it could be anything re-usable in multiple dags...
> +2/ Provide some XCom value as a python callable argument:+
> Let''s say I a have a task which is retrieving or calculating a value, and then storing it in an XCom for further use by other tasks:
> {code}
> value_producer_task = PythonOperator(
>     task_id=''value_producer_task'',
>     provide_context=True,
>     python_callable=produce_value,
>     op_args=[
> 	""my_db_connection_id"",
> 	""some_other_static_parameter"",
> 	""my_xcom_key""
>     ],
>     dag=dag
> )
> {code}
> Then I can just configure a PythonCallable task to use the produced value:
> {code}
> value_consumer_task = PythonOperator(
>     task_id=''value_consumer_task'',
>     provide_context=True,
>     python_callable=consume_value,
>     op_args=[
> 	""{{ task_instance.xcom_pull(task_ids=None, key=''my_xcom_key'') }}""
>     ],
>     dag=dag
> )
> {code}
> I quickly tried the following class:
> {code}
> from airflow.operators.python_operator import PythonOperator
> class MyPythonOperator(PythonOperator):
>     template_fields = PythonOperator.template_fields + (''op_args'', ''op_kwargs'')
> {code}
> and it worked like a charm.
> So could these 2 arguments be added to templated_fields? Or did I miss some major drawback to this change?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706925,24,153194482614.18502.898909040094927942.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 13:13:46-07,[GitHub] zheng-da commented on issue #11675: fix a bug in CachedOp.,"zheng-da commented on issue #11675: fix a bug in CachedOp.
URL: https://github.com/apache/incubator-mxnet/pull/11675#issuecomment-406059502
 
 
   the PR will be updated after https://github.com/apache/incubator-mxnet/pull/11566 is merged.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27170,54,JIRA.13118487.1510738273000.243311.1510738322249@Atlassian.JIRA,2155,Cedrik Neumann (JIRA),JIRA.13118487.1510738273000@Atlassian.JIRA,,,2017-11-15 01:32:02-08,"[jira] [Assigned] (AIRFLOW-1817) setup.py for S3 installs old boto
 library","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1817?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Cedrik Neumann reassigned AIRFLOW-1817:
---------------------------------------

    Assignee: Cedrik Neumann

> setup.py for S3 installs old boto library
> -----------------------------------------
>
>                 Key: AIRFLOW-1817
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1817
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Cedrik Neumann
>            Assignee: Cedrik Neumann
>
> In `setup.py` the s3 dependency still relies on the old boto library even though the s3 hook has been reimplemented with boto3.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27171,54,JIRA.13118487.1510738273000.243310.1510738322205@Atlassian.JIRA,2155,Cedrik Neumann (JIRA),JIRA.13118487.1510738273000@Atlassian.JIRA,,,2017-11-15 01:32:02-08,"[jira] [Created] (AIRFLOW-1817) setup.py for S3 installs old boto
 library","Cedrik Neumann created AIRFLOW-1817:
---------------------------------------

             Summary: setup.py for S3 installs old boto library
                 Key: AIRFLOW-1817
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1817
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Cedrik Neumann


In `setup.py` the s3 dependency still relies on the old boto library even though the s3 hook has been reimplemented with boto3.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27172,54,JIRA.13118487.1510738273000.243867.1510743360193@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13118487.1510738273000@Atlassian.JIRA,,,2017-11-15 02:56:00-08,"[jira] [Commented] (AIRFLOW-1817) setup.py for S3 installs old boto
 library","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1817?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16253276#comment-16253276 ] 

ASF subversion and git services commented on AIRFLOW-1817:
----------------------------------------------------------

Commit 5157b5a7631f2492bacfb705c79d128bd8d8d5d3 in incubator-airflow''s branch refs/heads/master from [~m1racoli]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=5157b5a ]

[AIRFLOW-1817] use boto3 for s3 dependency

Since S3Hook is reimplemented based on the AwsHook
using boto3, its package dependencies need to be
updated as well.

Closes #2790 from m1racoli/fix-setup-s3


> setup.py for S3 installs old boto library
> -----------------------------------------
>
>                 Key: AIRFLOW-1817
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1817
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Cedrik Neumann
>            Assignee: Cedrik Neumann
>
> In `setup.py` the s3 dependency still relies on the old boto library even though the s3 hook has been reimplemented with boto3.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27173,54,JIRA.13116890.1510125098000.246977.1510766821180@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13116890.1510125098000@Atlassian.JIRA,,,2017-11-15 09:27:01-08,"[jira] [Commented] (AIRFLOW-1791) Unexpected ""AttributeError:
 ''unicode'' object has no attribute ''val''"" from Variable.setdefault","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1791?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16253833#comment-16253833 ] 

Ash Berlin-Taylor commented on AIRFLOW-1791:
--------------------------------------------

Fixed by 1177, merged and will included in 1.9.0.

> Unexpected ""AttributeError: ''unicode'' object has no attribute ''val''"" from Variable.setdefault
> ---------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1791
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1791
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>    Affects Versions: Airflow 1.8
>         Environment: Python 2.7, Airflow 1.8.2
>            Reporter: Shawn Wang
>
> In Variable.setdefault method,
> {code:python}
>         obj = Variable.get(key, default_var=default_sentinel, deserialize_json=False)
>         if obj is default_sentinel:
>             // ...
>         else:
>             if deserialize_json:
>                 return json.loads(obj.val)
>             else:
>                 return obj.val
> {code}
> While obj is retrieved by ""get"" method which has already fetched the val attribute from obj, so this ""obj.val"" throws the AttributeError.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27174,54,JIRA.13066720.1493133634000.246996.1510767060154@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13066720.1493133634000@Atlassian.JIRA,,,2017-11-15 09:31:00-08,[jira] [Commented] (AIRFLOW-1146) izip use in Python 3.4,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1146?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16253839#comment-16253839 ] 

Ash Berlin-Taylor commented on AIRFLOW-1146:
--------------------------------------------

Fixed in 1.9.0

> izip use in Python 3.4
> ----------------------
>
>                 Key: AIRFLOW-1146
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1146
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: hive_hooks
>    Affects Versions: Airflow 1.8
>            Reporter: Alexander Panzhin
>
> Python 3 no longer has itertools.izip, but it is still used in airflow/hooks/hive_hooks.py
> This causes all kinds of havoc.
> This needs fixed, if this is to be used on Python 3+



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27175,54,JIRA.13118343.1510687944000.248341.1510776361306@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13118343.1510687944000@Atlassian.JIRA,,,2017-11-15 12:06:01-08,"[jira] [Commented] (AIRFLOW-1816) Add missing region param to
 DataProc{Pig,Hive,SparkSql}Operators","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1816?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16254077#comment-16254077 ] 

ASF subversion and git services commented on AIRFLOW-1816:
----------------------------------------------------------

Commit d04519e6051e39ec95c553c0f550092cfa418a38 in incubator-airflow''s branch refs/heads/master from [~DanSedov]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d04519e ]

[AIRFLOW-1816] Add region param to Dataproc operators

Closes #2788 from DanSedov/master


> Add missing region param to DataProc{Pig,Hive,SparkSql}Operators
> ----------------------------------------------------------------
>
>                 Key: AIRFLOW-1816
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1816
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Dan Sedov
>            Assignee: Dan Sedov
>            Priority: Minor
>
> Add region field to the remainder of Dataproc Jobs that were missed under AIRFLOW-1576.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27176,54,JIRA.13118343.1510687944000.248408.1510776780397@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13118343.1510687944000@Atlassian.JIRA,,,2017-11-15 12:13:00-08,"[jira] [Updated] (AIRFLOW-1816) Add missing region param to
 DataProc{Pig,Hive,SparkSql}Operators","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1816?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1816:
-------------------------------------
    Fix Version/s: 1.10.0

> Add missing region param to DataProc{Pig,Hive,SparkSql}Operators
> ----------------------------------------------------------------
>
>                 Key: AIRFLOW-1816
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1816
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Dan Sedov
>            Assignee: Dan Sedov
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Add region field to the remainder of Dataproc Jobs that were missed under AIRFLOW-1576.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27177,54,JIRA.13118343.1510687944000.248414.1510776780460@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13118343.1510687944000@Atlassian.JIRA,,,2017-11-15 12:13:00-08,"[jira] [Resolved] (AIRFLOW-1816) Add missing region param to
 DataProc{Pig,Hive,SparkSql}Operators","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1816?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1816.
--------------------------------------
    Resolution: Fixed

> Add missing region param to DataProc{Pig,Hive,SparkSql}Operators
> ----------------------------------------------------------------
>
>                 Key: AIRFLOW-1816
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1816
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Dan Sedov
>            Assignee: Dan Sedov
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Add region field to the remainder of Dataproc Jobs that were missed under AIRFLOW-1576.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27178,54,JIRA.13118649.1510780252000.248993.1510780260110@Atlassian.JIRA,2390,Soudipta Das (JIRA),JIRA.13118649.1510780252000@Atlassian.JIRA,,,2017-11-15 13:11:00-08,"[jira] [Created] (AIRFLOW-1818) Airflow task restarts after being
 marked fail","Soudipta Das created AIRFLOW-1818:
-------------------------------------

             Summary: Airflow task restarts after being marked fail
                 Key: AIRFLOW-1818
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1818
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Soudipta Das


Hi,

I am facing a bug in case where a task marks itself as failed after retrying for the max number of retries. And then the task restarts itself and marks itself with running. This interferes with the next run of the same task as sometimes they both run in parallel.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27179,54,JIRA.13117704.1510361841000.250265.1510786500101@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117704.1510361841000@Atlassian.JIRA,,,2017-11-15 14:55:00-08,"[jira] [Commented] (AIRFLOW-1805) Allow to supply Slack token
 through connection","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1805?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16254384#comment-16254384 ] 

ASF subversion and git services commented on AIRFLOW-1805:
----------------------------------------------------------

Commit d8e8f90142246ae5b02c1a0f9649ea5a419a5afc in incubator-airflow''s branch refs/heads/master from [~kevinyang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d8e8f90 ]

[AIRFLOW-1805] Allow Slack token to be passed through connection

Allow users to pass in Slack token through
connection which can provide better security. This
enables user to expose token only to workers
instead to both workers and schedulers.

Closes #2789 from
yrqls21/add_conn_supp_in_slack_op


> Allow to supply Slack token through connection
> ----------------------------------------------
>
>                 Key: AIRFLOW-1805
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1805
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Kevin Yang
>
> To prevent passing in Slack token directly in plain text, it is safer to pass in the token as ''password'' through connection.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27180,54,JIRA.13117704.1510361841000.250764.1510790580280@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13117704.1510361841000@Atlassian.JIRA,,,2017-11-15 16:03:00-08,"[jira] [Assigned] (AIRFLOW-1805) Allow to supply Slack token
 through connection","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1805?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kevin Yang reassigned AIRFLOW-1805:
-----------------------------------

    Assignee: Kevin Yang

> Allow to supply Slack token through connection
> ----------------------------------------------
>
>                 Key: AIRFLOW-1805
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1805
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>
> To prevent passing in Slack token directly in plain text, it is safer to pass in the token as ''password'' through connection.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27181,54,JIRA.13118748.1510790713000.250804.1510790760166@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13118748.1510790713000@Atlassian.JIRA,,,2017-11-15 16:06:00-08,[jira] [Created] (AIRFLOW-1819) Fix slack operator unittest bug,"Kevin Yang created AIRFLOW-1819:
-----------------------------------

             Summary: Fix slack operator unittest bug
                 Key: AIRFLOW-1819
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1819
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Kevin Yang


slack_operator.py unittest is failing and is not covering code paths for passing in api_params.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27182,54,JIRA.13118754.1510791920000.251160.1510791960093@Atlassian.JIRA,2305,William Pursell (JIRA),JIRA.13118754.1510791920000@Atlassian.JIRA,,,2017-11-15 16:26:00-08,"[jira] [Created] (AIRFLOW-1820) dagrun.dependency-check metric does
 not play nice with statsd","William Pursell created AIRFLOW-1820:
----------------------------------------

             Summary: dagrun.dependency-check metric does not play nice with statsd
                 Key: AIRFLOW-1820
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1820
             Project: Apache Airflow
          Issue Type: Bug
          Components: core
            Reporter: William Pursell
            Assignee: William Pursell
            Priority: Minor


The metric dagrun.dependency-check (https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L4594) is sent to statsd with spaces and colons in the metric name.  This is not ideal for statsd.  (I''m not really sure I see the point of putting a timestamp in a metric name at all!)  We should either do something like Stats.timing(""dagrun.dependency-check.{}{:%%%%Y%%%%m%%%%d_%%%%H%%%%M%%%%S}"". to use a nicer name, or just drop the timestamp completely.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27183,54,JIRA.13118748.1510790713000.251216.1510792380100@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13118748.1510790713000@Atlassian.JIRA,,,2017-11-15 16:33:00-08,[jira] [Assigned] (AIRFLOW-1819) Fix slack operator unittest bug,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1819?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kevin Yang reassigned AIRFLOW-1819:
-----------------------------------

    Assignee: Kevin Yang

> Fix slack operator unittest bug
> -------------------------------
>
>                 Key: AIRFLOW-1819
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1819
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>
> slack_operator.py unittest is failing and is not covering code paths for passing in api_params.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27184,54,JIRA.13117704.1510361841000.251217.1510792440176@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13117704.1510361841000@Atlassian.JIRA,,,2017-11-15 16:34:00-08,"[jira] [Work started] (AIRFLOW-1805) Allow to supply Slack token
 through connection","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1805?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1805 started by Kevin Yang.
-------------------------------------------
> Allow to supply Slack token through connection
> ----------------------------------------------
>
>                 Key: AIRFLOW-1805
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1805
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>
> To prevent passing in Slack token directly in plain text, it is safer to pass in the token as ''password'' through connection.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27185,54,JIRA.13118748.1510790713000.251218.1510792440190@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13118748.1510790713000@Atlassian.JIRA,,,2017-11-15 16:34:00-08,"[jira] [Work started] (AIRFLOW-1819) Fix slack operator unittest
 bug","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1819?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1819 started by Kevin Yang.
-------------------------------------------
> Fix slack operator unittest bug
> -------------------------------
>
>                 Key: AIRFLOW-1819
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1819
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>
> slack_operator.py unittest is failing and is not covering code paths for passing in api_params.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27186,54,JIRA.13118748.1510790713000.251371.1510793820093@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13118748.1510790713000@Atlassian.JIRA,,,2017-11-15 16:57:00-08,[jira] [Commented] (AIRFLOW-1819) Fix slack operator unittest bug,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1819?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16254570#comment-16254570 ] 

Kevin Yang commented on AIRFLOW-1819:
-------------------------------------

this jira fix bug in issue AIRFLOW-1805

> Fix slack operator unittest bug
> -------------------------------
>
>                 Key: AIRFLOW-1819
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1819
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>
> slack_operator.py unittest is failing and is not covering code paths for passing in api_params.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27187,54,JIRA.13117704.1510361841000.251379.1510793880264@Atlassian.JIRA,2374,Kevin Yang (JIRA),JIRA.13117704.1510361841000@Atlassian.JIRA,,,2017-11-15 16:58:00-08,"[jira] [Commented] (AIRFLOW-1805) Allow to supply Slack token
 through connection","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1805?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16254572#comment-16254572 ] 

Kevin Yang commented on AIRFLOW-1805:
-------------------------------------

Bug in this issue is fixed in AIRFLOW-1819

> Allow to supply Slack token through connection
> ----------------------------------------------
>
>                 Key: AIRFLOW-1805
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1805
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>
> To prevent passing in Slack token directly in plain text, it is safer to pass in the token as ''password'' through connection.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27188,54,JIRA.13118748.1510790713000.251734.1510797181320@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13118748.1510790713000@Atlassian.JIRA,,,2017-11-15 17:53:01-08,[jira] [Commented] (AIRFLOW-1819) Fix slack operator unittest bug,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1819?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16254627#comment-16254627 ] 

ASF subversion and git services commented on AIRFLOW-1819:
----------------------------------------------------------

Commit 3c8f7747b08ca5c23233799e56a040f60e3d0fc6 in incubator-airflow''s branch refs/heads/master from [~kevinyang]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3c8f774 ]

[AIRFLOW-1819] Fix slack operator unittest bug

Fix failing slack operator unittest and add test
coverage.

Closes #2791 from yrqls21/kevin-yang-fix-unit-test


> Fix slack operator unittest bug
> -------------------------------
>
>                 Key: AIRFLOW-1819
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1819
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Kevin Yang
>            Assignee: Kevin Yang
>
> slack_operator.py unittest is failing and is not covering code paths for passing in api_params.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27189,54,JIRA.13118786.1510801779000.252166.1510801800110@Atlassian.JIRA,1375,Joy Gao (JIRA),JIRA.13118786.1510801779000@Atlassian.JIRA,,,2017-11-15 19:10:00-08,"[jira] [Created] (AIRFLOW-1821) Default logging config file is
 confusing","Joy Gao created AIRFLOW-1821:
--------------------------------

             Summary: Default logging config file is confusing
                 Key: AIRFLOW-1821
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1821
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Joy Gao
            Assignee: Joy Gao


The current DEFAULT_LOGGING_CONFIG has 5 loggers for configurations:
- root
- airflow
- airflow.task
- airflow.task_runner
- airflow.processor
The number of loggers could be reduced to make configuration easier.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27190,54,JIRA.13118786.1510801779000.252169.1510801801173@Atlassian.JIRA,1375,Joy Gao (JIRA),JIRA.13118786.1510801779000@Atlassian.JIRA,,,2017-11-15 19:10:01-08,"[jira] [Updated] (AIRFLOW-1821) Default logging config file is
 confusing","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1821?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Joy Gao updated AIRFLOW-1821:
-----------------------------
    Description: 
The current DEFAULT_LOGGING_CONFIG has 5 loggers for configurations:
- root
- airflow
- airflow.task
- airflow.task_runner
- airflow.processor

The number of loggers could be reduced to make configuration easier.

  was:
The current DEFAULT_LOGGING_CONFIG has 5 loggers for configurations:
- root
- airflow
- airflow.task
- airflow.task_runner
- airflow.processor
The number of loggers could be reduced to make configuration easier.


> Default logging config file is confusing
> ----------------------------------------
>
>                 Key: AIRFLOW-1821
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1821
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Joy Gao
>            Assignee: Joy Gao
>
> The current DEFAULT_LOGGING_CONFIG has 5 loggers for configurations:
> - root
> - airflow
> - airflow.task
> - airflow.task_runner
> - airflow.processor
> The number of loggers could be reduced to make configuration easier.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27191,54,JIRA.13117757.1510437189000.253611.1510819440550@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117757.1510437189000@Atlassian.JIRA,,,2017-11-16 00:04:00-08,[jira] [Commented] (AIRFLOW-1811) Fix rendered Druid Operator,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1811?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16254902#comment-16254902 ] 

ASF subversion and git services commented on AIRFLOW-1811:
----------------------------------------------------------

Commit 54c03f3262babb0b5cbe335d429379ebb1440185 in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=54c03f3 ]

[AIRFLOW-1811] Fix render Druid operator

Set the correct fields to enable the visualisation
of the rendering
of the Druid indexing spec. Add some tests to make
sure that the
templating is working :-)

Closes #2783 from Fokko/AIRFLOW-1811-fix-druid-
operator


> Fix rendered Druid Operator
> ---------------------------
>
>                 Key: AIRFLOW-1811
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1811
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>
> Hi all,
> I''ve learned how the rendered field works in Airflow and how it can be used to visualise the job. I''ve did some small adjustments to the Druid operator to enable this.
> Cheers, Fokko



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27192,54,JIRA.13117757.1510437189000.253610.1510819440541@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117757.1510437189000@Atlassian.JIRA,,,2017-11-16 00:04:00-08,[jira] [Commented] (AIRFLOW-1811) Fix rendered Druid Operator,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1811?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16254901#comment-16254901 ] 

ASF subversion and git services commented on AIRFLOW-1811:
----------------------------------------------------------

Commit 54c03f3262babb0b5cbe335d429379ebb1440185 in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=54c03f3 ]

[AIRFLOW-1811] Fix render Druid operator

Set the correct fields to enable the visualisation
of the rendering
of the Druid indexing spec. Add some tests to make
sure that the
templating is working :-)

Closes #2783 from Fokko/AIRFLOW-1811-fix-druid-
operator


> Fix rendered Druid Operator
> ---------------------------
>
>                 Key: AIRFLOW-1811
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1811
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>
> Hi all,
> I''ve learned how the rendered field works in Airflow and how it can be used to visualise the job. I''ve did some small adjustments to the Druid operator to enable this.
> Cheers, Fokko



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27193,54,JIRA.13117757.1510437189000.253614.1510819500218@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13117757.1510437189000@Atlassian.JIRA,,,2017-11-16 00:05:00-08,[jira] [Resolved] (AIRFLOW-1811) Fix rendered Druid Operator,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1811?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong resolved AIRFLOW-1811.
---------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2783
[https://github.com/apache/incubator-airflow/pull/2783]

> Fix rendered Druid Operator
> ---------------------------
>
>                 Key: AIRFLOW-1811
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1811
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>             Fix For: 1.9.1
>
>
> Hi all,
> I''ve learned how the rendered field works in Airflow and how it can be used to visualise the job. I''ve did some small adjustments to the Druid operator to enable this.
> Cheers, Fokko



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27194,54,JIRA.13117222.1510223865000.257469.1510851720323@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-16 09:02:00-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16255600#comment-16255600 ] 

Chris Riccomini commented on AIRFLOW-1795:
------------------------------------------

We did the same thing for GCP a while ago. We did the approach you prefer: update all existing references, and add a note in UPDATING.md.

> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27195,54,JIRA.13117222.1510223865000.257473.1510851720637@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-16 09:02:00-08,"[jira] [Comment Edited] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16255600#comment-16255600 ] 

Chris Riccomini edited comment on AIRFLOW-1795 at 11/16/17 5:01 PM:
--------------------------------------------------------------------

We did the same thing for GCP a while ago. We did your least preferred approach: update all existing references, and add a note in UPDATING.md. :) It all worked out fine.


was (Author: criccomini):
We did the same thing for GCP a while ago. We did the approach you prefer: update all existing references, and add a note in UPDATING.md.

> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27196,54,JIRA.13117222.1510223865000.257479.1510851780373@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-16 09:03:00-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16255601#comment-16255601 ] 

Chris Riccomini commented on AIRFLOW-1795:
------------------------------------------

That said, all of these seem like fine-enough short term solutions to me.

> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27197,54,JIRA.13117222.1510223865000.257807.1510853880242@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-16 09:38:00-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16255672#comment-16255672 ] 

Ash Berlin-Taylor commented on AIRFLOW-1795:
--------------------------------------------

Argh I just noticed that the S3ToHiveOperator just won''t work as it still expects the boto2 return types from the S3Hook (for similar reasons to as changed in https://github.com/apache/incubator-airflow/pull/2773 - boto2 API was mocked, so tests still pass) I don''t have time (today) to fix that.

> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706939,24,153194867342.17579.3834437084589112738.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 14:17:53-07,"[GitHub] atiyo commented on issue #7637: Strange Validation and Training
 Losses at epoch change","atiyo commented on issue #7637: Strange Validation and Training Losses at epoch change
URL: https://github.com/apache/incubator-mxnet/issues/7637#issuecomment-406077456
 
 
   Still have yet to completely go through @vishaalkapoor''s very helpful post above, but I also concluded that it seems that this is not an issue with MXnet a while ago but my incorrect logging of the error. Closing the issue now.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27198,54,JIRA.13119034.1510864738000.259268.1510864740659@Atlassian.JIRA,2391,Jeremy Lewi (JIRA),JIRA.13119034.1510864738000@Atlassian.JIRA,,,2017-11-16 12:39:00-08,"[jira] [Created] (AIRFLOW-1823) API get_task_info is incompatible
 with manual runs created by UI","Jeremy Lewi created AIRFLOW-1823:
------------------------------------

             Summary: API get_task_info is incompatible with manual runs created by UI
                 Key: AIRFLOW-1823
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1823
             Project: Apache Airflow
          Issue Type: Bug
          Components: api
    Affects Versions: Airflow 2.0
         Environment: ubuntu
Airflow 1.9rc02
commit: https://github.com/apache/incubator-airflow/blob/master/airflow/www/api/experimental/endpoints.py#L126
            Reporter: Jeremy Lewi
            Priority: Minor


The API method [task_instance_info|https://github.com/apache/incubator-airflow/blob/master/airflow/www/api/experimental/endpoints.py#L126] doesn''t work with manual runs created by the UI.

The UI creates dag runs with ids with sub second precision in the name. An example of a run created by the UI is
2017-11-16T20:23:32.045330

The endpoint for  [task_instance_info|https://github.com/apache/incubator-airflow/blob/master/airflow/www/api/experimental/endpoints.py#L126] however assumes the dag run id is of the form ''%%%%Y-%%%%m-%%%%dT%%%%H:%%%%M:%%%%S''.

Runs triggered via the CLI generate run ids with the form expected by the API.





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27199,54,JIRA.13119033.1510864696000.259263.1510864740614@Atlassian.JIRA,2323,Sanjay Pillai (JIRA),JIRA.13119033.1510864696000@Atlassian.JIRA,,,2017-11-16 12:39:00-08,"[jira] [Created] (AIRFLOW-1822) Add aiohttp and gthread gunicorn
 workerclass option in cli","Sanjay Pillai created AIRFLOW-1822:
--------------------------------------

             Summary: Add aiohttp and gthread gunicorn workerclass option in cli
                 Key: AIRFLOW-1822
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1822
             Project: Apache Airflow
          Issue Type: Improvement
          Components: webserver
            Reporter: Sanjay Pillai
            Priority: Minor






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27200,54,JIRA.13119033.1510864696000.259298.1510865280222@Atlassian.JIRA,2323,Sanjay Pillai (JIRA),JIRA.13119033.1510864696000@Atlassian.JIRA,,,2017-11-16 12:48:00-08,"[jira] [Updated] (AIRFLOW-1822) Add gaiohttp and gthread gunicorn
 workerclass option in cli","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1822?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sanjay Pillai updated AIRFLOW-1822:
-----------------------------------
    Summary: Add gaiohttp and gthread gunicorn workerclass option in cli  (was: Add aiohttp and gthread gunicorn workerclass option in cli)

> Add gaiohttp and gthread gunicorn workerclass option in cli
> -----------------------------------------------------------
>
>                 Key: AIRFLOW-1822
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1822
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webserver
>            Reporter: Sanjay Pillai
>            Priority: Minor
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27201,54,JIRA.13119033.1510864696000.259413.1510866240958@Atlassian.JIRA,2323,Sanjay Pillai (JIRA),JIRA.13119033.1510864696000@Atlassian.JIRA,,,2017-11-16 13:04:00-08,"[jira] [Updated] (AIRFLOW-1822) Add gaiohttp and gthread gunicorn
 workerclass option in cli","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1822?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Sanjay Pillai updated AIRFLOW-1822:
-----------------------------------
    Description: 
gunicorn in min version has been updated to 19.40 
we need to add cli support for gthread and gaiohttp worker class

> Add gaiohttp and gthread gunicorn workerclass option in cli
> -----------------------------------------------------------
>
>                 Key: AIRFLOW-1822
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1822
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webserver
>            Reporter: Sanjay Pillai
>            Priority: Minor
>
> gunicorn in min version has been updated to 19.40 
> we need to add cli support for gthread and gaiohttp worker class



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27202,54,JIRA.13119113.1510892095000.263437.1510892100171@Atlassian.JIRA,2392,Snigdha Nair (JIRA),JIRA.13119113.1510892095000@Atlassian.JIRA,,,2017-11-16 20:15:00-08,[jira] [Created] (AIRFLOW-1825) Set Multi dag dependency,"Snigdha Nair created AIRFLOW-1825:
-------------------------------------

             Summary: Set Multi dag dependency 
                 Key: AIRFLOW-1825
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1825
             Project: Apache Airflow
          Issue Type: Task
            Reporter: Snigdha Nair


I have 3 dags A, B and C. Dag C should get triggered only after tasks in dag A and B completes. Is there a way to implement this in airflow? I am able to set dependency between dag A and C using Triggerdagrun Operator. But when I try to set dependency between dag B and C, C is getting triggered when either A or B completes. Can someone please help me in solving this? I understand that explains external task sensor Operator can be used. But it continuously polls if task in dag A and B is complete which might create performance hit over a period of time.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27203,54,JIRA.13119112.1510892065000.263436.1510892100048@Atlassian.JIRA,2392,Snigdha Nair (JIRA),JIRA.13119112.1510892065000@Atlassian.JIRA,,,2017-11-16 20:15:00-08,[jira] [Created] (AIRFLOW-1824) Set Multi dag dependency,"Snigdha Nair created AIRFLOW-1824:
-------------------------------------

             Summary: Set Multi dag dependency 
                 Key: AIRFLOW-1824
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1824
             Project: Apache Airflow
          Issue Type: Task
            Reporter: Snigdha Nair


I have 3 dags A, B and C. Dag C should get triggered only after tasks in dag A and B completes. Is there a way to implement this in airflow? I am able to set dependency between dag A and C using Triggerdagrun Operator. But when I try to set dependency between dag B and C, C is getting triggered when either A or B completes. Can someone please help me in solving this? I understand that explains external task sensor Operator can be used. But it continuously polls if task in dag A and B is complete which might create performance hit over a period of time.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706940,24,153194867350.17581.8299325396041898460.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 14:17:53-07,"[GitHub] atiyo closed issue #7637: Strange Validation and Training Losses at
 epoch change","atiyo closed issue #7637: Strange Validation and Training Losses at epoch change
URL: https://github.com/apache/incubator-mxnet/issues/7637
 
 
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27204,54,JIRA.13117222.1510223865000.266114.1510920720259@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-17 04:12:00-08,"[jira] [Updated] (AIRFLOW-1795) S3Hook no longer accepts s3_conn_id
 breaking build in ops/sensors and back-compat","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor updated AIRFLOW-1795:
---------------------------------------
    External issue URL: https://github.com/apache/incubator-airflow/pull/2795

> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27205,54,JIRA.13119211.1510921607000.266205.1510921620740@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13119211.1510921607000@Atlassian.JIRA,,,2017-11-17 04:27:00-08,"[jira] [Created] (AIRFLOW-1826) Update views to be able to use
 pendulum","Bolke de Bruin created AIRFLOW-1826:
---------------------------------------

             Summary: Update views to be able to use pendulum
                 Key: AIRFLOW-1826
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1826
             Project: Apache Airflow
          Issue Type: Sub-task
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27206,54,JIRA.13119214.1510922616000.266288.1510922640325@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13119214.1510922616000@Atlassian.JIRA,,,2017-11-17 04:44:00-08,[jira] [Created] (AIRFLOW-1827) Fix api endpoints time parsing,"Bolke de Bruin created AIRFLOW-1827:
---------------------------------------

             Summary: Fix api endpoints time parsing
                 Key: AIRFLOW-1827
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1827
             Project: Apache Airflow
          Issue Type: Sub-task
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27207,54,JIRA.13119232.1510927624000.266725.1510927680032@Atlassian.JIRA,2243,Andy Cooper (JIRA),JIRA.13119232.1510927624000@Atlassian.JIRA,,,2017-11-17 06:08:00-08,"[jira] [Created] (AIRFLOW-1828) Scheduler Performance Degrades
 Overttime","Andy Cooper created AIRFLOW-1828:
------------------------------------

             Summary: Scheduler Performance Degrades Overttime
                 Key: AIRFLOW-1828
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1828
             Project: Apache Airflow
          Issue Type: Bug
          Components: scheduler
    Affects Versions: 1.8.1, 1.8.2
            Reporter: Andy Cooper


Team,

We are using Airflow very heavily internally on multiple instances. Overtime, as we have added more and more tasks to Airflow we have begun to notice a degradation of scheduler performance. For our most heavy system we are noticing that it will eventually go from completing 15,000 task per hour to less than a 100. One other note for that particular system is that the ~30 DAGs are generated dynamically from a single DAG file.

We have also begun to see this on lesser used instances we are hosting as well. 

As a company we are happy to take a look at this ourselves and have in fact dug into the scheduler quite a bit. I am posting here more as an opportunity to gather more insights into what is happening here.

- What are the causes of this scheduler performance decrease? 

- Is the only known way to combat this performance decrease to restart the scheduler regularly? 

- Is restarting the scheduler on a time interval still the recommended way to handle this?

- In most cases it seems like most of the bottle neck is in moving tasks from null state to scheduled and from scheduled to queued. And often we will see only a portion of DAGs having tasks picked up from the queued state. What causes this and why does it only get worse over time?

Once we fully understand this problem we are happy to add to documentation or code base in order to resolve this problem or make it more clear for people going forward.





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27208,54,JIRA.13119113.1510892095000.267886.1510936560124@Atlassian.JIRA,2393,Jonathan Bender (JIRA),JIRA.13119113.1510892095000@Atlassian.JIRA,,,2017-11-17 08:36:00-08,[jira] [Commented] (AIRFLOW-1825) Set Multi dag dependency,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1825?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16257188#comment-16257188 ] 

Jonathan Bender commented on AIRFLOW-1825:
------------------------------------------

bq. Can someone please help me in solving this? I understand that explains external task sensor Operator can be used. But it continuously polls if task in dag A and B is complete which might create performance hit over a period of time.

The performance hit of polling the database for successful task instances every interval seems reasonable, if you have sane polling intervals.

We have strayed away from lots of ExternalTaskSensors as each one requires a task slot, which is some non-trivial memory overhead if you have enough dependencies which could wait for long periods of time. Additionally, each one means more task instances which puts more pressure on the db.

To combat this we just rolled a simple ""multi external"" sensor class which only completes when all of its child tasks are completed:
https://gist.github.com/jonbender/4da675e9385e2fbf66ff0bb591cc74d7

You could use something like that, or you could have a single DummyOperator like ""all_a_tasks_completed"" which is downstream of all tasks in a, and same goes for b, then you''d only have a single external dependency per dag.





> Set Multi dag dependency 
> -------------------------
>
>                 Key: AIRFLOW-1825
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1825
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Snigdha Nair
>
> I have 3 dags A, B and C. Dag C should get triggered only after tasks in dag A and B completes. Is there a way to implement this in airflow? I am able to set dependency between dag A and C using Triggerdagrun Operator. But when I try to set dependency between dag B and C, C is getting triggered when either A or B completes. Can someone please help me in solving this? I understand that explains external task sensor Operator can be used. But it continuously polls if task in dag A and B is complete which might create performance hit over a period of time.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27209,54,JIRA.13119294.1510944251000.269045.1510944300079@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13119294.1510944251000@Atlassian.JIRA,,,2017-11-17 10:45:00-08,"[jira] [Created] (AIRFLOW-1829) Support BigQuery schema updates as
 a side effect of a query job","Guillermo Rodr=C3=ADguez Cano created AIRFLOW-1829:
-------------------------------------------------

             Summary: Support BigQuery schema updates as a side effect of a=
 query job
                 Key: AIRFLOW-1829
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1829
             Project: Apache Airflow
          Issue Type: Improvement
          Components: contrib, gcp, hooks, operators
            Reporter: Guillermo Rodr=C3=ADguez Cano
            Priority: Critical


BigQuery hook supports schema updates as a side effect of a load job but no=
t for query jobs. Respectively GCS to BQ operator (which executes a load jo=
b) supports such possibility unlike its ''sister'' operator, BQ operator, whe=
n running a query with a table as destination.

Both operations, load and query, should support such feature (experimental =
as of this writing though)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27210,54,JIRA.13119294.1510944251000.269814.1510950060140@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13119294.1510944251000@Atlassian.JIRA,,,2017-11-17 12:21:00-08,"[jira] [Commented] (AIRFLOW-1829) Support BigQuery schema updates
 as a side effect of a query job","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1829?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
257508#comment-16257508 ]=20

Guillermo Rodr=C3=ADguez Cano commented on AIRFLOW-1829:
---------------------------------------------------

GitHub''s pull request solving the issue: https://github.com/apache/incubato=
r-airflow/pull/2796

> Support BigQuery schema updates as a side effect of a query job
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1829
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1829
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, hooks, operators
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Critical
>              Labels: easyfix, triaged
>
> BigQuery hook supports schema updates as a side effect of a load job but =
not for query jobs. Respectively GCS to BQ operator (which executes a load =
job) supports such possibility unlike its ''sister'' operator, BQ operator, w=
hen running a query with a table as destination.
> Both operations, load and query, should support such feature (experimenta=
l as of this writing though)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27211,54,JIRA.13119113.1510892095000.269869.1510950540083@Atlassian.JIRA,2392,Snigdha Nair (JIRA),JIRA.13119113.1510892095000@Atlassian.JIRA,,,2017-11-17 12:29:00-08,[jira] [Commented] (AIRFLOW-1825) Set Multi dag dependency,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1825?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16257516#comment-16257516 ] 

Snigdha Nair commented on AIRFLOW-1825:
---------------------------------------

Thanks for the response. I am not clear about the last part ""You could use something like that, or you could have a single DummyOperator like ""all_a_tasks_completed"" which is downstream of all tasks in a, and same goes for b, then you''d only have a single external dependency per dag."" Could you please explain this a bit more? I am not sure how this helps in having a single external dependency.

> Set Multi dag dependency 
> -------------------------
>
>                 Key: AIRFLOW-1825
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1825
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Snigdha Nair
>
> I have 3 dags A, B and C. Dag C should get triggered only after tasks in dag A and B completes. Is there a way to implement this in airflow? I am able to set dependency between dag A and C using Triggerdagrun Operator. But when I try to set dependency between dag B and C, C is getting triggered when either A or B completes. Can someone please help me in solving this? I understand that explains external task sensor Operator can be used. But it continuously polls if task in dag A and B is complete which might create performance hit over a period of time.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27212,54,JIRA.13119113.1510892095000.269935.1510951200559@Atlassian.JIRA,2393,Jonathan Bender (JIRA),JIRA.13119113.1510892095000@Atlassian.JIRA,,,2017-11-17 12:40:00-08,[jira] [Commented] (AIRFLOW-1825) Set Multi dag dependency,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1825?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16257529#comment-16257529 ] 

Jonathan Bender commented on AIRFLOW-1825:
------------------------------------------

In that case you would have two sensors, one for the dummy task in dag A, and one for the dummy task in dag B. If you''re concerned about the performance hit of two external task sensors vs one then I guess my other suggestion would be better!

> Set Multi dag dependency 
> -------------------------
>
>                 Key: AIRFLOW-1825
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1825
>             Project: Apache Airflow
>          Issue Type: Task
>            Reporter: Snigdha Nair
>
> I have 3 dags A, B and C. Dag C should get triggered only after tasks in dag A and B completes. Is there a way to implement this in airflow? I am able to set dependency between dag A and C using Triggerdagrun Operator. But when I try to set dependency between dag B and C, C is getting triggered when either A or B completes. Can someone please help me in solving this? I understand that explains external task sensor Operator can be used. But it continuously polls if task in dag A and B is complete which might create performance hit over a period of time.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27213,54,JIRA.13119325.1510952184000.270087.1510952220731@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13119325.1510952184000@Atlassian.JIRA,,,2017-11-17 12:57:00-08,"[jira] [Created] (AIRFLOW-1830) Support multiple domains in
 Google''s authentication backend","Guillermo Rodr=C3=ADguez Cano created AIRFLOW-1830:
-------------------------------------------------

             Summary: Support multiple domains in Google''s authentication b=
ackend
                 Key: AIRFLOW-1830
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1830
             Project: Apache Airflow
          Issue Type: Improvement
          Components: authentication, contrib
            Reporter: Guillermo Rodr=C3=ADguez Cano
            Priority: Minor


Google''s OAuth authentication backend supports only one domain but multiple=
 may be needed in some scenarios



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27214,54,JIRA.13119325.1510952184000.270137.1510952520917@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13119325.1510952184000@Atlassian.JIRA,,,2017-11-17 13:02:00-08,"[jira] [Commented] (AIRFLOW-1830) Support multiple domains in
 Google''s authentication backend","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1830?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
257565#comment-16257565 ]=20

Guillermo Rodr=C3=ADguez Cano commented on AIRFLOW-1830:
---------------------------------------------------

GitHub''s pull request addressing the issue: https://github.com/apache/incub=
ator-airflow/pull/2797

> Support multiple domains in Google''s authentication backend
> -----------------------------------------------------------
>
>                 Key: AIRFLOW-1830
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1830
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: authentication, contrib
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Minor
>              Labels: easyfix, triaged
>
> Google''s OAuth authentication backend supports only one domain but multip=
le may be needed in some scenarios



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27215,54,JIRA.13060704.1490973212000.271842.1510964400649@Atlassian.JIRA,2394,Jacopo Farina (JIRA),JIRA.13060704.1490973212000@Atlassian.JIRA,,,2017-11-17 16:20:00-08,"[jira] [Commented] (AIRFLOW-1061) Log URL in Task Instance list is
 incorrect when running locally","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1061?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16257797#comment-16257797 ] 

Jacopo Farina commented on AIRFLOW-1061:
----------------------------------------

The same thing happens when I run it locally and the port is 8090 instead of the default 8080. All the other links on the task execution list works, but the log one refers to the port 8080. If I manually edit the link to use 8080 it works again

> Log URL in Task Instance list is incorrect when running locally
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1061
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1061
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: Nicholas Duffy
>
> The Log URL in the Task Instance list links to BASE_URL + /admin/airflow/log (https://github.com/apache/incubator-airflow/blob/d8c0f59d5d627efb65b47264d5169e3626195839/airflow/models.py#L936) while it should link to only /admin/airflow/log.
> The current behavior causes the link to be incorrect if you are running Airflow locally. 
> Example:
> - Running Airflow on Vagrant on my local machine
> - URL is http://127.0.0.1:8080
> - Browse to task instance list at http://127.0.0.1:8080/admin/taskinstance/
> - Click the Log URL book glyphicon
> - The link takes you to http://mybaseurl.com/admin/airflow/log/
> - I expect the link to go to http://127.0.0.1:8080/admin/airflow/log/



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27216,54,JIRA.13060704.1490973212000.271970.1510965782412@Atlassian.JIRA,2394,Jacopo Farina (JIRA),JIRA.13060704.1490973212000@Atlassian.JIRA,,,2017-11-17 16:43:02-08,"[jira] [Commented] (AIRFLOW-1061) Log URL in Task Instance list is
 incorrect when running locally","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1061?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16257819#comment-16257819 ] 

Jacopo Farina commented on AIRFLOW-1061:
----------------------------------------

Found that a simple solution is to remove the BASE_URL from that line and have:

{code:python}
    @property
    def log_url(self):
        iso = self.execution_date.isoformat()
        return (
            ""/admin/airflow/log""
            ""?dag_id={self.dag_id}""
            ""&task_id={self.task_id}""
            ""&execution_date={iso}""
        ).format(**locals())
{code}

this works on any host and port because the server uses the same base path of the page.

> Log URL in Task Instance list is incorrect when running locally
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1061
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1061
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: Nicholas Duffy
>
> The Log URL in the Task Instance list links to BASE_URL + /admin/airflow/log (https://github.com/apache/incubator-airflow/blob/d8c0f59d5d627efb65b47264d5169e3626195839/airflow/models.py#L936) while it should link to only /admin/airflow/log.
> The current behavior causes the link to be incorrect if you are running Airflow locally. 
> Example:
> - Running Airflow on Vagrant on my local machine
> - URL is http://127.0.0.1:8080
> - Browse to task instance list at http://127.0.0.1:8080/admin/taskinstance/
> - Click the Log URL book glyphicon
> - The link takes you to http://mybaseurl.com/admin/airflow/log/
> - I expect the link to go to http://127.0.0.1:8080/admin/airflow/log/



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27217,54,JIRA.13117222.1510223865000.273336.1511010547659@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-18 05:09:07-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16258057#comment-16258057 ] 

ASF subversion and git services commented on AIRFLOW-1795:
----------------------------------------------------------

Commit 98df0d6e3b2e2b439ab46d6c9ba736777202414a in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=98df0d6 ]

[AIRFLOW-1795] Correctly call S3Hook after migration to boto3

In the migration of S3Hook to boto3 the connection
ID parameter changed
to `aws_conn_id`. This fixes the uses of
`s3_conn_id` in the code base
and adds a note to UPDATING.md about the change.

In correcting the tests for S3ToHiveTransfer I
noticed that
S3Hook.get_key was returning a dictionary, rather
then the S3.Object as
mentioned in it''s doc string. The important thing
that was missing was
ability to get the key name from the return a call
to get_wildcard_key.

Closes #2795 from
ashb/AIRFLOW-1795-s3hook_boto3_fixes


> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27218,54,JIRA.13117222.1510223865000.273337.1511010547669@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-18 05:09:07-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16258058#comment-16258058 ] 

ASF subversion and git services commented on AIRFLOW-1795:
----------------------------------------------------------

Commit 98df0d6e3b2e2b439ab46d6c9ba736777202414a in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=98df0d6 ]

[AIRFLOW-1795] Correctly call S3Hook after migration to boto3

In the migration of S3Hook to boto3 the connection
ID parameter changed
to `aws_conn_id`. This fixes the uses of
`s3_conn_id` in the code base
and adds a note to UPDATING.md about the change.

In correcting the tests for S3ToHiveTransfer I
noticed that
S3Hook.get_key was returning a dictionary, rather
then the S3.Object as
mentioned in it''s doc string. The important thing
that was missing was
ability to get the key name from the return a call
to get_wildcard_key.

Closes #2795 from
ashb/AIRFLOW-1795-s3hook_boto3_fixes


> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27219,54,JIRA.13117222.1510223865000.273355.1511010547832@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-18 05:09:07-08,"[jira] [Resolved] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1795.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2795
[https://github.com/apache/incubator-airflow/pull/2795]

> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27220,54,JIRA.13117222.1510223865000.273348.1511010547769@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-18 05:09:07-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16258061#comment-16258061 ] 

ASF subversion and git services commented on AIRFLOW-1795:
----------------------------------------------------------

Commit ed4390138de7889cb4e9b15eb0e0908e4584db79 in incubator-airflow''s branch refs/heads/v1-9-stable from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=ed43901 ]

[AIRFLOW-1795] Correctly call S3Hook after migration to boto3

In the migration of S3Hook to boto3 the connection
ID parameter changed
to `aws_conn_id`. This fixes the uses of
`s3_conn_id` in the code base
and adds a note to UPDATING.md about the change.

In correcting the tests for S3ToHiveTransfer I
noticed that
S3Hook.get_key was returning a dictionary, rather
then the S3.Object as
mentioned in it''s doc string. The important thing
that was missing was
ability to get the key name from the return a call
to get_wildcard_key.

Closes #2795 from
ashb/AIRFLOW-1795-s3hook_boto3_fixes

(cherry picked from commit 98df0d6e3b2e2b439ab46d6c9ba736777202414a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27221,54,JIRA.13117222.1510223865000.273351.1511010547798@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-18 05:09:07-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16258062#comment-16258062 ] 

ASF subversion and git services commented on AIRFLOW-1795:
----------------------------------------------------------

Commit ed4390138de7889cb4e9b15eb0e0908e4584db79 in incubator-airflow''s branch refs/heads/v1-9-stable from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=ed43901 ]

[AIRFLOW-1795] Correctly call S3Hook after migration to boto3

In the migration of S3Hook to boto3 the connection
ID parameter changed
to `aws_conn_id`. This fixes the uses of
`s3_conn_id` in the code base
and adds a note to UPDATING.md about the change.

In correcting the tests for S3ToHiveTransfer I
noticed that
S3Hook.get_key was returning a dictionary, rather
then the S3.Object as
mentioned in it''s doc string. The important thing
that was missing was
ability to get the key name from the return a call
to get_wildcard_key.

Closes #2795 from
ashb/AIRFLOW-1795-s3hook_boto3_fixes

(cherry picked from commit 98df0d6e3b2e2b439ab46d6c9ba736777202414a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27222,54,JIRA.13117222.1510223865000.273343.1511010547724@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-18 05:09:07-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16258060#comment-16258060 ] 

ASF subversion and git services commented on AIRFLOW-1795:
----------------------------------------------------------

Commit aa50d91722673aae9dbc88b08551cd5ff9bd1b9a in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=aa50d91 ]

[AIRFLOW-1795] Correctly call S3Hook after migration to boto3

In the migration of S3Hook to boto3 the connection
ID parameter changed
to `aws_conn_id`. This fixes the uses of
`s3_conn_id` in the code base
and adds a note to UPDATING.md about the change.

In correcting the tests for S3ToHiveTransfer I
noticed that
S3Hook.get_key was returning a dictionary, rather
then the S3.Object as
mentioned in it''s doc string. The important thing
that was missing was
ability to get the key name from the return a call
to get_wildcard_key.

Closes #2795 from
ashb/AIRFLOW-1795-s3hook_boto3_fixes

(cherry picked from commit 98df0d6e3b2e2b439ab46d6c9ba736777202414a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27223,54,JIRA.13117222.1510223865000.273342.1511010547711@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117222.1510223865000@Atlassian.JIRA,,,2017-11-18 05:09:07-08,"[jira] [Commented] (AIRFLOW-1795) S3Hook no longer accepts
 s3_conn_id breaking build in ops/sensors and back-compat","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1795?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16258059#comment-16258059 ] 

ASF subversion and git services commented on AIRFLOW-1795:
----------------------------------------------------------

Commit aa50d91722673aae9dbc88b08551cd5ff9bd1b9a in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=aa50d91 ]

[AIRFLOW-1795] Correctly call S3Hook after migration to boto3

In the migration of S3Hook to boto3 the connection
ID parameter changed
to `aws_conn_id`. This fixes the uses of
`s3_conn_id` in the code base
and adds a note to UPDATING.md about the change.

In correcting the tests for S3ToHiveTransfer I
noticed that
S3Hook.get_key was returning a dictionary, rather
then the S3.Object as
mentioned in it''s doc string. The important thing
that was missing was
ability to get the key name from the return a call
to get_wildcard_key.

Closes #2795 from
ashb/AIRFLOW-1795-s3hook_boto3_fixes

(cherry picked from commit 98df0d6e3b2e2b439ab46d6c9ba736777202414a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook no longer accepts s3_conn_id breaking build in ops/sensors and back-compat 
> ----------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1795
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1795
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> Found whilst testing Airflow 1.9.0rc1
> Previously the S3Hook accepted a parameter of {{s3_conn_id}}. As part of AIRFLOW-1520 we moved S3Hook to have a superclass of AWSHook, which accepts a {{aws_conn_id}} parameter instead.
> This break back-compat generally, and more specifically it breaks the built in S3KeySensor which does this:
> {code}
>     def poke(self, context):
>         from airflow.hooks.S3_hook import S3Hook
>         hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {code}
> There are a few other instances of s3_conn_id in the code base that will also probably need updating/tweaking.
> My first though was to add a shim mapping s3_conn_id to aws_conn_id in the S3Hook with a deprecation warning but the surface area with places where this is exposed is larger. I could add such a deprecation warning to all of these. Anyone have thoughts as to best way?
> - Rename all instances with deprecation warnings.
> - S3Hook accepts {{s3_conn_id}} and passes down to {{aws_conn_id}} in superclass.
> - Update existing references in code base to {{aws_conn_id}}, and not in updating about need to update in user code. (This is my least preferred option.)
> {noformat}
> airflow/operators/redshift_to_s3_operator.py
> 33:    :param s3_conn_id: reference to a specific S3 connection
> 34:    :type s3_conn_id: string
> 51:            s3_conn_id=''s3_default'',
> 62:        self.s3_conn_id = s3_conn_id
> 69:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/s3_file_transform_operator.py
> 40:    :param source_s3_conn_id: source s3 connection
> 41:    :type source_s3_conn_id: str
> 44:    :param dest_s3_conn_id: destination s3 connection
> 45:    :type dest_s3_conn_id: str
> 62:            source_s3_conn_id=''s3_default'',
> 63:            dest_s3_conn_id=''s3_default'',
> 68:        self.source_s3_conn_id = source_s3_conn_id
> 70:        self.dest_s3_conn_id = dest_s3_conn_id
> 75:        source_s3 = S3Hook(s3_conn_id=self.source_s3_conn_id)
> 76:        dest_s3 = S3Hook(s3_conn_id=self.dest_s3_conn_id)
> airflow/operators/s3_to_hive_operator.py
> 74:    :param s3_conn_id: source s3 connection
> 75:    :type s3_conn_id: str
> 102:            s3_conn_id=''s3_default'',
> 119:        self.s3_conn_id = s3_conn_id
> 130:        self.s3 = S3Hook(s3_conn_id=self.s3_conn_id)
> airflow/operators/sensors.py
> 504:    :param s3_conn_id: a reference to the s3 connection
> 505:    :type s3_conn_id: str
> 514:            s3_conn_id=''s3_default'',
> 531:        self.s3_conn_id = s3_conn_id
> 535:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> 568:            s3_conn_id=''s3_default'',
> 576:        self.s3_conn_id = s3_conn_id
> 582:        hook = S3Hook(s3_conn_id=self.s3_conn_id)
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27224,54,JIRA.13119675.1511188012000.280521.1511188020038@Atlassian.JIRA,1449,Daniel van der Ende (JIRA),JIRA.13119675.1511188012000@Atlassian.JIRA,,,2017-11-20 06:27:00-08,"[jira] [Created] (AIRFLOW-1831) Add Spark submit driver classpath
 parameter","Daniel van der Ende created AIRFLOW-1831:
--------------------------------------------

             Summary: Add Spark submit driver classpath parameter
                 Key: AIRFLOW-1831
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1831
             Project: Apache Airflow
          Issue Type: Improvement
          Components: hooks, operators
            Reporter: Daniel van der Ende
            Assignee: Daniel van der Ende
            Priority: Minor


Spark allows you to set the classpath for the driver only. This can save overhead, if you only need a specific jar on the driver, for instance. The Airflow spark_submit_operator/spark_submit_hook do not support this. This should be added.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27225,54,JIRA.13119688.1511191846000.280882.1511191860038@Atlassian.JIRA,2395,Michelle Huang (JIRA),JIRA.13119688.1511191846000@Atlassian.JIRA,,,2017-11-20 07:31:00-08,[jira] [Created] (AIRFLOW-1832) xcom_push is not reliable,"Michelle Huang created AIRFLOW-1832:
---------------------------------------

             Summary: xcom_push is not reliable
                 Key: AIRFLOW-1832
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1832
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Michelle Huang


we have a few ETL jobs that are hitting a third party API, extracting data onto the Airflow server and then transforming and inserting that data into Redshift. we''re using xcom_push in the extract function to store an identifier appended to the filename to be referenced in the transform function so we can grab the right file to transform. I''m noticing that xcom_push is not successfully pushing the value and xcom_pull is returning ""None"". this is happening across many of our scheduled jobs regularly and only just started happening on 11/16. manual triggers of the jobs run successfully to completion without issue. this only affects our scheduled runs and doesn''t happen to all of them.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27226,54,JIRA.13119705.1511194738000.281229.1511194740197@Atlassian.JIRA,1924,Connor Ameres (JIRA),JIRA.13119705.1511194738000@Atlassian.JIRA,,,2017-11-20 08:19:00-08,"[jira] [Created] (AIRFLOW-1833) Airflow ''retries'' parameter not
 being honored with CeleryExecutor","Connor Ameres created AIRFLOW-1833:
--------------------------------------

             Summary: Airflow ''retries'' parameter not being honored with CeleryExecutor
                 Key: AIRFLOW-1833
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1833
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Connor Ameres
         Attachments: Screen Shot 2017-11-20 at 11.13.04 AM.png, Screen Shot 2017-11-20 at 11.13.27 AM.png

I''ve noticed this for a few task_instances that end up being retried more times than the ''retries'' parameter that is passed to the constructor of the DAG in the default_args dictionary. I''ve attached the task instance attributes & task attributes sections to highlight this. 

Note: 
- retries: 1
- try_number: 6

We''re using docker containers w/ https://github.com/puckel/docker-airflow & the following versions of python and packages...

- python 2.7.9
- apache-airflow==1.8.1
- celery==3.1.17





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27227,54,JIRA.13119705.1511194738000.281234.1511194800114@Atlassian.JIRA,1924,Connor Ameres (JIRA),JIRA.13119705.1511194738000@Atlassian.JIRA,,,2017-11-20 08:20:00-08,"[jira] [Updated] (AIRFLOW-1833) Airflow ''retries'' parameter not
 being honored with CeleryExecutor","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1833?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Connor Ameres updated AIRFLOW-1833:
-----------------------------------
    Affects Version/s: 1.8.1

> Airflow ''retries'' parameter not being honored with CeleryExecutor
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1833
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1833
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.1
>            Reporter: Connor Ameres
>         Attachments: Screen Shot 2017-11-20 at 11.13.04 AM.png, Screen Shot 2017-11-20 at 11.13.27 AM.png
>
>
> I''ve noticed this for a few task_instances that end up being retried more times than the ''retries'' parameter that is passed to the constructor of the DAG in the default_args dictionary. I''ve attached the task instance attributes & task attributes sections to highlight this. 
> Note: 
> - retries: 1
> - try_number: 6
> We''re using docker containers w/ https://github.com/puckel/docker-airflow & the following versions of python and packages...
> - python 2.7.9
> - apache-airflow==1.8.1
> - celery==3.1.17



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27228,54,JIRA.13119756.1511206944000.282716.1511206980356@Atlassian.JIRA,2280,=?utf-8?Q?Christoph_H=C3=B6sler_=28JIRA=29?=,JIRA.13119756.1511206944000@Atlassian.JIRA,,,2017-11-20 11:43:00-08,"[jira] [Created] (AIRFLOW-1834) Unfold SubDAGs in Graph- and
 Tree-View","Christoph H=C3=B6sler created AIRFLOW-1834:
-----------------------------------------

             Summary: Unfold SubDAGs in Graph- and Tree-View
                 Key: AIRFLOW-1834
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1834
             Project: Apache Airflow
          Issue Type: Improvement
          Components: ui
    Affects Versions: Airflow 1.8
            Reporter: Christoph H=C3=B6sler


If one has a DAG with multiple nested SubDAGs, it is cumbersome to zoom int=
o each SubDAG to view its tasks. It would be helpful to ""unfold"" a SubDAG O=
perator and show its subtasks as part of the current DAG.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27229,54,JIRA.13119756.1511206944000.282728.1511207040599@Atlassian.JIRA,2280,=?utf-8?Q?Christoph_H=C3=B6sler_=28JIRA=29?=,JIRA.13119756.1511206944000@Atlassian.JIRA,,,2017-11-20 11:44:00-08,"[jira] [Updated] (AIRFLOW-1834) Unfold SubDAGs in Graph- and
 Tree-View","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1834?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Christoph H=C3=B6sler updated AIRFLOW-1834:
--------------------------------------
    Description: If one has a DAG with multiple nested SubDAGs, it is cumbe=
rsome with the current UI to zoom into each SubDAG to view its tasks. It wo=
uld be helpful to ""unfold"" a SubDAG Operator and show its subtasks as part =
of the current DAG.  (was: If one has a DAG with multiple nested SubDAGs, i=
t is cumbersome to zoom into each SubDAG to view its tasks. It would be hel=
pful to ""unfold"" a SubDAG Operator and show its subtasks as part of the cur=
rent DAG.)

> Unfold SubDAGs in Graph- and Tree-View
> --------------------------------------
>
>                 Key: AIRFLOW-1834
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1834
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: ui
>    Affects Versions: Airflow 1.8
>            Reporter: Christoph H=C3=B6sler
>
> If one has a DAG with multiple nested SubDAGs, it is cumbersome with the =
current UI to zoom into each SubDAG to view its tasks. It would be helpful =
to ""unfold"" a SubDAG Operator and show its subtasks as part of the current =
DAG.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27230,54,JIRA.13073389.1495190402000.283293.1511212020151@Atlassian.JIRA,1851,Alexander Bij (JIRA),JIRA.13073389.1495190402000@Atlassian.JIRA,,,2017-11-20 13:07:00-08,"[jira] [Assigned] (AIRFLOW-1229) Make ""Run Id"" column clickable in
 Browse -> DAG Runs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1229?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alexander Bij reassigned AIRFLOW-1229:
--------------------------------------

    Assignee: Alexander Bij

> Make ""Run Id"" column clickable in Browse -> DAG Runs
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1229
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1229
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webapp
>    Affects Versions: Airflow 1.8
>         Environment: Python3.4
>            Reporter: Erik Cederstrand
>            Assignee: Alexander Bij
>              Labels: patch
>         Attachments: dag_run_link.patch
>
>
> I''m triggering a lot of DAGs manually using ""airflow trigger_dag my_dag --run_id=some_unique_id"". I would like to be able in the UI to browse easily to this specific DAG run using the ""some_unique_id"" label. In the graph page of the DAG, I need to know the exact execution date, which is inconvenient, and in the Browse -> DAG Runs page I can search by ""some_unique_id"", but the ""Run Ids"" column is not clickable.
> The attached patch makes the aforementioned column clickable, so I''m sent directly to the graph view for that specific DAG run, not the DAG in general.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27231,54,JIRA.13073389.1495190402000.283310.1511212200209@Atlassian.JIRA,1851,Alexander Bij (JIRA),JIRA.13073389.1495190402000@Atlassian.JIRA,,,2017-11-20 13:10:00-08,"[jira] [Commented] (AIRFLOW-1229) Make ""Run Id"" column clickable in
 Browse -> DAG Runs","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1229?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16259852#comment-16259852 ] 

Alexander Bij commented on AIRFLOW-1229:
----------------------------------------

I was working on the same issue and created a pull-request on Airflow.
And i''ve added link to a the run-id which I found in your patch attachment.

> Make ""Run Id"" column clickable in Browse -> DAG Runs
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1229
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1229
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webapp
>    Affects Versions: Airflow 1.8
>         Environment: Python3.4
>            Reporter: Erik Cederstrand
>            Assignee: Alexander Bij
>              Labels: patch
>         Attachments: dag_run_link.patch
>
>
> I''m triggering a lot of DAGs manually using ""airflow trigger_dag my_dag --run_id=some_unique_id"". I would like to be able in the UI to browse easily to this specific DAG run using the ""some_unique_id"" label. In the graph page of the DAG, I need to know the exact execution date, which is inconvenient, and in the Browse -> DAG Runs page I can search by ""some_unique_id"", but the ""Run Ids"" column is not clickable.
> The attached patch makes the aforementioned column clickable, so I''m sent directly to the graph view for that specific DAG run, not the DAG in general.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27232,54,JIRA.13119808.1511224978000.284920.1511224980234@Atlassian.JIRA,2396,Bovard Doerschuk-Tiberi (JIRA),JIRA.13119808.1511224978000@Atlassian.JIRA,,,2017-11-20 16:43:00-08,"[jira] [Created] (AIRFLOW-1835) Documentation doesn''t have file
 format for variable file uploads","Bovard Doerschuk-Tiberi created AIRFLOW-1835:
------------------------------------------------

             Summary: Documentation doesn''t have file format for variable file uploads
                 Key: AIRFLOW-1835
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1835
             Project: Apache Airflow
          Issue Type: Bug
          Components: Documentation
            Reporter: Bovard Doerschuk-Tiberi
            Assignee: Bovard Doerschuk-Tiberi
            Priority: Trivial


Currently the documentation doesn''t tell you that to upload settings you need a json file.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27233,54,JIRA.13119325.1510952184000.286355.1511246940073@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13119325.1510952184000@Atlassian.JIRA,,,2017-11-20 22:49:00-08,"[jira] [Commented] (AIRFLOW-1830) Support multiple domains in
 Google''s authentication backend","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1830?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
260339#comment-16260339 ]=20

ASF subversion and git services commented on AIRFLOW-1830:
----------------------------------------------------------

Commit 6b1ceff7d2e1f50c6c2f4a7b27d4026b40d0c77d in incubator-airflow''s bran=
ch refs/heads/master from [~wileeam]
[ https://git-wip-us.apache.org/repos/asf?p=3Dincubator-airflow.git;h=3D6b1=
ceff ]

[AIRFLOW-1830] Support multiple domains in Google authentication backend

Closes #2797 from wileeam/multiple-domains-google-
auth


> Support multiple domains in Google''s authentication backend
> -----------------------------------------------------------
>
>                 Key: AIRFLOW-1830
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1830
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: authentication, contrib
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Priority: Minor
>              Labels: easyfix, triaged
>
> Google''s OAuth authentication backend supports only one domain but multip=
le may be needed in some scenarios



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27234,54,JIRA.13119851.1511248361000.286521.1511248380029@Atlassian.JIRA,2398,Kevin Zhang (JIRA),JIRA.13119851.1511248361000@Atlassian.JIRA,,,2017-11-20 23:13:00-08,"[jira] [Created] (AIRFLOW-1836) Airflow select keycloak as OAuth
 Provider","Kevin Zhang created AIRFLOW-1836:
------------------------------------

             Summary: Airflow select keycloak as OAuth Provider
                 Key: AIRFLOW-1836
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1836
             Project: Apache Airflow
          Issue Type: Improvement
          Components: contrib
    Affects Versions: 1.8.2
         Environment: Linux CentOS 7.3
Python 2.7.13
            Reporter: Kevin Zhang
             Fix For: 1.8.2


As the need of my project in hand, I had to integrate keycloak with airflow=
.During implemetation=EF=BC=8CI learnt from the github_enterprise_auth prov=
ided by airflow.When I found it used python module flask_oauthlib=EF=BC=8CI=
 thought the fastest way for me was to modify the parameters about connecti=
ng to keycloak.So I tried the idea and found it''s true.The next question is=
 to resolve the token from keycloak. I referred to keycloak and decided two=
 key -preferred_username and email.

In my environment, the keycloak is accessed by http.So I had to add the cod=
e
{code:python}
 import os
os.environ[''OAUTHLIB_INSECURE_TRANSPORT''] =3D ''1''
{code}




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27235,54,JIRA.13119851.1511248361000.286522.1511248500417@Atlassian.JIRA,2398,Kevin Zhang (JIRA),JIRA.13119851.1511248361000@Atlassian.JIRA,,,2017-11-20 23:15:00-08,"[jira] [Updated] (AIRFLOW-1836) Airflow select keycloak as OAuth
 Provider","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1836?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kevin Zhang updated AIRFLOW-1836:
---------------------------------
    Description:=20
As the need of my project in hand, I had to integrate keycloak with airflow=
.During implemetation=EF=BC=8CI learnt from the github_enterprise_auth prov=
ided by airflow.When I found it used python module flask_oauthlib=EF=BC=8CI=
 thought the fastest way for me was to modify the parameters about connecti=
ng to keycloak.So I tried the idea and found it''s true.The next question is=
 to resolve the token from keycloak. I referred to keycloak and decided two=
 key -preferred_username and email.

In my environment, the keycloak is accessed by http.So I had to add the cod=
e
{code:java}
 import os
os.environ[''OAUTHLIB_INSECURE_TRANSPORT''] =3D ''1''
{code}


  was:
As the need of my project in hand, I had to integrate keycloak with airflow=
.During implemetation=EF=BC=8CI learnt from the github_enterprise_auth prov=
ided by airflow.When I found it used python module flask_oauthlib=EF=BC=8CI=
 thought the fastest way for me was to modify the parameters about connecti=
ng to keycloak.So I tried the idea and found it''s true.The next question is=
 to resolve the token from keycloak. I referred to keycloak and decided two=
 key -preferred_username and email.

In my environment, the keycloak is accessed by http.So I had to add the cod=
e
{code:python}
 import os
os.environ[''OAUTHLIB_INSECURE_TRANSPORT''] =3D ''1''
{code}



> Airflow select keycloak as OAuth Provider
> -----------------------------------------
>
>                 Key: AIRFLOW-1836
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1836
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib
>    Affects Versions: 1.8.2
>         Environment: Linux CentOS 7.3
> Python 2.7.13
>            Reporter: Kevin Zhang
>             Fix For: 1.8.2
>
>
> As the need of my project in hand, I had to integrate keycloak with airfl=
ow.During implemetation=EF=BC=8CI learnt from the github_enterprise_auth pr=
ovided by airflow.When I found it used python module flask_oauthlib=EF=BC=
=8CI thought the fastest way for me was to modify the parameters about conn=
ecting to keycloak.So I tried the idea and found it''s true.The next questio=
n is to resolve the token from keycloak. I referred to keycloak and decided=
 two key -preferred_username and email.
> In my environment, the keycloak is accessed by http.So I had to add the c=
ode
> {code:java}
>  import os
> os.environ[''OAUTHLIB_INSECURE_TRANSPORT''] =3D ''1''
> {code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27236,54,JIRA.13119930.1511269553000.288496.1511269561290@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13119930.1511269553000@Atlassian.JIRA,,,2017-11-21 05:06:01-08,"[jira] [Created] (AIRFLOW-1837) Differing start_dates on tasks not
 respected by scheduler.","Ash Berlin-Taylor created AIRFLOW-1837:
------------------------------------------

             Summary: Differing start_dates on tasks not respected by scheduler.
                 Key: AIRFLOW-1837
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1837
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.9.0
            Reporter: Ash Berlin-Taylor


It it possible to specify start_date directly on tasks in dag, as well as on the DAG. This is correctly handled when creating dag runs, but it is seemingly ignored when scheduling tasks.

Given this example:

{code}
dag_args = {
    ""start_date"": datetime(2017, 9, 4),
}
dag = DAG(
    ""my-dag"",
    default_args=dag_args,
    schedule_interval=""0 0 * * Mon"",
)

# ...
with dag:
        op = PythonOperator(
            python_callable=fetcher.run,
            task_id=""fetch_all_respondents"",
            provide_context=True,
            # The ""unfiltered"" API calls are a lot quicker, so lets put them
            # ahead of any other filtered job in the queue.
            priority_weight=10,
            start_date=datetime(2014, 9, 1),
        )

        op = PythonOperator(
            python_callable=fetcher.run,
            task_id=""fetch_by_demographics"",
            op_kwargs={
                ''demo_names'': demo_names,
            },
            provide_context=True,
            priority_weight=5,
        )
{code}

I only want the fetch_all_respondents tasks to run for 2014..2017, and then from September 2017 I also want the fetch_by_demographics task to run. However right now both tasks are being scheduled from 2014-09-01.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27237,54,JIRA.13111746.1508863133000.288837.1511272020207@Atlassian.JIRA,2399,=?utf-8?Q?Patryk_Kobyli=C5=84ski_=28JIRA=29?=,JIRA.13111746.1508863133000@Atlassian.JIRA,,,2017-11-21 05:47:00-08,[jira] [Commented] (AIRFLOW-1753) Can''t install on windows 10,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1753?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
260746#comment-16260746 ]=20

Patryk Kobyli=C5=84ski commented on AIRFLOW-1753:
--------------------------------------------

Currently I have the same error although to this character has evolved for =
4 days (!). Previously, there were dozens of others I fought with. Is your =
product compatible with Windows10 at all? Because I already found the opini=
on that it is not. If so, please be kind enough to describe the different m=
ethods of solving errors in your pitiful documentation. No words in them!

> Can''t install on windows 10
> ---------------------------
>
>                 Key: AIRFLOW-1753
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1753
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>            Reporter: Lakshman Udayakantha
>
> When I installed airflow using ""pip install airflow command"" two errors p=
op up.
> 1.  link.exe failed with exit status 1158
> 2.\x86_amd64\\cl.exe'' failed with exit status 2
> first issue can be solved by reffering https://stackoverflow.com/question=
s/43858836/python-installing-clarifai-vs14-0-link-exe-failed-with-exit-stat=
us-1158/44563421#44563421.
> But second issue is still there. there was no any solution by googling al=
so. how to prevent that issue and install airflow on windows 10 X64.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27238,54,JIRA.13119958.1511274032000.289030.1511274060695@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13119958.1511274032000@Atlassian.JIRA,,,2017-11-21 06:21:00-08,[jira] [Created] (AIRFLOW-1838) Properly log collect_dags Exception,"Fokko Driesprong created AIRFLOW-1838:
-----------------------------------------

             Summary: Properly log collect_dags Exception
                 Key: AIRFLOW-1838
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1838
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Fokko Driesprong


As a user it would be nice to properly log the exceptions thrown in the collect_dags function to debug the faulty dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27239,54,JIRA.13111746.1508863133000.289379.1511276580242@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13111746.1508863133000@Atlassian.JIRA,,,2017-11-21 07:03:00-08,[jira] [Commented] (AIRFLOW-1753) Can''t install on windows 10,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1753?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16260862#comment-16260862 ] 

Ash Berlin-Taylor commented on AIRFLOW-1753:
--------------------------------------------

This is somewhat unrelated to airflow -- one of the python modules we depend upon needs to compile something, and python isn''t properly configured to find the toolchain.

The full output would include the name of the module that is being installed at the time of the error, but mire generally: look for a guide about installing python on Windows.

> Can''t install on windows 10
> ---------------------------
>
>                 Key: AIRFLOW-1753
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1753
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.0
>            Reporter: Lakshman Udayakantha
>
> When I installed airflow using ""pip install airflow command"" two errors pop up.
> 1.  link.exe failed with exit status 1158
> 2.\x86_amd64\\cl.exe'' failed with exit status 2
> first issue can be solved by reffering https://stackoverflow.com/questions/43858836/python-installing-clarifai-vs14-0-link-exe-failed-with-exit-status-1158/44563421#44563421.
> But second issue is still there. there was no any solution by googling also. how to prevent that issue and install airflow on windows 10 X64.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27240,54,JIRA.13040668.1486396038000.289895.1511279820118@Atlassian.JIRA,2400,Ross Donaldson (JIRA),JIRA.13040668.1486396038000@Atlassian.JIRA,,,2017-11-21 07:57:00-08,[jira] [Commented] (AIRFLOW-843) Store task exceptions in context,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-843?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16260940#comment-16260940 ] 

Ross Donaldson commented on AIRFLOW-843:
----------------------------------------

I''d like to volunteer to fix this. I can see two perfectly lovely approaches: 

1. Attach the `error` object in to the `context` dict `on_failure_callback` receives.
2. Modify the signature to `on_failure_callback` to accept the error as a (potentially optional) second argument. 

Any preferences? Otherwise, I have a gentle preference for #2, and will open a PR. 

> Store task exceptions in context
> --------------------------------
>
>                 Key: AIRFLOW-843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-843
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Scott Kruger
>            Priority: Minor
>
> If a task encounters an exception during execution, it should store the exception on the execution context so that other methods (namely `on_failure_callback` can access it.  This would help with custom error integrations, e.g. Sentry.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27241,54,JIRA.13040668.1486396038000.289937.1511280120880@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13040668.1486396038000@Atlassian.JIRA,,,2017-11-21 08:02:00-08,[jira] [Commented] (AIRFLOW-843) Store task exceptions in context,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-843?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16260949#comment-16260949 ] 

Ash Berlin-Taylor commented on AIRFLOW-843:
-------------------------------------------

 I think 1. is a ""softer"" approach that will mean existing on failure hooks in people''s code bases wont suddenly error (due to signature mismatch)

But we can go with 2. so long as it''s mentioned in updating.md

> Store task exceptions in context
> --------------------------------
>
>                 Key: AIRFLOW-843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-843
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Scott Kruger
>            Priority: Minor
>
> If a task encounters an exception during execution, it should store the exception on the execution context so that other methods (namely `on_failure_callback` can access it.  This would help with custom error integrations, e.g. Sentry.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27242,54,JIRA.13040668.1486396038000.290410.1511282400443@Atlassian.JIRA,1706,Scott Kruger (JIRA),JIRA.13040668.1486396038000@Atlassian.JIRA,,,2017-11-21 08:40:00-08,[jira] [Commented] (AIRFLOW-843) Store task exceptions in context,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-843?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16261015#comment-16261015 ] 

Scott Kruger commented on AIRFLOW-843:
--------------------------------------

I actually have a (stale) PR up for this: https://github.com/apache/incubator-airflow/pull/2135.  Feel free to use all/part of it.

> Store task exceptions in context
> --------------------------------
>
>                 Key: AIRFLOW-843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-843
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Scott Kruger
>            Priority: Minor
>
> If a task encounters an exception during execution, it should store the exception on the execution context so that other methods (namely `on_failure_callback` can access it.  This would help with custom error integrations, e.g. Sentry.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27243,54,JIRA.13007135.1474632750000.290466.1511282880201@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13007135.1474632750000@Atlassian.JIRA,,,2017-11-21 08:48:00-08,"[jira] [Assigned] (AIRFLOW-528) airflow-webserver.service does not
 fork","
     [ https://issues.apache.org/jira/browse/AIRFLOW-528?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Igors Vaitkus reassigned AIRFLOW-528:
-------------------------------------

    Assignee: Igors Vaitkus

> airflow-webserver.service does not fork
> ---------------------------------------
>
>                 Key: AIRFLOW-528
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-528
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>         Environment: Ubuntu 16.04
>            Reporter: Emil Lynge
>            Assignee: Igors Vaitkus
>            Priority: Minor
>              Labels: easyfix
>
> The systemd service for the webserver has Type:forking, but the execute command is missing the --daemon flag.
> This makes the startup fail as systemd never registers the service as active since it never forks.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27244,54,JIRA.13120009.1511285008000.290820.1511285040324@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-21 09:24:00-08,[jira] [Created] (AIRFLOW-1839) S3Hook.list_keys throws exception,"Ash Berlin-Taylor created AIRFLOW-1839:
------------------------------------------

             Summary: S3Hook.list_keys throws exception
                 Key: AIRFLOW-1839
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Ash Berlin-Taylor


{noformat}
      File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
        return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
      File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
        return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
    AttributeError: ''dict'' object has no attribute ''Key''
{noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27245,54,JIRA.13120047.1511293294000.292307.1511293320872@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120047.1511293294000@Atlassian.JIRA,,,2017-11-21 11:42:00-08,[jira] [Created] (AIRFLOW-1840) Fix Celery config,"Fokko Driesprong created AIRFLOW-1840:
-----------------------------------------

             Summary: Fix Celery config
                 Key: AIRFLOW-1840
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1840
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Fokko Driesprong


While configuring the Celery executor I keep running into this problem:

==> /var/log/airflow/scheduler.log <==
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/celery_executor.py"", line 83, in sync
    state = async.state
  File ""/usr/local/lib/python2.7/dist-packages/celery/result.py"", line 394, in state
    return self._get_task_meta()[''status'']
  File ""/usr/local/lib/python2.7/dist-packages/celery/result.py"", line 339, in _get_task_meta
    return self._maybe_set_cache(self.backend.get_task_meta(self.id))
  File ""/usr/local/lib/python2.7/dist-packages/celery/backends/base.py"", line 307, in get_task_meta
    meta = self._get_task_meta_for(task_id)
AttributeError: ''DisabledBackend'' object has no attribute ''_get_task_meta_for''



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27246,54,JIRA.13120047.1511293294000.292308.1511293320881@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120047.1511293294000@Atlassian.JIRA,,,2017-11-21 11:42:00-08,[jira] [Assigned] (AIRFLOW-1840) Fix Celery config,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1840?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong reassigned AIRFLOW-1840:
-----------------------------------------

    Assignee: Fokko Driesprong

> Fix Celery config
> -----------------
>
>                 Key: AIRFLOW-1840
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1840
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Assignee: Fokko Driesprong
>
> While configuring the Celery executor I keep running into this problem:
> ==> /var/log/airflow/scheduler.log <==
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/executors/celery_executor.py"", line 83, in sync
>     state = async.state
>   File ""/usr/local/lib/python2.7/dist-packages/celery/result.py"", line 394, in state
>     return self._get_task_meta()[''status'']
>   File ""/usr/local/lib/python2.7/dist-packages/celery/result.py"", line 339, in _get_task_meta
>     return self._maybe_set_cache(self.backend.get_task_meta(self.id))
>   File ""/usr/local/lib/python2.7/dist-packages/celery/backends/base.py"", line 307, in get_task_meta
>     meta = self._get_task_meta_for(task_id)
> AttributeError: ''DisabledBackend'' object has no attribute ''_get_task_meta_for''



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27247,54,JIRA.13120009.1511285008000.292436.1511294220561@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-21 11:57:00-08,[jira] [Updated] (AIRFLOW-1839) S3Hook.list_keys throws exception,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1839?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor updated AIRFLOW-1839:
---------------------------------------
    External issue URL: https://github.com/apache/incubator-airflow/pull/2805

> S3Hook.list_keys throws exception
> ---------------------------------
>
>                 Key: AIRFLOW-1839
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>     AttributeError: ''dict'' object has no attribute ''Key''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27248,54,JIRA.13120203.1511350073000.298423.1511350080266@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120203.1511350073000@Atlassian.JIRA,,,2017-11-22 03:28:00-08,"[jira] [Created] (AIRFLOW-1841) GoogleCloudStorageDownloadOperator
 filename parameter","Igors Vaitkus created AIRFLOW-1841:
--------------------------------------

             Summary: GoogleCloudStorageDownloadOperator filename parameter
                 Key: AIRFLOW-1841
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1841
             Project: Apache Airflow
          Issue Type: Improvement
          Components: contrib, gcp, operators
    Affects Versions: 1.8.0, 1.9.0
            Reporter: Igors Vaitkus
            Assignee: Igors Vaitkus
            Priority: Trivial


https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/contrib/operators/gcs_download_operator.py

Documentation - ""... If false, the downloaded data will not be stored on the local file system.""
But filename type is a string in the documentation, it should be a union of String and Bool, but better change it to None. And change Hook as well to None because all it does just check the presence of a value in the parameter.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27249,54,JIRA.13120203.1511350073000.298760.1511352840187@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120203.1511350073000@Atlassian.JIRA,,,2017-11-22 04:14:00-08,"[jira] [Work started] (AIRFLOW-1841)
 GoogleCloudStorageDownloadOperator filename parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1841?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1841 started by Igors Vaitkus.
----------------------------------------------
> GoogleCloudStorageDownloadOperator filename parameter
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1841
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1841
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>    Affects Versions: 1.8.0, 1.9.0
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Trivial
>              Labels: documentation
>
> https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/contrib/operators/gcs_download_operator.py
> Documentation - ""... If false, the downloaded data will not be stored on the local file system.""
> But filename type is a string in the documentation, it should be a union of String and Bool, but better change it to None. And change Hook as well to None because all it does just check the presence of a value in the parameter.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27250,54,JIRA.13120009.1511285008000.299519.1511359440466@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-22 06:04:00-08,[jira] [Commented] (AIRFLOW-1839) S3Hook.list_keys throws exception,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1839?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16262555#comment-16262555 ] 

ASF subversion and git services commented on AIRFLOW-1839:
----------------------------------------------------------

Commit 17e01248a1a5ba92675c865a6e5ccfb36603b3a5 in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=17e0124 ]

[AIRFLOW-1839] Fix more bugs in S3Hook boto -> boto3 migration

There were some more bugs as a result of the boto
to boto3 migration
that weren''t covered by existing tests. Now they
are fixed, and covered.
Hopefully I got everything this time.

Closes #2805 from ashb/AIRFLOW-1839-s3
-hook_loadsa-tests

(cherry picked from commit 2d5408935fc41c3d3b6618d8c563d1eecac06561)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook.list_keys throws exception
> ---------------------------------
>
>                 Key: AIRFLOW-1839
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>     AttributeError: ''dict'' object has no attribute ''Key''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27251,54,JIRA.13120009.1511285008000.299516.1511359440437@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-22 06:04:00-08,[jira] [Commented] (AIRFLOW-1839) S3Hook.list_keys throws exception,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1839?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16262554#comment-16262554 ] 

ASF subversion and git services commented on AIRFLOW-1839:
----------------------------------------------------------

Commit 2d5408935fc41c3d3b6618d8c563d1eecac06561 in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2d54089 ]

[AIRFLOW-1839] Fix more bugs in S3Hook boto -> boto3 migration

There were some more bugs as a result of the boto
to boto3 migration
that weren''t covered by existing tests. Now they
are fixed, and covered.
Hopefully I got everything this time.

Closes #2805 from ashb/AIRFLOW-1839-s3
-hook_loadsa-tests


> S3Hook.list_keys throws exception
> ---------------------------------
>
>                 Key: AIRFLOW-1839
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>     AttributeError: ''dict'' object has no attribute ''Key''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27252,54,JIRA.13120009.1511285008000.299521.1511359440486@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-22 06:04:00-08,[jira] [Commented] (AIRFLOW-1839) S3Hook.list_keys throws exception,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1839?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16262556#comment-16262556 ] 

ASF subversion and git services commented on AIRFLOW-1839:
----------------------------------------------------------

Commit 17e01248a1a5ba92675c865a6e5ccfb36603b3a5 in incubator-airflow''s branch refs/heads/v1-9-test from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=17e0124 ]

[AIRFLOW-1839] Fix more bugs in S3Hook boto -> boto3 migration

There were some more bugs as a result of the boto
to boto3 migration
that weren''t covered by existing tests. Now they
are fixed, and covered.
Hopefully I got everything this time.

Closes #2805 from ashb/AIRFLOW-1839-s3
-hook_loadsa-tests

(cherry picked from commit 2d5408935fc41c3d3b6618d8c563d1eecac06561)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook.list_keys throws exception
> ---------------------------------
>
>                 Key: AIRFLOW-1839
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>     AttributeError: ''dict'' object has no attribute ''Key''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27253,54,JIRA.13120009.1511285008000.299512.1511359440375@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-22 06:04:00-08,[jira] [Commented] (AIRFLOW-1839) S3Hook.list_keys throws exception,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1839?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16262552#comment-16262552 ] 

ASF subversion and git services commented on AIRFLOW-1839:
----------------------------------------------------------

Commit 2d5408935fc41c3d3b6618d8c563d1eecac06561 in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2d54089 ]

[AIRFLOW-1839] Fix more bugs in S3Hook boto -> boto3 migration

There were some more bugs as a result of the boto
to boto3 migration
that weren''t covered by existing tests. Now they
are fixed, and covered.
Hopefully I got everything this time.

Closes #2805 from ashb/AIRFLOW-1839-s3
-hook_loadsa-tests


> S3Hook.list_keys throws exception
> ---------------------------------
>
>                 Key: AIRFLOW-1839
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>
> {noformat}
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>     AttributeError: ''dict'' object has no attribute ''Key''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27254,54,JIRA.13120009.1511285008000.299528.1511359500629@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-22 06:05:00-08,[jira] [Commented] (AIRFLOW-1839) S3Hook.list_keys throws exception,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1839?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16262559#comment-16262559 ] 

ASF subversion and git services commented on AIRFLOW-1839:
----------------------------------------------------------

Commit d485453c0a746aea03b7eafdcdd6eb10e8ef1fe2 in incubator-airflow''s branch refs/heads/v1-9-stable from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d485453 ]

[AIRFLOW-1839] Fix more bugs in S3Hook boto -> boto3 migration

There were some more bugs as a result of the boto
to boto3 migration
that weren''t covered by existing tests. Now they
are fixed, and covered.
Hopefully I got everything this time.

Closes #2805 from ashb/AIRFLOW-1839-s3
-hook_loadsa-tests

(cherry picked from commit 2d5408935fc41c3d3b6618d8c563d1eecac06561)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook.list_keys throws exception
> ---------------------------------
>
>                 Key: AIRFLOW-1839
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> {noformat}
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>     AttributeError: ''dict'' object has no attribute ''Key''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27255,54,JIRA.13120009.1511285008000.299531.1511359500648@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-22 06:05:00-08,[jira] [Resolved] (AIRFLOW-1839) S3Hook.list_keys throws exception,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1839?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1839.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2805
[https://github.com/apache/incubator-airflow/pull/2805]

> S3Hook.list_keys throws exception
> ---------------------------------
>
>                 Key: AIRFLOW-1839
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> {noformat}
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>     AttributeError: ''dict'' object has no attribute ''Key''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27256,54,JIRA.13120247.1511359498000.299538.1511359500699@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120247.1511359498000@Atlassian.JIRA,,,2017-11-22 06:05:00-08,"[jira] [Created] (AIRFLOW-1842) Create gcs to gcs operator to copy,
 rename and both","Igors Vaitkus created AIRFLOW-1842:
--------------------------------------

             Summary: Create gcs to gcs operator to copy, rename and both
                 Key: AIRFLOW-1842
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1842
             Project: Apache Airflow
          Issue Type: Improvement
          Components: contrib, gcp, operators
            Reporter: Igors Vaitkus
            Assignee: Igors Vaitkus
            Priority: Minor


Copies an object from a Google Cloud Storage bucket to another, with renaming if requested.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27257,54,JIRA.13120009.1511285008000.299523.1511359500551@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120009.1511285008000@Atlassian.JIRA,,,2017-11-22 06:05:00-08,[jira] [Commented] (AIRFLOW-1839) S3Hook.list_keys throws exception,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1839?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16262557#comment-16262557 ] 

ASF subversion and git services commented on AIRFLOW-1839:
----------------------------------------------------------

Commit d485453c0a746aea03b7eafdcdd6eb10e8ef1fe2 in incubator-airflow''s branch refs/heads/v1-9-stable from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d485453 ]

[AIRFLOW-1839] Fix more bugs in S3Hook boto -> boto3 migration

There were some more bugs as a result of the boto
to boto3 migration
that weren''t covered by existing tests. Now they
are fixed, and covered.
Hopefully I got everything this time.

Closes #2805 from ashb/AIRFLOW-1839-s3
-hook_loadsa-tests

(cherry picked from commit 2d5408935fc41c3d3b6618d8c563d1eecac06561)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> S3Hook.list_keys throws exception
> ---------------------------------
>
>                 Key: AIRFLOW-1839
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1839
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> {noformat}
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in list_keys
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>       File ""/usr/local/lib/python3.5/site-packages/airflow/hooks/S3_hook.py"", line 104, in <listcomp>
>         return [k.Key for k in response[''Contents'']] if response.get(''Contents'') else None
>     AttributeError: ''dict'' object has no attribute ''Key''
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27258,54,JIRA.13120247.1511359498000.299670.1511360460203@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120247.1511359498000@Atlassian.JIRA,,,2017-11-22 06:21:00-08,"[jira] [Work started] (AIRFLOW-1842) Create gcs to gcs operator to
 copy, rename and both","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1842?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1842 started by Igors Vaitkus.
----------------------------------------------
> Create gcs to gcs operator to copy, rename and both
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1842
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1842
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>              Labels: features
>
> Copies an object from a Google Cloud Storage bucket to another, with renaming if requested.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27259,54,JIRA.13120203.1511350073000.307723.1511423460822@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120203.1511350073000@Atlassian.JIRA,,,2017-11-22 23:51:00-08,"[jira] [Commented] (AIRFLOW-1841)
 GoogleCloudStorageDownloadOperator filename parameter","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1841?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16263921#comment-16263921 ] 

ASF subversion and git services commented on AIRFLOW-1841:
----------------------------------------------------------

Commit cbd6e70411b276d4ad6bb471b3760a3f7925a548 in incubator-airflow''s branch refs/heads/master from [~vait]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=cbd6e70 ]

[AIRFLOW-1841] change False to None in operator and hook

Documentation stated it''s String type but in code
it was union of String and Bool.
Changed to to pure string by substituting False to
None since in operator and hook
code checks only for presence of value in
variable.
Make it more predictable by using simpler String
type.

Closes #2807 from litdeviant/gcs-operator-hook


> GoogleCloudStorageDownloadOperator filename parameter
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1841
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1841
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>    Affects Versions: 1.8.0, 1.9.0
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Trivial
>              Labels: documentation
>
> https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/contrib/operators/gcs_download_operator.py
> Documentation - ""... If false, the downloaded data will not be stored on the local file system.""
> But filename type is a string in the documentation, it should be a union of String and Bool, but better change it to None. And change Hook as well to None because all it does just check the presence of a value in the parameter.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
574974,135,1453275851.1134842377455.JavaMail.jira@ajax.apache.org,22134,Craig Russell (JIRA),1561588258.1134157088920.JavaMail.jira@ajax.apache.org,22134,Craig Russell (JIRA),2005-12-17 09:59:37-08,[jira] Commented: (JDO-257) Change return type of makePersistent from void to Object,"    [ http://issues.apache.org/jira/browse/JDO-257?page=comments#action_12360672 ] 

Craig Russell commented on JDO-257:
-----------------------------------

The methods attachCopy and attachCopyAll should be removed as part of this change. This will require corresponding changes to the runtime20 and tck20 projects as well.

> Change return type of makePersistent from void to Object
> --------------------------------------------------------
>
>          Key: JDO-257
>          URL: http://issues.apache.org/jira/browse/JDO-257
>      Project: JDO
>         Type: New Feature
>   Components: api20
>     Reporter: Craig Russell
>     Assignee: Craig Russell
>      Fix For: JDO 2 beta
>  Attachments: persistencemanager.patch
>
> The signatures of makePersistent and makePersistentAll are changed to return the persistent instances. This change was adopted by the expert group 8-Dec-2005.

-- 
This message is automatically generated by JIRA.
-
If you think it was sent incorrectly contact one of the administrators:
   http://issues.apache.org/jira/secure/Administrators.jspa
-
For more information on JIRA, see:
   http://www.atlassian.com/software/jira


",f
27260,54,JIRA.13120247.1511359498000.307725.1511423520145@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120247.1511359498000@Atlassian.JIRA,,,2017-11-22 23:52:00-08,"[jira] [Commented] (AIRFLOW-1842) Create gcs to gcs operator to
 copy, rename and both","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1842?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16263922#comment-16263922 ] 

ASF subversion and git services commented on AIRFLOW-1842:
----------------------------------------------------------

Commit 149195845d7062f7ee12f1d59f52c9d4a054c53f in incubator-airflow''s branch refs/heads/master from [~vait]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1491958 ]

[AIRFLOW-1842] Add gcs to gcs copy operator with renaming if required

Copies an object from a Google Cloud Storage
bucket to another Google
Cloud Storage bucket, with renaming if required.

Closes #2808 from litdeviant/gcs_to_gcs


> Create gcs to gcs operator to copy, rename and both
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1842
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1842
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>              Labels: features
>
> Copies an object from a Google Cloud Storage bucket to another, with renaming if requested.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27261,54,JIRA.13073389.1495190402000.308489.1511430421151@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13073389.1495190402000@Atlassian.JIRA,,,2017-11-23 01:47:01-08,"[jira] [Commented] (AIRFLOW-1229) Make ""Run Id"" column clickable in
 Browse -> DAG Runs","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1229?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16264068#comment-16264068 ] 

ASF subversion and git services commented on AIRFLOW-1229:
----------------------------------------------------------

Commit d76bf76de2d2d4d3041e4d1bf9304dce8a1bcefb in incubator-airflow''s branch refs/heads/master from [~abij]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d76bf76 ]

[AIRFLOW-1229] Add link to Run Id, incl execution_date

Add two UI improvement. 1: the links from ""DAG
runs"" to DAG graph view
include the execution_date. So you land on the
expected DAG, instead of
the last DAG run. 2: A new link is added for the
column ""Run Id"" with the
same behaviour.

Closes #2801 from abij/AIRFLOW-1229


> Make ""Run Id"" column clickable in Browse -> DAG Runs
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1229
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1229
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webapp
>    Affects Versions: Airflow 1.8
>         Environment: Python3.4
>            Reporter: Erik Cederstrand
>            Assignee: Alexander Bij
>              Labels: patch
>         Attachments: dag_run_link.patch
>
>
> I''m triggering a lot of DAGs manually using ""airflow trigger_dag my_dag --run_id=some_unique_id"". I would like to be able in the UI to browse easily to this specific DAG run using the ""some_unique_id"" label. In the graph page of the DAG, I need to know the exact execution date, which is inconvenient, and in the Browse -> DAG Runs page I can search by ""some_unique_id"", but the ""Run Ids"" column is not clickable.
> The attached patch makes the aforementioned column clickable, so I''m sent directly to the graph view for that specific DAG run, not the DAG in general.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27262,54,JIRA.13073389.1495190402000.308492.1511430421174@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13073389.1495190402000@Atlassian.JIRA,,,2017-11-23 01:47:01-08,"[jira] [Commented] (AIRFLOW-1229) Make ""Run Id"" column clickable in
 Browse -> DAG Runs","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1229?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16264069#comment-16264069 ] 

ASF subversion and git services commented on AIRFLOW-1229:
----------------------------------------------------------

Commit d76bf76de2d2d4d3041e4d1bf9304dce8a1bcefb in incubator-airflow''s branch refs/heads/master from [~abij]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d76bf76 ]

[AIRFLOW-1229] Add link to Run Id, incl execution_date

Add two UI improvement. 1: the links from ""DAG
runs"" to DAG graph view
include the execution_date. So you land on the
expected DAG, instead of
the last DAG run. 2: A new link is added for the
column ""Run Id"" with the
same behaviour.

Closes #2801 from abij/AIRFLOW-1229


> Make ""Run Id"" column clickable in Browse -> DAG Runs
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1229
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1229
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webapp
>    Affects Versions: Airflow 1.8
>         Environment: Python3.4
>            Reporter: Erik Cederstrand
>            Assignee: Alexander Bij
>              Labels: patch
>         Attachments: dag_run_link.patch
>
>
> I''m triggering a lot of DAGs manually using ""airflow trigger_dag my_dag --run_id=some_unique_id"". I would like to be able in the UI to browse easily to this specific DAG run using the ""some_unique_id"" label. In the graph page of the DAG, I need to know the exact execution date, which is inconvenient, and in the Browse -> DAG Runs page I can search by ""some_unique_id"", but the ""Run Ids"" column is not clickable.
> The attached patch makes the aforementioned column clickable, so I''m sent directly to the graph view for that specific DAG run, not the DAG in general.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27263,54,JIRA.13073389.1495190402000.308623.1511431980104@Atlassian.JIRA,1778,Erik Cederstrand (JIRA),JIRA.13073389.1495190402000@Atlassian.JIRA,,,2017-11-23 02:13:00-08,"[jira] [Commented] (AIRFLOW-1229) Make ""Run Id"" column clickable in
 Browse -> DAG Runs","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1229?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16264101#comment-16264101 ] 

Erik Cederstrand commented on AIRFLOW-1229:
-------------------------------------------

Thanks for taking the time to get this in, [~abij]!

> Make ""Run Id"" column clickable in Browse -> DAG Runs
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1229
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1229
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webapp
>    Affects Versions: Airflow 1.8
>         Environment: Python3.4
>            Reporter: Erik Cederstrand
>            Assignee: Alexander Bij
>              Labels: patch
>         Attachments: dag_run_link.patch
>
>
> I''m triggering a lot of DAGs manually using ""airflow trigger_dag my_dag --run_id=some_unique_id"". I would like to be able in the UI to browse easily to this specific DAG run using the ""some_unique_id"" label. In the graph page of the DAG, I need to know the exact execution date, which is inconvenient, and in the Browse -> DAG Runs page I can search by ""some_unique_id"", but the ""Run Ids"" column is not clickable.
> The attached patch makes the aforementioned column clickable, so I''m sent directly to the graph view for that specific DAG run, not the DAG in general.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27264,54,JIRA.13120434.1511432791000.308719.1511432820367@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120434.1511432791000@Atlassian.JIRA,,,2017-11-23 02:27:00-08,"[jira] [Created] (AIRFLOW-1843) Add Google Cloud Storage Sensor
 with prefix","Igors Vaitkus created AIRFLOW-1843:
--------------------------------------

             Summary: Add Google Cloud Storage Sensor with prefix
                 Key: AIRFLOW-1843
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1843
             Project: Apache Airflow
          Issue Type: Improvement
          Components: contrib, gcp
    Affects Versions: 1.8.1, 1.9.1
            Reporter: Igors Vaitkus
            Assignee: Igors Vaitkus
            Priority: Minor


Hook can do list objects in bucket with prefix so I need sensor which will check bucket with prefix if there any incoming files.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27265,54,JIRA.13120203.1511350073000.308725.1511432880534@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120203.1511350073000@Atlassian.JIRA,,,2017-11-23 02:28:00-08,"[jira] [Resolved] (AIRFLOW-1841) GoogleCloudStorageDownloadOperator
 filename parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1841?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Igors Vaitkus resolved AIRFLOW-1841.
------------------------------------
    Resolution: Resolved

> GoogleCloudStorageDownloadOperator filename parameter
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1841
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1841
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>    Affects Versions: 1.8.0, 1.9.0
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Trivial
>              Labels: documentation
>
> https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/contrib/operators/gcs_download_operator.py
> Documentation - ""... If false, the downloaded data will not be stored on the local file system.""
> But filename type is a string in the documentation, it should be a union of String and Bool, but better change it to None. And change Hook as well to None because all it does just check the presence of a value in the parameter.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27266,54,JIRA.13120203.1511350073000.308726.1511432940383@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120203.1511350073000@Atlassian.JIRA,,,2017-11-23 02:29:00-08,"[jira] [Closed] (AIRFLOW-1841) GoogleCloudStorageDownloadOperator
 filename parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1841?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Igors Vaitkus closed AIRFLOW-1841.
----------------------------------

> GoogleCloudStorageDownloadOperator filename parameter
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1841
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1841
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>    Affects Versions: 1.8.0, 1.9.0
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Trivial
>              Labels: documentation
>
> https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/contrib/operators/gcs_download_operator.py
> Documentation - ""... If false, the downloaded data will not be stored on the local file system.""
> But filename type is a string in the documentation, it should be a union of String and Bool, but better change it to None. And change Hook as well to None because all it does just check the presence of a value in the parameter.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27267,54,JIRA.13120434.1511432791000.308730.1511432940440@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120434.1511432791000@Atlassian.JIRA,,,2017-11-23 02:29:00-08,"[jira] [Work started] (AIRFLOW-1843) Add Google Cloud Storage
 Sensor with prefix","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1843?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1843 started by Igors Vaitkus.
----------------------------------------------
> Add Google Cloud Storage Sensor with prefix
> -------------------------------------------
>
>                 Key: AIRFLOW-1843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1843
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp
>    Affects Versions: 1.8.1, 1.9.1
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>
> Hook can do list objects in bucket with prefix so I need sensor which will check bucket with prefix if there any incoming files.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27268,54,JIRA.13120247.1511359498000.308969.1511434860876@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120247.1511359498000@Atlassian.JIRA,,,2017-11-23 03:01:00-08,"[jira] [Resolved] (AIRFLOW-1842) Create gcs to gcs operator to
 copy, rename and both","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1842?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Igors Vaitkus resolved AIRFLOW-1842.
------------------------------------
    Resolution: Resolved

> Create gcs to gcs operator to copy, rename and both
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1842
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1842
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>              Labels: features
>
> Copies an object from a Google Cloud Storage bucket to another, with renaming if requested.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27269,54,JIRA.13120247.1511359498000.308973.1511434860924@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13120247.1511359498000@Atlassian.JIRA,,,2017-11-23 03:01:00-08,"[jira] [Closed] (AIRFLOW-1842) Create gcs to gcs operator to copy,
 rename and both","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1842?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Igors Vaitkus closed AIRFLOW-1842.
----------------------------------

> Create gcs to gcs operator to copy, rename and both
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1842
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1842
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>              Labels: features
>
> Copies an object from a Google Cloud Storage bucket to another, with renaming if requested.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27270,54,JIRA.13089926.1500997946000.311248.1511457000340@Atlassian.JIRA,2401,Igors Vaitkus (JIRA),JIRA.13089926.1500997946000@Atlassian.JIRA,,,2017-11-23 09:10:00-08,"[jira] [Assigned] (AIRFLOW-1455) Move logging related configs out
 of airflow.cfg","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1455?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Igors Vaitkus reassigned AIRFLOW-1455:
--------------------------------------

    Assignee: Igors Vaitkus

> Move logging related configs out of airflow.cfg
> -----------------------------------------------
>
>                 Key: AIRFLOW-1455
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1455
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Allison Wang
>            Assignee: Igors Vaitkus
>
> All logging related configurations including {{LOG_BASE_FOLDER}}, {{REMOTE_LOG_BASE_FOLDER}}, {{LOG_LEVEL}} and {{LOG_FORMAT}} should be placed inside {{default_airflow_logging}}. This task also includes refactoring all occurrence of those variables and make them handler specific rather than global. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27271,54,JIRA.13120573.1511504935000.313083.1511504940152@Atlassian.JIRA,2403,hujiahua (JIRA),JIRA.13120573.1511504935000@Atlassian.JIRA,,,2017-11-23 22:29:00-08,"[jira] [Created] (AIRFLOW-1844) task would not be executed when
 celery broker recovery","hujiahua created AIRFLOW-1844:
---------------------------------

             Summary: task would not be executed when celery broker recovery
                 Key: AIRFLOW-1844
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1844
             Project: Apache Airflow
          Issue Type: Bug
          Components: celery, executor, scheduler
    Affects Versions: 1.8.0, 1.9.0
            Reporter: hujiahua


when the scheduler failed to send task  during celery broker not working, then the task will never send again and never be executed.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27272,54,JIRA.13120573.1511504935000.313245.1511506800044@Atlassian.JIRA,2403,hujiahua (JIRA),JIRA.13120573.1511504935000@Atlassian.JIRA,,,2017-11-23 23:00:00-08,"[jira] [Updated] (AIRFLOW-1844) task would not be executed when
 celery broker recovery","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1844?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

hujiahua updated AIRFLOW-1844:
------------------------------
    Description: When the scheduler fail to send task during celery broker not working, then the task will never send again and never be executed when celery broker recovery.  (was: when the scheduler failed to send task  during celery broker not working, then the task will never send again and never be executed.)

> task would not be executed when celery broker recovery
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1844
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1844
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery, executor, scheduler
>    Affects Versions: 1.8.0, 1.9.0
>            Reporter: hujiahua
>
> When the scheduler fail to send task during celery broker not working, then the task will never send again and never be executed when celery broker recovery.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27273,54,JIRA.13120624.1511526205000.314972.1511526240149@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13120624.1511526205000@Atlassian.JIRA,,,2017-11-24 04:24:00-08,"[jira] [Created] (AIRFLOW-1845) Modal background doesn''t cover wide
 or tall pages","Ash Berlin-Taylor created AIRFLOW-1845:
------------------------------------------

             Summary: Modal background doesn''t cover wide or tall pages
                 Key: AIRFLOW-1845
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1845
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.8.1, 1.9.0
            Reporter: Ash Berlin-Taylor
            Assignee: Ash Berlin-Taylor
         Attachments: Screen Shot 2017-11-24 at 12.19.50.png

If there is any kind of scrolling on the page behind a modal pop up then the grey backgorund behind the modal dialog doesn''t correctly cover outside the ""first"" page view. For example:

!Screen Shot 2017-11-24 at 12.19.50.png|thumbnail!

To reproduce: go to a long or tall dag tree view page, scroll first, then click on a task instance to get the modal popup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27274,54,JIRA.13120624.1511526205000.314981.1511526300149@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13120624.1511526205000@Atlassian.JIRA,,,2017-11-24 04:25:00-08,"[jira] [Updated] (AIRFLOW-1845) Modal background doesn''t cover wide
 or tall pages","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1845?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor updated AIRFLOW-1845:
---------------------------------------
    Description: 
If there is any kind of scrolling on the page behind a modal pop up then the grey backgorund behind the modal dialog doesn''t correctly cover outside the ""first"" page view. For example see the attached screenshot (which for some reason Jira isn''t embedding...)

!Screen Shot 2017-11-24 at 12.19.50.png|thumbnail!

To reproduce: go to a long or tall dag tree view page, scroll first, then click on a task instance to get the modal popup.

  was:
If there is any kind of scrolling on the page behind a modal pop up then the grey backgorund behind the modal dialog doesn''t correctly cover outside the ""first"" page view. For example:

!Screen Shot 2017-11-24 at 12.19.50.png|thumbnail!

To reproduce: go to a long or tall dag tree view page, scroll first, then click on a task instance to get the modal popup.


> Modal background doesn''t cover wide or tall pages
> -------------------------------------------------
>
>                 Key: AIRFLOW-1845
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1845
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.8.1, 1.9.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>         Attachments: Screen Shot 2017-11-24 at 12.19.50.png
>
>
> If there is any kind of scrolling on the page behind a modal pop up then the grey backgorund behind the modal dialog doesn''t correctly cover outside the ""first"" page view. For example see the attached screenshot (which for some reason Jira isn''t embedding...)
> !Screen Shot 2017-11-24 at 12.19.50.png|thumbnail!
> To reproduce: go to a long or tall dag tree view page, scroll first, then click on a task instance to get the modal popup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27275,54,JIRA.13119930.1511269553000.316236.1511545500176@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13119930.1511269553000@Atlassian.JIRA,,,2017-11-24 09:45:00-08,"[jira] [Commented] (AIRFLOW-1837) Differing start_dates on tasks
 not respected by scheduler.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1837?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16265460#comment-16265460 ] 

Ash Berlin-Taylor commented on AIRFLOW-1837:
--------------------------------------------

Looking at the code I''m not quite sure how I saw this behaviour. Or even if I really did. I will need to go and test this again.

> Differing start_dates on tasks not respected by scheduler.
> ----------------------------------------------------------
>
>                 Key: AIRFLOW-1837
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1837
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>
> It it possible to specify start_date directly on tasks in dag, as well as on the DAG. This is correctly handled when creating dag runs, but it is seemingly ignored when scheduling tasks.
> Given this example:
> {code}
> dag_args = {
>     ""start_date"": datetime(2017, 9, 4),
> }
> dag = DAG(
>     ""my-dag"",
>     default_args=dag_args,
>     schedule_interval=""0 0 * * Mon"",
> )
> # ...
> with dag:
>         op = PythonOperator(
>             python_callable=fetcher.run,
>             task_id=""fetch_all_respondents"",
>             provide_context=True,
>             # The ""unfiltered"" API calls are a lot quicker, so lets put them
>             # ahead of any other filtered job in the queue.
>             priority_weight=10,
>             start_date=datetime(2014, 9, 1),
>         )
>         op = PythonOperator(
>             python_callable=fetcher.run,
>             task_id=""fetch_by_demographics"",
>             op_kwargs={
>                 ''demo_names'': demo_names,
>             },
>             provide_context=True,
>             priority_weight=5,
>         )
> {code}
> I only want the fetch_all_respondents tasks to run for 2014..2017, and then from September 2017 I also want the fetch_by_demographics task to run. However right now both tasks are being scheduled from 2014-09-01.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27276,54,JIRA.13120624.1511526205000.316242.1511545560133@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13120624.1511526205000@Atlassian.JIRA,,,2017-11-24 09:46:00-08,"[jira] [Updated] (AIRFLOW-1845) Modal background doesn''t cover wide
 or tall pages","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1845?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor updated AIRFLOW-1845:
---------------------------------------
    Fix Version/s: 1.10.0
      Component/s: ui

> Modal background doesn''t cover wide or tall pages
> -------------------------------------------------
>
>                 Key: AIRFLOW-1845
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1845
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.1, 1.9.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>             Fix For: 1.10.0
>
>         Attachments: Screen Shot 2017-11-24 at 12.19.50.png
>
>
> If there is any kind of scrolling on the page behind a modal pop up then the grey backgorund behind the modal dialog doesn''t correctly cover outside the ""first"" page view. For example see the attached screenshot (which for some reason Jira isn''t embedding...)
> !Screen Shot 2017-11-24 at 12.19.50.png|thumbnail!
> To reproduce: go to a long or tall dag tree view page, scroll first, then click on a task instance to get the modal popup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27277,54,JIRA.13111827.1508885615000.316330.1511549040176@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13111827.1508885615000@Atlassian.JIRA,,,2017-11-24 10:44:00-08,"[jira] [Updated] (AIRFLOW-1755) URL Prefix for both Flower and Web
 admin","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1755?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1755:
---------------------------
    Component/s:     (was: core)
                     (was: cli)
                 webserver
                 webapp
                 celery

> URL Prefix for both Flower and Web admin
> ----------------------------------------
>
>                 Key: AIRFLOW-1755
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1755
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: celery, webapp, webserver
>    Affects Versions: 1.10.0
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>
> Similar to AIRFLOW-339.
> Should also fix AIRFLOW-964.
> I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
> {code}
> # Root URL to use for the web server
> web_server_url_prefix = /airflow
> ...
> # The root URL for Flower
> flower_url_prefix = /flower
> {code}
> This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27278,54,JIRA.13111827.1508885615000.316334.1511549040741@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13111827.1508885615000@Atlassian.JIRA,,,2017-11-24 10:44:00-08,"[jira] [Updated] (AIRFLOW-1755) URL Prefix for both Flower and Web
 admin","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1755?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1755:
---------------------------
    Affects Version/s: 1.10.0

> URL Prefix for both Flower and Web admin
> ----------------------------------------
>
>                 Key: AIRFLOW-1755
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1755
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: celery, webapp, webserver
>    Affects Versions: 1.10.0
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>
> Similar to AIRFLOW-339.
> Should also fix AIRFLOW-964.
> I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
> {code}
> # Root URL to use for the web server
> web_server_url_prefix = /airflow
> ...
> # The root URL for Flower
> flower_url_prefix = /flower
> {code}
> This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27279,54,JIRA.13111827.1508885615000.316332.1511549040699@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13111827.1508885615000@Atlassian.JIRA,,,2017-11-24 10:44:00-08,"[jira] [Updated] (AIRFLOW-1755) URL Prefix for both Flower and Web
 admin","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1755?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1755:
---------------------------
    Affects Version/s:     (was: Airflow 2.0)

> URL Prefix for both Flower and Web admin
> ----------------------------------------
>
>                 Key: AIRFLOW-1755
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1755
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: celery, webapp, webserver
>    Affects Versions: 1.10.0
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>
> Similar to AIRFLOW-339.
> Should also fix AIRFLOW-964.
> I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
> {code}
> # Root URL to use for the web server
> web_server_url_prefix = /airflow
> ...
> # The root URL for Flower
> flower_url_prefix = /flower
> {code}
> This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27280,54,JIRA.13111827.1508885615000.316336.1511549100183@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13111827.1508885615000@Atlassian.JIRA,,,2017-11-24 10:45:00-08,"[jira] [Updated] (AIRFLOW-1755) URL Prefix for both Flower and Web
 admin","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1755?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1755:
---------------------------
    Labels: prefix proxy reverse url  (was: )

> URL Prefix for both Flower and Web admin
> ----------------------------------------
>
>                 Key: AIRFLOW-1755
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1755
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: celery, webapp, webserver
>    Affects Versions: 1.10.0
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: prefix, proxy, reverse, url
>
> Similar to AIRFLOW-339.
> Should also fix AIRFLOW-964.
> I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
> {code}
> # Root URL to use for the web server
> web_server_url_prefix = /airflow
> ...
> # The root URL for Flower
> flower_url_prefix = /flower
> {code}
> This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27281,54,JIRA.13111827.1508885615000.316338.1511549100566@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13111827.1508885615000@Atlassian.JIRA,,,2017-11-24 10:45:00-08,"[jira] [Updated] (AIRFLOW-1755) URL Prefix for both Flower and Web
 admin","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1755?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1755:
---------------------------
    Description: 
Similar to AIRFLOW-339.
Should also fix AIRFLOW-964.

I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
{code}
# Root URL to use for the web server
web_server_url_prefix = /airflow
...
# The root URL for Flower
flower_url_prefix = /flower
{code}

This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.

See documentation updated in my commit for example on how to configure reverse proxy in front of airflow

  was:
Similar to AIRFLOW-339.
Should also fix AIRFLOW-964.

I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
{code}
# Root URL to use for the web server
web_server_url_prefix = /airflow
...
# The root URL for Flower
flower_url_prefix = /flower
{code}

This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.


> URL Prefix for both Flower and Web admin
> ----------------------------------------
>
>                 Key: AIRFLOW-1755
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1755
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: celery, webapp, webserver
>    Affects Versions: 1.10.0
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: prefix, proxy, reverse, url
>
> Similar to AIRFLOW-339.
> Should also fix AIRFLOW-964.
> I propose in this change 2 new settings on {{airflow.cfg}} and the CLI arguments:
> {code}
> # Root URL to use for the web server
> web_server_url_prefix = /airflow
> ...
> # The root URL for Flower
> flower_url_prefix = /flower
> {code}
> This will allow deploying airflow on only on a root FQDN, like {{airflow.mycompany.com}}, but also on a path.
> See documentation updated in my commit for example on how to configure reverse proxy in front of airflow



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27282,54,JIRA.13120682.1511561173000.316806.1511561220040@Atlassian.JIRA,2404,Jon Bruno (JIRA),JIRA.13120682.1511561173000@Atlassian.JIRA,,,2017-11-24 14:07:00-08,"[jira] [Created] (AIRFLOW-1846) Allow Ad Hoc Query and Charts pages
 to be disabled via configs","Jon Bruno created AIRFLOW-1846:
----------------------------------

             Summary: Allow Ad Hoc Query and Charts pages to be disabled via configs
                 Key: AIRFLOW-1846
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1846
             Project: Apache Airflow
          Issue Type: Improvement
          Components: webapp
            Reporter: Jon Bruno
            Priority: Minor


At the company I work for, we have pretty strict rules surrounding data exfiltration. One of the things stopping us from being able to access the Airflow web app from outside our environment is the ad hoc query page, which would allow us to access data where we shouldn''t.

As an administrator, i want to be able to disable the Ad Hoc query and Charts pages via application level configs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27283,54,JIRA.13120718.1511609594000.317692.1511609641101@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-25 03:34:01-08,[jira] [Assigned] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet reassigned AIRFLOW-1847:
------------------------------

    Assignee: Semet

> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> Webhook sensor. May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27284,54,JIRA.13120718.1511609594000.317691.1511609640082@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-25 03:34:00-08,[jira] [Created] (AIRFLOW-1847) Webhook Sensor,"Semet created AIRFLOW-1847:
------------------------------

             Summary: Webhook Sensor
                 Key: AIRFLOW-1847
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
             Project: Apache Airflow
          Issue Type: Improvement
          Components: core, operators
            Reporter: Semet
            Priority: Minor
         Attachments: airflow-webhook-proposal.png

Webhook sensor. May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27285,54,JIRA.13120718.1511609594000.317693.1511609700126@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-25 03:35:00-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
Webhook sensor. May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

!airflow-webhook-proposal.png|thumbnail!

  was:
Webhook sensor. May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> Webhook sensor. May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> !airflow-webhook-proposal.png|thumbnail!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27286,54,JIRA.13120718.1511609594000.317694.1511609700142@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-25 03:35:00-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
Webhook sensor. May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

!airflow-webhook-proposal.png!

  was:
Webhook sensor. May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

!airflow-webhook-proposal.png|thumbnail!


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> Webhook sensor. May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> !airflow-webhook-proposal.png!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27287,54,JIRA.13120718.1511609594000.317714.1511610961181@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-25 03:56:01-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
Webhook sensor. May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

Security, authentication and other related subject might be adresses in another ticket.

  was:
Webhook sensor. May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

!airflow-webhook-proposal.png!


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> Webhook sensor. May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.
> Documentation would be updated to describe the classic scheme to implement this use case, which would look like:
> !airflow-webhook-proposal.png!
> I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  
> Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
> {code}
> sensor = JsonWebHookSensor(
>             task_id=''my_task_id'',
>             name=""on_github_push""
>         )
> .. user is responsible to triggering the processing DAG himself.
> {code}
> In my github project, I register the following URL in webhook page:
> {code}
> http://airflow.myserver.com/api/experimental/webhook/on_github_push
> {code}
> From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.
> The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.
> Security, authentication and other related subject might be adresses in another ticket.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27288,54,JIRA.13120718.1511609594000.317716.1511611204483@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-25 04:00:04-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Possible evolutions:
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- Security, authentication and other related subject might be adresses in another ticket.

  was:
h1. Webhook sensor.
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Possible evolutions:
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- Security, authentication and other related subject might be adresses in another ticket.


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> h1. Webhook sensor
> May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.
> Documentation would be updated to describe the classic scheme to implement this use case, which would look like:
> !airflow-webhook-proposal.png!
> I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  
> h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
> {code}
> sensor = JsonWebHookSensor(
>             task_id=''my_task_id'',
>             name=""on_github_push""
>         )
> .. user is responsible to triggering the processing DAG himself.
> {code}
> In my github project, I register the following URL in webhook page:
> {code}
> http://airflow.myserver.com/api/experimental/webhook/on_github_push
> {code}
> From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.
> The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.
> h2. Possible evolutions:
> - use an external queue (redis, amqp) to handle lot of events
> - allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
> - Security, authentication and other related subject might be adresses in another ticket.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27289,54,JIRA.13120718.1511609594000.317715.1511611200284@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-25 04:00:00-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
h1. Webhook sensor.
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Possible evolutions:
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- Security, authentication and other related subject might be adresses in another ticket.

  was:
Webhook sensor. May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

Security, authentication and other related subject might be adresses in another ticket.


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> h1. Webhook sensor.
> May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.
> Documentation would be updated to describe the classic scheme to implement this use case, which would look like:
> !airflow-webhook-proposal.png!
> I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  
> h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
> {code}
> sensor = JsonWebHookSensor(
>             task_id=''my_task_id'',
>             name=""on_github_push""
>         )
> .. user is responsible to triggering the processing DAG himself.
> {code}
> In my github project, I register the following URL in webhook page:
> {code}
> http://airflow.myserver.com/api/experimental/webhook/on_github_push
> {code}
> From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.
> The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.
> h2. Possible evolutions:
> - use an external queue (redis, amqp) to handle lot of events
> - allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
> - Security, authentication and other related subject might be adresses in another ticket.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27290,54,JIRA.13120718.1511609594000.317737.1511612101703@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-25 04:15:01-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- 
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- Security, authentication and other related subject might be adresses in another ticket.

  was:
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Possible evolutions:
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- Security, authentication and other related subject might be adresses in another ticket.


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> h1. Webhook sensor
> May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.
> Documentation would be updated to describe the classic scheme to implement this use case, which would look like:
> !airflow-webhook-proposal.png!
> I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  
> h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
> {code}
> sensor = JsonWebHookSensor(
>             task_id=''my_task_id'',
>             name=""on_github_push""
>         )
> .. user is responsible to triggering the processing DAG himself.
> {code}
> In my github project, I register the following URL in webhook page:
> {code}
> http://airflow.myserver.com/api/experimental/webhook/on_github_push
> {code}
> From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.
> The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.
> h2. Documenation update
> - add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
> - describe the sensing dag + processing dag scheme and provide the github use case as real life example
> h2. Possible evolutions
> - 
> - use an external queue (redis, amqp) to handle lot of events
> - allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
> - Security, authentication and other related subject might be adresses in another ticket.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27291,54,JIRA.12966633.1462863705000.317949.1511619121065@Atlassian.JIRA,1903,Semet (JIRA),JIRA.12966633.1462863705000@Atlassian.JIRA,,,2017-11-25 06:12:01-08,"[jira] [Commented] (AIRFLOW-90) `[celery]` section needed even if
 CeleryExecutor not used","
    [ https://issues.apache.org/jira/browse/AIRFLOW-90?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16265726#comment-16265726 ] 

Semet commented on AIRFLOW-90:
------------------------------

Please close this issue since it is fixed

> `[celery]` section needed even if CeleryExecutor not used
> ---------------------------------------------------------
>
>                 Key: AIRFLOW-90
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-90
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery
>    Affects Versions: Airflow 1.7.0
>         Environment: * Airflow version: 1.7.0
> * Airflow components: webserver and scheduler with a postgres database and LocalExecutor
> * Relevant {{airflow.cfg}} settings: no {{[celery]}} section
> * Python Version: Python 2.7.3
> * Operating System: Linux ubu91 3.13.0-85-generic #129~precise1-Ubuntu SMP Fri Mar 18 17:38:08 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
> * Python packages: 
> {code}
> Babel==1.3
> Flask==0.10.1
> Flask-Admin==1.4.0
> Flask-Bcrypt==0.7.1
> Flask-Cache==0.13.1
> Flask-Login==0.2.11
> Flask-WTF==0.12
> JPype1==0.6.1
> JayDeBeApi==0.2.0
> Jinja2==2.8
> Mako==1.0.4
> Markdown==2.6.6
> MarkupSafe==0.23
> PyHive==0.1.7
> PySmbClient==0.1.3
> Pygments==2.1.3
> SQLAlchemy==1.0.12
> Sphinx==1.4
> Sphinx-PyPI-upload==0.2.1
> WTForms==2.1
> Werkzeug==0.11.5
> airflow==1.7.0
> alabaster==0.7.7
> alembic==0.8.5
> amqp==1.4.9
> anyjson==0.3.3
> argparse==1.2.1
> backports.ssl-match-hostname==3.5.0.1
> bcrypt==2.0.0
> billiard==3.3.0.23
> boto==2.39.0
> celery==3.1.23
> certifi==2016.2.28
> cffi==1.5.2
> chartkick==0.4.2
> check-manifest==0.31
> coverage==4.0.3
> coveralls==1.1
> croniter==0.3.12
> cryptography==1.3.1
> decorator==4.0.9
> devpi-client==2.5.0
> devpi-common==2.0.8
> dill==0.2.5
> docker-py==1.7.2
> docopt==0.6.2
> docutils==0.12
> enum34==1.1.2
> filechunkio==1.6
> flake8==2.5.4
> flower==0.9.0
> funcsigs==0.4
> future==0.15.2
> futures==3.0.5
> gunicorn==19.3.0
> hdfs==2.0.5
> hive-thrift-py==0.0.1
> idna==2.1
> imagesize==0.7.0
> ipaddress==1.0.16
> ipython==4.1.2
> ipython-genutils==0.1.0
> itsdangerous==0.24
> kombu==3.0.35
> ldap3==1.2.2
> lxml==3.6.0
> mccabe==0.4.0
> mock==1.3.0
> mysqlclient==1.3.7
> nose==1.3.7
> nose-exclude==0.4.1
> numpy==1.11.0
> pandas==0.18.0
> path.py==8.1.2
> pbr==1.8.1
> pep8==1.7.0
> pexpect==4.0.1
> pickleshare==0.6
> pkginfo==1.2.1
> pluggy==0.3.1
> psycopg2==2.6.1
> ptyprocess==0.5.1
> py==1.4.31
> pyOpenSSL==16.0.0
> pyasn1==0.1.9
> pycparser==2.14
> pydruid==0.2.3
> pyflakes==1.0.0
> pykerberos==1.1.10
> pytest==2.9.1
> pytest-cov==2.2.1
> python-dateutil==2.5.2
> python-editor==1.0
> pytz==2016.3
> redis==2.10.5
> requests==2.9.1
> setproctitle==1.1.9
> simplegeneric==0.8.1
> six==1.10.0
> slackclient==1.0.0
> snowballstemmer==1.2.1
> sphinx-argparse==0.1.15
> sphinx-rtd-theme==0.1.9
> statsd==3.2.1
> thrift==0.9.3
> tornado==4.2
> tox==2.3.1
> traitlets==4.2.1
> unicodecsv==0.14.1
> virtualenv==15.0.1
> websocket-client==0.35.0
> wheel==0.29.0
> wsgiref==0.1.2
> {code}
>            Reporter: Andreas Merkel
>            Priority: Trivial
>
> Now that you know a little about me, let me tell you about the issue I am having:
> * What did you expect to happen?
> I expect that if I don''t use the Celery Executor, I don''t need a {{[celery]}} section in my {{airflow.cfg}}.
> * What happened instead?
> If I remove the section, Airflow does not start. At the very least I need
> {code}
> [celery]
> celeryd_concurrency = 1 
> {code}
> regardless of the executor configured.
> * Stack trace, if appropriate:
> {code}
> Traceback (most recent call last):
>   File ""/home/PHI-TPS/amerkel/venv/airflow/bin/airflow"", line 13, in <module>
>     parser = get_parser()
>   File ""/home/PHI-TPS/amerkel/venv/airflow/local/lib/python2.7/site-packages/airflow/bin/cli.py"", line 751, in get_parser
>     default=configuration.get(''celery'', ''celeryd_concurrency''))
>   File ""/home/PHI-TPS/amerkel/venv/airflow/local/lib/python2.7/site-packages/airflow/configuration.py"", line 520, in get
>     return conf.get(section, key, **kwargs)
>   File ""/home/PHI-TPS/amerkel/venv/airflow/local/lib/python2.7/site-packages/airflow/configuration.py"", line 428, in get
>     ""in config"".format(**locals()))
> airflow.configuration.AirflowConfigException: section/key [celery/celeryd_concurrency] not found in config
> {code}
> h2. Reproducing the Issue
> Here is how you can reproduce this issue on your machine.
> * Example code that reproduces the issue, including a minimally illustrative DAG if necessary:
> {{airflow.cfg}}:
> {code}
> [core]
> airflow_home = $PWD
> dags_folder = $PWD/dags
> base_log_folder = $PWD/airflow_logs
> plugins_folder = $PWD/plugins
> executor = SequentialExecutor
> sql_alchemy_conn = sqlite:///airflow.db
> parallelism = 8
> dag_concurrency = 4
> max_active_runs_per_dag = 4
> load_examples = False
> donot_pickle = False
> fernet_key =  ; provided via environment
> [webserver]
> expose_config = true
> authenticate = False
> filter_by_owner = False
> [scheduler]
> job_heartbeat_sec = 5
> scheduler_heartbeat_sec = 5
> {code}
> * Reproduction steps:
> 1. Configure {{airflow.cfg}} as above
> 2. run {{airflow}} from the console
> 3. see the stack trace



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27292,54,JIRA.13080521.1497648697000.318127.1511627460368@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13080521.1497648697000@Atlassian.JIRA,,,2017-11-25 08:31:00-08,[jira] [Commented] (AIRFLOW-1314) Airflow kubernetes integration,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1314?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16265760#comment-16265760 ] 

Semet commented on AIRFLOW-1314:
--------------------------------

For information, I am working on an Helm chart for airflow with celery executor: https://github.com/Stibbons/kube-airflow

I plan to propose it to the kubernetes chart once I am satisfied with it. Still need to handle dag deployment properly

> Airflow kubernetes integration
> ------------------------------
>
>                 Key: AIRFLOW-1314
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1314
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib
>    Affects Versions: Airflow 2.0
>            Reporter: Daniel Imberman
>            Assignee: Daniel Imberman
>            Priority: Minor
>              Labels: features
>             Fix For: Airflow 2.0
>
>
> Kubernetes is a container-based cluster management system designed by google for easy application deployment. Companies such as Airbnb, Bloomberg, Palantir, and Google use kubernetes for a variety of large-scale solutions including data science, ETL, and app deployment. Integrating airflow into Kubernetes would increase viable use cases for airflow, promote airflow as a de facto workflow scheduler for Kubernetes, and create possibilities for improved security and robustness within airflow. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27293,54,JIRA.13120624.1511526205000.319752.1511706960916@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120624.1511526205000@Atlassian.JIRA,,,2017-11-26 06:36:00-08,"[jira] [Commented] (AIRFLOW-1845) Modal background doesn''t cover
 wide or tall pages","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1845?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16266023#comment-16266023 ] 

ASF subversion and git services commented on AIRFLOW-1845:
----------------------------------------------------------

Commit 87c6c83525ae2dc6f55e09e5e960dd47c5fa5acc in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=87c6c83 ]

[AIRFLOW-1845] Modal background now covers long or tall pages

If the page was scrolled before the dialog was
displayed then the grey
background would not cover the whole page
correctly.

Closes #2813 from ashb/AIRFLOW-1845-modal-
background-on-long-pages


> Modal background doesn''t cover wide or tall pages
> -------------------------------------------------
>
>                 Key: AIRFLOW-1845
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1845
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.1, 1.9.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>             Fix For: 1.10.0
>
>         Attachments: Screen Shot 2017-11-24 at 12.19.50.png
>
>
> If there is any kind of scrolling on the page behind a modal pop up then the grey backgorund behind the modal dialog doesn''t correctly cover outside the ""first"" page view. For example see the attached screenshot (which for some reason Jira isn''t embedding...)
> !Screen Shot 2017-11-24 at 12.19.50.png|thumbnail!
> To reproduce: go to a long or tall dag tree view page, scroll first, then click on a task instance to get the modal popup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27294,54,JIRA.13120624.1511526205000.319753.1511706960940@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120624.1511526205000@Atlassian.JIRA,,,2017-11-26 06:36:00-08,"[jira] [Commented] (AIRFLOW-1845) Modal background doesn''t cover
 wide or tall pages","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1845?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16266024#comment-16266024 ] 

ASF subversion and git services commented on AIRFLOW-1845:
----------------------------------------------------------

Commit 87c6c83525ae2dc6f55e09e5e960dd47c5fa5acc in incubator-airflow''s branch refs/heads/master from [~ashb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=87c6c83 ]

[AIRFLOW-1845] Modal background now covers long or tall pages

If the page was scrolled before the dialog was
displayed then the grey
background would not cover the whole page
correctly.

Closes #2813 from ashb/AIRFLOW-1845-modal-
background-on-long-pages


> Modal background doesn''t cover wide or tall pages
> -------------------------------------------------
>
>                 Key: AIRFLOW-1845
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1845
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.1, 1.9.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>             Fix For: 1.10.0
>
>         Attachments: Screen Shot 2017-11-24 at 12.19.50.png
>
>
> If there is any kind of scrolling on the page behind a modal pop up then the grey backgorund behind the modal dialog doesn''t correctly cover outside the ""first"" page view. For example see the attached screenshot (which for some reason Jira isn''t embedding...)
> !Screen Shot 2017-11-24 at 12.19.50.png|thumbnail!
> To reproduce: go to a long or tall dag tree view page, scroll first, then click on a task instance to get the modal popup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27295,54,JIRA.13120624.1511526205000.319757.1511706961155@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13120624.1511526205000@Atlassian.JIRA,,,2017-11-26 06:36:01-08,"[jira] [Resolved] (AIRFLOW-1845) Modal background doesn''t cover
 wide or tall pages","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1845?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1845.
-------------------------------------
       Resolution: Fixed
    Fix Version/s:     (was: 1.10.0)
                   1.9.1

Issue resolved by pull request #2813
[https://github.com/apache/incubator-airflow/pull/2813]

> Modal background doesn''t cover wide or tall pages
> -------------------------------------------------
>
>                 Key: AIRFLOW-1845
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1845
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.1, 1.9.0
>            Reporter: Ash Berlin-Taylor
>            Assignee: Ash Berlin-Taylor
>             Fix For: 1.9.1
>
>         Attachments: Screen Shot 2017-11-24 at 12.19.50.png
>
>
> If there is any kind of scrolling on the page behind a modal pop up then the grey backgorund behind the modal dialog doesn''t correctly cover outside the ""first"" page view. For example see the attached screenshot (which for some reason Jira isn''t embedding...)
> !Screen Shot 2017-11-24 at 12.19.50.png|thumbnail!
> To reproduce: go to a long or tall dag tree view page, scroll first, then click on a task instance to get the modal popup.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
574975,135,375144053.1134842616011.JavaMail.jira@ajax.apache.org,22134,Craig Russell (JIRA),397484532.1132073729539.JavaMail.jira@ajax.apache.org,22134,Craig Russell (JIRA),2005-12-17 10:03:36-08,[jira] Resolved: (JDO-214) Add a dtd and xsd for jdoquery documents,"     [ http://issues.apache.org/jira/browse/JDO-214?page=all ]
     
Craig Russell resolved JDO-214:
-------------------------------

    Resolution: Fixed

Committed revision 357357.

> Add a dtd and xsd for jdoquery documents
> ----------------------------------------
>
>          Key: JDO-214
>          URL: http://issues.apache.org/jira/browse/JDO-214
>      Project: JDO
>         Type: Bug
>   Components: api20
>     Reporter: Craig Russell
>     Assignee: Craig Russell
>  Attachments: jdoquery_2_0.dtd, xsd.patch
>
> JDO queries can be defined in their own files, separate from the jdo or orm files. As such, they need a document descriptor.

-- 
This message is automatically generated by JIRA.
-
If you think it was sent incorrectly contact one of the administrators:
   http://issues.apache.org/jira/secure/Administrators.jspa
-
For more information on JIRA, see:
   http://www.atlassian.com/software/jira


",f
27296,54,JIRA.13120247.1511359498000.319759.1511707020073@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120247.1511359498000@Atlassian.JIRA,,,2017-11-26 06:37:00-08,"[jira] [Commented] (AIRFLOW-1842) Create gcs to gcs operator to
 copy, rename and both","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1842?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16266026#comment-16266026 ] 

ASF subversion and git services commented on AIRFLOW-1842:
----------------------------------------------------------

Commit 4247ff0228afd06b8210321dcf660e6e7aca7b62 in incubator-airflow''s branch refs/heads/master from [~kaxilnaik]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4247ff0 ]

[AIRFLOW-1842] Fixed Super class name for the gcs to gcs copy operator

Fixed incorrect Super class name in gcs to gcs
copy operator from `GoogleCloudStorageOperatorToGo
ogleCloudStorageOperator` to
`GoogleCloudStorageToGoogleCloudStorageOperator`.

Closes #2812 from kaxil/patch-2


> Create gcs to gcs operator to copy, rename and both
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1842
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1842
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>              Labels: features
>
> Copies an object from a Google Cloud Storage bucket to another, with renaming if requested.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27297,54,JIRA.13119958.1511274032000.319764.1511707140754@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13119958.1511274032000@Atlassian.JIRA,,,2017-11-26 06:39:00-08,"[jira] [Commented] (AIRFLOW-1838) Properly log collect_dags
 Exception","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1838?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16266028#comment-16266028 ] 

ASF subversion and git services commented on AIRFLOW-1838:
----------------------------------------------------------

Commit f5df0d3437e206e2a8bc9586c4fc2383fa0cf043 in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f5df0d3 ]

[AIRFLOW-1838] Properly log collect_dags exception

As a user it would be nice to properly log the
exceptions
thrown in the collect_dags function to debug the
faulty dags

Closes #2803 from Fokko/AIRFLOW-1838-Properly-log-
collect-dags-exception


> Properly log collect_dags Exception
> -----------------------------------
>
>                 Key: AIRFLOW-1838
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1838
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>
> As a user it would be nice to properly log the exceptions thrown in the collect_dags function to debug the faulty dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27298,54,JIRA.13119958.1511274032000.319765.1511707140763@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13119958.1511274032000@Atlassian.JIRA,,,2017-11-26 06:39:00-08,"[jira] [Commented] (AIRFLOW-1838) Properly log collect_dags
 Exception","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1838?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16266029#comment-16266029 ] 

ASF subversion and git services commented on AIRFLOW-1838:
----------------------------------------------------------

Commit f5df0d3437e206e2a8bc9586c4fc2383fa0cf043 in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f5df0d3 ]

[AIRFLOW-1838] Properly log collect_dags exception

As a user it would be nice to properly log the
exceptions
thrown in the collect_dags function to debug the
faulty dags

Closes #2803 from Fokko/AIRFLOW-1838-Properly-log-
collect-dags-exception


> Properly log collect_dags Exception
> -----------------------------------
>
>                 Key: AIRFLOW-1838
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1838
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>
> As a user it would be nice to properly log the exceptions thrown in the collect_dags function to debug the faulty dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27299,54,JIRA.13119958.1511274032000.319770.1511707200374@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13119958.1511274032000@Atlassian.JIRA,,,2017-11-26 06:40:00-08,"[jira] [Resolved] (AIRFLOW-1838) Properly log collect_dags
 Exception","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1838?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1838.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2803
[https://github.com/apache/incubator-airflow/pull/2803]

> Properly log collect_dags Exception
> -----------------------------------
>
>                 Key: AIRFLOW-1838
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1838
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>             Fix For: 1.9.1
>
>
> As a user it would be nice to properly log the exceptions thrown in the collect_dags function to debug the faulty dags.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27300,54,JIRA.13120718.1511609594000.320116.1511716380066@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-26 09:13:00-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- 
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- for higher throughput, kafka?
- Security, authentication and other related subject might be adresses in another ticket.

  was:
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- 
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- Security, authentication and other related subject might be adresses in another ticket.


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> h1. Webhook sensor
> May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.
> Documentation would be updated to describe the classic scheme to implement this use case, which would look like:
> !airflow-webhook-proposal.png!
> I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  
> h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
> {code}
> sensor = JsonWebHookSensor(
>             task_id=''my_task_id'',
>             name=""on_github_push""
>         )
> .. user is responsible to triggering the processing DAG himself.
> {code}
> In my github project, I register the following URL in webhook page:
> {code}
> http://airflow.myserver.com/api/experimental/webhook/on_github_push
> {code}
> From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.
> The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.
> h2. Documenation update
> - add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
> - describe the sensing dag + processing dag scheme and provide the github use case as real life example
> h2. Possible evolutions
> - 
> - use an external queue (redis, amqp) to handle lot of events
> - allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
> - for higher throughput, kafka?
> - Security, authentication and other related subject might be adresses in another ticket.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27301,54,JIRA.13120718.1511609594000.320117.1511716440058@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-26 09:14:00-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- for higher throughput, kafka?
- Security, authentication and other related subject might be adresses in another ticket.

  was:
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- 
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- for higher throughput, kafka?
- Security, authentication and other related subject might be adresses in another ticket.


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> h1. Webhook sensor
> May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.
> Documentation would be updated to describe the classic scheme to implement this use case, which would look like:
> !airflow-webhook-proposal.png!
> I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  
> h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
> {code}
> sensor = JsonWebHookSensor(
>             task_id=''my_task_id'',
>             name=""on_github_push""
>         )
> .. user is responsible to triggering the processing DAG himself.
> {code}
> In my github project, I register the following URL in webhook page:
> {code}
> http://airflow.myserver.com/api/experimental/webhook/on_github_push
> {code}
> From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.
> The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.
> h2. Documenation update
> - add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
> - describe the sensing dag + processing dag scheme and provide the github use case as real life example
> h2. Possible evolutions
> - use an external queue (redis, amqp) to handle lot of events
> - allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
> - for higher throughput, kafka?
> - Security, authentication and other related subject might be adresses in another ticket.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27302,54,JIRA.13120718.1511609594000.320120.1511716860046@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-26 09:21:00-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- use an external queue (redis, amqp) to handle lot of events
- subscribe in a pub/sub ?
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- for higher throughput, kafka?
- Security, authentication and other related subject might be adresses in another ticket.

  was:
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- use an external queue (redis, amqp) to handle lot of events
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- for higher throughput, kafka?
- Security, authentication and other related subject might be adresses in another ticket.


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> h1. Webhook sensor
> May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.
> Documentation would be updated to describe the classic scheme to implement this use case, which would look like:
> !airflow-webhook-proposal.png!
> I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  
> h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
> {code}
> sensor = JsonWebHookSensor(
>             task_id=''my_task_id'',
>             name=""on_github_push""
>         )
> .. user is responsible to triggering the processing DAG himself.
> {code}
> In my github project, I register the following URL in webhook page:
> {code}
> http://airflow.myserver.com/api/experimental/webhook/on_github_push
> {code}
> From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.
> The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.
> h2. Documenation update
> - add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
> - describe the sensing dag + processing dag scheme and provide the github use case as real life example
> h2. Possible evolutions
> - use an external queue (redis, amqp) to handle lot of events
> - subscribe in a pub/sub ?
> - allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
> - for higher throughput, kafka?
> - Security, authentication and other related subject might be adresses in another ticket.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27303,54,JIRA.13120718.1511609594000.320124.1511717040057@Atlassian.JIRA,1903,Semet (JIRA),JIRA.13120718.1511609594000@Atlassian.JIRA,,,2017-11-26 09:24:00-08,[jira] [Updated] (AIRFLOW-1847) Webhook Sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1847?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Semet updated AIRFLOW-1847:
---------------------------
    Description: 
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- use an external queue (redis, amqp) to handle lot of events
- subscribe in a pub/sub system such as WAMP?
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- for higher throughput, kafka?
- Security, authentication and other related subject might be adresses in another ticket.

  was:
h1. Webhook sensor
May require a hook in the experimental API
Register an api endpoint and wait for input on each.

It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)

Use Case:
- A Dag registers a WebHook sensor named {{<webhookname>}}
- An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
- I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
- when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
- sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.

If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.

To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.

Documentation would be updated to describe the classic scheme to implement this use case, which would look like:

!airflow-webhook-proposal.png!

I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  

h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
{code}
sensor = JsonWebHookSensor(
            task_id=''my_task_id'',
            name=""on_github_push""
        )
.. user is responsible to triggering the processing DAG himself.
{code}

In my github project, I register the following URL in webhook page:

{code}
http://airflow.myserver.com/api/experimental/webhook/on_github_push
{code}

>From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.

The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.

h2. Documenation update
- add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
- describe the sensing dag + processing dag scheme and provide the github use case as real life example


h2. Possible evolutions
- use an external queue (redis, amqp) to handle lot of events
- subscribe in a pub/sub ?
- allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
- for higher throughput, kafka?
- Security, authentication and other related subject might be adresses in another ticket.


> Webhook Sensor
> --------------
>
>                 Key: AIRFLOW-1847
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1847
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core, operators
>            Reporter: Semet
>            Assignee: Semet
>            Priority: Minor
>              Labels: api, sensors, webhook
>         Attachments: airflow-webhook-proposal.png
>
>
> h1. Webhook sensor
> May require a hook in the experimental API
> Register an api endpoint and wait for input on each.
> It is different than the {{dag_runs}} api in that the format is not airflow specific, it is just a callback web url called by an external system on some even with its application specific content. The content in really important and need to be sent to the dag (as XCom?)
> Use Case:
> - A Dag registers a WebHook sensor named {{<webhookname>}}
> - An custom endpoint is exposed at {{http://myairflow.server/api/experimental/webhook/<webhookname>}}.
> - I set this URL in the external system I wish to use the webhook from. Ex: github/gitlab project webhook
> - when the external application performs a request to this URL, this is automatically sent to the WebHook sensor. For simplicity, we can have a JsonWebHookSensor that would be able to carry any kind of json content.
> - sensor only job would be normally to trigger the exection of a DAG, providing it with the json content as xcom.
> If there are several requests at the same time, the system should be scalable enough to not die or not slow down the webui. It is also possible to instantiate an independant flask/gunicorn server to split the load. It would mean it runs on another port, but this could be just an option in the configuration file or even a complete independant application ({{airflow webhookserver}}). I saw recent changes integrated gunicorn in airflow core, guess it can help this use case.
> To support the charge, I think it is good that the part in the API just post the received request in an internal queue so the Sensor can handle them later without risk of missing one.
> Documentation would be updated to describe the classic scheme to implement this use case, which would look like:
> !airflow-webhook-proposal.png!
> I think it is good to split it into 2 DAGs, one for linear handling of the messages and triggering new DAG, and the processing DAG that might be executed in parallel.  
> h2. Example usage in Sensor DAG: trigger a DAG on GitHub Push Event
> {code}
> sensor = JsonWebHookSensor(
>             task_id=''my_task_id'',
>             name=""on_github_push""
>         )
> .. user is responsible to triggering the processing DAG himself.
> {code}
> In my github project, I register the following URL in webhook page:
> {code}
> http://airflow.myserver.com/api/experimental/webhook/on_github_push
> {code}
> From now on, on push, github will send a [json with this format|https://developer.github.com/v3/activity/events/types/#pushevent] to the previous URL.
> The {{JsonWebHookSensor}} receives the payload, and a new dag is triggered in this Sensing Dag.
> h2. Documenation update
> - add new item in the [scheduling documentation|https://pythonhosted.org/airflow/scheduler.html] about how to trigger a DAG using a webhook
> - describe the sensing dag + processing dag scheme and provide the github use case as real life example
> h2. Possible evolutions
> - use an external queue (redis, amqp) to handle lot of events
> - subscribe in a pub/sub system such as WAMP?
> - allow batch processing (trigger processing DAG on n events or after a timeout, gathering n messages alltogether)
> - for higher throughput, kafka?
> - Security, authentication and other related subject might be adresses in another ticket.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27304,54,JIRA.13117734.1510403686000.320200.1511720040300@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117734.1510403686000@Atlassian.JIRA,,,2017-11-26 10:14:00-08,"[jira] [Resolved] (AIRFLOW-1810) Remove unused mysql import in
 migrations","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1810?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1810.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2782
[https://github.com/apache/incubator-airflow/pull/2782]

> Remove unused mysql import in migrations
> ----------------------------------------
>
>                 Key: AIRFLOW-1810
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1810
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Sanjay Pillai
>            Assignee: Sanjay Pillai
>            Priority: Minor
>             Fix For: 1.9.1
>
>
> sqlalachemy.dialects.mysql is imported in few migraitons, but not used. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27305,54,JIRA.13117734.1510403686000.320197.1511720040261@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117734.1510403686000@Atlassian.JIRA,,,2017-11-26 10:14:00-08,"[jira] [Commented] (AIRFLOW-1810) Remove unused mysql import in
 migrations","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1810?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16266117#comment-16266117 ] 

ASF subversion and git services commented on AIRFLOW-1810:
----------------------------------------------------------

Commit 0422157864a9369a5f83aae51a72fd85ffeea87b in incubator-airflow''s branch refs/heads/master from [~sanjay.pillai]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=0422157 ]

[AIRFLOW-1810] Remove unused mysql import in migrations.

Closes #2782 from MortalViews/master


> Remove unused mysql import in migrations
> ----------------------------------------
>
>                 Key: AIRFLOW-1810
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1810
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Sanjay Pillai
>            Assignee: Sanjay Pillai
>            Priority: Minor
>             Fix For: 1.9.1
>
>
> sqlalachemy.dialects.mysql is imported in few migraitons, but not used. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27306,54,JIRA.13118754.1510791920000.320204.1511720160091@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13118754.1510791920000@Atlassian.JIRA,,,2017-11-26 10:16:00-08,"[jira] [Commented] (AIRFLOW-1820) dagrun.dependency-check metric
 does not play nice with statsd","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1820?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16266120#comment-16266120 ] 

ASF subversion and git services commented on AIRFLOW-1820:
----------------------------------------------------------

Commit d4816667e5f3bbed293c86b1a23db5d30cf38f2a in incubator-airflow''s branch refs/heads/master from [~wrp]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d481666 ]

[AIRFLOW-1820] Remove timestamp from metric name

Closes #2792 from wrp/datetime


> dagrun.dependency-check metric does not play nice with statsd
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1820
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1820
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>            Reporter: William Pursell
>            Assignee: William Pursell
>            Priority: Minor
>   Original Estimate: 10m
>  Remaining Estimate: 10m
>
> The metric dagrun.dependency-check (https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L4594) is sent to statsd with spaces and colons in the metric name.  This is not ideal for statsd.  (I''m not really sure I see the point of putting a timestamp in a metric name at all!)  We should either do something like Stats.timing(""dagrun.dependency-check.{}{:%%%%Y%%%%m%%%%d_%%%%H%%%%M%%%%S}"". to use a nicer name, or just drop the timestamp completely.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27307,54,JIRA.13118754.1510791920000.320207.1511720220189@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13118754.1510791920000@Atlassian.JIRA,,,2017-11-26 10:17:00-08,"[jira] [Resolved] (AIRFLOW-1820) dagrun.dependency-check metric
 does not play nice with statsd","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1820?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1820.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2792
[https://github.com/apache/incubator-airflow/pull/2792]

> dagrun.dependency-check metric does not play nice with statsd
> -------------------------------------------------------------
>
>                 Key: AIRFLOW-1820
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1820
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: core
>            Reporter: William Pursell
>            Assignee: William Pursell
>            Priority: Minor
>             Fix For: 1.9.1
>
>   Original Estimate: 10m
>  Remaining Estimate: 10m
>
> The metric dagrun.dependency-check (https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L4594) is sent to statsd with spaces and colons in the metric name.  This is not ideal for statsd.  (I''m not really sure I see the point of putting a timestamp in a metric name at all!)  We should either do something like Stats.timing(""dagrun.dependency-check.{}{:%%%%Y%%%%m%%%%d_%%%%H%%%%M%%%%S}"". to use a nicer name, or just drop the timestamp completely.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27308,54,JIRA.13116697.1510066613000.320251.1511720640100@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13116697.1510066613000@Atlassian.JIRA,,,2017-11-26 10:24:00-08,[jira] [Commented] (AIRFLOW-1790) AWS Batch Operator Suppport,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1790?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16266128#comment-16266128 ] 

ASF subversion and git services commented on AIRFLOW-1790:
----------------------------------------------------------

Commit 68d3a80dcb427bcb9e3bee354fbff3d0379d9c1c in incubator-airflow''s branch refs/heads/master from [~hprudent]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=68d3a80 ]

[AIRFLOW-1790] Add support for AWS Batch operator

Closes #2762 from hprudent/aws-batch


> AWS Batch Operator Suppport
> ---------------------------
>
>                 Key: AIRFLOW-1790
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1790
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: operators
>    Affects Versions: 1.9.0
>            Reporter: Hugo Prudente
>            Assignee: Hugo Prudente
>
> Add support to AWS Batch operator



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27309,54,JIRA.12982154.1466638602000.321099.1511748240085@Atlassian.JIRA,1428,Narek Amirbekian (JIRA),JIRA.12982154.1466638602000@Atlassian.JIRA,,,2017-11-26 18:04:00-08,[jira] [Closed] (AIRFLOW-272) Create a DagRun sensor,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-272?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Narek Amirbekian closed AIRFLOW-272.
------------------------------------
    Resolution: Won''t Fix

> Create a DagRun sensor
> ----------------------
>
>                 Key: AIRFLOW-272
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-272
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Narek Amirbekian
>            Assignee: Narek Amirbekian
>            Priority: Minor
>
> There is no way to tell a task or sub-dag to not run until a certain DagRun is complete. We could implemnt a DagRunStateSensor similar to the ExternalTaskSensor.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27310,54,JIRA.13073389.1495190402000.321988.1511766300203@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13073389.1495190402000@Atlassian.JIRA,,,2017-11-26 23:05:00-08,"[jira] [Closed] (AIRFLOW-1229) Make ""Run Id"" column clickable in
 Browse -> DAG Runs","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1229?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1229.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

> Make ""Run Id"" column clickable in Browse -> DAG Runs
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1229
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1229
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: webapp
>    Affects Versions: Airflow 1.8
>         Environment: Python3.4
>            Reporter: Erik Cederstrand
>            Assignee: Alexander Bij
>              Labels: patch
>             Fix For: 1.9.1
>
>         Attachments: dag_run_link.patch
>
>
> I''m triggering a lot of DAGs manually using ""airflow trigger_dag my_dag --run_id=some_unique_id"". I would like to be able in the UI to browse easily to this specific DAG run using the ""some_unique_id"" label. In the graph page of the DAG, I need to know the exact execution date, which is inconvenient, and in the Browse -> DAG Runs page I can search by ""some_unique_id"", but the ""Run Ids"" column is not clickable.
> The attached patch makes the aforementioned column clickable, so I''m sent directly to the graph view for that specific DAG run, not the DAG in general.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27311,54,JIRA.13120247.1511359498000.321993.1511766360827@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120247.1511359498000@Atlassian.JIRA,,,2017-11-26 23:06:00-08,"[jira] [Reopened] (AIRFLOW-1842) Create gcs to gcs operator to
 copy, rename and both","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1842?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong reopened AIRFLOW-1842:
---------------------------------------

> Create gcs to gcs operator to copy, rename and both
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1842
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1842
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>              Labels: features
>             Fix For: 1.9.1
>
>
> Copies an object from a Google Cloud Storage bucket to another, with renaming if requested.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27312,54,JIRA.13120247.1511359498000.321996.1511766360857@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120247.1511359498000@Atlassian.JIRA,,,2017-11-26 23:06:00-08,"[jira] [Closed] (AIRFLOW-1842) Create gcs to gcs operator to copy,
 rename and both","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1842?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1842.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

> Create gcs to gcs operator to copy, rename and both
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1842
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1842
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>              Labels: features
>             Fix For: 1.9.1
>
>
> Copies an object from a Google Cloud Storage bucket to another, with renaming if requested.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27313,54,JIRA.13120203.1511350073000.322002.1511766360920@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120203.1511350073000@Atlassian.JIRA,,,2017-11-26 23:06:00-08,"[jira] [Closed] (AIRFLOW-1841) GoogleCloudStorageDownloadOperator
 filename parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1841?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1841.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

> GoogleCloudStorageDownloadOperator filename parameter
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1841
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1841
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>    Affects Versions: 1.8.0, 1.9.0
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Trivial
>              Labels: documentation
>             Fix For: 1.9.1
>
>
> https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/contrib/operators/gcs_download_operator.py
> Documentation - ""... If false, the downloaded data will not be stored on the local file system.""
> But filename type is a string in the documentation, it should be a union of String and Bool, but better change it to None. And change Hook as well to None because all it does just check the presence of a value in the parameter.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27314,54,JIRA.13120203.1511350073000.321999.1511766360894@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120203.1511350073000@Atlassian.JIRA,,,2017-11-26 23:06:00-08,"[jira] [Reopened] (AIRFLOW-1841) GoogleCloudStorageDownloadOperator
 filename parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1841?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong reopened AIRFLOW-1841:
---------------------------------------

> GoogleCloudStorageDownloadOperator filename parameter
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1841
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1841
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp, operators
>    Affects Versions: 1.8.0, 1.9.0
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Trivial
>              Labels: documentation
>             Fix For: 1.9.1
>
>
> https://github.com/apache/incubator-airflow/blob/v1-8-stable/airflow/contrib/operators/gcs_download_operator.py
> Documentation - ""... If false, the downloaded data will not be stored on the local file system.""
> But filename type is a string in the documentation, it should be a union of String and Bool, but better change it to None. And change Hook as well to None because all it does just check the presence of a value in the parameter.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706983,24,153196195282.27023.3976558110406408047.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 17:59:12-07,"[GitHub] tornadomeet commented on issue #11806: [MXNET-687] Fix spatial
 transformer op","tornadomeet commented on issue #11806: [MXNET-687] Fix spatial transformer op
URL: https://github.com/apache/incubator-mxnet/pull/11806#issuecomment-406120194
 
 
   @anirudh2290 the change of `spatial_transformer.cu` LGTM. the other part @szha 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27315,54,JIRA.13119675.1511188012000.322008.1511766420190@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13119675.1511188012000@Atlassian.JIRA,,,2017-11-26 23:07:00-08,"[jira] [Closed] (AIRFLOW-1831) Add Spark submit driver classpath
 parameter","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1831?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1831.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

> Add Spark submit driver classpath parameter
> -------------------------------------------
>
>                 Key: AIRFLOW-1831
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1831
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: hooks, operators
>            Reporter: Daniel van der Ende
>            Assignee: Daniel van der Ende
>            Priority: Minor
>             Fix For: 1.9.1
>
>
> Spark allows you to set the classpath for the driver only. This can save overhead, if you only need a specific jar on the driver, for instance. The Airflow spark_submit_operator/spark_submit_hook do not support this. This should be added.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27316,54,JIRA.13119325.1510952184000.322005.1511766420164@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13119325.1510952184000@Atlassian.JIRA,,,2017-11-26 23:07:00-08,"[jira] [Closed] (AIRFLOW-1830) Support multiple domains in Google''s
 authentication backend","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1830?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1830.
-------------------------------------
       Resolution: Fixed
         Assignee: Guillermo Rodr=C3=ADguez Cano
    Fix Version/s: 1.9.1

> Support multiple domains in Google''s authentication backend
> -----------------------------------------------------------
>
>                 Key: AIRFLOW-1830
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1830
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: authentication, contrib
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>            Assignee: Guillermo Rodr=C3=ADguez Cano
>            Priority: Minor
>              Labels: easyfix, triaged
>             Fix For: 1.9.1
>
>
> Google''s OAuth authentication backend supports only one domain but multip=
le may be needed in some scenarios



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27317,54,JIRA.13117919.1510576163000.322013.1511766480608@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13117919.1510576163000@Atlassian.JIRA,,,2017-11-26 23:08:00-08,"[jira] [Closed] (AIRFLOW-1813) SSH Operator errors on commands with
 no output","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1813?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1813.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

> SSH Operator errors on commands with no output
> ----------------------------------------------
>
>                 Key: AIRFLOW-1813
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1813
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Rob Keevil
>             Fix For: 1.9.1
>
>   Original Estimate: 1h
>  Remaining Estimate: 1h
>
> The SSH Operator will throw an empty ""SSH operator error"" when running commands that do not immediately log something to the terminal.  This is due to a call to stdout.channel.recv when the channel currently has a 0-size buffer, either because the command has not yet logged anything, or never will (e.g. sleep 5).  A simple check of the buffer size before reading will fix this issue, will link a PR shortly



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27318,54,JIRA.13118487.1510738273000.322010.1511766480560@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13118487.1510738273000@Atlassian.JIRA,,,2017-11-26 23:08:00-08,"[jira] [Closed] (AIRFLOW-1817) setup.py for S3 installs old boto
 library","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1817?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1817.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

> setup.py for S3 installs old boto library
> -----------------------------------------
>
>                 Key: AIRFLOW-1817
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1817
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Cedrik Neumann
>            Assignee: Cedrik Neumann
>             Fix For: 1.9.1
>
>
> In `setup.py` the s3 dependency still relies on the old boto library even though the s3 hook has been reimplemented with boto3.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27319,54,JIRA.13117658.1510343120000.322017.1511766480670@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13117658.1510343120000@Atlassian.JIRA,,,2017-11-26 23:08:00-08,"[jira] [Closed] (AIRFLOW-1801) Url encode all execution dates in
 the UI","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1801?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1801.
-------------------------------------
       Resolution: Fixed
         Assignee: Bolke de Bruin
    Fix Version/s: 1.9.1

> Url encode all execution dates in the UI
> ----------------------------------------
>
>                 Key: AIRFLOW-1801
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1801
>             Project: Apache Airflow
>          Issue Type: Sub-task
>          Components: ui
>    Affects Versions: 1.9.0
>            Reporter: Bolke de Bruin
>            Assignee: Bolke de Bruin
>             Fix For: 1.9.1
>
>
> Execution dates are not properly url encoded therefore losing timezone information



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27320,54,JIRA.13099790.1504605491000.322019.1511766540161@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13099790.1504605491000@Atlassian.JIRA,,,2017-11-26 23:09:00-08,"[jira] [Closed] (AIRFLOW-1563) OSError while attempting to symlink
 latest log folder","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1563?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1563.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

> OSError while attempting to symlink latest log folder
> -----------------------------------------------------
>
>                 Key: AIRFLOW-1563
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1563
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: cli
>    Affects Versions: Airflow 1.8
>            Reporter: Niels Zeilemaker
>            Assignee: Niels Zeilemaker
>             Fix For: 1.9.1
>
>
> We''re getting an OSError due a azure fileshare volume mount being used for the logs
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 882, in scheduler
>     job.run()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 200, in run
>     self._execute()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 1312, in _execute
>     self._execute_helper(processor_manager)
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/jobs.py"", line 1409, in _execute_helper
>     simple_dags = processor_manager.heartbeat()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/dag_processing.py"", line 631, in heartbeat
>     self.symlink_latest_log_directory()
>   File ""/usr/local/lib/python2.7/dist-packages/airflow/utils/dag_processing.py"", line 525, in symlink_latest_log_directory
>     os.symlink(log_directory, latest_log_directory_path)
> OSError: [Errno 95] Operation not supported



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27321,54,JIRA.13120967.1511794207000.324535.1511794260195@Atlassian.JIRA,2246,Crystal Qian (JIRA),JIRA.13120967.1511794207000@Atlassian.JIRA,,,2017-11-27 06:51:00-08,"[jira] [Created] (AIRFLOW-1848) Dataflow operator py_file arg
 doesn''t require .py suffix","Crystal Qian created AIRFLOW-1848:
-------------------------------------

             Summary: Dataflow operator py_file arg doesn''t require .py suffix
                 Key: AIRFLOW-1848
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1848
             Project: Apache Airflow
          Issue Type: Bug
          Components: contrib
            Reporter: Crystal Qian
            Assignee: Crystal Qian
            Priority: Minor






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27322,54,JIRA.13120988.1511796793000.324776.1511796840243@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120988.1511796793000@Atlassian.JIRA,,,2017-11-27 07:34:00-08,"[jira] [Created] (AIRFLOW-1850) Sqoop password is overwritten by
 reference","Fokko Driesprong created AIRFLOW-1850:
-----------------------------------------

             Summary: Sqoop password is overwritten by reference
                 Key: AIRFLOW-1850
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1850
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Fokko Driesprong


The sqoop operator masks the password so it does not appear in the logs. This is good, but the password itself is overwritten since the map is passed by reference.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27323,54,JIRA.13120987.1511796786000.324775.1511796840229@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120987.1511796786000@Atlassian.JIRA,,,2017-11-27 07:34:00-08,"[jira] [Created] (AIRFLOW-1849) Sqoop password is overwritten by
 reference","Fokko Driesprong created AIRFLOW-1849:
-----------------------------------------

             Summary: Sqoop password is overwritten by reference
                 Key: AIRFLOW-1849
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1849
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Fokko Driesprong


The sqoop operator masks the password so it does not appear in the logs. This is good, but the password itself is overwritten since the map is passed by reference.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27324,54,JIRA.13120987.1511796786000.324794.1511797080141@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120987.1511796786000@Atlassian.JIRA,,,2017-11-27 07:38:00-08,"[jira] [Closed] (AIRFLOW-1849) Sqoop password is overwritten by
 reference","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1849?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong closed AIRFLOW-1849.
-------------------------------------
    Resolution: Duplicate

> Sqoop password is overwritten by reference
> ------------------------------------------
>
>                 Key: AIRFLOW-1849
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1849
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>
> The sqoop operator masks the password so it does not appear in the logs. This is good, but the password itself is overwritten since the map is passed by reference.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27325,54,JIRA.13120988.1511796793000.324797.1511797080275@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13120988.1511796793000@Atlassian.JIRA,,,2017-11-27 07:38:00-08,"[jira] [Assigned] (AIRFLOW-1850) Sqoop password is overwritten by
 reference","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1850?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong reassigned AIRFLOW-1850:
-----------------------------------------

    Assignee: Fokko Driesprong

> Sqoop password is overwritten by reference
> ------------------------------------------
>
>                 Key: AIRFLOW-1850
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1850
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Assignee: Fokko Driesprong
>
> The sqoop operator masks the password so it does not appear in the logs. This is good, but the password itself is overwritten since the map is passed by reference.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27326,54,JIRA.13121036.1511805924000.325804.1511805960249@Atlassian.JIRA,2408,Tylar Murray (JIRA),JIRA.13121036.1511805924000@Atlassian.JIRA,,,2017-11-27 10:06:00-08,"[jira] [Created] (AIRFLOW-1851) next/previous buttons on DAG Tree
 View UI","Tylar Murray created AIRFLOW-1851:
-------------------------------------

             Summary: next/previous buttons on DAG Tree View UI
                 Key: AIRFLOW-1851
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1851
             Project: Apache Airflow
          Issue Type: New Feature
          Components: webapp
            Reporter: Tylar Murray
            Priority: Minor


I would be surprised if this was not already requested/documented somewhere, but I was not able to find it. 

Next and Previous buttons in the webapp tree view would be very useful for me. Currently I have to look at the earliest/latest run showing and adjust the Base date manually to ""page"" through tasks. 

Seems pretty straightforward so it might make a good first issue, but then again I have no idea what the webapp code looks like.

I''m happy to provide more info on request, but I think that about covers it.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27327,54,JIRA.13042441.1486939198000.326398.1511809023161@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13042441.1486939198000@Atlassian.JIRA,,,2017-11-27 10:57:03-08,[jira] [Commented] (AIRFLOW-868) Add PostgresToGCSOperator,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-868?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267254#comment-16267254 ] 

ASF subversion and git services commented on AIRFLOW-868:
---------------------------------------------------------

Commit d8fa2e9049328341dc58a635b34f04fa52de543e in incubator-airflow''s branch refs/heads/master from [~ajbosco]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d8fa2e9 ]

[AIRFLOW-868] Add postgres_to_gcs operator and unittests

Adds a postgres_to_gcs operator to contrib so that a user can copy a
dump from postgres to google cloud storage. Tests write to local
NamedTemporayFiles so we correctly test serializing encoded ndjson in
both python3 and python2.7.


> Add PostgresToGCSOperator
> -------------------------
>
>                 Key: AIRFLOW-868
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-868
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Adam Boscarino
>            Assignee: Adam Boscarino
>            Priority: Trivial
>
> As a user, I would like the ability to extract data from a Postgres database to Google Cloud Storage in a manner similar to the existing MySQL implementation. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27328,54,JIRA.13102370.1505420120000.326394.1511809023123@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13102370.1505420120000@Atlassian.JIRA,,,2017-11-27 10:57:03-08,"[jira] [Commented] (AIRFLOW-1613) Make
 MySqlToGoogleCloudStorageOperator compaitible with python3","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1613?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267253#comment-16267253 ] 

ASF subversion and git services commented on AIRFLOW-1613:
----------------------------------------------------------

Commit 2f79610a3ef726e88dec238de000d9295ae7d2a9 in incubator-airflow''s branch refs/heads/master from Devon Peticolas
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2f79610 ]

[AIRFLOW-1613] make mysql_to_gcs_operator py3 compatible

Uses `__future__.unicode_literals` and replaces calling `json.dumps`
with `json.dump` followed by `tmp_file_handle.write` to write json lines
to the ndjson file. When using python3, `json.dump` will return a
unicode string instead of a byte string, therefore we encode the unicode
string to `utf-8` which is compatible with bigquery (see:
https://cloud.google.com/bigquery/docs/loading-data#loading_encoded_data).


> Make MySqlToGoogleCloudStorageOperator compaitible with python3
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1613
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1613
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Joy Gao
>            Assignee: Joy Gao
>             Fix For: 1.9.0
>
>
> 1. 
> In Python 3, map(...) returns an iterator, which can only be iterated over once. 
> Therefore the current implementation will return an empty list after the first iteration of schema:
> {code}
>         schema = map(lambda schema_tuple: schema_tuple[0], cursor.description)
>         file_no = 0
>         tmp_file_handle = NamedTemporaryFile(delete=True)
>         tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}
>         for row in cursor:
>             # Convert datetime objects to utc seconds, and decimals to floats
>             row = map(self.convert_types, row)
>             row_dict = dict(zip(schema, row))
> {code}
> 2.
> File opened as binary, but string are written to it. Get error `a bytes-like object is required, not ''str''`. Use mode=''w'' instead.
> 3.
> Operator currently does not support binary columns in mysql.  We should support uploading binary columns from mysql to cloud storage as it''s a pretty common use-case. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27329,54,JIRA.13042441.1486939198000.326523.1511809141159@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13042441.1486939198000@Atlassian.JIRA,,,2017-11-27 10:59:01-08,[jira] [Resolved] (AIRFLOW-868) Add PostgresToGCSOperator,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-868?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-868.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> Add PostgresToGCSOperator
> -------------------------
>
>                 Key: AIRFLOW-868
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-868
>             Project: Apache Airflow
>          Issue Type: New Feature
>            Reporter: Adam Boscarino
>            Assignee: Adam Boscarino
>            Priority: Trivial
>             Fix For: 1.10.0
>
>
> As a user, I would like the ability to extract data from a Postgres database to Google Cloud Storage in a manner similar to the existing MySQL implementation. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27330,54,JIRA.13102370.1505420120000.326531.1511809141562@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13102370.1505420120000@Atlassian.JIRA,,,2017-11-27 10:59:01-08,"[jira] [Resolved] (AIRFLOW-1613) Make
 MySqlToGoogleCloudStorageOperator compaitible with python3","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1613?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1613.
--------------------------------------
       Resolution: Fixed
    Fix Version/s:     (was: 1.9.0)
                   1.10.0

> Make MySqlToGoogleCloudStorageOperator compaitible with python3
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1613
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1613
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Joy Gao
>            Assignee: Joy Gao
>             Fix For: 1.10.0
>
>
> 1. 
> In Python 3, map(...) returns an iterator, which can only be iterated over once. 
> Therefore the current implementation will return an empty list after the first iteration of schema:
> {code}
>         schema = map(lambda schema_tuple: schema_tuple[0], cursor.description)
>         file_no = 0
>         tmp_file_handle = NamedTemporaryFile(delete=True)
>         tmp_file_handles = {self.filename.format(file_no): tmp_file_handle}
>         for row in cursor:
>             # Convert datetime objects to utc seconds, and decimals to floats
>             row = map(self.convert_types, row)
>             row_dict = dict(zip(schema, row))
> {code}
> 2.
> File opened as binary, but string are written to it. Get error `a bytes-like object is required, not ''str''`. Use mode=''w'' instead.
> 3.
> Operator currently does not support binary columns in mysql.  We should support uploading binary columns from mysql to cloud storage as it''s a pretty common use-case. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27331,54,JIRA.13120434.1511432791000.326904.1511811360595@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120434.1511432791000@Atlassian.JIRA,,,2017-11-27 11:36:00-08,"[jira] [Commented] (AIRFLOW-1843) Add Google Cloud Storage Sensor
 with prefix","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1843?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267327#comment-16267327 ] 

ASF subversion and git services commented on AIRFLOW-1843:
----------------------------------------------------------

Commit d8115e982b19e39fb96362c1acb3bf1715bc9180 in incubator-airflow''s branch refs/heads/master from [~vait]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d8115e9 ]

[AIRFLOW-1843] Add Google Cloud Storage Sensor with prefix

Sensor for checking if there any files in bucket
at certain prefix

Closes #2809 from litdeviant/gcs_prefix_sensor


> Add Google Cloud Storage Sensor with prefix
> -------------------------------------------
>
>                 Key: AIRFLOW-1843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1843
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp
>    Affects Versions: 1.8.1, 1.9.1
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Hook can do list objects in bucket with prefix so I need sensor which will check bucket with prefix if there any incoming files.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27332,54,JIRA.13120434.1511432791000.326909.1511811360652@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13120434.1511432791000@Atlassian.JIRA,,,2017-11-27 11:36:00-08,"[jira] [Resolved] (AIRFLOW-1843) Add Google Cloud Storage Sensor
 with prefix","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1843?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1843.
--------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> Add Google Cloud Storage Sensor with prefix
> -------------------------------------------
>
>                 Key: AIRFLOW-1843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1843
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: contrib, gcp
>    Affects Versions: 1.8.1, 1.9.1
>            Reporter: Igors Vaitkus
>            Assignee: Igors Vaitkus
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Hook can do list objects in bucket with prefix so I need sensor which will check bucket with prefix if there any incoming files.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27333,54,JIRA.13117663.1510343855000.327468.1511815200729@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117663.1510343855000@Atlassian.JIRA,,,2017-11-27 12:40:00-08,"[jira] [Commented] (AIRFLOW-1804) Add airflow.cfg option for time
 zone configuration","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1804?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267454#comment-16267454 ] 

ASF subversion and git services commented on AIRFLOW-1804:
----------------------------------------------------------

Commit a47255fb2dad6035d03fe7acc50d2e0e65639c3e in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=a47255f ]

[AIRFLOW-1804] Add time zone configuration options

Time zone defaults to UTC as is the default now in order
to maintain backwards compatibility.


> Add airflow.cfg option for time zone configuration
> --------------------------------------------------
>
>                 Key: AIRFLOW-1804
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1804
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27334,54,JIRA.13117729.1510401682000.327472.1511815200771@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117729.1510401682000@Atlassian.JIRA,,,2017-11-27 12:40:00-08,"[jira] [Commented] (AIRFLOW-1808) Convert all utcnow() to timezone
 aware utcnow()","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1808?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267455#comment-16267455 ] 

ASF subversion and git services commented on AIRFLOW-1808:
----------------------------------------------------------

Commit c857436b7565777695c8336dbac2ef60a74d71d1 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=c857436 ]

[AIRFLOW-1808] Convert all utcnow() to time zone aware

datetime.utcnow() does not set time zone information.


> Convert all utcnow() to timezone aware utcnow()
> -----------------------------------------------
>
>                 Key: AIRFLOW-1808
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1808
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27335,54,JIRA.13117724.1510396623000.327473.1511815200790@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117724.1510396623000@Atlassian.JIRA,,,2017-11-27 12:40:00-08,"[jira] [Commented] (AIRFLOW-1807) Make sure all date time fields
 store in UTC and are always time zone aware","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1807?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267456#comment-16267456 ] 

ASF subversion and git services commented on AIRFLOW-1807:
----------------------------------------------------------

Commit 2f168634aac4aa138f00634bbb9f4e3993346ffa in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=2f16863 ]

[AIRFLOW-1807] Force use of time zone aware db fields

This change will check if all date times being stored are
indeed timezone aware.


> Make sure all date time fields store in UTC and are always time zone aware
> --------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1807
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1807
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27336,54,JIRA.13117730.1510401757000.327483.1511815200917@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117730.1510401757000@Atlassian.JIRA,,,2017-11-27 12:40:00-08,[jira] [Commented] (AIRFLOW-1809) Fix all tests that use dates,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1809?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267458#comment-16267458 ] 

ASF subversion and git services commented on AIRFLOW-1809:
----------------------------------------------------------

Commit 9624f5f24e00299c66adfd799d2be59fabd17f03 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=9624f5f ]

[AIRFLOW-1809] Update tests to use timezone aware objects


> Fix all tests that use dates
> ----------------------------
>
>                 Key: AIRFLOW-1809
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1809
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27337,54,JIRA.13117723.1510396492000.327484.1511815200934@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117723.1510396492000@Atlassian.JIRA,,,2017-11-27 12:40:00-08,[jira] [Commented] (AIRFLOW-1806) Make scheduler aware of timezones,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1806?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267459#comment-16267459 ] 

ASF subversion and git services commented on AIRFLOW-1806:
----------------------------------------------------------

Commit 8aadc3112539f760bbd8b0454137e7a40091458c in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=8aadc31 ]

[AIRFLOW-1806] Use naive datetime when using cron


> Make scheduler aware of timezones
> ---------------------------------
>
>                 Key: AIRFLOW-1806
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1806
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>
> When supplied with time zone aware date times 
> dag.following_schedule and dag.previous_schedule should do arithmetic in local time in order to take care of day light savings time



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27338,54,JIRA.13117723.1510396492000.327481.1511815200898@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117723.1510396492000@Atlassian.JIRA,,,2017-11-27 12:40:00-08,[jira] [Commented] (AIRFLOW-1806) Make scheduler aware of timezones,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1806?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267457#comment-16267457 ] 

ASF subversion and git services commented on AIRFLOW-1806:
----------------------------------------------------------

Commit dcac3e97a4e1b4429e4baf9d8ab2a7eb4139ad74 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=dcac3e9 ]

[AIRFLOW-1806] Use naive datetime for cron scheduling

Converting to naive time is required in order to make sure
to run at exact times for crons.
E.g. if you specify to run at 8:00pm every day you do not
want suddenly to run at 7:00pm due to DST.


> Make scheduler aware of timezones
> ---------------------------------
>
>                 Key: AIRFLOW-1806
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1806
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>
> When supplied with time zone aware date times 
> dag.following_schedule and dag.previous_schedule should do arithmetic in local time in order to take care of day light savings time



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27339,54,JIRA.13119214.1510922616000.327487.1511815200966@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13119214.1510922616000@Atlassian.JIRA,,,2017-11-27 12:40:00-08,[jira] [Commented] (AIRFLOW-1827) Fix api endpoints time parsing,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1827?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267460#comment-16267460 ] 

ASF subversion and git services commented on AIRFLOW-1827:
----------------------------------------------------------

Commit f43c0e9ba59b9e89f2932f3a34254bf675a291ff in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f43c0e9 ]

[AIRFLOW-1827] Fix api endpoint date parsing


> Fix api endpoints time parsing
> ------------------------------
>
>                 Key: AIRFLOW-1827
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1827
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27340,54,JIRA.13119211.1510921607000.327493.1511815260786@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13119211.1510921607000@Atlassian.JIRA,,,2017-11-27 12:41:00-08,"[jira] [Commented] (AIRFLOW-1826) Update views to be able to use
 pendulum","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1826?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267461#comment-16267461 ] 

ASF subversion and git services commented on AIRFLOW-1826:
----------------------------------------------------------

Commit 518a41acf319af27d49bdc0c84bda64b6b8af0b3 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=518a41a ]

[AIRFLOW-1826] Update views to use timezone aware objects


> Update views to be able to use pendulum
> ---------------------------------------
>
>                 Key: AIRFLOW-1826
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1826
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27341,54,JIRA.13117662.1510343683000.327495.1511815260802@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117662.1510343683000@Atlassian.JIRA,,,2017-11-27 12:41:00-08,"[jira] [Commented] (AIRFLOW-1803) Add documentation on time zone
 usage","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1803?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267462#comment-16267462 ] 

ASF subversion and git services commented on AIRFLOW-1803:
----------------------------------------------------------

Commit f1ab56cc6ad3b9419af94aaa333661c105185883 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f1ab56c ]

[AIRFLOW-1803] Time zone documentation


> Add documentation on time zone usage
> ------------------------------------
>
>                 Key: AIRFLOW-1803
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1803
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27342,54,JIRA.13117659.1510343499000.327498.1511815260827@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13117659.1510343499000@Atlassian.JIRA,,,2017-11-27 12:41:00-08,"[jira] [Commented] (AIRFLOW-1802) Convert database datetime fields
 to time zone aware fields","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1802?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267463#comment-16267463 ] 

ASF subversion and git services commented on AIRFLOW-1802:
----------------------------------------------------------

Commit d99053106e58ec377333c64f68ee84ed1dcdf61c in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=d990531 ]

Merge pull request #2781 from bolkedebruin/AIRFLOW-1802


> Convert database datetime fields to time zone aware fields
> ----------------------------------------------------------
>
>                 Key: AIRFLOW-1802
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1802
>             Project: Apache Airflow
>          Issue Type: Sub-task
>    Affects Versions: 1.9.0
>            Reporter: Bolke de Bruin
>            Assignee: Bolke de Bruin
>
> DateTime fields are currently naive in the databases. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27343,54,JIRA.12984446.1467137330000.327534.1511815320721@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.12984446.1467137330000@Atlassian.JIRA,,,2017-11-27 12:42:00-08,[jira] [Resolved] (AIRFLOW-288) Make system timezone configurable,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-288?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-288.
------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Make system timezone configurable
> ---------------------------------
>
>                 Key: AIRFLOW-288
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-288
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Vineet Goel
>            Assignee: Vineet Goel
>             Fix For: 1.10.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27344,54,JIRA.13109009.1507832929000.327568.1511815381311@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13109009.1507832929000@Atlassian.JIRA,,,2017-11-27 12:43:01-08,"[jira] [Resolved] (AIRFLOW-1710) Add timezone setting to Airflow
 DAG","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1710?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1710.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Add timezone setting to Airflow DAG
> -----------------------------------
>
>                 Key: AIRFLOW-1710
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1710
>             Project: Apache Airflow
>          Issue Type: Sub-task
>          Components: DAG
>    Affects Versions: 1.9.0
>            Reporter: Chris Riccomini
>            Assignee: Bolke de Bruin
>             Fix For: 1.10.0
>
>
> We have some use cases where we''d like to run DAGs pegged to a specific timezone.
> Customers want things to happen in their time zone. If they have DST, it''s not a constant offset from UTC. If they aren''t in the US, their DST isn''t a constant offset from California''s DST (where we run). GB, for example.
> One way to solve this would be to have the DAG start with PythonOperator task that calculates the difference between UTC and the expected timezone, and sleeps for that amount of time.
> Another (cleaner?) way would be to add a field to the DAG model that allows the DAG author to specify the timezone that the DAG should be scheduled in. For example, we could have our Airflow box continue to run on UTC, but schedule a specific DAG for Pacific/US, which would adjust according to daylight savings time. We could schedule other DAGs to run on GB DST, etc.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27345,54,JIRA.13117659.1510343499000.327575.1511815381384@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117659.1510343499000@Atlassian.JIRA,,,2017-11-27 12:43:01-08,"[jira] [Resolved] (AIRFLOW-1802) Convert database datetime fields
 to time zone aware fields","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1802?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1802.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Convert database datetime fields to time zone aware fields
> ----------------------------------------------------------
>
>                 Key: AIRFLOW-1802
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1802
>             Project: Apache Airflow
>          Issue Type: Sub-task
>    Affects Versions: 1.9.0
>            Reporter: Bolke de Bruin
>            Assignee: Bolke de Bruin
>             Fix For: 1.10.0
>
>
> DateTime fields are currently naive in the databases. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27346,54,JIRA.13117663.1510343855000.327586.1511815440627@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117663.1510343855000@Atlassian.JIRA,,,2017-11-27 12:44:00-08,"[jira] [Resolved] (AIRFLOW-1804) Add airflow.cfg option for time
 zone configuration","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1804?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1804.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Add airflow.cfg option for time zone configuration
> --------------------------------------------------
>
>                 Key: AIRFLOW-1804
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1804
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>             Fix For: 1.10.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27347,54,JIRA.13117662.1510343683000.327584.1511815440613@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117662.1510343683000@Atlassian.JIRA,,,2017-11-27 12:44:00-08,"[jira] [Resolved] (AIRFLOW-1803) Add documentation on time zone
 usage","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1803?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1803.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Add documentation on time zone usage
> ------------------------------------
>
>                 Key: AIRFLOW-1803
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1803
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>             Fix For: 1.10.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27348,54,JIRA.13117723.1510396492000.327591.1511815501243@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117723.1510396492000@Atlassian.JIRA,,,2017-11-27 12:45:01-08,[jira] [Resolved] (AIRFLOW-1806) Make scheduler aware of timezones,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1806?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1806.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Make scheduler aware of timezones
> ---------------------------------
>
>                 Key: AIRFLOW-1806
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1806
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>             Fix For: 1.10.0
>
>
> When supplied with time zone aware date times 
> dag.following_schedule and dag.previous_schedule should do arithmetic in local time in order to take care of day light savings time



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27349,54,JIRA.13117724.1510396623000.327601.1511815501343@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117724.1510396623000@Atlassian.JIRA,,,2017-11-27 12:45:01-08,"[jira] [Resolved] (AIRFLOW-1807) Make sure all date time fields
 store in UTC and are always time zone aware","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1807?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1807.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Make sure all date time fields store in UTC and are always time zone aware
> --------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1807
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1807
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>             Fix For: 1.10.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27350,54,JIRA.13117729.1510401682000.327608.1511815501393@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117729.1510401682000@Atlassian.JIRA,,,2017-11-27 12:45:01-08,"[jira] [Resolved] (AIRFLOW-1808) Convert all utcnow() to timezone
 aware utcnow()","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1808?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1808.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Convert all utcnow() to timezone aware utcnow()
> -----------------------------------------------
>
>                 Key: AIRFLOW-1808
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1808
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>             Fix For: 1.10.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27351,54,JIRA.13117730.1510401757000.327610.1511815560327@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13117730.1510401757000@Atlassian.JIRA,,,2017-11-27 12:46:00-08,[jira] [Resolved] (AIRFLOW-1809) Fix all tests that use dates,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1809?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1809.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Fix all tests that use dates
> ----------------------------
>
>                 Key: AIRFLOW-1809
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1809
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>             Fix For: 1.10.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27352,54,JIRA.13119211.1510921607000.327612.1511815560347@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13119211.1510921607000@Atlassian.JIRA,,,2017-11-27 12:46:00-08,"[jira] [Resolved] (AIRFLOW-1826) Update views to be able to use
 pendulum","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1826?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1826.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Update views to be able to use pendulum
> ---------------------------------------
>
>                 Key: AIRFLOW-1826
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1826
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>             Fix For: 1.10.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27353,54,JIRA.13119214.1510922616000.327617.1511815620335@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13119214.1510922616000@Atlassian.JIRA,,,2017-11-27 12:47:00-08,[jira] [Resolved] (AIRFLOW-1827) Fix api endpoints time parsing,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1827?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1827.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

Issue resolved by pull request #2781
[https://github.com/apache/incubator-airflow/pull/2781]

> Fix api endpoints time parsing
> ------------------------------
>
>                 Key: AIRFLOW-1827
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1827
>             Project: Apache Airflow
>          Issue Type: Sub-task
>            Reporter: Bolke de Bruin
>             Fix For: 1.10.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27354,54,JIRA.13121097.1511818397000.328097.1511818440128@Atlassian.JIRA,2410,Trevor Joynson (JIRA),JIRA.13121097.1511818397000@Atlassian.JIRA,,,2017-11-27 13:34:00-08,[jira] [Created] (AIRFLOW-1852) Allow hostname to be overridable,"Trevor Joynson created AIRFLOW-1852:
---------------------------------------

             Summary: Allow hostname to be overridable
                 Key: AIRFLOW-1852
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1852
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Trevor Joynson


* https://github.com/apache/incubator-airflow/pull/2472

This makes running Airflow tremendously easier in common
production deployments that need a little more than just
a bare `socket.getfqdn()` hostname for service discovery
per running instance.

Personally, I just place the Kubernetes Pod FQDN (or even IP) here.

Question: Since the web server calls out to the individual
worker nodes to snag logs, what happens if one dies midway?
I may later look into that, because that scares me slightly.
I feel like workers should not ever hold such state, but that''s purely a personal bias.

Thanks,
Trevor



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27355,54,JIRA.13121097.1511818397000.328119.1511818620934@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13121097.1511818397000@Atlassian.JIRA,,,2017-11-27 13:37:00-08,[jira] [Commented] (AIRFLOW-1852) Allow hostname to be overridable,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1852?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267607#comment-16267607 ] 

Ash Berlin-Taylor commented on AIRFLOW-1852:
--------------------------------------------

Where is the hostname currently used in airflow? (I''ve been running fine without worrying about this, as I''m sure are lots of other people.)

{quote}
Since the web server calls out to the individual worker nodes to snag logs, what happens if one dies midway?
{quote}

There''s support for writing task logs to GCS or S3 for more persistent storage.

> Allow hostname to be overridable
> --------------------------------
>
>                 Key: AIRFLOW-1852
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1852
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Trevor Joynson
>
> * https://github.com/apache/incubator-airflow/pull/2472
> This makes running Airflow tremendously easier in common
> production deployments that need a little more than just
> a bare `socket.getfqdn()` hostname for service discovery
> per running instance.
> Personally, I just place the Kubernetes Pod FQDN (or even IP) here.
> Question: Since the web server calls out to the individual
> worker nodes to snag logs, what happens if one dies midway?
> I may later look into that, because that scares me slightly.
> I feel like workers should not ever hold such state, but that''s purely a personal bias.
> Thanks,
> Trevor



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27356,54,JIRA.13121097.1511818397000.328117.1511818620915@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13121097.1511818397000@Atlassian.JIRA,,,2017-11-27 13:37:00-08,[jira] [Commented] (AIRFLOW-1852) Allow hostname to be overridable,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1852?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16267606#comment-16267606 ] 

Ash Berlin-Taylor commented on AIRFLOW-1852:
--------------------------------------------

Where is the hostname currently used in airflow? (I''ve been running fine without worrying about this, as I''m sure are lots of other people.)

{quote}
Since the web server calls out to the individual worker nodes to snag logs, what happens if one dies midway?
{quote}

There''s support for writing task logs to GCS or S3 for more persistent storage.

> Allow hostname to be overridable
> --------------------------------
>
>                 Key: AIRFLOW-1852
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1852
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Trevor Joynson
>
> * https://github.com/apache/incubator-airflow/pull/2472
> This makes running Airflow tremendously easier in common
> production deployments that need a little more than just
> a bare `socket.getfqdn()` hostname for service discovery
> per running instance.
> Personally, I just place the Kubernetes Pod FQDN (or even IP) here.
> Question: Since the web server calls out to the individual
> worker nodes to snag logs, what happens if one dies midway?
> I may later look into that, because that scares me slightly.
> I feel like workers should not ever hold such state, but that''s purely a personal bias.
> Thanks,
> Trevor



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706984,24,153196203569.28772.7478535051419481615.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 18:00:35-07,"[GitHub] pengzhao-intel commented on issue #8532: mxnet-mkl (v0.12.0) crash
 when using (conda-installed) numpy with MKL","pengzhao-intel commented on issue #8532: mxnet-mkl (v0.12.0) crash when using (conda-installed) numpy with MKL
URL: https://github.com/apache/incubator-mxnet/issues/8532#issuecomment-406120408
 
 
   @fhieber @tdomhan could you help confirm the fix?
   If the problem is solved, we can close this bug finally ;)
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27357,54,JIRA.13121097.1511818397000.328123.1511818620971@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13121097.1511818397000@Atlassian.JIRA,,,2017-11-27 13:37:00-08,"[jira] [Issue Comment Deleted] (AIRFLOW-1852) Allow hostname to be
 overridable","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1852?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Ash Berlin-Taylor updated AIRFLOW-1852:
---------------------------------------
    Comment: was deleted

(was: Where is the hostname currently used in airflow? (I''ve been running fine without worrying about this, as I''m sure are lots of other people.)

{quote}
Since the web server calls out to the individual worker nodes to snag logs, what happens if one dies midway?
{quote}

There''s support for writing task logs to GCS or S3 for more persistent storage.)

> Allow hostname to be overridable
> --------------------------------
>
>                 Key: AIRFLOW-1852
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1852
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Trevor Joynson
>
> * https://github.com/apache/incubator-airflow/pull/2472
> This makes running Airflow tremendously easier in common
> production deployments that need a little more than just
> a bare `socket.getfqdn()` hostname for service discovery
> per running instance.
> Personally, I just place the Kubernetes Pod FQDN (or even IP) here.
> Question: Since the web server calls out to the individual
> worker nodes to snag logs, what happens if one dies midway?
> I may later look into that, because that scares me slightly.
> I feel like workers should not ever hold such state, but that''s purely a personal bias.
> Thanks,
> Trevor



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27358,54,JIRA.13065768.1492751782000.331739.1511853480291@Atlassian.JIRA,1930,Jepson (JIRA),JIRA.13065768.1492751782000@Atlassian.JIRA,,,2017-11-27 23:18:00-08,"[jira] [Closed] (AIRFLOW-1134) airflow webserver -p 8080,can''t
 start the webserver","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1134?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Jepson closed AIRFLOW-1134.
---------------------------

> airflow webserver -p 8080,can''t start the webserver
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1134
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1134
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webserver
>    Affects Versions: Airflow 1.8
>         Environment: centos6.5+python2.7.5+pip
> airflow1.8.0
>            Reporter: Jepson
>            Assignee: Jepson
>              Labels: features, patch, test
>   Original Estimate: 672h
>  Remaining Estimate: 672h
>
> [root@hadoopdn-04 ~]# /usr/local/python27/bin/airflow webserver -D
> [2017-04-21 12:59:41,341] {__init__.py:57} INFO - Using executor SequentialExecutor
> Namespace(access_logfile=''-'', daemon=True, debug=False, error_logfile=''-'', func=<function webserver at 0x48fd320>, hostname=''0.0.0.0'', log_file=None, pid=None, port=8080, ssl_cert=None, ssl_key=None, stderr=None, stdout=None, subcommand=''webserver'', worker_timeout=120, workerclass=''sync'', workers=4)
>   ____________       _____________
>  ____    |__( )_________  __/__  /________      __
> ____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
> ___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
>  _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
>  
> /usr/local/python27/lib/python2.7/site-packages/flask/exthook.py:71: ExtDeprecationWarning: Importing flask.ext.cache is deprecated, use flask_cache instead.
>   .format(x=modname), ExtDeprecationWarning
> [2017-04-21 12:59:42,719] [1616] {models.py:167} INFO - Filling up the DagBag from /root/learnproject/app/airflow/dags
> Running the Gunicorn Server with:
> Workers: 4 sync
> Host: 0.0.0.0:8080
> Timeout: 120
> Logfiles: - -
> =================================================================            
> Traceback (most recent call last):
>   File ""/usr/local/python27/bin/airflow"", line 29, in <module>
>     args.func(args)
>   File ""/usr/local/python27/lib/python2.7/site-packages/airflow/bin/cli.py"", line 791, in webserver
>     gunicorn_master_proc = subprocess.Popen(run_args)
>   File ""/usr/local/python27/lib/python2.7/subprocess.py"", line 711, in __init__
>     errread, errwrite)
>   File ""/usr/local/python27/lib/python2.7/subprocess.py"", line 1308, in _execute_child
>     raise child_exception
> OSError: [Errno 2] No such file or directory



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27359,54,JIRA.13121233.1511860494000.332495.1511860501934@Atlassian.JIRA,2411,Yee Ting Li (JIRA),JIRA.13121233.1511860494000@Atlassian.JIRA,,,2017-11-28 01:15:01-08,"[jira] [Created] (AIRFLOW-1853) tree view of manually triggered
 dags overpopulate page","Yee Ting Li created AIRFLOW-1853:
------------------------------------

             Summary: tree view of manually triggered dags overpopulate page
                 Key: AIRFLOW-1853
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1853
             Project: Apache Airflow
          Issue Type: Bug
          Components: webapp
    Affects Versions: 1.8.2
         Environment: running from https://github.com/puckel/docker-airflow
            Reporter: Yee Ting Li


i have a DAG that i only ever run via an external trigger (via TriggerDagRunOperator). They show up fine under the Tree View, however, the page appears to ignore a filter for the last 25/50... number of runs. Instead, the entire history of all the run''s show up on the page - ie if i had triggered the DAG 5 times, it will show all 5... if i have triggered it 500 times it will show all 500 dag runs.

this also means that loading the page is very very slow with a large number of (triggered) dag runs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27360,54,JIRA.13120967.1511794207000.332749.1511863080099@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120967.1511794207000@Atlassian.JIRA,,,2017-11-28 01:58:00-08,"[jira] [Commented] (AIRFLOW-1848) Dataflow operator py_file arg
 doesn''t require .py suffix","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1848?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16268479#comment-16268479 ] 

ASF subversion and git services commented on AIRFLOW-1848:
----------------------------------------------------------

Commit 02112197c60948ac4579fdee5cf09a0fcdabdd7a in incubator-airflow''s branch refs/heads/master from [~cjqian]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=0211219 ]

[AIRFLOW-1848][Airflow-1848] Fix DataFlowPythonOperator py_file extension doc comment

Closes #2816 from cjqian/1848


> Dataflow operator py_file arg doesn''t require .py suffix
> --------------------------------------------------------
>
>                 Key: AIRFLOW-1848
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1848
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Crystal Qian
>            Assignee: Crystal Qian
>            Priority: Minor
>   Original Estimate: 1h
>  Remaining Estimate: 1h
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27361,54,JIRA.13120967.1511794207000.332752.1511863140350@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13120967.1511794207000@Atlassian.JIRA,,,2017-11-28 01:59:00-08,"[jira] [Resolved] (AIRFLOW-1848) Dataflow operator py_file arg
 doesn''t require .py suffix","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1848?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1848.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2816
[https://github.com/apache/incubator-airflow/pull/2816]

> Dataflow operator py_file arg doesn''t require .py suffix
> --------------------------------------------------------
>
>                 Key: AIRFLOW-1848
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1848
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib
>            Reporter: Crystal Qian
>            Assignee: Crystal Qian
>            Priority: Minor
>             Fix For: 1.9.1
>
>   Original Estimate: 1h
>  Remaining Estimate: 1h
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27362,54,JIRA.13121243.1511863834000.332808.1511863860139@Atlassian.JIRA,2412,Milan van der Meer (JIRA),JIRA.13121243.1511863834000@Atlassian.JIRA,,,2017-11-28 02:11:00-08,"[jira] [Created] (AIRFLOW-1854) Improve Spark submit hook for
 cluster mode","Milan van der Meer created AIRFLOW-1854:
-------------------------------------------

             Summary: Improve Spark submit hook for cluster mode
                 Key: AIRFLOW-1854
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1854
             Project: Apache Airflow
          Issue Type: Improvement
          Components: hooks
            Reporter: Milan van der Meer
            Assignee: Milan van der Meer
            Priority: Minor


*We are already working on this issue and making a PR soon*

When executing a Spark submit to a standalone cluster using the Spark submit hook, it will get a return code from the Spark submit action and not the Spark job itself.
This means when a Spark submit is executed and successfully received by the cluster, the Airflow job will be successful, even when the Spark job fails on the cluster later on.

Suggested solution:
* When you execute a Spark submit, the response will contain a driver ID.
* Use this driver ID to poll the cluster for the driver state.
* Based on the drivers state, the job will be successful or failed.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27363,54,JIRA.13121243.1511863834000.332817.1511863920394@Atlassian.JIRA,2412,Milan van der Meer (JIRA),JIRA.13121243.1511863834000@Atlassian.JIRA,,,2017-11-28 02:12:00-08,"[jira] [Work started] (AIRFLOW-1854) Improve Spark submit hook for
 cluster mode","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1854?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1854 started by Milan van der Meer.
---------------------------------------------------
> Improve Spark submit hook for cluster mode
> ------------------------------------------
>
>                 Key: AIRFLOW-1854
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1854
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: hooks
>            Reporter: Milan van der Meer
>            Assignee: Milan van der Meer
>            Priority: Minor
>              Labels: features
>
> *We are already working on this issue and making a PR soon*
> When executing a Spark submit to a standalone cluster using the Spark submit hook, it will get a return code from the Spark submit action and not the Spark job itself.
> This means when a Spark submit is executed and successfully received by the cluster, the Airflow job will be successful, even when the Spark job fails on the cluster later on.
> Suggested solution:
> * When you execute a Spark submit, the response will contain a driver ID.
> * Use this driver ID to poll the cluster for the driver state.
> * Based on the drivers state, the job will be successful or failed.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27364,54,JIRA.13121243.1511863834000.332829.1511864100659@Atlassian.JIRA,2412,Milan van der Meer (JIRA),JIRA.13121243.1511863834000@Atlassian.JIRA,,,2017-11-28 02:15:00-08,"[jira] [Updated] (AIRFLOW-1854) Improve Spark submit hook for
 cluster mode","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1854?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Milan van der Meer updated AIRFLOW-1854:
----------------------------------------
    Description: 
*We are already working on this issue and making a PR soon*

When executing a Spark submit to a standalone cluster using the Spark submit hook, it will get a return code from the Spark submit action and not the Spark job itself.
This means when a Spark submit is executed and successfully received by the cluster, the Airflow job will be successful, even when the Spark job fails on the cluster later on.

Suggested solution:
* When you execute a Spark submit, the logs will contain a driver ID.
* Use this driver ID to poll the cluster for the driver state.
* Based on the drivers state, the job will be successful or failed.

  was:
*We are already working on this issue and making a PR soon*

When executing a Spark submit to a standalone cluster using the Spark submit hook, it will get a return code from the Spark submit action and not the Spark job itself.
This means when a Spark submit is executed and successfully received by the cluster, the Airflow job will be successful, even when the Spark job fails on the cluster later on.

Suggested solution:
* When you execute a Spark submit, the response will contain a driver ID.
* Use this driver ID to poll the cluster for the driver state.
* Based on the drivers state, the job will be successful or failed.


> Improve Spark submit hook for cluster mode
> ------------------------------------------
>
>                 Key: AIRFLOW-1854
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1854
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: hooks
>            Reporter: Milan van der Meer
>            Assignee: Milan van der Meer
>            Priority: Minor
>              Labels: features
>
> *We are already working on this issue and making a PR soon*
> When executing a Spark submit to a standalone cluster using the Spark submit hook, it will get a return code from the Spark submit action and not the Spark job itself.
> This means when a Spark submit is executed and successfully received by the cluster, the Airflow job will be successful, even when the Spark job fails on the cluster later on.
> Suggested solution:
> * When you execute a Spark submit, the logs will contain a driver ID.
> * Use this driver ID to poll the cluster for the driver state.
> * Based on the drivers state, the job will be successful or failed.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27365,54,JIRA.13121243.1511863834000.332839.1511864220164@Atlassian.JIRA,2412,Milan van der Meer (JIRA),JIRA.13121243.1511863834000@Atlassian.JIRA,,,2017-11-28 02:17:00-08,"[jira] [Updated] (AIRFLOW-1854) Improve Spark submit hook for
 cluster mode","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1854?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Milan van der Meer updated AIRFLOW-1854:
----------------------------------------
    Description: 
*We are already working on this issue and will submit a PR soon*

When executing a Spark submit to a standalone cluster using the Spark submit hook, it will get a return code from the Spark submit action and not the Spark job itself.
This means when a Spark submit is executed and successfully received by the cluster, the Airflow job will be successful, even when the Spark job fails on the cluster later on.

Suggested solution:
* When you execute a Spark submit in cluster mode, the logs will contain a driver ID.
* Use this driver ID to poll the cluster for the driver state.
* Based on the drivers state, the job will be successful or failed.

  was:
*We are already working on this issue and making a PR soon*

When executing a Spark submit to a standalone cluster using the Spark submit hook, it will get a return code from the Spark submit action and not the Spark job itself.
This means when a Spark submit is executed and successfully received by the cluster, the Airflow job will be successful, even when the Spark job fails on the cluster later on.

Suggested solution:
* When you execute a Spark submit, the logs will contain a driver ID.
* Use this driver ID to poll the cluster for the driver state.
* Based on the drivers state, the job will be successful or failed.


> Improve Spark submit hook for cluster mode
> ------------------------------------------
>
>                 Key: AIRFLOW-1854
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1854
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: hooks
>            Reporter: Milan van der Meer
>            Assignee: Milan van der Meer
>            Priority: Minor
>              Labels: features
>
> *We are already working on this issue and will submit a PR soon*
> When executing a Spark submit to a standalone cluster using the Spark submit hook, it will get a return code from the Spark submit action and not the Spark job itself.
> This means when a Spark submit is executed and successfully received by the cluster, the Airflow job will be successful, even when the Spark job fails on the cluster later on.
> Suggested solution:
> * When you execute a Spark submit in cluster mode, the logs will contain a driver ID.
> * Use this driver ID to poll the cluster for the driver state.
> * Based on the drivers state, the job will be successful or failed.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27366,54,JIRA.13121253.1511865347000.332987.1511865360191@Atlassian.JIRA,2413,Kaxil Naik (JIRA),JIRA.13121253.1511865347000@Atlassian.JIRA,,,2017-11-28 02:36:00-08,"[jira] [Created] (AIRFLOW-1855) Add an Operator to copy files with
 a specific delimiter in a directory from one GCS bucket to another","Kaxil Naik created AIRFLOW-1855:
-----------------------------------

             Summary: Add an Operator to copy files with a specific delimiter in a directory from one GCS bucket to another
                 Key: AIRFLOW-1855
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1855
             Project: Apache Airflow
          Issue Type: New Feature
          Components: contrib, gcp
    Affects Versions: 1.10.0
            Reporter: Kaxil Naik
            Assignee: Kaxil Naik
            Priority: Minor
             Fix For: 1.10.0


Use case: Copy all the CSV/JSON files from a particular directory in a Bucket to another bucket and in a specific directory (or the same).

Proposed Approach:
- Add ''delimiter'' argument in GCP hook to filter files with a particular delimiter.
- Get the list of files to copy and filter it with delimiter using ''list'' method in GCP hook
- Use loop and ''copy'' method in GCP hook. 


Note: Under the hood GCS has no directories. Files are just objects.




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27367,54,JIRA.13116697.1510066613000.333894.1511870342054@Atlassian.JIRA,2380,Hugo Prudente (JIRA),JIRA.13116697.1510066613000@Atlassian.JIRA,,,2017-11-28 03:59:02-08,[jira] [Reopened] (AIRFLOW-1790) AWS Batch Operator Suppport,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1790?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Hugo Prudente reopened AIRFLOW-1790:
------------------------------------

It''s not merged yet, so I''m reopening to evaluation as the Pull request was not evaluated yet.

> AWS Batch Operator Suppport
> ---------------------------
>
>                 Key: AIRFLOW-1790
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1790
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: operators
>    Affects Versions: 1.9.0
>            Reporter: Hugo Prudente
>            Assignee: Hugo Prudente
>
> Add support to AWS Batch operator



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27368,54,JIRA.13042742.1487030758000.335213.1511882521105@Atlassian.JIRA,2304,Alberto Calderari (JIRA),JIRA.13042742.1487030758000@Atlassian.JIRA,,,2017-11-28 07:22:01-08,"[jira] [Commented] (AIRFLOW-873) Tests forcing availability of
 hive_metastore module","
    [ https://issues.apache.org/jira/browse/AIRFLOW-873?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16268902#comment-16268902 ] 

Alberto Calderari commented on AIRFLOW-873:
-------------------------------------------

[~wrp][~andyxhadji] I solved  with pip install apache-airflow[hive] in the virtualenv.

> Tests forcing availability of hive_metastore module
> ---------------------------------------------------
>
>                 Key: AIRFLOW-873
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-873
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: krishnabhupatiraju
>            Assignee: krishnabhupatiraju
>            Priority: Minor
>
> PR 2012 leads to an attempt to import hive_metastore any time unit tests are run, even if these aren''t the unit tests being run. hive_metastore is not a required module for Airflow, but now any machine without it can''t run any local unit tests! I put the traceback I''m seeing below.
> The unit test file causing this import needs to be guarded so it only runs in if its dependencies are available (see hive_operator.py in the same tests directory for one example, or the explicit skip guards in the postgres/mysql tests that ensure they only run in the right travis environment).
> ======================================================================
> ERROR: Failure: ImportError (No module named ''hive_metastore'')
> ----------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/failure.py"", line 39, in runTest
>     raise self.exc_val.with_traceback(self.tb)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/loader.py"", line 418, in loadTestsFromName
>     addr.filename, addr.module)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/importer.py"", line 47, in importFromPath
>     return self.importFromDir(dir_path, fqname)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/site-packages/nose/importer.py"", line 94, in importFromDir
>     mod = load_module(part_fqname, fh, filename, desc)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/imp.py"", line 244, in load_module
>     return load_package(name, filename)
>   File ""/Users/jlowin/anaconda3/lib/python3.5/imp.py"", line 216, in load_package
>     return _load(spec)
>   File ""<frozen importlib._bootstrap>"", line 693, in _load
>   File ""<frozen importlib._bootstrap>"", line 673, in _load_unlocked
>   File ""<frozen importlib._bootstrap_external>"", line 665, in exec_module
>   File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
>   File ""/Users/jlowin/git/airflow/tests/__init__.py"", line 24, in <module>
>     from .operators import *
>   File ""/Users/jlowin/git/airflow/tests/operators/__init__.py"", line 20, in <module>
>     from .s3_to_hive_operator import *
>   File ""/Users/jlowin/git/airflow/tests/operators/s3_to_hive_operator.py"", line 25, in <module>
>     from airflow.operators.s3_to_hive_operator import S3ToHiveTransfer
>   File ""/Users/jlowin/git/airflow/airflow/operators/s3_to_hive_operator.py"", line 27, in <module>
>     from airflow.hooks.hive_hooks import HiveCliHook
>   File ""/Users/jlowin/git/airflow/airflow/hooks/hive_hooks.py"", line 28, in <module>
>     import hive_metastore
> ImportError: No module named ''hive_metastore''



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27369,54,JIRA.13121349.1511882533000.335216.1511882580320@Atlassian.JIRA,2414,Ikar Pohorsky (JIRA),JIRA.13121349.1511882533000@Atlassian.JIRA,,,2017-11-28 07:23:00-08,"[jira] [Created] (AIRFLOW-1856) How to allow airflow dags for
 concrete user(s) only?","Ikar Pohorsky created AIRFLOW-1856:
--------------------------------------

             Summary: How to allow airflow dags for concrete user(s) only?
                 Key: AIRFLOW-1856
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1856
             Project: Apache Airflow
          Issue Type: Bug
          Components: authentication, ui, webapp
            Reporter: Ikar Pohorsky


The problem is pretty simple. I need to limit airflow web users to see and execute only certain DAGs and tasks.

If possible, I''d prefer not to use [Kerberos|https://airflow.incubator.apache.org/security.html#kerberos] nor [OAuth|https://airflow.incubator.apache.org/security.html#oauth-authentication].

The [Multi-tenancy|https://airflow.incubator.apache.org/security.html#multi-tenancy] option seems like an option to go, but couldn''t make it work the way I expect.

My current setup:

* added airflow web users _test_ and _ikar_ via [Web Authentication / Password|https://airflow.incubator.apache.org/security.html#password]
* my unix username is _ikar_ with a home in _/home/ikar_
* no _test_ unix user
* airflow _1.8.2_ is installed in _/home/ikar/airflow_
* added two DAGs with one task:
** one with _owner_ set to _ikar_
** one with _owner_ set to _test_
* airflow.cfg:
{code}
[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /home/ikar/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
# This path must be absolute
dags_folder = /home/ikar/airflow-test/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = /home/ikar/airflow/logs

# Airflow can store logs remotely in AWS S3 or Google Cloud Storage. Users
# must supply a remote location URL (starting with either ''s3://...'' or
# ''gs://...'') and an Airflow connection id that provides access to the storage
# location.
remote_base_log_folder =
remote_log_conn_id =
# Use server-side encryption for logs stored in S3
encrypt_s3_logs = False                                                                                                                                                                  
# DEPRECATED option for remote log storage, use remote_base_log_folder instead!                                                                                                          
s3_log_folder =                                                                                                                                                                          
                                                                                                                                                                                                                  
# The executor class that airflow should use. Choices include                                                                                                                            
# SequentialExecutor, LocalExecutor, CeleryExecutor                                                                                                                                      
executor = SequentialExecutor                                                                                                                                                            
                                                                                                                                                                                                                  
# The SqlAlchemy connection string to the metadata database.                                                                                                                             
# SqlAlchemy supports many different database engine, more information                                                                                                                   
# their website                                                                                                                                                                          
sql_alchemy_conn = sqlite:////home/ikar/airflow/airflow.db

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool.
sql_alchemy_pool_size = 5

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. This config does
# not apply to sqlite.
sql_alchemy_pool_recycle = 3600

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# When not using pools, tasks are run in the ""default pool"",
# whose size is guided by this config element
non_pooled_task_slot_count = 128

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the examples that ship with Airflow. It''s good to
# get started, but you probably want to set this to False in a production
# environment
load_examples = False

# Where your Airflow plugins are stored
plugins_folder = /home/ikar/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = cryptography_not_found_storing_passwords_in_plain_text

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import while filling the DagBag
dagbag_import_timeout = 30

# The class to use for running task instances in a subprocess
task_runner = BashTaskRunner

# If set, tasks without a `run_as_user` argument will be run with this user
# Can be used to de-elevate a sudo user running Airflow when executing tasks
default_impersonation =

# What security module to use (for example kerberos):
security =

# Turn unit test mode on (overwrites many configuration options with test
# values at runtime)
unit_test_mode = False

[cli]
# In what way should the cli access the API. The LocalClient will use the
# database directly, while the json_client will use the api running on the
# webserver
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8888

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.default

[operators]
# The default owner assigned to each new operator, unless
# provided explicitly or passed via `default_args`
default_owner = Airflow
default_cpus = 1
default_ram = 512
default_disk = 512
default_gpus = 0


[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to point links to the right web server
base_url = http://localhost:8888

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8888

# Paths to the SSL certificate and key for the web server. When both are
# provided SSL will be enabled. This does not change the web server port.
web_server_ssl_cert =
web_server_ssl_key =

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time. When set to 0, worker refresh is
# disabled. When nonzero, airflow periodically refreshes webserver workers by
# bringing up new ones and killing old ones.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 30

# Secret key used to run your flask app
secret_key = temporary_key

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use. Choices include
# sync (default), eventlet, gevent
worker_class = sync

# Log files for the gunicorn webserver. ''-'' means log to stderr.
access_logfile = -
error_logfile = -

# Expose the configuration file in the web server
expose_config = False

# Set to true to turn on authentication:
# http://pythonhosted.org/airflow/security.html#web-authentication
authenticate = True
auth_backend = airflow.contrib.auth.backends.password_auth

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = True

# Filtering mode. Choices include user (default) and ldapgroup.
# Ldap group filtering requires using the ldap backend
#
# Note that the ldap server needs the ""memberOf"" overlay to be set up
# in order to user the ldapgroup mode.
owner_mode = user

# Default DAG orientation. Valid values are:
# LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
dag_orientation = LR

# Puts the webserver in demonstration mode; blurs the names of Operators for
# privacy.
demo_mode = False

# The amount of time (in secs) webserver will wait for initial handshake
# while fetching logs from other worker machine
log_fetch_timeout_sec = 5

# By default, the webserver shows paused DAGs. Flip this to hide paused
# DAGs by default
hide_paused_dags_by_default = False

[email]
email_backend = airflow.utils.email.send_email_smtp


[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
# Uncomment and set the user/pass settings if you want to use SMTP AUTH
# smtp_user = airflow
# smtp_password = airflow
smtp_port = 25
smtp_mail_from = airflow@airflow.com


[celery]
# This section only applies if you are using the CeleryExecutor in
# [core] section above

# The app name that will be used by celery
celery_app_name = airflow.executors.celery_executor

# The concurrency that will be used when starting workers with the
# ""airflow worker"" command. This defines the number of task instances that
# a worker will take, so size up your workers based on the resources on
# your worker box and the nature of your tasks
celeryd_concurrency = 4

# When you start an airflow worker, airflow starts a tiny web server
# subprocess to serve the workers local log files to the airflow main
# web server, who then builds pages and sends them to users. This defines
# the port on which the logs are served. It needs to be unused, and open
# visible from the main web server to connect into the workers.
worker_log_server_port = 8793

# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
broker_url = sqla+mysql://airflow:airflow@localhost:3306/airflow

# Another key Celery setting
celery_result_backend = db+mysql://airflow:airflow@localhost:3306/airflow

# Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
# it `airflow flower`. This defines the IP that Celery Flower runs on
flower_host = 0.0.0.0

# This defines the port that Celery Flower runs on
flower_port = 5555

# Default queue that tasks get assigned to and that worker listen on.
default_queue = default


[scheduler]
# Task instances listen for external kill signal (when you clear tasks
# from the CLI or the UI), this defines the frequency at which they should
# listen (in seconds).
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks (look at the
# scheduler section in the docs for more information). This defines
# how often the scheduler should run (in seconds).
scheduler_heartbeat_sec = 5

# after how much time should the scheduler terminate in seconds
# -1 indicates to run continuously (see also num_runs)
run_duration = -1

# after how much time a new DAGs should be picked up from the filesystem
min_file_process_interval = 0

dag_dir_list_interval = 300

# How often should stats be printed to the logs
print_stats_interval = 30

child_process_log_directory = /home/ikar/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB. If the job has
# not heartbeat in this many seconds, the scheduler will mark the
# associated task instance as failed and will re-schedule the task.
scheduler_zombie_task_threshold = 300

# Turn off scheduler catchup by setting this to False.
# Default behavior is unchanged and
# Command Line Backfills still work, but the scheduler
# will not do scheduler catchup if this is False,
# however it can be set on a per DAG basis in the
# DAG definition (catchup)
catchup_by_default = False

# Statsd (https://github.com/etsy/statsd) integration settings
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

# The scheduler can run multiple threads in parallel to schedule dags.
# This defines how many threads will run. However airflow will never
# use more threads than the amount of cpu cores available.
max_threads = 2

authenticate = False


[mesos]
# Mesos master address which MesosExecutor will connect to.
master = localhost:5050

# The framework name which Airflow scheduler will register itself as on mesos
framework_name = Airflow

# Number of cpu cores required for running one task instance using
# ''airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>''
# command on a mesos slave
task_cpu = 1

# Memory in MB required for running one task instance using
# ''airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>''
# command on a mesos slave
task_memory = 256

# Enable framework checkpointing for mesos
# See http://mesos.apache.org/documentation/latest/slave-recovery/
checkpoint = False

# Failover timeout in milliseconds.
# When checkpointing is enabled and this option is set, Mesos waits
# until the configured timeout for
# the MesosExecutor framework to re-register after a failover. Mesos
# shuts down running tasks if the
# MesosExecutor framework fails to re-register within this timeframe.
# failover_timeout = 604800

# Enable framework authentication for mesos
# See http://mesos.apache.org/documentation/latest/configuration/
authenticate = False

# Mesos credentials, if authentication is enabled
# default_principal = admin
# default_secret = admin


[kerberos]
ccache = /tmp/airflow_krb5_ccache
# gets augmented with fqdn
principal = airflow
reinit_frequency = 3600
kinit_path = kinit
keytab = airflow.keytab


[github_enterprise]
api_rev = v3


[admin]
# UI to hide sensitive variable fields when set to True
hide_sensitive_variable_fields = True

{code}
* sample DAG:
{code}
from datetime import datetime

from airflow import DAG
from airflow.operators.bash_operator import BashOperator

from core.path import REPO_PATH

test_template = """"""
cd {{ params.path }}; python3 -m unittest --verbose {{ params.script }}
""""""

with DAG(
    dag_id=""daily_tests"",
    schedule_interval=""30 4 * * *"",
    default_args={''start_date'': datetime(2017, 8, 25, hour=5)}
) as dag:
    BashOperator(
        task_id=""platform_test"",
        owner=""ikar"",
        bash_command=test_template,
        params={''path'': ""{}/tests/daily"".format(REPO_PATH), ''script'': ""test_platform.py""},
    )
{code}

I''d expect that _test_ user will only see DAG with owner set to _test_ but both users can see and execute both DAGs.

Searching issues by owner works great.

Couldn''t find any detailed documentation on how to setup the user restrictions for airflow DAGs.

Can anyone help? Am I missing something?

Is it possible that both user I''ve created are superusers? If so, how to make them non-superusers?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27370,54,JIRA.13084633.1499205090000.335700.1511886840251@Atlassian.JIRA,2072,=?utf-8?Q?Victor_Villas_B=C3=B4as_Chaves_=28JIRA=29?=,JIRA.13084633.1499205090000@Atlassian.JIRA,,,2017-11-28 08:34:00-08,"[jira] [Commented] (AIRFLOW-1372) ""Failed to fetch log file from
 worker"" using distributed workers","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1372?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
269007#comment-16269007 ]=20

Victor Villas B=C3=B4as Chaves commented on AIRFLOW-1372:
----------------------------------------------------

I believe this is not a bug in Airflow, but with the Docker setup or someth=
ing external. I''m no longer able to reproduce it.

> ""Failed to fetch log file from worker"" using distributed workers
> ----------------------------------------------------------------
>
>                 Key: AIRFLOW-1372
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1372
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: logging, ui
>    Affects Versions: 1.8.0
>         Environment: System: Windows 10 Home (Docker Toolbox)
> docker 17.05.0-ce
> docker-compose 1.13.0
> docker-machine 0.11.0
>            Reporter: Victor Villas B=C3=B4as Chaves
>
> Multiple users reproduced:
> > crorella: When I manually trigger the DAG from the UI it fails, if it r=
uns via the scheduler I can see the logs.
> > licryle: same here, I can''t see logs when launching from the UI, nor wi=
th airflow trigger_dag, however it seems to work when starting with airflow=
 backfill.
> Previous discussion: https://github.com/puckel/docker-airflow/issues/44



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27371,54,JIRA.13084633.1499205090000.335703.1511886840282@Atlassian.JIRA,2072,=?utf-8?Q?Victor_Villas_B=C3=B4as_Chaves_=28JIRA=29?=,JIRA.13084633.1499205090000@Atlassian.JIRA,,,2017-11-28 08:34:00-08,"[jira] [Closed] (AIRFLOW-1372) ""Failed to fetch log file from
 worker"" using distributed workers","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1372?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Victor Villas B=C3=B4as Chaves closed AIRFLOW-1372.
----------------------------------------------
    Resolution: Cannot Reproduce

> ""Failed to fetch log file from worker"" using distributed workers
> ----------------------------------------------------------------
>
>                 Key: AIRFLOW-1372
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1372
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: logging, ui
>    Affects Versions: 1.8.0
>         Environment: System: Windows 10 Home (Docker Toolbox)
> docker 17.05.0-ce
> docker-compose 1.13.0
> docker-machine 0.11.0
>            Reporter: Victor Villas B=C3=B4as Chaves
>
> Multiple users reproduced:
> > crorella: When I manually trigger the DAG from the UI it fails, if it r=
uns via the scheduler I can see the logs.
> > licryle: same here, I can''t see logs when launching from the UI, nor wi=
th airflow trigger_dag, however it seems to work when starting with airflow=
 backfill.
> Previous discussion: https://github.com/puckel/docker-airflow/issues/44



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27372,54,JIRA.13121373.1511888826000.335992.1511888880086@Atlassian.JIRA,2415,Mark Whitfield (JIRA),JIRA.13121373.1511888826000@Atlassian.JIRA,,,2017-11-28 09:08:00-08,"[jira] [Created] (AIRFLOW-1857) Tree View in Web UI shows all
 DAGRuns for Dags with execution interval @once or None","Mark Whitfield created AIRFLOW-1857:
---------------------------------------

             Summary: Tree View in Web UI shows all DAGRuns for Dags with execution interval @once or None
                 Key: AIRFLOW-1857
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1857
             Project: Apache Airflow
          Issue Type: Bug
          Components: ui, webserver
            Reporter: Mark Whitfield
            Priority: Minor


The DAG''s tree view in the web UI relies on the DAG''s [execution interval|https://github.com/apache/incubator-airflow/blob/master/airflow/www/views.py#L1178-L1189] to limit the number of DAG runs displayed. For DAGs with intervals `@once` or `None` this falls back on the default of displayiing *all* historical runs of the DAG (back to 2000, anyway).

I have use cases (and believe others do too) involving DAGs which are run exclusively by manual triggering, whether through the UI, API call, or TriggerDagRunOperator. For long-lived DAGs of this kind, the UI ends up displaying hundreds of runs, which is not only slow to load and cumbersome to navigate, but after a while will begin to crash the (containerized, resource-limited) webserver.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706991,24,153196460465.396.42879960791676391.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 18:43:24-07,"[GitHub] szha commented on issue #11807: enable CPU kernel for all RNN layer
 forward","szha commented on issue #11807: enable CPU kernel for all RNN layer forward
URL: https://github.com/apache/incubator-mxnet/pull/11807#issuecomment-406127559
 
 
   Plenty with {cpu, gpu} * {ndarray, symbol} coverage.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27373,54,JIRA.13121373.1511888826000.335997.1511888940190@Atlassian.JIRA,2415,Mark Whitfield (JIRA),JIRA.13121373.1511888826000@Atlassian.JIRA,,,2017-11-28 09:09:00-08,"[jira] [Updated] (AIRFLOW-1857) Tree View in Web UI shows all
 DAGRuns for Dags with execution interval @once or None","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1857?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark Whitfield updated AIRFLOW-1857:
------------------------------------
    Description: 
The tree view in the web UI relies on the DAG''s [execution interval|https://github.com/apache/incubator-airflow/blob/master/airflow/www/views.py#L1178-L1189] to limit the number of DAG runs displayed. For DAGs with intervals `@once` or `None` this falls back on the default of displayiing *all* historical runs of the DAG (back to 2000, anyway).

I have use cases (and believe others do too) involving DAGs which are run exclusively by manual triggering, whether through the UI, API call, or TriggerDagRunOperator. For long-lived DAGs of this kind, the UI ends up displaying hundreds of runs, which is not only slow to load and cumbersome to navigate, but after a while will begin to crash the (containerized, resource-limited) webserver.

  was:
The DAG''s tree view in the web UI relies on the DAG''s [execution interval|https://github.com/apache/incubator-airflow/blob/master/airflow/www/views.py#L1178-L1189] to limit the number of DAG runs displayed. For DAGs with intervals `@once` or `None` this falls back on the default of displayiing *all* historical runs of the DAG (back to 2000, anyway).

I have use cases (and believe others do too) involving DAGs which are run exclusively by manual triggering, whether through the UI, API call, or TriggerDagRunOperator. For long-lived DAGs of this kind, the UI ends up displaying hundreds of runs, which is not only slow to load and cumbersome to navigate, but after a while will begin to crash the (containerized, resource-limited) webserver.


> Tree View in Web UI shows all DAGRuns for Dags with execution interval @once or None
> ------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1857
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1857
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui, webserver
>            Reporter: Mark Whitfield
>            Priority: Minor
>
> The tree view in the web UI relies on the DAG''s [execution interval|https://github.com/apache/incubator-airflow/blob/master/airflow/www/views.py#L1178-L1189] to limit the number of DAG runs displayed. For DAGs with intervals `@once` or `None` this falls back on the default of displayiing *all* historical runs of the DAG (back to 2000, anyway).
> I have use cases (and believe others do too) involving DAGs which are run exclusively by manual triggering, whether through the UI, API call, or TriggerDagRunOperator. For long-lived DAGs of this kind, the UI ends up displaying hundreds of runs, which is not only slow to load and cumbersome to navigate, but after a while will begin to crash the (containerized, resource-limited) webserver.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27374,54,JIRA.13121373.1511888826000.336190.1511890140126@Atlassian.JIRA,2415,Mark Whitfield (JIRA),JIRA.13121373.1511888826000@Atlassian.JIRA,,,2017-11-28 09:29:00-08,"[jira] [Updated] (AIRFLOW-1857) Tree View in Web UI does not limit
 DAGRuns for Dags with execution interval @once or None","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1857?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark Whitfield updated AIRFLOW-1857:
------------------------------------
    Summary: Tree View in Web UI does not limit DAGRuns for Dags with execution interval @once or None  (was: Tree View in Web UI shows all DAGRuns for Dags with execution interval @once or None)

> Tree View in Web UI does not limit DAGRuns for Dags with execution interval @once or None
> -----------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1857
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1857
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui, webserver
>            Reporter: Mark Whitfield
>            Priority: Minor
>
> The tree view in the web UI relies on the DAG''s [execution interval|https://github.com/apache/incubator-airflow/blob/master/airflow/www/views.py#L1178-L1189] to limit the number of DAG runs displayed. For DAGs with intervals `@once` or `None` this falls back on the default of displayiing *all* historical runs of the DAG (back to 2000, anyway).
> I have use cases (and believe others do too) involving DAGs which are run exclusively by manual triggering, whether through the UI, API call, or TriggerDagRunOperator. For long-lived DAGs of this kind, the UI ends up displaying hundreds of runs, which is not only slow to load and cumbersome to navigate, but after a while will begin to crash the (containerized, resource-limited) webserver.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27375,54,JIRA.13121384.1511890735000.336318.1511890740343@Atlassian.JIRA,2072,=?utf-8?Q?Victor_Villas_B=C3=B4as_Chaves_=28JIRA=29?=,JIRA.13121384.1511890735000@Atlassian.JIRA,,,2017-11-28 09:39:00-08,"[jira] [Created] (AIRFLOW-1858) Allow connection extras to be
 defined with environment variables","Victor Villas B=C3=B4as Chaves created AIRFLOW-1858:
--------------------------------------------------

             Summary: Allow connection extras to be defined with environmen=
t variables
                 Key: AIRFLOW-1858
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1858
             Project: Apache Airflow
          Issue Type: New Feature
          Components: configuration
            Reporter: Victor Villas B=C3=B4as Chaves


Defining connections with environment variables is currently documented as =
an alternative to storing them in the database and manipulating them in the=
 UI or CLI.

But there''s currently no way to define the `extras` if using environment va=
riables, usually required by complex connections like AWS or Presto, so it''=
s a feature incomplete alternative.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27376,54,JIRA.13121406.1511894690000.337015.1511894700304@Atlassian.JIRA,2072,=?utf-8?Q?Victor_Villas_B=C3=B4as_Chaves_=28JIRA=29?=,JIRA.13121406.1511894690000@Atlassian.JIRA,,,2017-11-28 10:45:00-08,"[jira] [Created] (AIRFLOW-1859) Allow specification of
 cursor_factory with PostgresHook","Victor Villas B=C3=B4as Chaves created AIRFLOW-1859:
--------------------------------------------------

             Summary: Allow specification of cursor_factory with PostgresHo=
ok
                 Key: AIRFLOW-1859
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1859
             Project: Apache Airflow
          Issue Type: New Feature
          Components: hooks
            Reporter: Victor Villas B=C3=B4as Chaves


PostgresHook uses psycopg2 by default, but it''s not exposing the cursor_fac=
tory parameter which is very useful - specially for using DictCursor and co=
lumn name based access to retrieved records.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27377,54,JIRA.13121470.1511907122000.338823.1511907180108@Atlassian.JIRA,2321,Zelko Nikolic (JIRA),JIRA.13121470.1511907122000@Atlassian.JIRA,,,2017-11-28 14:13:00-08,"[jira] [Created] (AIRFLOW-1860) When adding variables in Web GUI
 they are case-insensitive, but everywhere else they are case-sensitive","Zelko Nikolic created AIRFLOW-1860:
--------------------------------------

             Summary: When adding variables in Web GUI they are case-insensitive, but everywhere else they are case-sensitive
                 Key: AIRFLOW-1860
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1860
             Project: Apache Airflow
          Issue Type: Bug
          Components: webapp
    Affects Versions: Airflow 1.8
            Reporter: Zelko Nikolic
            Priority: Minor


1. Open Web GUI -> Variables
2. Add a variable named ""ABCD"" and enter some value for it.
3. Add a variable named ""abcd""... you can''t. It says that variable already exists. As if variable names are case-insensitive. But they are not.
4. Because if you try accessing the first variable from the code, using Variable.get(""abcd""), it will say that variable doesn''t exist.

So, from the code variables are case-sensitive, but when Web GUI is adding them, it treats them case-insensitive. To make things even weirder, when you sort the variables by name in Web GUI, it sorts uppercase first, then lowercase. Obviously, even Web GUI isn''t consistent with itself, because it''s treating them sometimes case-sensitive, sometimes case-insensitive.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706992,24,153196478100.2988.15964380136752429659.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 18:46:21-07,[GitHub] szha commented on issue #11810: website build and deploy failing,"szha commented on issue #11810: website build and deploy failing
URL: https://github.com/apache/incubator-mxnet/issues/11810#issuecomment-406128037
 
 
   Where''s the above log from?

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27378,54,JIRA.13121475.1511907413000.338868.1511907420463@Atlassian.JIRA,2321,Zelko Nikolic (JIRA),JIRA.13121475.1511907413000@Atlassian.JIRA,,,2017-11-28 14:17:00-08,"[jira] [Created] (AIRFLOW-1861) If a variable is lowercase and
 contains words like ""secret"" it is encrypted, but if variable is uppercase,
 then it''s not encrypted","Zelko Nikolic created AIRFLOW-1861:
--------------------------------------

             Summary: If a variable is lowercase and contains words like ""secret"" it is encrypted, but if variable is uppercase, then it''s not encrypted
                 Key: AIRFLOW-1861
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1861
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: Airflow 1.8
            Reporter: Zelko Nikolic


This is such a big inconsistency that it''s basically a bug. Concluding whether a variable should be encrypted or not shouldn''t even be decided by its name, even less by its case. 

There should be a checkbox so when one clicks it, the variable is encrypted, otherwise it''s not. But the way it works now, making this feature case-sensitive, is inexplicable. Why would someone do something like that????



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27379,54,JIRA.13121470.1511907122000.338869.1511907480104@Atlassian.JIRA,2321,Zelko Nikolic (JIRA),JIRA.13121470.1511907122000@Atlassian.JIRA,,,2017-11-28 14:18:00-08,"[jira] [Updated] (AIRFLOW-1860) When adding variables in Web GUI
 they are case-insensitive, but everywhere else they are case-sensitive","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1860?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Zelko Nikolic updated AIRFLOW-1860:
-----------------------------------
    Description: 
1. Open Web GUI -> Variables
2. Add a variable named ""ABCD"" and enter some value for it.
3. Now add a variable named ""abcd"" and enter some value... wait... you can''t. It says that variable already exists. As if variable names are case-insensitive. But they are not.
4. Because if you try accessing the first variable from the code, using Variable.get(""abcd""), it will say that variable doesn''t exist.

So, from the code variables are case-sensitive, but when Web GUI is adding them, it treats them case-insensitive. To make things even weirder, when you sort the variables by name in Web GUI, it sorts uppercase first, then lowercase. Obviously, even Web GUI isn''t consistent with itself, because it''s treating them sometimes case-sensitive, sometimes case-insensitive.

  was:
1. Open Web GUI -> Variables
2. Add a variable named ""ABCD"" and enter some value for it.
3. Add a variable named ""abcd""... you can''t. It says that variable already exists. As if variable names are case-insensitive. But they are not.
4. Because if you try accessing the first variable from the code, using Variable.get(""abcd""), it will say that variable doesn''t exist.

So, from the code variables are case-sensitive, but when Web GUI is adding them, it treats them case-insensitive. To make things even weirder, when you sort the variables by name in Web GUI, it sorts uppercase first, then lowercase. Obviously, even Web GUI isn''t consistent with itself, because it''s treating them sometimes case-sensitive, sometimes case-insensitive.


> When adding variables in Web GUI they are case-insensitive, but everywhere else they are case-sensitive
> -------------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1860
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1860
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webapp
>    Affects Versions: Airflow 1.8
>            Reporter: Zelko Nikolic
>            Priority: Minor
>
> 1. Open Web GUI -> Variables
> 2. Add a variable named ""ABCD"" and enter some value for it.
> 3. Now add a variable named ""abcd"" and enter some value... wait... you can''t. It says that variable already exists. As if variable names are case-insensitive. But they are not.
> 4. Because if you try accessing the first variable from the code, using Variable.get(""abcd""), it will say that variable doesn''t exist.
> So, from the code variables are case-sensitive, but when Web GUI is adding them, it treats them case-insensitive. To make things even weirder, when you sort the variables by name in Web GUI, it sorts uppercase first, then lowercase. Obviously, even Web GUI isn''t consistent with itself, because it''s treating them sometimes case-sensitive, sometimes case-insensitive.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27380,54,JIRA.13121470.1511907122000.338881.1511907540677@Atlassian.JIRA,2321,Zelko Nikolic (JIRA),JIRA.13121470.1511907122000@Atlassian.JIRA,,,2017-11-28 14:19:00-08,"[jira] [Updated] (AIRFLOW-1860) When adding variables in Web GUI
 they are case-insensitive, but everywhere else they are case-sensitive","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1860?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Zelko Nikolic updated AIRFLOW-1860:
-----------------------------------
    Description: 
1. Open Web GUI -> Variables
2. Add a variable named ""ABCD"" and enter some value for it.
3. Now add a variable named ""abcd"", enter some value and save it. Oh, you can''t. It says that variable already exists. As if variable names are case-insensitive. But they are not.
4. Because if you try accessing the first variable from the code, using Variable.get(""abcd""), it will say that variable doesn''t exist.

So, from the code variables are case-sensitive, but when Web GUI is adding them, it treats them case-insensitive. To make things even weirder, when you sort the variables by name in Web GUI, it sorts uppercase first, then lowercase. Obviously, even Web GUI isn''t consistent with itself, because it''s treating them sometimes case-sensitive, sometimes case-insensitive.

  was:
1. Open Web GUI -> Variables
2. Add a variable named ""ABCD"" and enter some value for it.
3. Now add a variable named ""abcd"" and enter some value... wait... you can''t. It says that variable already exists. As if variable names are case-insensitive. But they are not.
4. Because if you try accessing the first variable from the code, using Variable.get(""abcd""), it will say that variable doesn''t exist.

So, from the code variables are case-sensitive, but when Web GUI is adding them, it treats them case-insensitive. To make things even weirder, when you sort the variables by name in Web GUI, it sorts uppercase first, then lowercase. Obviously, even Web GUI isn''t consistent with itself, because it''s treating them sometimes case-sensitive, sometimes case-insensitive.


> When adding variables in Web GUI they are case-insensitive, but everywhere else they are case-sensitive
> -------------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1860
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1860
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: webapp
>    Affects Versions: Airflow 1.8
>            Reporter: Zelko Nikolic
>            Priority: Minor
>
> 1. Open Web GUI -> Variables
> 2. Add a variable named ""ABCD"" and enter some value for it.
> 3. Now add a variable named ""abcd"", enter some value and save it. Oh, you can''t. It says that variable already exists. As if variable names are case-insensitive. But they are not.
> 4. Because if you try accessing the first variable from the code, using Variable.get(""abcd""), it will say that variable doesn''t exist.
> So, from the code variables are case-sensitive, but when Web GUI is adding them, it treats them case-insensitive. To make things even weirder, when you sort the variables by name in Web GUI, it sorts uppercase first, then lowercase. Obviously, even Web GUI isn''t consistent with itself, because it''s treating them sometimes case-sensitive, sometimes case-insensitive.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27381,54,JIRA.13121484.1511912159000.339647.1511912220434@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:37:00-08,"[jira] [Created] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","Mark S Weiss created AIRFLOW-1862:
-------------------------------------

             Summary: redshift_to_s3_operator fails on BOOLEAN column in source table
                 Key: AIRFLOW-1862
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
             Project: Apache Airflow
          Issue Type: Bug
          Components: redshift
    Affects Versions: Airflow 1.8
            Reporter: Mark S Weiss
            Assignee: Mark S Weiss
             Fix For: Airflow 1.8


The {{airflow/operators/redshift_to_s3_operator}} module generates an `UNLOAD` query using an SQL fragment for the `column_castings` that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

<noformat>
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
</noformat>

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27382,54,JIRA.13121484.1511912159000.339663.1511912220567@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:37:00-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

<noformat>
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
<noformat>

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

<noformat>
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
</noformat>

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> <noformat>
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=#SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> <noformat>
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> <noformat>
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> </noformat>



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27383,54,JIRA.13121484.1511912159000.339662.1511912220560@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:37:00-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

<noformat>
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
</noformat>

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an `UNLOAD` query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

<noformat>
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
</noformat>

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> <noformat>
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=#SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> </noformat>
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> <noformat>
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> </noformat>



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27384,54,JIRA.13121484.1511912159000.339659.1511912220541@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:37:00-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an `UNLOAD` query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

<noformat>
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
</noformat>

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an `UNLOAD` query using an SQL fragment for the `column_castings` that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

<noformat>
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
</noformat>

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an `UNLOAD` query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> <noformat>
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=#SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> </noformat>
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> <noformat>
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> </noformat>



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27385,54,JIRA.13121484.1511912159000.339664.1511912280196@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:38:00-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

<noformat>
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
<noformat>

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> {noformat}
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=#SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> {noformat}
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> <noformat>
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> </noformat>



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27386,54,JIRA.13121484.1511912159000.339670.1511912280270@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:38:00-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{noformat}



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{/noformat}




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> {noformat}
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=#SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> {noformat}
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> {noformat}
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27387,54,JIRA.13121484.1511912159000.339669.1511912280260@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:38:00-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{/noformat}



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

<noformat>
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
</noformat>




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> {noformat}
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=#SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> {noformat}
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> {noformat}
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> {/noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27388,54,JIRA.13121484.1511912159000.339671.1511912340578@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:39:00-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
* Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
* Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{noformat}



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{noformat}




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> {noformat}
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=#SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> {noformat}
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> * Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> * Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> {noformat}
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27389,54,JIRA.13121484.1511912159000.339694.1511912341917@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:39:01-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=# SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
* Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
* Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{noformat}



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=#SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
* Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
* Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{noformat}




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> {noformat}
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=# SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> {noformat}
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> * Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> * Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> {noformat}
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27390,54,JIRA.13121484.1511912159000.339709.1511912400732@Atlassian.JIRA,2416,Mark S Weiss (JIRA),JIRA.13121484.1511912159000@Atlassian.JIRA,,,2017-11-28 15:40:00-08,"[jira] [Updated] (AIRFLOW-1862) redshift_to_s3_operator fails on
 BOOLEAN column in source table","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1862?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Mark S Weiss updated AIRFLOW-1862:
----------------------------------
    Description: 
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=# SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
* Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
* Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        columns_query = """"""SELECT column_name, data_type
                            FROM information_schema.columns
                            WHERE table_schema = ''{0}''
                            AND   table_name = ''{1}''
                            ORDER BY ordinal_position
                        """""".format(self.schema, self.table)

        ....
        ....

        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{noformat}



  was:
The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.

This can be trivially verified in Redshift using the following code:

{noformat}
my_db=# CREATE TABLE temp (BOOLEAN flag);
my_db=# INSERT INTO temp (flag) VALUES(false);
my_db=# SELECT CAST (flag AS text) FROM temp;

ERROR:  cannot cast type boolean to character varying
{noformat}

The solution is to handle the case of {{BOOLEAN}} columns, by:
* Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
* Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87

I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.

{noformat}
        column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
                                       if types[i] != ''boolean'' else
                                       ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
                                       for i in xrange(len(columns))])
{noformat}




> redshift_to_s3_operator fails on BOOLEAN column in source table
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1862
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1862
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: redshift
>    Affects Versions: Airflow 1.8
>            Reporter: Mark S Weiss
>            Assignee: Mark S Weiss
>              Labels: easyfix
>             Fix For: Airflow 1.8
>
>
> The {{airflow/operators/redshift_to_s3_operator}} module generates an {{UNLOAD}} query using an SQL fragment for the {{column_castings}} that is generated by this line: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> This is a bug, because a {{CAST()}} in Redshift on a column of type {{BOOLEAN}} will raise an error and abort execution of the SQL statement. The error raised is {{ERROR:  cannot cast type boolean to character varying}}.
> This can be trivially verified in Redshift using the following code:
> {noformat}
> my_db=# CREATE TABLE temp (BOOLEAN flag);
> my_db=# INSERT INTO temp (flag) VALUES(false);
> my_db=# SELECT CAST (flag AS text) FROM temp;
> ERROR:  cannot cast type boolean to character varying
> {noformat}
> The solution is to handle the case of {{BOOLEAN}} columns, by:
> * Modifying the {{columns_query}} here https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L75 to also select column {{data_type}}
> * Modify the expression here to use the {{data_type}} to generate a valid {{CAST}} here: https://github.com/apache/incubator-airflow/blob/master/airflow/operators/redshift_to_s3_operator.py#L87
> I have implemented and locally tested this alternative, which I believe is both minimal in scope and robust in all cases, and will submit a pull request.
> {noformat}
>         columns_query = """"""SELECT column_name, data_type
>                             FROM information_schema.columns
>                             WHERE table_schema = ''{0}''
>                             AND   table_name = ''{1}''
>                             ORDER BY ordinal_position
>                         """""".format(self.schema, self.table)
>         ....
>         ....
>         column_castings = ('', '').join([""CAST({0} AS text) AS {1}"".format(columns[i], columns[i])
>                                        if types[i] != ''boolean'' else
>                                        ""CAST(CAST({0} AS SMALLINT)AS text) AS {1}"".format(columns[i], columns[i])
>                                        for i in xrange(len(columns))])
> {noformat}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27391,54,JIRA.13121553.1511935112000.342390.1511935140067@Atlassian.JIRA,2411,Yee Ting Li (JIRA),JIRA.13121553.1511935112000@Atlassian.JIRA,,,2017-11-28 21:59:00-08,"[jira] [Created] (AIRFLOW-1863) should the gantt chart provide a
 drop down of the dag run like the grpah view?","Yee Ting Li created AIRFLOW-1863:
------------------------------------

             Summary: should the gantt chart provide a drop down of the dag run like the grpah view?
                 Key: AIRFLOW-1863
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1863
             Project: Apache Airflow
          Issue Type: Bug
          Components: webapp
    Affects Versions: 1.8.2
            Reporter: Yee Ting Li


the gantt chart is a great way of displaying the efficiency of a dag run. however, the current display of the run ''time'' is somewhat arbitrary and does not provide meaningful ways of determining how specific a dag performed (eg with manually triggered dags).

i think it would be useful to have a drop down of the dag runs (as it currently does with the Graph View) presented on the Gantt View.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706993,24,153196483637.3780.7142797765445332298.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 18:47:16-07,"[GitHub] dbsxdbsx commented on issue #9026: why is it so slow
 (MXNET0.12)even with NVIDIA V100 GPU?","dbsxdbsx commented on issue #9026: why is it so slow (MXNET0.12)even with NVIDIA V100 GPU?
URL: https://github.com/apache/incubator-mxnet/issues/9026#issuecomment-406128181
 
 
   @safrooze Thanks

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27392,54,JIRA.13121555.1511936067000.342469.1511936100572@Atlassian.JIRA,2411,Yee Ting Li (JIRA),JIRA.13121555.1511936067000@Atlassian.JIRA,,,2017-11-28 22:15:00-08,"[jira] [Created] (AIRFLOW-1864) Dags with large number of Tasks are
 very slow","Yee Ting Li created AIRFLOW-1864:
------------------------------------

             Summary: Dags with large number of Tasks are very slow
                 Key: AIRFLOW-1864
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1864
             Project: Apache Airflow
          Issue Type: Bug
          Components: scheduler
    Affects Versions: 1.8.2
         Environment: job_heartbeat_sec = 1
scheduler_heartbeat_sec = 1
            Reporter: Yee Ting Li


i have a dag which according to the scheduler logs([DAG File Processing Stats), take about 10 seconds to process. it has about 20 odd tasks, mostly linear in nature.

it takes a long time for the dag to complete, even though the individual tasks are fast - many only a few seconds with a couple of longer tasks.

looking at the scheduler logs more closely, it seems that most of the time it is waiting for jobs to be put into the executor. i am assuming (without looking at the code), that during the time the dag is being processed, no jobs are being queued or being send to the executor. it seems as though the entire scheduler is basically waiting for the dag processing to finish before it does anything - which means that larger dags inherently do no scale and linear graphs take a significant amount of time to finish.

is there a way to improve the responsiveness of new jobs being queued and executed beyond the two parameters (job_heartbeat_sec and scheduler_heartbeat_sec)?






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27393,54,JIRA.13121620.1511955424000.344711.1511955480023@Atlassian.JIRA,2417,Nunemunthala Sudheer (JIRA),JIRA.13121620.1511955424000@Atlassian.JIRA,,,2017-11-29 03:38:00-08,"[jira] [Created] (AIRFLOW-1865) As a Novice user, can a
 non-technical create a DAG workflow using Airflow UI interface","Nunemunthala Sudheer created AIRFLOW-1865:
---------------------------------------------

             Summary: As a Novice user, can a non-technical create a DAG workflow using Airflow UI interface
                 Key: AIRFLOW-1865
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1865
             Project: Apache Airflow
          Issue Type: Improvement
          Components: DAG
            Reporter: Nunemunthala Sudheer


We are exploring some open source tools for scheduling our jobs and observed Airflow as promising tool.
In Airflow, we have a facility to create a DAG through UI. But, we did not find a way to provide the workflow for the DAG created through Airflow UI. Is there a way to do it through UI or any other simple way apart from python script for non technical users.




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27394,54,JIRA.13121639.1511960157000.344970.1511960160140@Atlassian.JIRA,2413,Kaxil Naik (JIRA),JIRA.13121639.1511960157000@Atlassian.JIRA,,,2017-11-29 04:56:00-08,"[jira] [Created] (AIRFLOW-1866) Fix missing parameters in docstring
 for copy function in gcs_hook","Kaxil Naik created AIRFLOW-1866:
-----------------------------------

             Summary: Fix missing parameters in docstring for copy function in gcs_hook
                 Key: AIRFLOW-1866
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1866
             Project: Apache Airflow
          Issue Type: Task
          Components: contrib, gcp
            Reporter: Kaxil Naik
            Assignee: Kaxil Naik
            Priority: Minor


The docstrings for copy method in gcs_hook.py is outdated. PyCharm warns that (source_bucket, source_object) have missing docstrings.





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27395,54,JIRA.13120988.1511796793000.345349.1511964360566@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120988.1511796793000@Atlassian.JIRA,,,2017-11-29 06:06:00-08,"[jira] [Commented] (AIRFLOW-1850) Sqoop password is overwritten by
 reference","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1850?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16270774#comment-16270774 ] 

ASF subversion and git services commented on AIRFLOW-1850:
----------------------------------------------------------

Commit 4135c82bf4cd07d5e18a9fabda5d62e27a23ca3f in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4135c82 ]

[AIRFLOW-1850] Copy cmd before masking

The cmd is first copied before the password is
masked. This ensures
that the orignal cmd isn''t changed. Replacing the
password with a
masked value replaces the password in the original
command since it
is passed by reference.

Closes #2817 from Fokko/AIRFLOW-1850-copy-cmd-
before-replacing-password


> Sqoop password is overwritten by reference
> ------------------------------------------
>
>                 Key: AIRFLOW-1850
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1850
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Assignee: Fokko Driesprong
>
> The sqoop operator masks the password so it does not appear in the logs. This is good, but the password itself is overwritten since the map is passed by reference.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27396,54,JIRA.13120988.1511796793000.345350.1511964360574@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13120988.1511796793000@Atlassian.JIRA,,,2017-11-29 06:06:00-08,"[jira] [Commented] (AIRFLOW-1850) Sqoop password is overwritten by
 reference","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1850?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16270775#comment-16270775 ] 

ASF subversion and git services commented on AIRFLOW-1850:
----------------------------------------------------------

Commit 4135c82bf4cd07d5e18a9fabda5d62e27a23ca3f in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=4135c82 ]

[AIRFLOW-1850] Copy cmd before masking

The cmd is first copied before the password is
masked. This ensures
that the orignal cmd isn''t changed. Replacing the
password with a
masked value replaces the password in the original
command since it
is passed by reference.

Closes #2817 from Fokko/AIRFLOW-1850-copy-cmd-
before-replacing-password


> Sqoop password is overwritten by reference
> ------------------------------------------
>
>                 Key: AIRFLOW-1850
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1850
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Assignee: Fokko Driesprong
>
> The sqoop operator masks the password so it does not appear in the logs. This is good, but the password itself is overwritten since the map is passed by reference.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
706994,24,153196483649.3782.4985747683208358891.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 18:47:16-07,"[GitHub] dbsxdbsx closed issue #9026: why is it so slow (MXNET0.12)even with
 NVIDIA V100 GPU?","dbsxdbsx closed issue #9026: why is it so slow (MXNET0.12)even with NVIDIA V100 GPU?
URL: https://github.com/apache/incubator-mxnet/issues/9026
 
 
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27397,54,JIRA.13116257.1509892666000.345563.1511965980694@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13116257.1509892666000@Atlassian.JIRA,,,2017-11-29 06:33:00-08,[jira] [Commented] (AIRFLOW-1785) Enable Python 3 tests,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1785?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16270815#comment-16270815 ] 

ASF subversion and git services commented on AIRFLOW-1785:
----------------------------------------------------------

Commit 8e7b0abed297ae53821feb35bac55390d7a8e2bf in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=8e7b0ab ]

[AIRFLOW-1785] Enable Python 3 tests

Enable tests under Python 3 to make sure that
tests run under Python
3.

Closes #2755 from Fokko/AIRFLOW-1785-Enable-
Python3-tests


> Enable Python 3 tests
> ---------------------
>
>                 Key: AIRFLOW-1785
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1785
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>             Fix For: 1.9.1
>
>
> Some of the tests are being skipped when running Python 3.4. We would like to enable these tests to make sure that everything runs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27398,54,JIRA.13116257.1509892666000.345562.1511965980682@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13116257.1509892666000@Atlassian.JIRA,,,2017-11-29 06:33:00-08,[jira] [Commented] (AIRFLOW-1785) Enable Python 3 tests,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1785?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16270814#comment-16270814 ] 

ASF subversion and git services commented on AIRFLOW-1785:
----------------------------------------------------------

Commit 8e7b0abed297ae53821feb35bac55390d7a8e2bf in incubator-airflow''s branch refs/heads/master from [~Fokko]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=8e7b0ab ]

[AIRFLOW-1785] Enable Python 3 tests

Enable tests under Python 3 to make sure that
tests run under Python
3.

Closes #2755 from Fokko/AIRFLOW-1785-Enable-
Python3-tests


> Enable Python 3 tests
> ---------------------
>
>                 Key: AIRFLOW-1785
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1785
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>             Fix For: 1.9.1
>
>
> Some of the tests are being skipped when running Python 3.4. We would like to enable these tests to make sure that everything runs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27399,54,JIRA.13116257.1509892666000.345566.1511965980720@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13116257.1509892666000@Atlassian.JIRA,,,2017-11-29 06:33:00-08,[jira] [Assigned] (AIRFLOW-1785) Enable Python 3 tests,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1785?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong reassigned AIRFLOW-1785:
-----------------------------------------

    Assignee: Fokko Driesprong

> Enable Python 3 tests
> ---------------------
>
>                 Key: AIRFLOW-1785
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1785
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Assignee: Fokko Driesprong
>             Fix For: 1.9.1
>
>
> Some of the tests are being skipped when running Python 3.4. We would like to enable these tests to make sure that everything runs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27400,54,JIRA.13116257.1509892666000.345568.1511965980738@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13116257.1509892666000@Atlassian.JIRA,,,2017-11-29 06:33:00-08,[jira] [Resolved] (AIRFLOW-1785) Enable Python 3 tests,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1785?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong resolved AIRFLOW-1785.
---------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2755
[https://github.com/apache/incubator-airflow/pull/2755]

> Enable Python 3 tests
> ---------------------
>
>                 Key: AIRFLOW-1785
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1785
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Fokko Driesprong
>            Assignee: Fokko Driesprong
>             Fix For: 1.9.1
>
>
> Some of the tests are being skipped when running Python 3.4. We would like to enable these tests to make sure that everything runs.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27401,54,JIRA.13121349.1511882533000.346138.1511969280500@Atlassian.JIRA,2418,Yogi Valani (JIRA),JIRA.13121349.1511882533000@Atlassian.JIRA,,,2017-11-29 07:28:00-08,"[jira] [Commented] (AIRFLOW-1856) How to allow airflow dags for
 concrete user(s) only?","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1856?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16270941#comment-16270941 ] 

Yogi Valani commented on AIRFLOW-1856:
--------------------------------------

Looks like its already discussed here: https://cwiki.apache.org/confluence/display/AIRFLOW/DAGs+UI

> How to allow airflow dags for concrete user(s) only?
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1856
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1856
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: authentication, ui, webapp
>            Reporter: Ikar Pohorsky
>
> The problem is pretty simple. I need to limit airflow web users to see and execute only certain DAGs and tasks.
> If possible, I''d prefer not to use [Kerberos|https://airflow.incubator.apache.org/security.html#kerberos] nor [OAuth|https://airflow.incubator.apache.org/security.html#oauth-authentication].
> The [Multi-tenancy|https://airflow.incubator.apache.org/security.html#multi-tenancy] option seems like an option to go, but couldn''t make it work the way I expect.
> My current setup:
> * added airflow web users _test_ and _ikar_ via [Web Authentication / Password|https://airflow.incubator.apache.org/security.html#password]
> * my unix username is _ikar_ with a home in _/home/ikar_
> * no _test_ unix user
> * airflow _1.8.2_ is installed in _/home/ikar/airflow_
> * added two DAGs with one task:
> ** one with _owner_ set to _ikar_
> ** one with _owner_ set to _test_
> * airflow.cfg:
> {code}
> [core]
> # The home folder for airflow, default is ~/airflow
> airflow_home = /home/ikar/airflow
> # The folder where your airflow pipelines live, most likely a
> # subfolder in a code repository
> # This path must be absolute
> dags_folder = /home/ikar/airflow-test/dags
> # The folder where airflow should store its log files
> # This path must be absolute
> base_log_folder = /home/ikar/airflow/logs
> # Airflow can store logs remotely in AWS S3 or Google Cloud Storage. Users
> # must supply a remote location URL (starting with either ''s3://...'' or
> # ''gs://...'') and an Airflow connection id that provides access to the storage
> # location.
> remote_base_log_folder =
> remote_log_conn_id =
> # Use server-side encryption for logs stored in S3
> encrypt_s3_logs = False                                                                                                                                                                  
> # DEPRECATED option for remote log storage, use remote_base_log_folder instead!                                                                                                          
> s3_log_folder =                                                                                                                                                                          
>                                                                                                                                                                                                                   
> # The executor class that airflow should use. Choices include                                                                                                                            
> # SequentialExecutor, LocalExecutor, CeleryExecutor                                                                                                                                      
> executor = SequentialExecutor                                                                                                                                                            
>                                                                                                                                                                                                                   
> # The SqlAlchemy connection string to the metadata database.                                                                                                                             
> # SqlAlchemy supports many different database engine, more information                                                                                                                   
> # their website                                                                                                                                                                          
> sql_alchemy_conn = sqlite:////home/ikar/airflow/airflow.db
> # The SqlAlchemy pool size is the maximum number of database connections
> # in the pool.
> sql_alchemy_pool_size = 5
> # The SqlAlchemy pool recycle is the number of seconds a connection
> # can be idle in the pool before it is invalidated. This config does
> # not apply to sqlite.
> sql_alchemy_pool_recycle = 3600
> # The amount of parallelism as a setting to the executor. This defines
> # the max number of task instances that should run simultaneously
> # on this airflow installation
> parallelism = 32
> # The number of task instances allowed to run concurrently by the scheduler
> dag_concurrency = 16
> # Are DAGs paused by default at creation
> dags_are_paused_at_creation = True
> # When not using pools, tasks are run in the ""default pool"",
> # whose size is guided by this config element
> non_pooled_task_slot_count = 128
> # The maximum number of active DAG runs per DAG
> max_active_runs_per_dag = 16
> # Whether to load the examples that ship with Airflow. It''s good to
> # get started, but you probably want to set this to False in a production
> # environment
> load_examples = False
> # Where your Airflow plugins are stored
> plugins_folder = /home/ikar/airflow/plugins
> # Secret key to save connection passwords in the db
> fernet_key = cryptography_not_found_storing_passwords_in_plain_text
> # Whether to disable pickling dags
> donot_pickle = False
> # How long before timing out a python file import while filling the DagBag
> dagbag_import_timeout = 30
> # The class to use for running task instances in a subprocess
> task_runner = BashTaskRunner
> # If set, tasks without a `run_as_user` argument will be run with this user
> # Can be used to de-elevate a sudo user running Airflow when executing tasks
> default_impersonation =
> # What security module to use (for example kerberos):
> security =
> # Turn unit test mode on (overwrites many configuration options with test
> # values at runtime)
> unit_test_mode = False
> [cli]
> # In what way should the cli access the API. The LocalClient will use the
> # database directly, while the json_client will use the api running on the
> # webserver
> api_client = airflow.api.client.local_client
> endpoint_url = http://localhost:8888
> [api]
> # How to authenticate users of the API
> auth_backend = airflow.api.auth.backend.default
> [operators]
> # The default owner assigned to each new operator, unless
> # provided explicitly or passed via `default_args`
> default_owner = Airflow
> default_cpus = 1
> default_ram = 512
> default_disk = 512
> default_gpus = 0
> [webserver]
> # The base url of your website as airflow cannot guess what domain or
> # cname you are using. This is used in automated emails that
> # airflow sends to point links to the right web server
> base_url = http://localhost:8888
> # The ip specified when starting the web server
> web_server_host = 0.0.0.0
> # The port on which to run the web server
> web_server_port = 8888
> # Paths to the SSL certificate and key for the web server. When both are
> # provided SSL will be enabled. This does not change the web server port.
> web_server_ssl_cert =
> web_server_ssl_key =
> # Number of seconds the gunicorn webserver waits before timing out on a worker
> web_server_worker_timeout = 120
> # Number of workers to refresh at a time. When set to 0, worker refresh is
> # disabled. When nonzero, airflow periodically refreshes webserver workers by
> # bringing up new ones and killing old ones.
> worker_refresh_batch_size = 1
> # Number of seconds to wait before refreshing a batch of workers.
> worker_refresh_interval = 30
> # Secret key used to run your flask app
> secret_key = temporary_key
> # Number of workers to run the Gunicorn web server
> workers = 4
> # The worker class gunicorn should use. Choices include
> # sync (default), eventlet, gevent
> worker_class = sync
> # Log files for the gunicorn webserver. ''-'' means log to stderr.
> access_logfile = -
> error_logfile = -
> # Expose the configuration file in the web server
> expose_config = False
> # Set to true to turn on authentication:
> # http://pythonhosted.org/airflow/security.html#web-authentication
> authenticate = True
> auth_backend = airflow.contrib.auth.backends.password_auth
> # Filter the list of dags by owner name (requires authentication to be enabled)
> filter_by_owner = True
> # Filtering mode. Choices include user (default) and ldapgroup.
> # Ldap group filtering requires using the ldap backend
> #
> # Note that the ldap server needs the ""memberOf"" overlay to be set up
> # in order to user the ldapgroup mode.
> owner_mode = user
> # Default DAG orientation. Valid values are:
> # LR (Left->Right), TB (Top->Bottom), RL (Right->Left), BT (Bottom->Top)
> dag_orientation = LR
> # Puts the webserver in demonstration mode; blurs the names of Operators for
> # privacy.
> demo_mode = False
> # The amount of time (in secs) webserver will wait for initial handshake
> # while fetching logs from other worker machine
> log_fetch_timeout_sec = 5
> # By default, the webserver shows paused DAGs. Flip this to hide paused
> # DAGs by default
> hide_paused_dags_by_default = False
> [email]
> email_backend = airflow.utils.email.send_email_smtp
> [smtp]
> # If you want airflow to send emails on retries, failure, and you want to use
> # the airflow.utils.email.send_email_smtp function, you have to configure an
> # smtp server here
> smtp_host = localhost
> smtp_starttls = True
> smtp_ssl = False
> # Uncomment and set the user/pass settings if you want to use SMTP AUTH
> # smtp_user = airflow
> # smtp_password = airflow
> smtp_port = 25
> smtp_mail_from = airflow@airflow.com
> [celery]
> # This section only applies if you are using the CeleryExecutor in
> # [core] section above
> # The app name that will be used by celery
> celery_app_name = airflow.executors.celery_executor
> # The concurrency that will be used when starting workers with the
> # ""airflow worker"" command. This defines the number of task instances that
> # a worker will take, so size up your workers based on the resources on
> # your worker box and the nature of your tasks
> celeryd_concurrency = 4
> # When you start an airflow worker, airflow starts a tiny web server
> # subprocess to serve the workers local log files to the airflow main
> # web server, who then builds pages and sends them to users. This defines
> # the port on which the logs are served. It needs to be unused, and open
> # visible from the main web server to connect into the workers.
> worker_log_server_port = 8793
> # The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
> # a sqlalchemy database. Refer to the Celery documentation for more
> # information.
> broker_url = sqla+mysql://airflow:airflow@localhost:3306/airflow
> # Another key Celery setting
> celery_result_backend = db+mysql://airflow:airflow@localhost:3306/airflow
> # Celery Flower is a sweet UI for Celery. Airflow has a shortcut to start
> # it `airflow flower`. This defines the IP that Celery Flower runs on
> flower_host = 0.0.0.0
> # This defines the port that Celery Flower runs on
> flower_port = 5555
> # Default queue that tasks get assigned to and that worker listen on.
> default_queue = default
> [scheduler]
> # Task instances listen for external kill signal (when you clear tasks
> # from the CLI or the UI), this defines the frequency at which they should
> # listen (in seconds).
> job_heartbeat_sec = 5
> # The scheduler constantly tries to trigger new tasks (look at the
> # scheduler section in the docs for more information). This defines
> # how often the scheduler should run (in seconds).
> scheduler_heartbeat_sec = 5
> # after how much time should the scheduler terminate in seconds
> # -1 indicates to run continuously (see also num_runs)
> run_duration = -1
> # after how much time a new DAGs should be picked up from the filesystem
> min_file_process_interval = 0
> dag_dir_list_interval = 300
> # How often should stats be printed to the logs
> print_stats_interval = 30
> child_process_log_directory = /home/ikar/airflow/logs/scheduler
> # Local task jobs periodically heartbeat to the DB. If the job has
> # not heartbeat in this many seconds, the scheduler will mark the
> # associated task instance as failed and will re-schedule the task.
> scheduler_zombie_task_threshold = 300
> # Turn off scheduler catchup by setting this to False.
> # Default behavior is unchanged and
> # Command Line Backfills still work, but the scheduler
> # will not do scheduler catchup if this is False,
> # however it can be set on a per DAG basis in the
> # DAG definition (catchup)
> catchup_by_default = False
> # Statsd (https://github.com/etsy/statsd) integration settings
> statsd_on = False
> statsd_host = localhost
> statsd_port = 8125
> statsd_prefix = airflow
> # The scheduler can run multiple threads in parallel to schedule dags.
> # This defines how many threads will run. However airflow will never
> # use more threads than the amount of cpu cores available.
> max_threads = 2
> authenticate = False
> [mesos]
> # Mesos master address which MesosExecutor will connect to.
> master = localhost:5050
> # The framework name which Airflow scheduler will register itself as on mesos
> framework_name = Airflow
> # Number of cpu cores required for running one task instance using
> # ''airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>''
> # command on a mesos slave
> task_cpu = 1
> # Memory in MB required for running one task instance using
> # ''airflow run <dag_id> <task_id> <execution_date> --local -p <pickle_id>''
> # command on a mesos slave
> task_memory = 256
> # Enable framework checkpointing for mesos
> # See http://mesos.apache.org/documentation/latest/slave-recovery/
> checkpoint = False
> # Failover timeout in milliseconds.
> # When checkpointing is enabled and this option is set, Mesos waits
> # until the configured timeout for
> # the MesosExecutor framework to re-register after a failover. Mesos
> # shuts down running tasks if the
> # MesosExecutor framework fails to re-register within this timeframe.
> # failover_timeout = 604800
> # Enable framework authentication for mesos
> # See http://mesos.apache.org/documentation/latest/configuration/
> authenticate = False
> # Mesos credentials, if authentication is enabled
> # default_principal = admin
> # default_secret = admin
> [kerberos]
> ccache = /tmp/airflow_krb5_ccache
> # gets augmented with fqdn
> principal = airflow
> reinit_frequency = 3600
> kinit_path = kinit
> keytab = airflow.keytab
> [github_enterprise]
> api_rev = v3
> [admin]
> # UI to hide sensitive variable fields when set to True
> hide_sensitive_variable_fields = True
> {code}
> * sample DAG:
> {code}
> from datetime import datetime
> from airflow import DAG
> from airflow.operators.bash_operator import BashOperator
> from core.path import REPO_PATH
> test_template = """"""
> cd {{ params.path }}; python3 -m unittest --verbose {{ params.script }}
> """"""
> with DAG(
>     dag_id=""daily_tests"",
>     schedule_interval=""30 4 * * *"",
>     default_args={''start_date'': datetime(2017, 8, 25, hour=5)}
> ) as dag:
>     BashOperator(
>         task_id=""platform_test"",
>         owner=""ikar"",
>         bash_command=test_template,
>         params={''path'': ""{}/tests/daily"".format(REPO_PATH), ''script'': ""test_platform.py""},
>     )
> {code}
> I''d expect that _test_ user will only see DAG with owner set to _test_ but both users can see and execute both DAGs.
> Searching issues by owner works great.
> Couldn''t find any detailed documentation on how to setup the user restrictions for airflow DAGs.
> Can anyone help? Am I missing something?
> Is it possible that both user I''ve created are superusers? If so, how to make them non-superusers?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27402,54,JIRA.13121722.1511984815000.348301.1511984820058@Atlassian.JIRA,1706,Scott Kruger (JIRA),JIRA.13121722.1511984815000@Atlassian.JIRA,,,2017-11-29 11:47:00-08,"[jira] [Created] (AIRFLOW-1867) sendgrid fails on python3 with
 attachments","Scott Kruger created AIRFLOW-1867:
-------------------------------------

             Summary: sendgrid fails on python3 with attachments
                 Key: AIRFLOW-1867
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1867
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Scott Kruger
            Priority: Minor


Sendgrid emails raise an exception on python 3 when attaching files due to {{base64.b64encode}} returning {{bytes}} rather than {{unicode/string}} (see: https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/utils/sendgrid.py#L69).  The fix is simple: decode the base64 data to `utf-8`.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27403,54,JIRA.13040668.1486396038000.348879.1511989260123@Atlassian.JIRA,1706,Scott Kruger (JIRA),JIRA.13040668.1486396038000@Atlassian.JIRA,,,2017-11-29 13:01:00-08,[jira] [Commented] (AIRFLOW-843) Store task exceptions in context,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-843?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16271533#comment-16271533 ] 

Scott Kruger commented on AIRFLOW-843:
--------------------------------------

PR is rebased

> Store task exceptions in context
> --------------------------------
>
>                 Key: AIRFLOW-843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-843
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Scott Kruger
>            Priority: Minor
>
> If a task encounters an exception during execution, it should store the exception on the execution context so that other methods (namely `on_failure_callback` can access it.  This would help with custom error integrations, e.g. Sentry.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27404,54,JIRA.13121760.1511995648000.349904.1511995681371@Atlassian.JIRA,2419,nathan warshauer (JIRA),JIRA.13121760.1511995648000@Atlassian.JIRA,,,2017-11-29 14:48:01-08,"[jira] [Created] (AIRFLOW-1868) Packaged Dags not added to dag
 table, unable to execute tasks","nathan warshauer created AIRFLOW-1868:
-----------------------------------------

             Summary: Packaged Dags not added to dag table, unable to execute tasks
                 Key: AIRFLOW-1868
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1868
             Project: Apache Airflow
          Issue Type: Bug
         Environment: airflow 1.8.2, celery, rabbitMQ, mySQL, aws
            Reporter: nathan warshauer
         Attachments: Screen Shot 2017-11-29 at 2.31.02 PM.png, Screen Shot 2017-11-29 at 4.40.39 PM.png, Screen Shot 2017-11-29 at 4.42.39 PM.png

.zip files in the dag directory do not appear to be getting added to the dag table on the airflow database.  When a .zip file is placed within the dags folder and it contains executable .py files, the dag_id should be added to the dag table and airflow should allow the dag to be unpaused and run through the web server.
SELECT distinct dag.dag_id AS dag_dag_id FROM dag confirms the dag does not exist in the dags table but shows up on the UI with the warning message ""This Dag seems to be existing only locally"" however the dag exists in all 3 dag directories (master and two workers) and the airflow.cfg has donot_pickle = True
When the dag is triggered manually via airflow trigger_dag <dag_id> the process goes to the web server and does not execute any tasks.  When I go to the task and click start through the UI the task will execute successfully and shows the attached state upon completion.  When I do not do this process the tasks will not enter the queue and the run sits idle as the 3rd attached image shows.
Basically, the dag CAN run manually from the zip BUT the scheduler and underlying database tables appear to not be functioning correctly for packaged dags.
Please let me know if I can provide any additional information regarding this issue, or if you all have any leads that I can check out for resolving this.

dag = DAG(''MY-DAG-NAME'', 
  default_args=default_args, 
  schedule_interval=''*/5 * * * *'',
  max_active_runs=1,
  dagrun_timeout=timedelta(minutes=4, seconds=30))

default_args = {
  ''depends_on_past'': False,
  ''email'': [''airflow@airflow.com''],
  ''email_on_failure'': True,
  ''email_on_retry'': False,
  ''owner'': ''airflow'',
  ''provide_context'': True,
  ''retries'': 0,
  ''retry_delay'': timedelta(minutes=5),
  ''start_date'': datetime(2017,11,28)
}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27405,54,JIRA.13040668.1486396038000.350014.1511996220117@Atlassian.JIRA,1706,Scott Kruger (JIRA),JIRA.13040668.1486396038000@Atlassian.JIRA,,,2017-11-29 14:57:00-08,[jira] [Commented] (AIRFLOW-843) Store task exceptions in context,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-843?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16271719#comment-16271719 ] 

Scott Kruger commented on AIRFLOW-843:
--------------------------------------

The PR is now passing; can anybody review and merge?

> Store task exceptions in context
> --------------------------------
>
>                 Key: AIRFLOW-843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-843
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Scott Kruger
>            Priority: Minor
>
> If a task encounters an exception during execution, it should store the exception on the execution context so that other methods (namely `on_failure_callback` can access it.  This would help with custom error integrations, e.g. Sentry.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27406,54,JIRA.13121798.1512004701000.351558.1512004740379@Atlassian.JIRA,2305,William Pursell (JIRA),JIRA.13121798.1512004701000@Atlassian.JIRA,,,2017-11-29 17:19:00-08,"[jira] [Created] (AIRFLOW-1869) Logging in gcs_task_handler
 discards too many error messages","William Pursell created AIRFLOW-1869:
----------------------------------------

             Summary: Logging in gcs_task_handler discards too many error messages
                 Key: AIRFLOW-1869
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1869
             Project: Apache Airflow
          Issue Type: Improvement
          Components: core
            Reporter: William Pursell
            Assignee: William Pursell
            Priority: Minor
             Fix For: Airflow 2.0






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27407,54,JIRA.13121798.1512004701000.351565.1512004920474@Atlassian.JIRA,2305,William Pursell (JIRA),JIRA.13121798.1512004701000@Atlassian.JIRA,,,2017-11-29 17:22:00-08,"[jira] [Updated] (AIRFLOW-1869) Logging in gcs_task_handler
 discards too many error messages","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1869?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

William Pursell updated AIRFLOW-1869:
-------------------------------------
    Description: Many exceptions are caught and effectively discarded in the gcs task log reader.  It makes debugging difficult.  The logs should be more verbose and include the exception strings.

> Logging in gcs_task_handler discards too many error messages
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1869
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1869
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core
>            Reporter: William Pursell
>            Assignee: William Pursell
>            Priority: Minor
>             Fix For: Airflow 2.0
>
>
> Many exceptions are caught and effectively discarded in the gcs task log reader.  It makes debugging difficult.  The logs should be more verbose and include the exception strings.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27408,54,JIRA.13116697.1510066613000.355337.1512050160242@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13116697.1510066613000@Atlassian.JIRA,,,2017-11-30 05:56:00-08,[jira] [Commented] (AIRFLOW-1790) AWS Batch Operator Suppport,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1790?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16272689#comment-16272689 ] 

Bolke de Bruin commented on AIRFLOW-1790:
-----------------------------------------

Why do you think it is not merged yet?

https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/awsbatch_operator.py


> AWS Batch Operator Suppport
> ---------------------------
>
>                 Key: AIRFLOW-1790
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1790
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: operators
>    Affects Versions: 1.9.0
>            Reporter: Hugo Prudente
>            Assignee: Hugo Prudente
>
> Add support to AWS Batch operator



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27409,54,JIRA.13121919.1512050251000.355351.1512050280140@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13121919.1512050251000@Atlassian.JIRA,,,2017-11-30 05:58:00-08,[jira] [Created] (AIRFLOW-1870) Enable flake8 tests,"Bolke de Bruin created AIRFLOW-1870:
---------------------------------------

             Summary: Enable flake8 tests
                 Key: AIRFLOW-1870
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1870
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
707000,24,153196759176.12895.3478289293199802145.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-18 19:33:11-07,"[GitHub] aaronmarkham commented on issue #11810: website build and deploy
 failing","aaronmarkham commented on issue #11810: website build and deploy failing
URL: https://github.com/apache/incubator-mxnet/issues/11810#issuecomment-406135331
 
 
   http://jenkins.mxnet-ci.amazon-ml.com/job/restricted-website-build/120/console

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27410,54,JIRA.12969653.1463173893000.355488.1512051661271@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.12969653.1463173893000@Atlassian.JIRA,,,2017-11-30 06:21:01-08,"[jira] [Commented] (AIRFLOW-115) Migrate and Refactor AWS
 integration to use boto3 and better structured hooks","
    [ https://issues.apache.org/jira/browse/AIRFLOW-115?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16272715#comment-16272715 ] 

Bolke de Bruin commented on AIRFLOW-115:
----------------------------------------

I think this is resolved in master/1.9.0. Please confirm.

> Migrate and Refactor AWS integration to use boto3 and better structured hooks
> -----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-115
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-115
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: aws, boto3, hooks
>            Reporter: Arthur Wiedmer
>            Assignee: Arthur Wiedmer
>            Priority: Minor
>
> h2. Current State
> The current AWS integration is mostly done through the S3Hook, which uses non standard credentials parsing on top of using boto instead of boto3 which is the current supported AWS sdk for Python.
> h2. Proposal
> an AWSHook should be provided that maps Airflow connections to the boto3 API. Operators working with s3, as well as other AWS services would then inherit from this hook but extend the functionality with service specific methods like get_key for S3, start_cluster for EMR, enqueue for SQS, send_email for SES etc...
> * AWSHook
> ** S3Hook
> ** EMRHook
> ** SQSHook
> ** SESHook
> ...
>  



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27411,54,JIRA.12969653.1463173893000.355508.1512051661415@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.12969653.1463173893000@Atlassian.JIRA,,,2017-11-30 06:21:01-08,"[jira] [Resolved] (AIRFLOW-115) Migrate and Refactor AWS
 integration to use boto3 and better structured hooks","
     [ https://issues.apache.org/jira/browse/AIRFLOW-115?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-115.
------------------------------------
    Resolution: Fixed

> Migrate and Refactor AWS integration to use boto3 and better structured hooks
> -----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-115
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-115
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: aws, boto3, hooks
>            Reporter: Arthur Wiedmer
>            Assignee: Arthur Wiedmer
>            Priority: Minor
>
> h2. Current State
> The current AWS integration is mostly done through the S3Hook, which uses non standard credentials parsing on top of using boto instead of boto3 which is the current supported AWS sdk for Python.
> h2. Proposal
> an AWSHook should be provided that maps Airflow connections to the boto3 API. Operators working with s3, as well as other AWS services would then inherit from this hook but extend the functionality with service specific methods like get_key for S3, start_cluster for EMR, enqueue for SQS, send_email for SES etc...
> * AWSHook
> ** S3Hook
> ** EMRHook
> ** SQSHook
> ** SESHook
> ...
>  



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27412,54,JIRA.13121926.1512051917000.355531.1512051960189@Atlassian.JIRA,2420,Clinton Boys (JIRA),JIRA.13121926.1512051917000@Atlassian.JIRA,,,2017-11-30 06:26:00-08,"[jira] [Created] (AIRFLOW-1871) Make UI scheduler element red only
 when active DAG runs exceeds max_active_runs","Clinton Boys created AIRFLOW-1871:
-------------------------------------

             Summary: Make UI scheduler element red only when active DAG runs exceeds max_active_runs
                 Key: AIRFLOW-1871
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1871
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Clinton Boys
            Priority: Minor


The UI should show red in the schedule label only when the DAG run number is strictly greater than the maximum. This allows for ""normal"" behaviour when max_active_runs = 1.




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27413,54,JIRA.13083090.1498646060000.355578.1512052261608@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13083090.1498646060000@Atlassian.JIRA,,,2017-11-30 06:31:01-08,"[jira] [Closed] (AIRFLOW-1355) Unable to launch jobs : DAGs not
 being executed.","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1355?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin closed AIRFLOW-1355.
-----------------------------------
    Resolution: Incomplete

> Unable to launch jobs : DAGs not being executed.
> ------------------------------------------------
>
>                 Key: AIRFLOW-1355
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1355
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG
>    Affects Versions: Airflow 1.8
>         Environment: Mac OS and Ubuntu
>            Reporter: Pavan KN
>            Priority: Critical
>
> Steps to re-produce:
> 1. Create new installation
> 2. Launch Airflow
> 3. Enable a DAG and trigger it manually
> DAG/Job won''t get executed. Will stay in Running status, but no execution starts and continues to stay at same status.
> Same issues are there with Sequential, Local and Celeri executors.
> Happening in 1.8 version. Tried on multiple Mac machines and on Ubuntu.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27414,54,JIRA.13083090.1498646060000.355574.1512052261568@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13083090.1498646060000@Atlassian.JIRA,,,2017-11-30 06:31:01-08,"[jira] [Commented] (AIRFLOW-1355) Unable to launch jobs : DAGs not
 being executed.","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1355?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16272729#comment-16272729 ] 

Bolke de Bruin commented on AIRFLOW-1355:
-----------------------------------------

This is a mix of issues and sometimes not related not Airflow at all (Docker). Will therefore close.

> Unable to launch jobs : DAGs not being executed.
> ------------------------------------------------
>
>                 Key: AIRFLOW-1355
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1355
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG
>    Affects Versions: Airflow 1.8
>         Environment: Mac OS and Ubuntu
>            Reporter: Pavan KN
>            Priority: Critical
>
> Steps to re-produce:
> 1. Create new installation
> 2. Launch Airflow
> 3. Enable a DAG and trigger it manually
> DAG/Job won''t get executed. Will stay in Running status, but no execution starts and continues to stay at same status.
> Same issues are there with Sequential, Local and Celeri executors.
> Happening in 1.8 version. Tried on multiple Mac machines and on Ubuntu.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27415,54,JIRA.13121919.1512050251000.355736.1512053880356@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13121919.1512050251000@Atlassian.JIRA,,,2017-11-30 06:58:00-08,[jira] [Commented] (AIRFLOW-1870) Enable flake8 tests,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1870?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16272763#comment-16272763 ] 

ASF subversion and git services commented on AIRFLOW-1870:
----------------------------------------------------------

Commit b9c82c0400017b24c2670392d8b850b2fb0de8eb in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=b9c82c0 ]

[AIRFLOW-1870] Enable flake8 tests

Flake8 tests now run for diffs

Closes #2829 from bolkedebruin/use_flake8


> Enable flake8 tests
> -------------------
>
>                 Key: AIRFLOW-1870
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1870
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.1
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27416,54,JIRA.13121919.1512050251000.355739.1512053880386@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13121919.1512050251000@Atlassian.JIRA,,,2017-11-30 06:58:00-08,[jira] [Resolved] (AIRFLOW-1870) Enable flake8 tests,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1870?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1870.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2829
[https://github.com/apache/incubator-airflow/pull/2829]

> Enable flake8 tests
> -------------------
>
>                 Key: AIRFLOW-1870
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1870
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.1
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27417,54,JIRA.13122016.1512071496000.358384.1512071520552@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13122016.1512071496000@Atlassian.JIRA,,,2017-11-30 11:52:00-08,"[jira] [Created] (AIRFLOW-1872) All parent handlers should be tried
 for set_context","Bolke de Bruin created AIRFLOW-1872:
---------------------------------------

             Summary: All parent handlers should be tried for set_context
                 Key: AIRFLOW-1872
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1872
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Bolke de Bruin
             Fix For: 1.9.0






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27418,54,JIRA.13122055.1512083786000.360418.1512083820077@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13122055.1512083786000@Atlassian.JIRA,,,2017-11-30 15:17:00-08,"[jira] [Created] (AIRFLOW-1873) Task operator logs appear in wrong
 numbered log file","Ash Berlin-Taylor created AIRFLOW-1873:
------------------------------------------

             Summary: Task operator logs appear in wrong numbered log file
                 Key: AIRFLOW-1873
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1873
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.9.0
            Reporter: Ash Berlin-Taylor


The logs for the running operators appear in the ""next"" task number.

For example, for the first try for a given task instance the ""collecting dag"" etc appear in 1.log, but log messages from the operator itself appear in 2.log.

1.log:

{noformat}
[2017-11-30 23:14:44,189] {cli.py:374} INFO - Running on host 4f1698e8ae61
[2017-11-30 23:14:44,254] {models.py:1173} INFO - Dependencies all met for <TaskInstance: tests.test-logging 2017-11-20 00:00:00 [queued]>
[2017-11-30 23:14:44,265] {models.py:1173} INFO - Dependencies all met for <TaskInstance: tests.test-logging 2017-11-20 00:00:00 [queued]>
[2017-11-30 23:14:44,266] {models.py:1383} INFO -
--------------------------------------------------------------------------------
Starting attempt 1 of 1
--------------------------------------------------------------------------------

[2017-11-30 23:14:44,290] {models.py:1404} INFO - Executing <Task(PythonOperator): test-logging> on 2017-11-20 00:00:00
[2017-11-30 23:14:44,291] {base_task_runner.py:115} INFO - Running: [''bash'', ''-c'', ''airflow run tests test-logging 2017-11-20T00:00:00 --job_id 4 --raw -sd /usr/local/airflow/dags/example/csv_to_parquet.py'']
[2017-11-30 23:14:50,054] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,052] {configuration.py:206} WARNING - section/key [celery/celery_ssl_active] not found in config
[2017-11-30 23:14:50,056] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,052] {default_celery.py:41} WARNING - Celery Executor will run without SSL
[2017-11-30 23:14:50,058] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,054] {__init__.py:45} INFO - Using executor CeleryExecutor
[2017-11-30 23:14:50,529] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,529] {models.py:189} INFO - Filling up the DagBag from /usr/local/airflow/dags/example/csv_to_parquet.py
[2017-11-30 23:14:50,830] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,825] {python_operator.py:90} INFO - Done. Returned value was: None
{noformat}

2.log:

{noformat}
[2017-11-30 23:14:50,749] {cli.py:374} INFO - Running on host 4f1698e8ae61
[2017-11-30 23:14:50,820] {logging_mixin.py:84} INFO - Hi from /usr/local/airflow/dags/example/csv_to_parquet.py

[2017-11-30 23:14:50,824] {csv_to_parquet.py:21} ERROR - Hello
{noformat}

Notice the timestamps - the contents of 2.log appear just before the last line of 1.log, and should be in the same log file (there is only a single run of this task instance)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)
",t
43851,54,154329845673.7121.139537548359068540.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 22:00:56-08,"[GitHub] dlamblin opened a new pull request #4242: [AIRFLOW-XXX] Correct
 typos in UPDATING.md `master`","dlamblin opened a new pull request #4242: [AIRFLOW-XXX] Correct typos in UPDATING.md `master`
URL: https://github.com/apache/incubator-airflow/pull/4242
 
 
   Make sure you have checked _all_ steps below.
   
   ### Jira
   
   - [x] My PR addresses fixing a typo in the documentation and so prepends commit with \[AIRFLOW-XXX\].
   ### Description
   
   Applies to `master`; pull request 4241 applies to `v1-10-stable.
   
   - Started with ""habe"", ""serever"" and ""certificiate"" needing to be:  
     ""have"", ""server"", and ""certificate"".
   - Ran a check, ignoring British and US accepted spellings.
   - Kept jargon. EG admin, aync, auth, backend, config, dag, s3, utils, etc.
   - Took exception to:
     - ""num of dag run"" meaning ""number of dag runs"",
     - ""upness"" is normally for quarks,
     - ""url"" being lower-case, and
     - sftp example having an excess file ending.
   - Python documentation writes ""builtin"" hyphenated, cases ""PYTHONPATH"".
   - Gave up on mixed use of ""dag"" and ""DAG"" as well as long line lengths.
   
   ### Tests
   
   - [x] My PR does not need testing for this extremely good reason: It is scoped to documentation
   
   ### Commits
   
   - [x] My commits all reference Jira issues in their subject lines, and I have squashed multiple commits if they address the same issue. In addition, my commits follow the guidelines from ""[How to write a good git commit message](http://chris.beams.io/posts/git-commit/)"":
     1. Subject is separated from body by a blank line
     1. Subject is limited to 50 characters (not including Jira issue reference)
     1. Subject does not end with a period
     1. Subject uses the imperative mood (""add"", not ""adding"")
     1. Body wraps at 72 characters
     1. Body explains ""what"" and ""why"", not ""how""
   
   ### Documentation
   
   - [x] Updates only existing documentation.
   
   ### Code Quality
   
   - [x] Passes `flake8`
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
43852,54,154329861271.8621.8388150786178703307.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 22:03:32-08,"[GitHub] dlamblin commented on issue #4241: [AIRFLOW-XXX] Correct typos in
 UPDATING.md","dlamblin commented on issue #4241: [AIRFLOW-XXX] Correct typos in UPDATING.md
URL: https://github.com/apache/incubator-airflow/pull/4241#issuecomment-441935323
 
 
   Welcome; not sure how you mange the diffs from the master and stable branch to this doc.
   See also https://github.com/apache/incubator-airflow/pull/4242

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27419,54,JIRA.13122055.1512083786000.360865.1512086700428@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13122055.1512083786000@Atlassian.JIRA,,,2017-11-30 16:05:00-08,"[jira] [Updated] (AIRFLOW-1873) Task operator logs appear in wrong
 numbered log file","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1873?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1873:
-------------------------------------
    Fix Version/s: 1.9.0

> Task operator logs appear in wrong numbered log file
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1873
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1873
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: 1.9.0
>            Reporter: Ash Berlin-Taylor
>             Fix For: 1.9.0
>
>
> The logs for the running operators appear in the ""next"" task number.
> For example, for the first try for a given task instance the ""collecting dag"" etc appear in 1.log, but log messages from the operator itself appear in 2.log.
> 1.log:
> {noformat}
> [2017-11-30 23:14:44,189] {cli.py:374} INFO - Running on host 4f1698e8ae61
> [2017-11-30 23:14:44,254] {models.py:1173} INFO - Dependencies all met for <TaskInstance: tests.test-logging 2017-11-20 00:00:00 [queued]>
> [2017-11-30 23:14:44,265] {models.py:1173} INFO - Dependencies all met for <TaskInstance: tests.test-logging 2017-11-20 00:00:00 [queued]>
> [2017-11-30 23:14:44,266] {models.py:1383} INFO -
> --------------------------------------------------------------------------------
> Starting attempt 1 of 1
> --------------------------------------------------------------------------------
> [2017-11-30 23:14:44,290] {models.py:1404} INFO - Executing <Task(PythonOperator): test-logging> on 2017-11-20 00:00:00
> [2017-11-30 23:14:44,291] {base_task_runner.py:115} INFO - Running: [''bash'', ''-c'', ''airflow run tests test-logging 2017-11-20T00:00:00 --job_id 4 --raw -sd /usr/local/airflow/dags/example/csv_to_parquet.py'']
> [2017-11-30 23:14:50,054] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,052] {configuration.py:206} WARNING - section/key [celery/celery_ssl_active] not found in config
> [2017-11-30 23:14:50,056] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,052] {default_celery.py:41} WARNING - Celery Executor will run without SSL
> [2017-11-30 23:14:50,058] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,054] {__init__.py:45} INFO - Using executor CeleryExecutor
> [2017-11-30 23:14:50,529] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,529] {models.py:189} INFO - Filling up the DagBag from /usr/local/airflow/dags/example/csv_to_parquet.py
> [2017-11-30 23:14:50,830] {base_task_runner.py:98} INFO - Subtask: [2017-11-30 23:14:50,825] {python_operator.py:90} INFO - Done. Returned value was: None
> {noformat}
> 2.log:
> {noformat}
> [2017-11-30 23:14:50,749] {cli.py:374} INFO - Running on host 4f1698e8ae61
> [2017-11-30 23:14:50,820] {logging_mixin.py:84} INFO - Hi from /usr/local/airflow/dags/example/csv_to_parquet.py
> [2017-11-30 23:14:50,824] {csv_to_parquet.py:21} ERROR - Hello
> {noformat}
> Notice the timestamps - the contents of 2.log appear just before the last line of 1.log, and should be in the same log file (there is only a single run of this task instance)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27420,54,JIRA.13116697.1510066613000.363839.1512122520765@Atlassian.JIRA,2380,Hugo Prudente (JIRA),JIRA.13116697.1510066613000@Atlassian.JIRA,,,2017-12-01 02:02:00-08,[jira] [Resolved] (AIRFLOW-1790) AWS Batch Operator Suppport,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1790?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Hugo Prudente resolved AIRFLOW-1790.
------------------------------------
    Resolution: Fixed

Thanks, I just saw that it was properly merged I must have checked on the branch by mistake!

> AWS Batch Operator Suppport
> ---------------------------
>
>                 Key: AIRFLOW-1790
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1790
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: operators
>    Affects Versions: 1.9.0
>            Reporter: Hugo Prudente
>            Assignee: Hugo Prudente
>
> Add support to AWS Batch operator



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27421,54,JIRA.13122165.1512132418000.364574.1512132420664@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13122165.1512132418000@Atlassian.JIRA,,,2017-12-01 04:47:00-08,"[jira] [Created] (AIRFLOW-1874) Support standard SQL in Check,
 ValueCheck and IntervalCheck BigQuery operators","Guillermo Rodr=C3=ADguez Cano created AIRFLOW-1874:
-------------------------------------------------

             Summary: Support standard SQL in Check, ValueCheck and Interva=
lCheck BigQuery operators
                 Key: AIRFLOW-1874
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1874
             Project: Apache Airflow
          Issue Type: Bug
          Components: contrib, gcp, operators
            Reporter: Guillermo Rodr=C3=ADguez Cano


BigQueryCheckOperator, BigQueryValueCheckOperator and BigQueryIntervalCheck=
Operator do not support disabling use of default legacy SQL in BigQuery.

This is a major blocker to support correct migration to standard SQL when q=
ueries are complicated. For example, a query that can be queried in legacy =
SQL may be blocked from any subsequent view done in standard SQL that this =
view uses as the queries are bound to either standard or legacy SQL but not=
 a mix.

These operators inherit from base ones of the same name (without the BigQue=
ry prefix) from Airflow which may make the process more complicated as the =
flag to use standard SQL should be enabled because the underlying BigQueryH=
ook has the corresponding parameter, use_legacy_sql, set to True, when runn=
ing a query. But it is not possible to pass parameters all the way to it vi=
a the aforementioned operators.

The workaround of including #standardSQL and a new line before the query do=
esn''t work either as there is mismatch. BigQuery reports the following in f=
act: ""Query text specifies use_legacy_sql:false, while API options specify:=
true""



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27422,54,JIRA.13122165.1512132418000.364611.1512133021327@Atlassian.JIRA,2195,=?utf-8?Q?Guillermo_Rodr=C3=ADguez_Cano_=28JIRA=29?=,JIRA.13122165.1512132418000@Atlassian.JIRA,,,2017-12-01 04:57:01-08,"[jira] [Updated] (AIRFLOW-1874) Support standard SQL in Check,
 ValueCheck and IntervalCheck BigQuery operators","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1874?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Guillermo Rodr=C3=ADguez Cano updated AIRFLOW-1874:
----------------------------------------------
    Description:=20
BigQueryCheckOperator, BigQueryValueCheckOperator and BigQueryIntervalCheck=
Operator do not support disabling use of default legacy SQL in BigQuery.

This is a major blocker to support correct migration to standard SQL when q=
ueries are complicated. For example, a query that can be queried in legacy =
SQL may be blocked from any subsequent view done in standard SQL that this =
view uses as the queries are bound to either standard or legacy SQL but not=
 a mix.

These operators inherit from base ones of the same name (without the BigQue=
ry prefix) from Airflow which may make the process more complicated as the =
flag to use standard SQL should be enabled because the underlying BigQueryH=
ook has the corresponding parameter, use_legacy_sql, set to True, when runn=
ing a query. But it is not possible to pass parameters all the way to it vi=
a the aforementioned operators.

The workaround of including #standardSQL and a new line before the query do=
esn''t work either as there is mismatch. BigQuery reports the following in f=
act: ""Query text specifies use_legacy_sql:false, while API options specify:=
true""
A workaround for queries on views using standard SQL is to persist the resu=
lt of the query in a temporary table, then run the check operation and ther=
eafter delete the temporary table.=20

  was:
BigQueryCheckOperator, BigQueryValueCheckOperator and BigQueryIntervalCheck=
Operator do not support disabling use of default legacy SQL in BigQuery.

This is a major blocker to support correct migration to standard SQL when q=
ueries are complicated. For example, a query that can be queried in legacy =
SQL may be blocked from any subsequent view done in standard SQL that this =
view uses as the queries are bound to either standard or legacy SQL but not=
 a mix.

These operators inherit from base ones of the same name (without the BigQue=
ry prefix) from Airflow which may make the process more complicated as the =
flag to use standard SQL should be enabled because the underlying BigQueryH=
ook has the corresponding parameter, use_legacy_sql, set to True, when runn=
ing a query. But it is not possible to pass parameters all the way to it vi=
a the aforementioned operators.

The workaround of including #standardSQL and a new line before the query do=
esn''t work either as there is mismatch. BigQuery reports the following in f=
act: ""Query text specifies use_legacy_sql:false, while API options specify:=
true""


> Support standard SQL in Check, ValueCheck and IntervalCheck BigQuery oper=
ators
> -------------------------------------------------------------------------=
-----
>
>                 Key: AIRFLOW-1874
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1874
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: contrib, gcp, operators
>            Reporter: Guillermo Rodr=C3=ADguez Cano
>
> BigQueryCheckOperator, BigQueryValueCheckOperator and BigQueryIntervalChe=
ckOperator do not support disabling use of default legacy SQL in BigQuery.
> This is a major blocker to support correct migration to standard SQL when=
 queries are complicated. For example, a query that can be queried in legac=
y SQL may be blocked from any subsequent view done in standard SQL that thi=
s view uses as the queries are bound to either standard or legacy SQL but n=
ot a mix.
> These operators inherit from base ones of the same name (without the BigQ=
uery prefix) from Airflow which may make the process more complicated as th=
e flag to use standard SQL should be enabled because the underlying BigQuer=
yHook has the corresponding parameter, use_legacy_sql, set to True, when ru=
nning a query. But it is not possible to pass parameters all the way to it =
via the aforementioned operators.
> The workaround of including #standardSQL and a new line before the query =
doesn''t work either as there is mismatch. BigQuery reports the following in=
 fact: ""Query text specifies use_legacy_sql:false, while API options specif=
y:true""
> A workaround for queries on views using standard SQL is to persist the re=
sult of the query in a temporary table, then run the check operation and th=
ereafter delete the temporary table.=20



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27423,54,JIRA.13091078.1501467019000.364630.1512133620155@Atlassian.JIRA,2208,Junyoung Park (JIRA),JIRA.13091078.1501467019000@Atlassian.JIRA,,,2017-12-01 05:07:00-08,"[jira] [Commented] (AIRFLOW-1475)  Change EmrBaseSensor
 NON_TERMINAL_STATES and FAILED_STATE to list","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1475?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16274358#comment-16274358 ] 

Junyoung Park commented on AIRFLOW-1475:
----------------------------------------

[~victorddiniz] I have already worked on it. May I take this issue?

>  Change EmrBaseSensor NON_TERMINAL_STATES and FAILED_STATE to list
> ------------------------------------------------------------------
>
>                 Key: AIRFLOW-1475
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1475
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: aws
>    Affects Versions: Airflow 1.8
>            Reporter: Victor Duarte Diniz Monteiro
>            Assignee: Victor Duarte Diniz Monteiro
>            Priority: Trivial
>             Fix For: Airflow 1.8
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27424,54,JIRA.13088824.1500569821000.364634.1512133740234@Atlassian.JIRA,2208,Junyoung Park (JIRA),JIRA.13088824.1500569821000@Atlassian.JIRA,,,2017-12-01 05:09:00-08,"[jira] [Assigned] (AIRFLOW-1436) EmrJobFlowSensor consideres
 Cancelled step as Successful","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1436?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Junyoung Park reassigned AIRFLOW-1436:
--------------------------------------

    Assignee: Junyoung Park

> EmrJobFlowSensor consideres Cancelled step as Successful
> --------------------------------------------------------
>
>                 Key: AIRFLOW-1436
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1436
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: aws
>            Reporter: Martin Zlocha
>            Assignee: Junyoung Park
>            Priority: Trivial
>
> When steps are added to an EMR cluster and then cancelled, the EmrJobFlowSensor considers them as successful but they should be failed because they didn''t complete.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27425,54,JIRA.13122181.1512139582000.365180.1512139620201@Atlassian.JIRA,1699,Alan Cruickshank (JIRA),JIRA.13122181.1512139582000@Atlassian.JIRA,,,2017-12-01 06:47:00-08,"[jira] [Created] (AIRFLOW-1875) Front page UI breaks on second page
 or past 25 entries","Alan Cruickshank created AIRFLOW-1875:
-----------------------------------------

             Summary: Front page UI breaks on second page or past 25 entries
                 Key: AIRFLOW-1875
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1875
             Project: Apache Airflow
          Issue Type: Bug
          Components: ui
    Affects Versions: 1.8.2
            Reporter: Alan Cruickshank
            Priority: Minor
         Attachments: Capture-Normal1.PNG, Capture-SecondPage.PNG, Capture-ViewMore.PNG

On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.

Normal styling (top of first page):
!Capture-Normal1.PNG|thumbnail!

Broken styling (top of second page):
!Capture-SecondPage.PNG|thumbnail!

Also broken styling (this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
!Capture-ViewMore.PNG|thumbnail!

Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27426,54,JIRA.13122181.1512139582000.365182.1512139680126@Atlassian.JIRA,1699,Alan Cruickshank (JIRA),JIRA.13122181.1512139582000@Atlassian.JIRA,,,2017-12-01 06:48:00-08,"[jira] [Updated] (AIRFLOW-1875) Front page UI breaks on second page
 or past 25 entries","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1875?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alan Cruickshank updated AIRFLOW-1875:
--------------------------------------
    Description: 
On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.

Normal styling (top of first page):
!Capture-SecondPage.PNG!

Broken styling (top of second page):
!Capture-SecondPage.PNG!

Also broken styling (this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
!Capture-ViewMore.PNG!

Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.




  was:
On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.

Normal styling (top of first page):
!Capture-Normal1.PNG|thumbnail!

Broken styling (top of second page):
!Capture-SecondPage.PNG|thumbnail!

Also broken styling (this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
!Capture-ViewMore.PNG|thumbnail!

Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.





> Front page UI breaks on second page or past 25 entries
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1875
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1875
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.2
>            Reporter: Alan Cruickshank
>            Priority: Minor
>              Labels: ui
>         Attachments: Capture-Normal1.PNG, Capture-SecondPage.PNG, Capture-ViewMore.PNG
>
>
> On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.
> Normal styling (top of first page):
> !Capture-SecondPage.PNG!
> Broken styling (top of second page):
> !Capture-SecondPage.PNG!
> Also broken styling (this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
> !Capture-ViewMore.PNG!
> Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27427,54,JIRA.13122181.1512139582000.365186.1512139740095@Atlassian.JIRA,1699,Alan Cruickshank (JIRA),JIRA.13122181.1512139582000@Atlassian.JIRA,,,2017-12-01 06:49:00-08,"[jira] [Updated] (AIRFLOW-1875) Front page UI breaks on second page
 or past 25 entries","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1875?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alan Cruickshank updated AIRFLOW-1875:
--------------------------------------
    Description: 
On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.

The on/off switch just shows as a checkbox rather than the normal styling and the traffic light icons don''t show. Otherwise the UI is normal.

*Normal styling (top of first page):*
!Capture-SecondPage.PNG!

*Broken styling (top of second page):*
!Capture-SecondPage.PNG!

*Also broken styling *(this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
!Capture-ViewMore.PNG!

Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.




  was:
On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.

Normal styling (top of first page):
!Capture-SecondPage.PNG!

Broken styling (top of second page):
!Capture-SecondPage.PNG!

Also broken styling (this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
!Capture-ViewMore.PNG!

Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.





> Front page UI breaks on second page or past 25 entries
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1875
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1875
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.2
>            Reporter: Alan Cruickshank
>            Priority: Minor
>              Labels: ui
>         Attachments: Capture-Normal1.PNG, Capture-SecondPage.PNG, Capture-ViewMore.PNG
>
>
> On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.
> The on/off switch just shows as a checkbox rather than the normal styling and the traffic light icons don''t show. Otherwise the UI is normal.
> *Normal styling (top of first page):*
> !Capture-SecondPage.PNG!
> *Broken styling (top of second page):*
> !Capture-SecondPage.PNG!
> *Also broken styling *(this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
> !Capture-ViewMore.PNG!
> Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27428,54,JIRA.13122181.1512139582000.365187.1512139740117@Atlassian.JIRA,1699,Alan Cruickshank (JIRA),JIRA.13122181.1512139582000@Atlassian.JIRA,,,2017-12-01 06:49:00-08,"[jira] [Updated] (AIRFLOW-1875) Front page UI breaks on second page
 or past 25 entries","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1875?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Alan Cruickshank updated AIRFLOW-1875:
--------------------------------------
    Description: 
On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.

The on/off switch just shows as a checkbox rather than the normal styling and the traffic light icons don''t show. Otherwise the UI is normal.

*Normal styling (top of first page):*
!Capture-SecondPage.PNG!

*Broken styling (top of second page):*
!Capture-SecondPage.PNG!

*Also broken styling* (this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
!Capture-ViewMore.PNG!

Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.




  was:
On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.

The on/off switch just shows as a checkbox rather than the normal styling and the traffic light icons don''t show. Otherwise the UI is normal.

*Normal styling (top of first page):*
!Capture-SecondPage.PNG!

*Broken styling (top of second page):*
!Capture-SecondPage.PNG!

*Also broken styling *(this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
!Capture-ViewMore.PNG!

Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.





> Front page UI breaks on second page or past 25 entries
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1875
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1875
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: ui
>    Affects Versions: 1.8.2
>            Reporter: Alan Cruickshank
>            Priority: Minor
>              Labels: ui
>         Attachments: Capture-Normal1.PNG, Capture-SecondPage.PNG, Capture-ViewMore.PNG
>
>
> On the main DAGs frontpage (`/admin/`), the normal styling for DAGs fails past the first page, or if you select to view more than 25, then for those further DAGs.
> The on/off switch just shows as a checkbox rather than the normal styling and the traffic light icons don''t show. Otherwise the UI is normal.
> *Normal styling (top of first page):*
> !Capture-SecondPage.PNG!
> *Broken styling (top of second page):*
> !Capture-SecondPage.PNG!
> *Also broken styling* (this time first page, showing the same two DAGs as before, but now with ""show 50 entries"" rather than the default ""show 25 entries"":
> !Capture-ViewMore.PNG!
> Happy to assist in solving (submitting PR etc...) but I don''t quite know where to start and I''m hoping based on the pattern of failure that someone knows exactly what might be causing this.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
43853,54,154329862864.8812.10116179824324779328.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 22:03:48-08,"[GitHub] dlamblin commented on issue #4242: [AIRFLOW-XXX] Correct typos in
 UPDATING.md `master`","dlamblin commented on issue #4242: [AIRFLOW-XXX] Correct typos in UPDATING.md `master`
URL: https://github.com/apache/incubator-airflow/pull/4242#issuecomment-441935438
 
 
   Not sure how you mange the diffs from the master and stable branch to this doc.
   See also https://github.com/apache/incubator-airflow/pull/4241

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27429,54,JIRA.13122258.1512158366000.367628.1512158400864@Atlassian.JIRA,2305,William Pursell (JIRA),JIRA.13122258.1512158366000@Atlassian.JIRA,,,2017-12-01 12:00:00-08,"[jira] [Created] (AIRFLOW-1876) Subtask logs are not easily
 distinguised","William Pursell created AIRFLOW-1876:
----------------------------------------

             Summary: Subtask logs are not easily distinguised
                 Key: AIRFLOW-1876
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1876
             Project: Apache Airflow
          Issue Type: Improvement
          Components: core
            Reporter: William Pursell
            Assignee: William Pursell
            Priority: Minor


Currently, when the scheduler is outputting all subtask logs to the same stream, it is not easy to distinguish which logs come from which task.  It would be nice if there were some convenient way to filter the logs from a given task.  For example, putting the task id after the word ''Subtask''

For example:

diff --git a/airflow/task_runner/base_task_runner.py b/airflow/task_runner/base_task_runner.py                                 
index bc0edcf3..e40f6ea9 100644                                
--- a/airflow/task_runner/base_task_runner.py                  
+++ b/airflow/task_runner/base_task_runner.py                  
@@ -95,7 +95,11 @@ class BaseTaskRunner(LoggingMixin):         
                 line = line.decode(''utf-8'')                   
             if len(line) == 0:                                
                 break                                         
-            self.log.info(u''Subtask %%%%s: %%%%s'', self._task_instance, line.rstrip(''\n''))                                          
+            self.log.info(                                    
+                u''Subtask %%%%d: %%%%s'',                            
+                self._task_instance.job_id,                   
+                line.rstrip(''\n'')                             
+            )                                                 
                                                               
     def run_command(self, run_with, join_args=False):         
         """""" 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27430,54,JIRA.13122297.1512168496000.369127.1512168540299@Atlassian.JIRA,2238,Dylan Rajguru (JIRA),JIRA.13122297.1512168496000@Atlassian.JIRA,,,2017-12-01 14:49:00-08,[jira] [Created] (AIRFLOW-1877) airflow initdb fails on SQLServer,"Dylan Rajguru created AIRFLOW-1877:
--------------------------------------

             Summary: airflow initdb fails on SQLServer
                 Key: AIRFLOW-1877
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1877
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Dylan Rajguru


file ""/opt/incubator-airflow/airflow/migrations/versions/0e2a74e0fc9f_add_time_zone_awareness.py"", line 85, in upgrade
    op.alter_column(table_name=''chart'', column_name=''last_modified'', type_=sa.TIMESTAMP(timezone=True))

INFO  [alembic.runtime.migration] Running upgrade d2ae31099d61 -> 0e2a74e0fc9f, Add time zone awareness
Traceback (most recent call last):
  File ""pymssql.pyx"", line 447, in pymssql.Cursor.execute (pymssql.c:7119)
  File ""_mssql.pyx"", line 1011, in _mssql.MSSQLConnection.execute_query (_mssql.c:11586)
  File ""_mssql.pyx"", line 1042, in _mssql.MSSQLConnection.execute_query (_mssql.c:11466)
  File ""_mssql.pyx"", line 1175, in _mssql.MSSQLConnection.format_and_run_query (_mssql.c:12746)
  File ""_mssql.pyx"", line 1586, in _mssql.check_cancel_and_raise (_mssql.c:16880)
  File ""_mssql.pyx"", line 1630, in _mssql.maybe_raise_MSSQLDatabaseException (_mssql.c:17524)
_mssql.MSSQLDatabaseException: (4927, b""Cannot alter column ''last_modified'' to be data type timestamp.DB-Lib error message 20018, severity 16:\nGeneral SQL Server error: Check messages from the SQL Server\n"")



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27431,54,JIRA.13121253.1511865347000.369550.1512172320400@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13121253.1511865347000@Atlassian.JIRA,,,2017-12-01 15:52:00-08,"[jira] [Commented] (AIRFLOW-1855) Add an Operator to copy files
 with a specific delimiter in a directory from one GCS bucket to another","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1855?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275208#comment-16275208 ] 

ASF subversion and git services commented on AIRFLOW-1855:
----------------------------------------------------------

Commit 3e321790d537696b8fd1a97dcac2ab7c469fecea in incubator-airflow''s branch refs/heads/master from [~kaxilnaik]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3e32179 ]

[AIRFLOW-1855][AIRFLOW-1866] Add GCS Copy Operator to copy multiple files

Closes #2819 from kaxil/master


> Add an Operator to copy files with a specific delimiter in a directory from one GCS bucket to another
> -----------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1855
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1855
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>    Affects Versions: 1.10.0
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>            Priority: Minor
>             Fix For: 1.10.0
>
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> Use case: Copy all the CSV/JSON files from a particular directory in a Bucket to another bucket and in a specific directory (or the same).
> Proposed Approach:
> - Add ''delimiter'' argument in GCP hook to filter files with a particular delimiter.
> - Get the list of files to copy and filter it with delimiter using ''list'' method in GCP hook
> - Use loop and ''copy'' method in GCP hook. 
> Note: Under the hood GCS has no directories. Files are just objects.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27432,54,JIRA.13121639.1511960157000.369552.1512172380473@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13121639.1511960157000@Atlassian.JIRA,,,2017-12-01 15:53:00-08,"[jira] [Commented] (AIRFLOW-1866) Fix missing parameters in
 docstring for copy function in gcs_hook","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1866?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275209#comment-16275209 ] 

ASF subversion and git services commented on AIRFLOW-1866:
----------------------------------------------------------

Commit 3e321790d537696b8fd1a97dcac2ab7c469fecea in incubator-airflow''s branch refs/heads/master from [~kaxilnaik]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3e32179 ]

[AIRFLOW-1855][AIRFLOW-1866] Add GCS Copy Operator to copy multiple files

Closes #2819 from kaxil/master


> Fix missing parameters in docstring for copy function in gcs_hook
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1866
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1866
>             Project: Apache Airflow
>          Issue Type: Task
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>            Priority: Minor
>   Original Estimate: 1h
>  Remaining Estimate: 1h
>
> The docstrings for copy method in gcs_hook.py is outdated. PyCharm warns that (source_bucket, source_object) have missing docstrings.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27433,54,JIRA.13121253.1511865347000.369626.1512172943600@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13121253.1511865347000@Atlassian.JIRA,,,2017-12-01 16:02:23-08,"[jira] [Resolved] (AIRFLOW-1855) Add an Operator to copy files with
 a specific delimiter in a directory from one GCS bucket to another","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1855?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1855.
--------------------------------------
    Resolution: Fixed

> Add an Operator to copy files with a specific delimiter in a directory from one GCS bucket to another
> -----------------------------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1855
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1855
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>    Affects Versions: 1.10.0
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>            Priority: Minor
>             Fix For: 1.10.0
>
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> Use case: Copy all the CSV/JSON files from a particular directory in a Bucket to another bucket and in a specific directory (or the same).
> Proposed Approach:
> - Add ''delimiter'' argument in GCP hook to filter files with a particular delimiter.
> - Get the list of files to copy and filter it with delimiter using ''list'' method in GCP hook
> - Use loop and ''copy'' method in GCP hook. 
> Note: Under the hood GCS has no directories. Files are just objects.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27434,54,JIRA.13121639.1511960157000.369629.1512172980148@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13121639.1511960157000@Atlassian.JIRA,,,2017-12-01 16:03:00-08,"[jira] [Resolved] (AIRFLOW-1866) Fix missing parameters in
 docstring for copy function in gcs_hook","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1866?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1866.
--------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> Fix missing parameters in docstring for copy function in gcs_hook
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1866
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1866
>             Project: Apache Airflow
>          Issue Type: Task
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>            Priority: Minor
>             Fix For: 1.10.0
>
>   Original Estimate: 1h
>  Remaining Estimate: 1h
>
> The docstrings for copy method in gcs_hook.py is outdated. PyCharm warns that (source_bucket, source_object) have missing docstrings.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27435,54,JIRA.13122338.1512202694000.370982.1512202740048@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13122338.1512202694000@Atlassian.JIRA,,,2017-12-02 00:19:00-08,"[jira] [Created] (AIRFLOW-1878) stdout/stderr redirection does not
 work previously initialized StreamHandlers","Bolke de Bruin created AIRFLOW-1878:
---------------------------------------

             Summary: stdout/stderr redirection does not work previously initialized StreamHandlers
                 Key: AIRFLOW-1878
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1878
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Bolke de Bruin






--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27436,54,JIRA.13122016.1512071496000.371059.1512205020268@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122016.1512071496000@Atlassian.JIRA,,,2017-12-02 00:57:00-08,"[jira] [Commented] (AIRFLOW-1872) All parent handlers should be
 tried for set_context","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1872?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275486#comment-16275486 ] 

ASF subversion and git services commented on AIRFLOW-1872:
----------------------------------------------------------

Commit f18e2550543e455c9701af0995bc393ee6a97b47 in incubator-airflow''s branch refs/heads/v1-9-stable from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=f18e255 ]

[AIRFLOW-1872] Set context for all handlers including parents

Previously setting the context was not propagated
to the parent
loggers. Unfortnately, in case of a non explicitly
defined logger
the returned logger is shallow, ie. it does not
have handlers
defined. So to set the context it is required to
walk the tree.

Closes #2831 from bolkedebruin/fix_logging

(cherry picked from commit 406d738b1cf657b5ee6163bc26ab6fdea891576d)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> All parent handlers should be tried for set_context
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1872
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1872
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27437,54,JIRA.13122016.1512071496000.371056.1512205020249@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122016.1512071496000@Atlassian.JIRA,,,2017-12-02 00:57:00-08,"[jira] [Commented] (AIRFLOW-1872) All parent handlers should be
 tried for set_context","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1872?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275485#comment-16275485 ] 

ASF subversion and git services commented on AIRFLOW-1872:
----------------------------------------------------------

Commit 08bada5224a74a1e82f8791f61787539a1f45562 in incubator-airflow''s branch refs/heads/v1-9-test from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=08bada5 ]

[AIRFLOW-1872] Set context for all handlers including parents

Previously setting the context was not propagated
to the parent
loggers. Unfortnately, in case of a non explicitly
defined logger
the returned logger is shallow, ie. it does not
have handlers
defined. So to set the context it is required to
walk the tree.

Closes #2831 from bolkedebruin/fix_logging

(cherry picked from commit 406d738b1cf657b5ee6163bc26ab6fdea891576d)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> All parent handlers should be tried for set_context
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1872
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1872
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27438,54,JIRA.13122016.1512071496000.371055.1512205020235@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122016.1512071496000@Atlassian.JIRA,,,2017-12-02 00:57:00-08,"[jira] [Commented] (AIRFLOW-1872) All parent handlers should be
 tried for set_context","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1872?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275484#comment-16275484 ] 

ASF subversion and git services commented on AIRFLOW-1872:
----------------------------------------------------------

Commit 406d738b1cf657b5ee6163bc26ab6fdea891576d in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=406d738 ]

[AIRFLOW-1872] Set context for all handlers including parents

Previously setting the context was not propagated
to the parent
loggers. Unfortnately, in case of a non explicitly
defined logger
the returned logger is shallow, ie. it does not
have handlers
defined. So to set the context it is required to
walk the tree.

Closes #2831 from bolkedebruin/fix_logging


> All parent handlers should be tried for set_context
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1872
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1872
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27439,54,JIRA.13122016.1512071496000.371066.1512205080339@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13122016.1512071496000@Atlassian.JIRA,,,2017-12-02 00:58:00-08,"[jira] [Resolved] (AIRFLOW-1872) All parent handlers should be
 tried for set_context","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1872?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1872.
-------------------------------------
    Resolution: Fixed

Issue resolved by pull request #2831
[https://github.com/apache/incubator-airflow/pull/2831]

> All parent handlers should be tried for set_context
> ---------------------------------------------------
>
>                 Key: AIRFLOW-1872
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1872
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.0
>
>




--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27440,54,JIRA.13122343.1512206525000.371081.1512206580026@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13122343.1512206525000@Atlassian.JIRA,,,2017-12-02 01:23:00-08,[jira] [Created] (AIRFLOW-1879) Use TaskInstance log in cli.run,"Bolke de Bruin created AIRFLOW-1879:
---------------------------------------

             Summary: Use TaskInstance log in cli.run
                 Key: AIRFLOW-1879
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1879
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Bolke de Bruin


There is no reason the initalize the log in cli.py for airflow.task. It is fine to handle this by the taskinstance itself



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27441,54,JIRA.13122351.1512214738000.371268.1512214740021@Atlassian.JIRA,2040,Ash Berlin-Taylor (JIRA),JIRA.13122351.1512214738000@Atlassian.JIRA,,,2017-12-02 03:39:00-08,"[jira] [Created] (AIRFLOW-1880) TaskInstance.log_filepath property
 doesn''t account for custom format","Ash Berlin-Taylor created AIRFLOW-1880:
------------------------------------------

             Summary: TaskInstance.log_filepath property doesn''t account for custom format
                 Key: AIRFLOW-1880
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1880
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: 1.9.0
            Reporter: Ash Berlin-Taylor
            Priority: Trivial
             Fix For: 1.9.1


The ""log_filepath"" property on a TaskInstance doesn''t output the right path after [AIRFLOW-1582] which changed the default and let the format be customized.

This is a minor bug that doesn''t affect very much - it''s displayed in the admin UI, and is included in the failure email. Beyond that it isn''t used anywhere by Airflow.

Also I wonder if the log_filepath should be an attribute of the Job model, so that if the format is changed in the future we can still find the right historic log file?





--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27442,54,JIRA.13122356.1512218725000.371423.1512218760022@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13122356.1512218725000@Atlassian.JIRA,,,2017-12-02 04:46:00-08,[jira] [Created] (AIRFLOW-1881) Don''t double log operator output,"Bolke de Bruin created AIRFLOW-1881:
---------------------------------------

             Summary: Don''t double log operator output
                 Key: AIRFLOW-1881
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1881
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Bolke de Bruin


Operators log to stdout by default instead of the `airflow.task` parent, this results in double logging



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27443,54,JIRA.13099337.1504306043000.371721.1512228240911@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13099337.1504306043000@Atlassian.JIRA,,,2017-12-02 07:24:00-08,"[jira] [Commented] (AIRFLOW-1559) MySQL warnings about aborted
 connections, missing engine disposal","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1559?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275601#comment-16275601 ] 

ASF subversion and git services commented on AIRFLOW-1559:
----------------------------------------------------------

Commit 3bde95e599b56af9bb98caf66ac8248d6b7b9094 in incubator-airflow''s branch refs/heads/master from [~StephanErb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=3bde95e ]

[AIRFLOW-1559] Make database pooling optional

In situations where a database is heavily loaded with connections it
can be beneficial for operators to (temporarily) reduce the connection
footprint of Airflow on the database. This is particularly important
when Airflow or self-made extensions do not dispose the connection
pool when terminating.

Disabling the connection pool comes with a slowdown but that may be
acceptable in many deployment scenarios.


> MySQL warnings about aborted connections, missing engine disposal
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1559
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1559
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: db
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>             Fix For: 1.9.1
>
>
> We''re seeing a flood of warnings about aborted connections in our MySQL logs. 
> {code}
> Aborted connection 56720 to db: ''airflow'' user: ''foo'' host: ''x.x.x.x'' (Got an error reading communication packets)
> {code}
> It appears this is because we''re not performing [engine disposal|http://docs.sqlalchemy.org/en/latest/core/connections.html#engine-disposal]. The most common source of this warning is from the scheduler, when it kicks off new processes to process the DAG files. Calling dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L403 greatly reduced these messages. However, the worker is still causing some of these, I assume from when we spin up processes to run tasks. We do call dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L1394-L1396, but I think it''s a bit early. Not sure if there''s a place we can put this cleanup to ensure it''s done everywhere.
> Quick script to reproduce this warning message:
> {code}
> from airflow import settings
> from airflow.models import Connection
> session = settings.Session()
> session.query(Connection).count()
> session.close()
> # not calling settings.engine.dispose()
> {code}
> Reproduced with Airflow 1.8.1, MySQL 5.7, and SQLAlchemy 1.1.13. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27444,54,JIRA.13099337.1504306043000.371724.1512228241156@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13099337.1504306043000@Atlassian.JIRA,,,2017-12-02 07:24:01-08,"[jira] [Commented] (AIRFLOW-1559) MySQL warnings about aborted
 connections, missing engine disposal","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1559?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275602#comment-16275602 ] 

ASF subversion and git services commented on AIRFLOW-1559:
----------------------------------------------------------

Commit 5a303ebbc572cee7c9c30be84ebf625357360d4b in incubator-airflow''s branch refs/heads/master from [~StephanErb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=5a303eb ]

[AIRFLOW-1559] Close file handles in subprocesses

All file descriptors except 0, 1 and 2 will be closed before the
child process is executed. This is the default on Python 3.2 and
above. This patch ensures consistent behaviour for older Python
versions.

Resources will be released once the main thread disposes
them, independent of the longevity of its subprocesses.

Background information:

* https://www.python.org/dev/peps/pep-0446/
* https://bugs.python.org/issue7213


> MySQL warnings about aborted connections, missing engine disposal
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1559
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1559
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: db
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>             Fix For: 1.9.1
>
>
> We''re seeing a flood of warnings about aborted connections in our MySQL logs. 
> {code}
> Aborted connection 56720 to db: ''airflow'' user: ''foo'' host: ''x.x.x.x'' (Got an error reading communication packets)
> {code}
> It appears this is because we''re not performing [engine disposal|http://docs.sqlalchemy.org/en/latest/core/connections.html#engine-disposal]. The most common source of this warning is from the scheduler, when it kicks off new processes to process the DAG files. Calling dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L403 greatly reduced these messages. However, the worker is still causing some of these, I assume from when we spin up processes to run tasks. We do call dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L1394-L1396, but I think it''s a bit early. Not sure if there''s a place we can put this cleanup to ensure it''s done everywhere.
> Quick script to reproduce this warning message:
> {code}
> from airflow import settings
> from airflow.models import Connection
> session = settings.Session()
> session.query(Connection).count()
> session.close()
> # not calling settings.engine.dispose()
> {code}
> Reproduced with Airflow 1.8.1, MySQL 5.7, and SQLAlchemy 1.1.13. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27445,54,JIRA.13106155.1506722912000.371729.1512228241343@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13106155.1506722912000@Atlassian.JIRA,,,2017-12-02 07:24:01-08,"[jira] [Commented] (AIRFLOW-1665) Airflow webserver/scheduler don''t
 handle database disconnects (mysql)","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1665?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275604#comment-16275604 ] 

ASF subversion and git services commented on AIRFLOW-1665:
----------------------------------------------------------

Commit 94deac34eca869a0accbc6affe7640b09dab1530 in incubator-airflow''s branch refs/heads/master from [~StephanErb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=94deac3 ]

[AIRFLOW-1665] Reconnect on database errors

This change enables the scheduler to recover from temporary database
errors and downtimes. The same holds true for the webserver if run
without its regular worker refresh.

The reconnect logic is based on a truncated exponential binary backoff
to ensure reconnect attempts don''t overload the database.

Included changes:

* Switch to recommended pessimistic disconnect handling for engines
  http://docs.sqlalchemy.org/en/rel_1_1/core/pooling.html#disconnect-handling-pessimistic
* Remove legacy pool-based disconnect handling.
* Ensure event handlers are registered for each newly created engine.
  Engines are re-initialized in child processes so this is crucial for
  correctness.

This commit is based on a contribution by @vklogin
https://github.com/apache/incubator-airflow/pull/2744


> Airflow webserver/scheduler don''t handle database disconnects (mysql)
> ---------------------------------------------------------------------
>
>                 Key: AIRFLOW-1665
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1665
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: Vasanth Kumar
>            Assignee: Vasanth Kumar
>              Labels: database, reconnect
>             Fix For: 1.9.1
>
>
> Airflow webserver & scheduler don''t handle database disconnects.  The process appear to error out and either exit or are left in an off state.  This was observed when using mysql.
> I don''t see any database reconnect configuration or code.
> Stack tace for scheduler:
>   File ""...../MySQLdb/connections.py"", line 204, in __init__
>     super(Connection, self).__init__(*args, **kwargs2)
> sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (2002, ""Can''t connect to local MySQL server through socket ''/tmp/mysql.sock'' (2)"")



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27446,54,JIRA.13099337.1504306043000.371725.1512228241240@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13099337.1504306043000@Atlassian.JIRA,,,2017-12-02 07:24:01-08,"[jira] [Commented] (AIRFLOW-1559) MySQL warnings about aborted
 connections, missing engine disposal","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1559?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16275603#comment-16275603 ] 

ASF subversion and git services commented on AIRFLOW-1559:
----------------------------------------------------------

Commit 6bf1a6edaf13d3e255c47488f2747a2b8ebeff6c in incubator-airflow''s branch refs/heads/master from [~StephanErb]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=6bf1a6e ]

[AIRFLOW-1559] Dispose SQLAlchemy engines on exit

When a forked process or the entire interpreter terminates, we have
to close all pooled database connections. The database can run out
of connections otherwise. At a minimum, it will print errors in its
log file.

By using an atexit handler we ensure that connections are closed
for each interpreter and Gunicorn worker termination. Only usages
of multiprocessing.Process require special handling as those
terminate via os._exit() which does not run finalizers.

This commit is based on a contribution by @dhuang
https://github.com/apache/incubator-airflow/pull/2767


> MySQL warnings about aborted connections, missing engine disposal
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1559
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1559
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: db
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>             Fix For: 1.9.1
>
>
> We''re seeing a flood of warnings about aborted connections in our MySQL logs. 
> {code}
> Aborted connection 56720 to db: ''airflow'' user: ''foo'' host: ''x.x.x.x'' (Got an error reading communication packets)
> {code}
> It appears this is because we''re not performing [engine disposal|http://docs.sqlalchemy.org/en/latest/core/connections.html#engine-disposal]. The most common source of this warning is from the scheduler, when it kicks off new processes to process the DAG files. Calling dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L403 greatly reduced these messages. However, the worker is still causing some of these, I assume from when we spin up processes to run tasks. We do call dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L1394-L1396, but I think it''s a bit early. Not sure if there''s a place we can put this cleanup to ensure it''s done everywhere.
> Quick script to reproduce this warning message:
> {code}
> from airflow import settings
> from airflow.models import Connection
> session = settings.Session()
> session.query(Connection).count()
> session.close()
> # not calling settings.engine.dispose()
> {code}
> Reproduced with Airflow 1.8.1, MySQL 5.7, and SQLAlchemy 1.1.13. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27447,54,JIRA.13099337.1504306043000.371734.1512228241474@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13099337.1504306043000@Atlassian.JIRA,,,2017-12-02 07:24:01-08,"[jira] [Resolved] (AIRFLOW-1559) MySQL warnings about aborted
 connections, missing engine disposal","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1559?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1559.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2822
[https://github.com/apache/incubator-airflow/pull/2822]

> MySQL warnings about aborted connections, missing engine disposal
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1559
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1559
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: db
>            Reporter: Daniel Huang
>            Assignee: Daniel Huang
>            Priority: Minor
>             Fix For: 1.9.1
>
>
> We''re seeing a flood of warnings about aborted connections in our MySQL logs. 
> {code}
> Aborted connection 56720 to db: ''airflow'' user: ''foo'' host: ''x.x.x.x'' (Got an error reading communication packets)
> {code}
> It appears this is because we''re not performing [engine disposal|http://docs.sqlalchemy.org/en/latest/core/connections.html#engine-disposal]. The most common source of this warning is from the scheduler, when it kicks off new processes to process the DAG files. Calling dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/jobs.py#L403 greatly reduced these messages. However, the worker is still causing some of these, I assume from when we spin up processes to run tasks. We do call dispose in https://github.com/apache/incubator-airflow/blob/master/airflow/models.py#L1394-L1396, but I think it''s a bit early. Not sure if there''s a place we can put this cleanup to ensure it''s done everywhere.
> Quick script to reproduce this warning message:
> {code}
> from airflow import settings
> from airflow.models import Connection
> session = settings.Session()
> session.query(Connection).count()
> session.close()
> # not calling settings.engine.dispose()
> {code}
> Reproduced with Airflow 1.8.1, MySQL 5.7, and SQLAlchemy 1.1.13. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27448,54,JIRA.13106155.1506722912000.371737.1512228241521@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13106155.1506722912000@Atlassian.JIRA,,,2017-12-02 07:24:01-08,"[jira] [Resolved] (AIRFLOW-1665) Airflow webserver/scheduler don''t
 handle database disconnects (mysql)","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1665?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1665.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2822
[https://github.com/apache/incubator-airflow/pull/2822]

> Airflow webserver/scheduler don''t handle database disconnects (mysql)
> ---------------------------------------------------------------------
>
>                 Key: AIRFLOW-1665
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1665
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: Vasanth Kumar
>            Assignee: Vasanth Kumar
>              Labels: database, reconnect
>             Fix For: 1.9.1
>
>
> Airflow webserver & scheduler don''t handle database disconnects.  The process appear to error out and either exit or are left in an off state.  This was observed when using mysql.
> I don''t see any database reconnect configuration or code.
> Stack tace for scheduler:
>   File ""...../MySQLdb/connections.py"", line 204, in __init__
>     super(Connection, self).__init__(*args, **kwargs2)
> sqlalchemy.exc.OperationalError: (_mysql_exceptions.OperationalError) (2002, ""Can''t connect to local MySQL server through socket ''/tmp/mysql.sock'' (2)"")



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27449,54,JIRA.12981387.1466522366000.373638.1512304860336@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.12981387.1466522366000@Atlassian.JIRA,,,2017-12-03 04:41:00-08,[jira] [Closed] (AIRFLOW-266) Intermittent test failures,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-266?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin closed AIRFLOW-266.
----------------------------------
    Resolution: Cannot Reproduce

We dont see this anymore on master

> Intermittent test failures
> --------------------------
>
>                 Key: AIRFLOW-266
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-266
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: tests
>    Affects Versions: Airflow 1.7.1.2
>            Reporter: Chris Riccomini
>
> I''m seeing intermittent failures of the ""Test that the scheduler handles queued tasks correctly"" test:
> {noformat}
> ======================================================================
> FAIL: Test that the scheduler handles queued tasks correctly
> ----------------------------------------------------------------------
> Traceback (most recent call last):
>   File ""/home/travis/build/apache/incubator-airflow/tests/jobs.py"", line 309, in test_scheduler_pooled_tasks
>     self.assertEqual(ti.state, State.QUEUED)
> nose.proxy.AssertionError: None != ''queued''
> {noformat}
> Here''s one example: https://travis-ci.org/apache/incubator-airflow/jobs/139142905
> Note: other builds in the same commit pass: https://travis-ci.org/apache/incubator-airflow/builds/139142900



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27450,54,JIRA.12997770.1471395849000.373646.1512305220225@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.12997770.1471395849000@Atlassian.JIRA,,,2017-12-03 04:47:00-08,"[jira] [Closed] (AIRFLOW-435) Multiprocessing Scheduler is very
 slow","
     [ https://issues.apache.org/jira/browse/AIRFLOW-435?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin closed AIRFLOW-435.
----------------------------------
    Resolution: Invalid

Configuration issue.

> Multiprocessing Scheduler is very slow
> --------------------------------------
>
>                 Key: AIRFLOW-435
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-435
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: George Leslie-Waksman
>            Assignee: Paul Yang
>
> The PR https://github.com/apache/incubator-airflow/pull/1636 has dramatically slowed down the scheduler. Running code prior to 1636 will result in rapid scheduling of many tasks. After 1636, tasks can wait in a null state for minutes without being scheduled.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27451,54,JIRA.13122459.1512329902000.374326.1512329940522@Atlassian.JIRA,2421,Yannick Einsweiler (JIRA),JIRA.13122459.1512329902000@Atlassian.JIRA,,,2017-12-03 11:39:00-08,"[jira] [Created] (AIRFLOW-1882) Add ignoreUnknownValues option to
 gcs_to_bq operator","Yannick Einsweiler created AIRFLOW-1882:
-------------------------------------------

             Summary: Add ignoreUnknownValues option to gcs_to_bq operator
                 Key: AIRFLOW-1882
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1882
             Project: Apache Airflow
          Issue Type: Improvement
    Affects Versions: 1.8.2
            Reporter: Yannick Einsweiler


Allows to load csv''s that have columns not defined in schema. For instance when lines end with a dummy/extra separator. BigQuery considers it as an extra column and won''t load the file if option is not passed on. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27452,54,JIRA.13122459.1512329902000.374425.1512331320050@Atlassian.JIRA,2421,Yannick Einsweiler (JIRA),JIRA.13122459.1512329902000@Atlassian.JIRA,,,2017-12-03 12:02:00-08,"[jira] [Updated] (AIRFLOW-1882) Add ignoreUnknownValues option to
 gcs_to_bq operator","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1882?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Yannick Einsweiler updated AIRFLOW-1882:
----------------------------------------
    Description: Would allow to load csv''s that have columns not defined in schema. For instance when lines end with a dummy/extra separator. BigQuery considers it as an extra column and won''t load the file if option is not passed.   (was: Allows to load csv''s that have columns not defined in schema. For instance when lines end with a dummy/extra separator. BigQuery considers it as an extra column and won''t load the file if option is not passed on. )

> Add ignoreUnknownValues option to gcs_to_bq operator
> ----------------------------------------------------
>
>                 Key: AIRFLOW-1882
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1882
>             Project: Apache Airflow
>          Issue Type: Improvement
>    Affects Versions: 1.8.2
>            Reporter: Yannick Einsweiler
>
> Would allow to load csv''s that have columns not defined in schema. For instance when lines end with a dummy/extra separator. BigQuery considers it as an extra column and won''t load the file if option is not passed. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
43854,54,154329874766.10931.12654260149570516772.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 22:05:47-08,"[GitHub] feng-tao commented on issue #4241: [AIRFLOW-XXX] Correct typos in
 UPDATING.md","feng-tao commented on issue #4241: [AIRFLOW-XXX] Correct typos in UPDATING.md
URL: https://github.com/apache/incubator-airflow/pull/4241#issuecomment-441936124
 
 
   just found out that you are updating the v10 branch. cc @ashb , I am not sure if we still update the release branch though.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27453,54,JIRA.13122469.1512346936000.374980.1512346980074@Atlassian.JIRA,2413,Kaxil Naik (JIRA),JIRA.13122469.1512346936000@Atlassian.JIRA,,,2017-12-03 16:23:00-08,"[jira] [Created] (AIRFLOW-1883) Get File Size for objects in Google
 Cloud Storage","Kaxil Naik created AIRFLOW-1883:
-----------------------------------

             Summary: Get File Size for objects in Google Cloud Storage
                 Key: AIRFLOW-1883
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1883
             Project: Apache Airflow
          Issue Type: New Feature
          Components: contrib, gcp
            Reporter: Kaxil Naik
            Assignee: Kaxil Naik


I would want to get file size for objects in Google Cloud Storage. The use case the files are uploaded to GCS bucket I would want to validate whether the file uploaded correctly be matching the file size.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27454,54,JIRA.13122469.1512346936000.374985.1512347470568@Atlassian.JIRA,2413,Kaxil Naik (JIRA),JIRA.13122469.1512346936000@Atlassian.JIRA,,,2017-12-03 16:31:10-08,"[jira] [Updated] (AIRFLOW-1883) Get File Size for objects in Google
 Cloud Storage","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1883?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kaxil Naik updated AIRFLOW-1883:
--------------------------------
    Description: I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly be matching the file size.  (was: I would want to get file size for objects in Google Cloud Storage. The use case the files are uploaded to GCS bucket I would want to validate whether the file uploaded correctly be matching the file size.)

> Get File Size for objects in Google Cloud Storage
> -------------------------------------------------
>
>                 Key: AIRFLOW-1883
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1883
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly be matching the file size.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27455,54,JIRA.13122469.1512346936000.374986.1512347470582@Atlassian.JIRA,2413,Kaxil Naik (JIRA),JIRA.13122469.1512346936000@Atlassian.JIRA,,,2017-12-03 16:31:10-08,"[jira] [Updated] (AIRFLOW-1883) Get File Size for objects in Google
 Cloud Storage","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1883?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kaxil Naik updated AIRFLOW-1883:
--------------------------------
    Description: I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.  (was: I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly be matching the file size.)

> Get File Size for objects in Google Cloud Storage
> -------------------------------------------------
>
>                 Key: AIRFLOW-1883
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1883
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27456,54,JIRA.13122469.1512346936000.374989.1512347520030@Atlassian.JIRA,2413,Kaxil Naik (JIRA),JIRA.13122469.1512346936000@Atlassian.JIRA,,,2017-12-03 16:32:00-08,"[jira] [Updated] (AIRFLOW-1883) Get File Size for objects in Google
 Cloud Storage","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1883?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kaxil Naik updated AIRFLOW-1883:
--------------------------------
    Description: 
I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.

Proposed Approach:
- Added a get_file_size() hook in gcs hook.
- Create an operator that allows to get the file size for a file.

  was:I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.


> Get File Size for objects in Google Cloud Storage
> -------------------------------------------------
>
>                 Key: AIRFLOW-1883
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1883
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.
> Proposed Approach:
> - Added a get_file_size() hook in gcs hook.
> - Create an operator that allows to get the file size for a file.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27457,54,JIRA.13122469.1512346936000.374990.1512347580093@Atlassian.JIRA,2413,Kaxil Naik (JIRA),JIRA.13122469.1512346936000@Atlassian.JIRA,,,2017-12-03 16:33:00-08,"[jira] [Updated] (AIRFLOW-1883) Get File Size for objects in Google
 Cloud Storage","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1883?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Kaxil Naik updated AIRFLOW-1883:
--------------------------------
    Description: 
I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.

Proposed Approach:
- Added a get_file_size() hook in gcs hook.
- Create an operator that allows getting the file size for a file.

  was:
I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.

Proposed Approach:
- Added a get_file_size() hook in gcs hook.
- Create an operator that allows to get the file size for a file.


> Get File Size for objects in Google Cloud Storage
> -------------------------------------------------
>
>                 Key: AIRFLOW-1883
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1883
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.
> Proposed Approach:
> - Added a get_file_size() hook in gcs hook.
> - Create an operator that allows getting the file size for a file.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
43855,54,154329926586.15731.9548358972842085922.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 22:14:25-08,"[GitHub] feng-tao closed pull request #4242: [AIRFLOW-XXX] Correct typos in
 UPDATING.md `master`","feng-tao closed pull request #4242: [AIRFLOW-XXX] Correct typos in UPDATING.md `master`
URL: https://github.com/apache/incubator-airflow/pull/4242
 
 
   

This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:

As this is a foreign pull request (from a fork), the diff is supplied
below (as it won''t show otherwise due to GitHub magic):

diff --git a/UPDATING.md b/UPDATING.md
index af448cfff8..88dc78c810 100644
--- a/UPDATING.md
+++ b/UPDATING.md
@@ -80,7 +80,7 @@ airflow users --delete --username jondoe
 
 ### StatsD Metrics
 
-The `scheduler_heartbeat` metric has been changed from a gauge to a counter. Each loop of the scheduler will increment the counter by 1. This provides a higher degree of visibility and allows for better integration with Prometheus using the [StatsD Exporter](https://github.com/prometheus/statsd_exporter). Scheduler upness can be determined by graphing and alerting using a rate. If the scheduler goes down, the rate will drop to 0.
+The `scheduler_heartbeat` metric has been changed from a gauge to a counter. Each loop of the scheduler will increment the counter by 1. This provides a higher degree of visibility and allows for better integration with Prometheus using the [StatsD Exporter](https://github.com/prometheus/statsd_exporter). The scheduler''s activity status can be determined by graphing and alerting using a rate of change of the counter. If the scheduler goes down, the rate will drop to 0.
 
 ### Custom auth backends interface change
 
@@ -110,12 +110,12 @@ should be inside the ""Instances"" dict)
 
 ### LDAP Auth Backend now requires TLS
 
-Connecting to an LDAP serever over plain text is not supported anymore. The
+Connecting to an LDAP server over plain text is not supported anymore. The
 certificate presented by the LDAP server must be signed by a trusted
-certificiate, or you must provide the `cacert` option under `[ldap]` in the
+certificate, or you must provide the `cacert` option under `[ldap]` in the
 config file.
 
-If you want to use LDAP auth backend without TLS then you will habe to create a
+If you want to use LDAP auth backend without TLS then you will have to create a
 custom-auth backend based on
 https://github.com/apache/incubator-airflow/blob/1.10.0/airflow/contrib/auth/backends/ldap_auth.py
 
@@ -133,7 +133,7 @@ The method name was changed to be compatible with the Python 3.7 async/await key
 
 ### Add a configuration variable(default_dag_run_display_number) to control numbers of dag run for display
 
-Add a configuration variable(default_dag_run_display_number) under webserver section to control num of dag run to show in UI.
+Add a configuration variable(default_dag_run_display_number) under webserver section to control the number of dag runs to show in UI.
 
 ### Default executor for SubDagOperator is changed to SequentialExecutor
 
@@ -166,7 +166,7 @@ There are five roles created for Airflow by default: Admin, User, Op, Viewer, an
 - AWS Batch Operator renamed property queue to job_queue to prevent conflict with the internal queue from CeleryExecutor - AIRFLOW-2542
 - Users created and stored in the old users table will not be migrated automatically. FAB''s built-in authentication support must be reconfigured.
 - Airflow dag home page is now `/home` (instead of `/admin`).
-- All ModelViews in Flask-AppBuilder follow a different pattern from Flask-Admin. The `/admin` part of the url path will no longer exist. For example: `/admin/connection` becomes `/connection/list`, `/admin/connection/new` becomes `/connection/add`, `/admin/connection/edit` becomes `/connection/edit`, etc.
+- All ModelViews in Flask-AppBuilder follow a different pattern from Flask-Admin. The `/admin` part of the URL path will no longer exist. For example: `/admin/connection` becomes `/connection/list`, `/admin/connection/new` becomes `/connection/add`, `/admin/connection/edit` becomes `/connection/edit`, etc.
 - Due to security concerns, the new webserver will no longer support the features in the `Data Profiling` menu of old UI, including `Ad Hoc Query`, `Charts`, and `Known Events`.
 - HiveServer2Hook.get_results() always returns a list of tuples, even when a single column is queried, as per Python API 2.
 
@@ -249,7 +249,7 @@ SSH Hook now uses the Paramiko library to create an ssh client connection, inste
 
 - update SSHHook constructor
 - use SSHOperator class in place of SSHExecuteOperator which is removed now. Refer to test_ssh_operator.py for usage info.
-- SFTPOperator is added to perform secure file transfer from serverA to serverB. Refer to test_sftp_operator.py.py for usage info.
+- SFTPOperator is added to perform secure file transfer from serverA to serverB. Refer to test_sftp_operator.py for usage info.
 - No updates are required if you are using ftpHook, it will continue to work as is.
 
 ### S3Hook switched to use Boto3
@@ -279,7 +279,7 @@ Once a logger has determined that a message needs to be processed, it is passed
 
 #### Changes in Airflow Logging
 
-Airflow''s logging mechanism has been refactored to use Python???s builtin `logging` module to perform logging of the application. By extending classes with the existing `LoggingMixin`, all the logging will go through a central logger. Also the `BaseHook` and `BaseOperator` already extend this class, so it is easily available to do logging.
+Airflow''s logging mechanism has been refactored to use Python???s built-in `logging` module to perform logging of the application. By extending classes with the existing `LoggingMixin`, all the logging will go through a central logger. Also the `BaseHook` and `BaseOperator` already extend this class, so it is easily available to do logging.
 
 The main benefit is easier configuration of the logging by setting a single centralized python file. Disclaimer; there is still some inline configuration, but this will be removed eventually. The new logging class is defined by setting the dotted classpath in your `~/airflow/airflow.cfg` file:
 
@@ -439,7 +439,7 @@ If you are using S3, the instructions should be largely the same as the Google c
 - Copy the logging configuration from [`airflow/config_templates/airflow_logging_settings.py`](https://github.com/apache/incubator-airflow/blob/master/airflow/config_templates/airflow_local_settings.py).
 - Place it in a directory inside the Python import path `PYTHONPATH`. If you are using Python 2.7, ensuring that any `__init__.py` files exist so that it is importable.
 - Update the config by setting the path of `REMOTE_BASE_LOG_FOLDER` explicitly in the config. The `REMOTE_BASE_LOG_FOLDER` key is not used anymore.
-- Set the `logging_config_class` to the filename and dict. For example, if you place `custom_logging_config.py` on the base of your pythonpath, you will need to set `logging_config_class = custom_logging_config.LOGGING_CONFIG` in your config as Airflow 1.8.
+- Set the `logging_config_class` to the filename and dict. For example, if you place `custom_logging_config.py` on the base of your `PYTHONPATH`, you will need to set `logging_config_class = custom_logging_config.LOGGING_CONFIG` in your config as Airflow 1.8.
 
 ### New Features
 


 

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27458,54,JIRA.13122660.1512416352000.381399.1512416400786@Atlassian.JIRA,2422,Grant Nicholas (JIRA),JIRA.13122660.1512416352000@Atlassian.JIRA,,,2017-12-04 11:40:00-08,"[jira] [Created] (AIRFLOW-1884) Ensure scheduler is crash safe for
 externally triggered and backfilled dagruns","Grant Nicholas created AIRFLOW-1884:
---------------------------------------

             Summary: Ensure scheduler is crash safe for externally triggered and backfilled dagruns
                 Key: AIRFLOW-1884
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1884
             Project: Apache Airflow
          Issue Type: Bug
            Reporter: Grant Nicholas
            Assignee: Grant Nicholas


Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 

I found the changeset this regression happened in, it is this one:
https://issues.apache.org/jira/browse/AIRFLOW-1059

This change reverts the special casing logic so that all dagruns have orphaned tasks reset on startup of the scheduler. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27459,54,JIRA.13040190.1486145345000.381464.1512416580897@Atlassian.JIRA,2423,gaba (JIRA),JIRA.13040190.1486145345000@Atlassian.JIRA,,,2017-12-04 11:43:00-08,"[jira] [Commented] (AIRFLOW-835) SMTP Mail delivery fails with
 server using CRAM-MD5 auth","
    [ https://issues.apache.org/jira/browse/AIRFLOW-835?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16277363#comment-16277363 ] 

gaba commented on AIRFLOW-835:
------------------------------

I''m having exactly the same issue in version 1.8.0

> SMTP Mail delivery fails with server using CRAM-MD5 auth
> --------------------------------------------------------
>
>                 Key: AIRFLOW-835
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-835
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: utils
>    Affects Versions: Airflow 1.7.1
>         Environment: https://hub.docker.com/_/python/ (debian:jessie + python2.7 in docker)
>            Reporter: Joseph Harris
>            Priority: Minor
>
> Traceback when sending email from smtp-server configured to offer CRAM-MD5 (in all cases, tls included). This occurs because the configuration module returns the password as a futures.types.newstr, instead of a plain str (see below for gory details of why this breaks).
> Traceback (most recent call last):
>   File ""/usr/local/lib/python2.7/site-packages/airflow/models.py"", line 1308, in handle_failure
>     self.email_alert(error, is_retry=False)
>   File ""/usr/local/lib/python2.7/site-packages/airflow/models.py"", line 1425, in email_alert
>     send_email(task.email, title, body)
>   File ""/usr/local/lib/python2.7/site-packages/airflow/utils/email.py"", line 43, in send_email
>     return backend(to, subject, html_content, files=files, dryrun=dryrun)
>   File ""/usr/local/lib/python2.7/site-packages/airflow/utils/email.py"", line 79, in send_email_smtp
>     send_MIME_email(SMTP_MAIL_FROM, to, msg, dryrun)
>   File ""/usr/local/lib/python2.7/site-packages/airflow/utils/email.py"", line 95, in send_MIME_email
>     s.login(SMTP_USER, SMTP_PASSWORD)
>   File ""/usr/local/lib/python2.7/smtplib.py"", line 607, in login
>     (code, resp) = self.docmd(encode_cram_md5(resp, user, password))
>   File ""/usr/local/lib/python2.7/smtplib.py"", line 571, in encode_cram_md5
>     response = user + "" "" + hmac.HMAC(password, challenge).hexdigest()
>   File ""/usr/local/lib/python2.7/hmac.py"", line 75, in __init__
>     self.outer.update(key.translate(trans_5C))
>   File ""/usr/local/lib/python2.7/site-packages/future/types/newstr.py"", line 390, in translate
>     if ord(c) in table:
> TypeError: ''in <string>'' requires string as left operand, not int
> SMTP configs:
> [email]
> email_backend = airflow.utils.email.send_email_smtp
> [smtp]
> smtp_host = {a_smtp_server}
> smtp_port = 587
> smtp_starttls = True
> smtp_ssl = False
> smtp_user = {a_username}
> smtp_password = {a_password}
> smtp_mail_from = {a_email_addr}
> *Gory details
> If the server offers CRAM-MD5, smptlib prefers this by default, and will try to use hmac.HMAC to hash the password:
> https://hg.python.org/cpython/file/2.7/Lib/smtplib.py#l602
> https://hg.python.org/cpython/file/2.7/Lib/smtplib.py#l571
> But if the password is a newstr, newstr.translate expects a dict mapping instead of str, and raises an exception.
> https://hg.python.org/cpython/file/2.7/Lib/hmac.py#l75
> All of this occurs after a successful SMTP.ehlo(), so it''s probably not crap container networking
> Could be resolved by passing the smtp password as a futures.types.newbytes, as this behaves as expected:
> from future.types import newstr, newbytes
> import hmac
> # Make str / newstr types
> test = ''a_string''
> test_newstr = newstr(test)
> test_newbytes = newbytes(test)
> msg = ''future problems''
> # Test 1 - Try to do a HMAC:
> # fine
> hmac.HMAC(test, msg)
> # fails horribly
> hmac.HMAC(test_newstr, msg)
> # is completely fine
> hmac.HMAC(test_newbytes, msg)



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27460,54,JIRA.13040668.1486396038000.381670.1512417780317@Atlassian.JIRA,2400,Ross Donaldson (JIRA),JIRA.13040668.1486396038000@Atlassian.JIRA,,,2017-12-04 12:03:00-08,[jira] [Commented] (AIRFLOW-843) Store task exceptions in context,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-843?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16277398#comment-16277398 ] 

Ross Donaldson commented on AIRFLOW-843:
----------------------------------------

Oh dang, that''s excellent. Thanks, [~thesquelched]!

(I can''t review or merge, but I would super adore seeing this)

> Store task exceptions in context
> --------------------------------
>
>                 Key: AIRFLOW-843
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-843
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Scott Kruger
>            Priority: Minor
>
> If a task encounters an exception during execution, it should store the exception on the execution context so that other methods (namely `on_failure_callback` can access it.  This would help with custom error integrations, e.g. Sentry.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27461,54,JIRA.13122660.1512416352000.381676.1512417840111@Atlassian.JIRA,2422,Grant Nicholas (JIRA),JIRA.13122660.1512416352000@Atlassian.JIRA,,,2017-12-04 12:04:00-08,"[jira] [Updated] (AIRFLOW-1884) Ensure scheduler is crash safe for
 externally triggered and backfilled dagruns","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1884?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Grant Nicholas updated AIRFLOW-1884:
------------------------------------
    External issue URL: https://github.com/apache/incubator-airflow/pull/2843

> Ensure scheduler is crash safe for externally triggered and backfilled dagruns
> ------------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1884
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1884
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Grant Nicholas
>            Assignee: Grant Nicholas
>
> Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 
> I found the changeset this regression happened in, it is this one:
> https://issues.apache.org/jira/browse/AIRFLOW-1059
> This change reverts the special casing logic so that all dagruns have orphaned tasks reset on startup of the scheduler. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27462,54,JIRA.13122660.1512416352000.381824.1512419220333@Atlassian.JIRA,2422,Grant Nicholas (JIRA),JIRA.13122660.1512416352000@Atlassian.JIRA,,,2017-12-04 12:27:00-08,"[jira] [Updated] (AIRFLOW-1884) Ensure scheduler is crash safe for
 externally triggered dagruns","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1884?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Grant Nicholas updated AIRFLOW-1884:
------------------------------------
    Summary: Ensure scheduler is crash safe for externally triggered dagruns  (was: Ensure scheduler is crash safe for externally triggered and backfilled dagruns)

> Ensure scheduler is crash safe for externally triggered dagruns
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1884
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1884
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Grant Nicholas
>            Assignee: Grant Nicholas
>
> Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 
> I found the changeset this regression happened in, it is this one:
> https://issues.apache.org/jira/browse/AIRFLOW-1059
> This change reverts the special casing logic so that all dagruns have orphaned tasks reset on startup of the scheduler. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27463,54,JIRA.13122660.1512416352000.381833.1512419220387@Atlassian.JIRA,2422,Grant Nicholas (JIRA),JIRA.13122660.1512416352000@Atlassian.JIRA,,,2017-12-04 12:27:00-08,"[jira] [Updated] (AIRFLOW-1884) Ensure scheduler is crash safe for
 externally triggered dagruns","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1884?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Grant Nicholas updated AIRFLOW-1884:
------------------------------------
    Description: 
Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 

I found the changeset this regression happened in, it is this one:
https://issues.apache.org/jira/browse/AIRFLOW-1059

This change reverts the special casing logic so that externally triggered dagruns have orphaned tasks reset on startup of the scheduler. 

  was:
Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 

I found the changeset this regression happened in, it is this one:
https://issues.apache.org/jira/browse/AIRFLOW-1059

This change reverts the special casing logic so that all dagruns have orphaned tasks reset on startup of the scheduler. 


> Ensure scheduler is crash safe for externally triggered dagruns
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1884
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1884
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Grant Nicholas
>            Assignee: Grant Nicholas
>
> Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 
> I found the changeset this regression happened in, it is this one:
> https://issues.apache.org/jira/browse/AIRFLOW-1059
> This change reverts the special casing logic so that externally triggered dagruns have orphaned tasks reset on startup of the scheduler. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
926040,195,153154203989.25560.15993493693654678047.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-07-13 21:20:39-07,"[GitHub] rdhabalia removed a comment on issue #2130: Add function metrics
 with function-stats to get metrics on-demand","rdhabalia removed a comment on issue #2130: Add function metrics with function-stats to get metrics on-demand
URL: https://github.com/apache/incubator-pulsar/pull/2130#issuecomment-404997489
 
 
   retest this please

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27464,54,JIRA.13122660.1512416352000.381834.1512419280367@Atlassian.JIRA,2422,Grant Nicholas (JIRA),JIRA.13122660.1512416352000@Atlassian.JIRA,,,2017-12-04 12:28:00-08,"[jira] [Updated] (AIRFLOW-1884) Ensure scheduler is crash safe for
 externally triggered dagruns","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1884?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Grant Nicholas updated AIRFLOW-1884:
------------------------------------
    Description: 
Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 

I found the changeset this regression happened in, it is this one:
https://issues.apache.org/jira/browse/AIRFLOW-1059

This change reverts the special casing logic so that externally triggered dagruns have orphaned tasks reset on startup of the scheduler. Backfilled dagruns are still not crash safe, so if that needs to be fixed it will be done in another PR. 

  was:
Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 

I found the changeset this regression happened in, it is this one:
https://issues.apache.org/jira/browse/AIRFLOW-1059

This change reverts the special casing logic so that externally triggered dagruns have orphaned tasks reset on startup of the scheduler. 


> Ensure scheduler is crash safe for externally triggered dagruns
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1884
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1884
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Grant Nicholas
>            Assignee: Grant Nicholas
>
> Orphaned task instances are only reset for dagruns that are both not externally triggered and not backfilled. This violates the crash safety property of the scheduler, ie) if the scheduler crashes in the middle of one of these dagruns then tasks can be stuck in the ""Queued"" state forever and never executed. 
> I found the changeset this regression happened in, it is this one:
> https://issues.apache.org/jira/browse/AIRFLOW-1059
> This change reverts the special casing logic so that externally triggered dagruns have orphaned tasks reset on startup of the scheduler. Backfilled dagruns are still not crash safe, so if that needs to be fixed it will be done in another PR. 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27465,54,JIRA.13122469.1512346936000.382964.1512425460163@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122469.1512346936000@Atlassian.JIRA,,,2017-12-04 14:11:00-08,"[jira] [Commented] (AIRFLOW-1883) Get File Size for objects in
 Google Cloud Storage","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1883?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16277623#comment-16277623 ] 

ASF subversion and git services commented on AIRFLOW-1883:
----------------------------------------------------------

Commit 8d2f430732331c003d2c82f9a0c435e013281fe9 in incubator-airflow''s branch refs/heads/master from [~kaxilnaik]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=8d2f430 ]

[AIRFLOW-1883] Get File Size for objects in Google Cloud Storage

Closes #2840 from kaxil/Get_File_Size


> Get File Size for objects in Google Cloud Storage
> -------------------------------------------------
>
>                 Key: AIRFLOW-1883
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1883
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.
> Proposed Approach:
> - Added a get_file_size() hook in gcs hook.
> - Create an operator that allows getting the file size for a file.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27466,54,JIRA.13122469.1512346936000.382977.1512425520248@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13122469.1512346936000@Atlassian.JIRA,,,2017-12-04 14:12:00-08,"[jira] [Updated] (AIRFLOW-1883) Get File Size for objects in Google
 Cloud Storage","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1883?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini updated AIRFLOW-1883:
-------------------------------------
    Fix Version/s: 1.10.0

> Get File Size for objects in Google Cloud Storage
> -------------------------------------------------
>
>                 Key: AIRFLOW-1883
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1883
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>             Fix For: 1.10.0
>
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.
> Proposed Approach:
> - Added a get_file_size() hook in gcs hook.
> - Create an operator that allows getting the file size for a file.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27467,54,JIRA.13122469.1512346936000.382981.1512425581056@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13122469.1512346936000@Atlassian.JIRA,,,2017-12-04 14:13:01-08,"[jira] [Resolved] (AIRFLOW-1883) Get File Size for objects in
 Google Cloud Storage","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1883?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1883.
--------------------------------------
    Resolution: Fixed

> Get File Size for objects in Google Cloud Storage
> -------------------------------------------------
>
>                 Key: AIRFLOW-1883
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1883
>             Project: Apache Airflow
>          Issue Type: New Feature
>          Components: contrib, gcp
>            Reporter: Kaxil Naik
>            Assignee: Kaxil Naik
>             Fix For: 1.10.0
>
>   Original Estimate: 24h
>  Remaining Estimate: 24h
>
> I would want to get file size for objects in Google Cloud Storage. The use case is when the files are uploaded to GCS bucket I would want to validate whether the file is uploaded correctly by matching the file size.
> Proposed Approach:
> - Added a get_file_size() hook in gcs hook.
> - Create an operator that allows getting the file size for a file.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27468,54,JIRA.13122795.1512461800000.386170.1512461820191@Atlassian.JIRA,2424,John Barker (JIRA),JIRA.13122795.1512461800000@Atlassian.JIRA,,,2017-12-05 00:17:00-08,"[jira] [Created] (AIRFLOW-1885) Exception when polling ready
 workers and a gunicorn worker becomes a zombie","John Barker created AIRFLOW-1885:
------------------------------------

             Summary: Exception when polling ready workers and a gunicorn worker becomes a zombie
                 Key: AIRFLOW-1885
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1885
             Project: Apache Airflow
          Issue Type: Bug
    Affects Versions: Airflow 1.8
            Reporter: John Barker


If one of the gunicorn workers happens to become a zombie between `children()` and `cmdline()` calls to psutil in `get_num_ready_workers_running`:

{code}
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 28, in <module>
    args.func(args)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
    restart_workers(gunicorn_master_proc, num_workers)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
    num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
    proc for proc in workers
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
    if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
IndexError: list index out of range
{code}



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27469,54,JIRA.13122356.1512218725000.386185.1512462000217@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122356.1512218725000@Atlassian.JIRA,,,2017-12-05 00:20:00-08,[jira] [Commented] (AIRFLOW-1881) Don''t double log operator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1881?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278181#comment-16278181 ] 

ASF subversion and git services commented on AIRFLOW-1881:
----------------------------------------------------------

Commit 97383f76d03481e3925781e5bba65a9630c4751a in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=97383f7 ]

[AIRFLOW-1881] Make operator log in task log

Previously operators logged under
airflow.operators or
airflow.contrib.operators. This unifies them under
airflow.task.operators allowing the task log to
pick
them up and not have ''double'' logging.

Closes #2838 from bolkedebruin/AIRFLOW-1881


> Don''t double log operator output
> --------------------------------
>
>                 Key: AIRFLOW-1881
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1881
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.1
>
>
> Operators log to stdout by default instead of the `airflow.task` parent, this results in double logging



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27470,54,JIRA.13122356.1512218725000.386184.1512462000210@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122356.1512218725000@Atlassian.JIRA,,,2017-12-05 00:20:00-08,[jira] [Commented] (AIRFLOW-1881) Don''t double log operator output,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1881?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278180#comment-16278180 ] 

ASF subversion and git services commented on AIRFLOW-1881:
----------------------------------------------------------

Commit 97383f76d03481e3925781e5bba65a9630c4751a in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=97383f7 ]

[AIRFLOW-1881] Make operator log in task log

Previously operators logged under
airflow.operators or
airflow.contrib.operators. This unifies them under
airflow.task.operators allowing the task log to
pick
them up and not have ''double'' logging.

Closes #2838 from bolkedebruin/AIRFLOW-1881


> Don''t double log operator output
> --------------------------------
>
>                 Key: AIRFLOW-1881
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1881
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.1
>
>
> Operators log to stdout by default instead of the `airflow.task` parent, this results in double logging



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27471,54,JIRA.13122356.1512218725000.386191.1512462000275@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13122356.1512218725000@Atlassian.JIRA,,,2017-12-05 00:20:00-08,[jira] [Resolved] (AIRFLOW-1881) Don''t double log operator output,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1881?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1881.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2838
[https://github.com/apache/incubator-airflow/pull/2838]

> Don''t double log operator output
> --------------------------------
>
>                 Key: AIRFLOW-1881
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1881
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.1
>
>
> Operators log to stdout by default instead of the `airflow.task` parent, this results in double logging



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27472,54,JIRA.13122795.1512461800000.386201.1512462180067@Atlassian.JIRA,2424,John Barker (JIRA),JIRA.13122795.1512461800000@Atlassian.JIRA,,,2017-12-05 00:23:00-08,"[jira] [Updated] (AIRFLOW-1885) Exception when polling ready
 workers and a gunicorn worker becomes a zombie","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1885?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

John Barker updated AIRFLOW-1885:
---------------------------------
    Description: 
If one of the gunicorn workers happens to become a zombie between `children()` and `cmdline()` calls to psutil in `get_num_ready_workers_running` will raise an IndexError:

{code}
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 28, in <module>
    args.func(args)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
    restart_workers(gunicorn_master_proc, num_workers)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
    num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
    proc for proc in workers
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
    if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
IndexError: list index out of range
{code}

In version 4.2 of psutil, `cmdline` can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so ensure that an array is actually returned from `cmdline` before doing the `in` check.

  was:
If one of the gunicorn workers happens to become a zombie between `children()` and `cmdline()` calls to psutil in `get_num_ready_workers_running`:

{code}
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 28, in <module>
    args.func(args)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
    restart_workers(gunicorn_master_proc, num_workers)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
    num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
    proc for proc in workers
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
    if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
IndexError: list index out of range
{code}


> Exception when polling ready workers and a gunicorn worker becomes a zombie
> ---------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1885
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1885
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: John Barker
>
> If one of the gunicorn workers happens to become a zombie between `children()` and `cmdline()` calls to psutil in `get_num_ready_workers_running` will raise an IndexError:
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
>     restart_workers(gunicorn_master_proc, num_workers)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
>     num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
>     proc for proc in workers
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
>     if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
> IndexError: list index out of range
> {code}
> In version 4.2 of psutil, `cmdline` can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so ensure that an array is actually returned from `cmdline` before doing the `in` check.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27473,54,JIRA.13122795.1512461800000.386216.1512462240366@Atlassian.JIRA,2424,John Barker (JIRA),JIRA.13122795.1512461800000@Atlassian.JIRA,,,2017-12-05 00:24:00-08,"[jira] [Updated] (AIRFLOW-1885) IndexError when polling ready
 workers and a gunicorn worker becomes a zombie","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1885?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

John Barker updated AIRFLOW-1885:
---------------------------------
    Summary: IndexError when polling ready workers and a gunicorn worker becomes a zombie  (was: Exception when polling ready workers and a gunicorn worker becomes a zombie)

> IndexError when polling ready workers and a gunicorn worker becomes a zombie
> ----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1885
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1885
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: John Barker
>            Assignee: John Barker
>
> If one of the gunicorn workers happens to become a zombie between {{children()}} and {{cmdline()}} calls to psutil in {{get_num_ready_workers_running}} will raise an IndexError:
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
>     restart_workers(gunicorn_master_proc, num_workers)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
>     num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
>     proc for proc in workers
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
>     if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
> IndexError: list index out of range
> {code}
> In version 4.2 of psutil, {{cmdline}} can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so ensure that an array is actually returned from {{cmdline}} before doing the {{in}} check.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
875541,186,CAH-hyAfJ3rKWF7fqm_UJDC8mi6b4AWV0kh+k8cenkH8CkGYZ+g@mail.gmail.com,30940,John Sanda,NULL,,,2017-08-17 15:18:12-07,docker client API version used by invoker,"I am trying set up OpenWhisk using
https://github.com/apache/incubator-openwhisk-deploy-kube. I ran into a
problem with the invoker. It is stuck in a crash loop with this message:

[ERROR] [#sid_101] [ContainerUtils] stdout:  stderr: /usr/bin/docker: Error
response from daemon: client is newer than server (client API version:
1.24, server API version: 1.22). See ''/usr/bin/docker run --help''.
[marker:invoker_docker.run_error:29093:13]

The docker API version on my machine is 1.22. Is there a way I can change
the version of the docker client API that the invoker uses? I have been
looking in the gradle scripts but have not found anything yet.

Thanks

John
",f
27474,54,JIRA.13122795.1512461800000.386213.1512462240227@Atlassian.JIRA,2424,John Barker (JIRA),JIRA.13122795.1512461800000@Atlassian.JIRA,,,2017-12-05 00:24:00-08,"[jira] [Assigned] (AIRFLOW-1885) Exception when polling ready
 workers and a gunicorn worker becomes a zombie","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1885?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

John Barker reassigned AIRFLOW-1885:
------------------------------------

    Assignee: John Barker

> Exception when polling ready workers and a gunicorn worker becomes a zombie
> ---------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1885
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1885
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: John Barker
>            Assignee: John Barker
>
> If one of the gunicorn workers happens to become a zombie between {{children()}} and {{cmdline()}} calls to psutil in {{get_num_ready_workers_running}} will raise an IndexError:
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
>     restart_workers(gunicorn_master_proc, num_workers)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
>     num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
>     proc for proc in workers
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
>     if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
> IndexError: list index out of range
> {code}
> In version 4.2 of psutil, {{cmdline}} can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so ensure that an array is actually returned from {{cmdline}} before doing the {{in}} check.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27475,54,JIRA.13122795.1512461800000.386210.1512462240205@Atlassian.JIRA,2424,John Barker (JIRA),JIRA.13122795.1512461800000@Atlassian.JIRA,,,2017-12-05 00:24:00-08,"[jira] [Updated] (AIRFLOW-1885) Exception when polling ready
 workers and a gunicorn worker becomes a zombie","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1885?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

John Barker updated AIRFLOW-1885:
---------------------------------
    Description: 
If one of the gunicorn workers happens to become a zombie between {{children()}} and {{cmdline()}} calls to psutil in {{get_num_ready_workers_running}} will raise an IndexError:

{code}
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 28, in <module>
    args.func(args)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
    restart_workers(gunicorn_master_proc, num_workers)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
    num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
    proc for proc in workers
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
    if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
IndexError: list index out of range
{code}

In version 4.2 of psutil, {{cmdline}} can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so ensure that an array is actually returned from {{cmdline}} before doing the {{in}} check.

  was:
If one of the gunicorn workers happens to become a zombie between `children()` and `cmdline()` calls to psutil in `get_num_ready_workers_running` will raise an IndexError:

{code}
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 28, in <module>
    args.func(args)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
    restart_workers(gunicorn_master_proc, num_workers)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
    num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
    proc for proc in workers
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
    if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
IndexError: list index out of range
{code}

In version 4.2 of psutil, `cmdline` can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so ensure that an array is actually returned from `cmdline` before doing the `in` check.


> Exception when polling ready workers and a gunicorn worker becomes a zombie
> ---------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1885
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1885
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: John Barker
>
> If one of the gunicorn workers happens to become a zombie between {{children()}} and {{cmdline()}} calls to psutil in {{get_num_ready_workers_running}} will raise an IndexError:
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
>     restart_workers(gunicorn_master_proc, num_workers)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
>     num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
>     proc for proc in workers
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
>     if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
> IndexError: list index out of range
> {code}
> In version 4.2 of psutil, {{cmdline}} can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so ensure that an array is actually returned from {{cmdline}} before doing the {{in}} check.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27476,54,JIRA.13122795.1512461800000.386249.1512462480184@Atlassian.JIRA,2424,John Barker (JIRA),JIRA.13122795.1512461800000@Atlassian.JIRA,,,2017-12-05 00:28:00-08,"[jira] [Updated] (AIRFLOW-1885) IndexError when polling ready
 workers and a gunicorn worker becomes a zombie","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1885?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

John Barker updated AIRFLOW-1885:
---------------------------------
    Description: 
If one of the gunicorn workers happens to become a zombie between {{children()}} and {{cmdline()}} calls to psutil in {{get_num_ready_workers_running}} will raise an IndexError:

{code}
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 28, in <module>
    args.func(args)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
    restart_workers(gunicorn_master_proc, num_workers)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
    num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
    proc for proc in workers
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
    if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
IndexError: list index out of range
{code}

In version 4.2 of psutil, {{cmdline}} can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so one must ensure that an array is returned with at least one item from {{cmdline}} before doing the {{in}} check.

  was:
If one of the gunicorn workers happens to become a zombie between {{children()}} and {{cmdline()}} calls to psutil in {{get_num_ready_workers_running}} will raise an IndexError:

{code}
Traceback (most recent call last):
  File ""/usr/local/bin/airflow"", line 28, in <module>
    args.func(args)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
    restart_workers(gunicorn_master_proc, num_workers)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
    num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
    proc for proc in workers
  File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
    if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
IndexError: list index out of range
{code}

In version 4.2 of psutil, {{cmdline}} can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so ensure that an array is actually returned from {{cmdline}} before doing the {{in}} check.


> IndexError when polling ready workers and a gunicorn worker becomes a zombie
> ----------------------------------------------------------------------------
>
>                 Key: AIRFLOW-1885
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1885
>             Project: Apache Airflow
>          Issue Type: Bug
>    Affects Versions: Airflow 1.8
>            Reporter: John Barker
>            Assignee: John Barker
>
> If one of the gunicorn workers happens to become a zombie between {{children()}} and {{cmdline()}} calls to psutil in {{get_num_ready_workers_running}} will raise an IndexError:
> {code}
> Traceback (most recent call last):
>   File ""/usr/local/bin/airflow"", line 28, in <module>
>     args.func(args)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 803, in webserver
>     restart_workers(gunicorn_master_proc, num_workers)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 687, in restart_workers
>     num_ready_workers_running = get_num_ready_workers_running(gunicorn_master_proc)
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 663, in get_num_ready_workers_running
>     proc for proc in workers
>   File ""/usr/local/lib/python3.5/dist-packages/airflow/bin/cli.py"", line 664, in <listcomp>
>     if settings.GUNICORN_WORKER_READY_PREFIX in proc.cmdline()[0]
> IndexError: list index out of range
> {code}
> In version 4.2 of psutil, {{cmdline}} can return an empty array if the process is zombied: https://github.com/giampaolo/psutil/blob/release-4.2.0/psutil/_pslinux.py#L1007 so one must ensure that an array is returned with at least one item from {{cmdline}} before doing the {{in}} check.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27477,54,JIRA.13050106.1489170459000.386519.1512465240346@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13050106.1489170459000@Atlassian.JIRA,,,2017-12-05 01:14:00-08,[jira] [Commented] (AIRFLOW-966) Celery Broker Transport Options,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-966?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278241#comment-16278241 ] 

ASF subversion and git services commented on AIRFLOW-966:
---------------------------------------------------------

Commit aa737a582c687e7105ef934ffc4da3dc78438235 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=aa737a5 ]

[AIRFLOW-966] Make celery broker_transport_options configurable

Required for changing visibility timeout and other
options required
for Redis/SQS.

Closes #2842 from bolkedebruin/AIRFLOW-966


> Celery Broker Transport Options
> -------------------------------
>
>                 Key: AIRFLOW-966
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-966
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: celery, executor
>            Reporter: Desiree Cox
>            Assignee: Desiree Cox
>             Fix For: 1.9.1
>
>
> To use the CeleryExecutor with a Redis Sentinel broker, Celery must be provided with a dictionary of broker_transport_options with a key/value pair telling it the name of the desired redis service. There should be a broker_transport_options in the Celery section of airflow.cfg and CeleryExecutor should pass the options into the Celery worker configuration.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27478,54,JIRA.13050106.1489170459000.386532.1512465240435@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13050106.1489170459000@Atlassian.JIRA,,,2017-12-05 01:14:00-08,[jira] [Resolved] (AIRFLOW-966) Celery Broker Transport Options,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-966?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong resolved AIRFLOW-966.
--------------------------------------
       Resolution: Fixed
    Fix Version/s:     (was: 1.8.3)
                       (was: 1.9.0)
                   1.9.1

Issue resolved by pull request #2842
[https://github.com/apache/incubator-airflow/pull/2842]

> Celery Broker Transport Options
> -------------------------------
>
>                 Key: AIRFLOW-966
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-966
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: celery, executor
>            Reporter: Desiree Cox
>            Assignee: Desiree Cox
>             Fix For: 1.9.1
>
>
> To use the CeleryExecutor with a Redis Sentinel broker, Celery must be provided with a dictionary of broker_transport_options with a key/value pair telling it the name of the desired redis service. There should be a broker_transport_options in the Celery section of airflow.cfg and CeleryExecutor should pass the options into the Celery worker configuration.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27479,54,JIRA.13050106.1489170459000.386524.1512465240389@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13050106.1489170459000@Atlassian.JIRA,,,2017-12-05 01:14:00-08,[jira] [Commented] (AIRFLOW-966) Celery Broker Transport Options,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-966?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278242#comment-16278242 ] 

ASF subversion and git services commented on AIRFLOW-966:
---------------------------------------------------------

Commit aa737a582c687e7105ef934ffc4da3dc78438235 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=aa737a5 ]

[AIRFLOW-966] Make celery broker_transport_options configurable

Required for changing visibility timeout and other
options required
for Redis/SQS.

Closes #2842 from bolkedebruin/AIRFLOW-966


> Celery Broker Transport Options
> -------------------------------
>
>                 Key: AIRFLOW-966
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-966
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: celery, executor
>            Reporter: Desiree Cox
>            Assignee: Desiree Cox
>             Fix For: 1.9.1
>
>
> To use the CeleryExecutor with a Redis Sentinel broker, Celery must be provided with a dictionary of broker_transport_options with a key/value pair telling it the name of the desired redis service. There should be a broker_transport_options in the Celery section of airflow.cfg and CeleryExecutor should pass the options into the Celery worker configuration.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27480,54,JIRA.12990672.1468912243000.386547.1512465300975@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.12990672.1468912243000@Atlassian.JIRA,,,2017-12-05 01:15:00-08,"[jira] [Commented] (AIRFLOW-342)  exception in ''airflow scheduler''
 : Connection reset by peer","
    [ https://issues.apache.org/jira/browse/AIRFLOW-342?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278247#comment-16278247 ] 

ASF subversion and git services commented on AIRFLOW-342:
---------------------------------------------------------

Commit bdafb12f8dd9e6fcf508b0df869f9cc45bb9593d in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=bdafb12 ]

[AIRFLOW-342] Do not use amqp, rpc as result backend

amqp and rpc (and redis most likely) cannot store
results for tasks
long enough.

Closes #2830 from bolkedebruin/AIRFLOW-342


>  exception in ''airflow scheduler'' : Connection reset by peer
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-342
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-342
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery, scheduler
>    Affects Versions: Airflow 1.7.1.3
>         Environment: OS: Red Hat Enterprise Linux Server 7.2 (Maipo)
> Python: 2.7.5
> Airflow: 1.7.1.3
>            Reporter: Hila Visan
>            Assignee: Hila Visan
>
> ''airflow scheduler'' command throws an exception when running it. 
> Despite the exception, the workers run the tasks from the queues as expected.
> Error details:
>  
> [2016-06-30 19:00:10,130] {jobs.py:758} ERROR - [Errno 104] Connection reset by peer
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 755, in _execute
>     executor.heartbeat()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/base_executor.py"", line 107, in heartbeat
>     self.sync()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/celery_executor.py"", line 74, in sync
>     state = async.state
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 394, in state
>     return self._get_task_meta()[''status'']
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 339, in _get_task_meta
>     return self._maybe_set_cache(self.backend.get_task_meta(self.id))
>   File ""/usr/lib/python2.7/site-packages/celery/backends/amqp.py"", line 163, in get_task_meta
>     binding.declare()
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 521, in declare
>    self.exchange.declare(nowait)
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 174, in declare
>     nowait=nowait, passive=passive,
>   File ""/usr/lib/python2.7/site-packages/amqp/channel.py"", line 615, in exchange_declare
>     self._send_method((40, 10), args)
>   File ""/usr/lib/python2.7/site-packages/amqp/abstract_channel.py"", line 56, in _send_method
>     self.channel_id, method_sig, args, content,
>   File ""/usr/lib/python2.7/site-packages/amqp/method_framing.py"", line 221, in write_method
>     write_frame(1, channel, payload)
>   File ""/usr/lib/python2.7/site-packages/amqp/transport.py"", line 182, in write_frame
>     frame_type, channel, size, payload, 0xce,
>   File ""/usr/lib64/python2.7/socket.py"", line 224, in meth
>     return getattr(self._sock,name)(*args)
> error: [Errno 104] Connection reset by peer
> [2016-06-30 19:00:10,131] {jobs.py:759} ERROR - Tachycardia!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27481,54,JIRA.12990672.1468912243000.386589.1512465361127@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.12990672.1468912243000@Atlassian.JIRA,,,2017-12-05 01:16:01-08,"[jira] [Commented] (AIRFLOW-342)  exception in ''airflow scheduler''
 : Connection reset by peer","
    [ https://issues.apache.org/jira/browse/AIRFLOW-342?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278249#comment-16278249 ] 

ASF subversion and git services commented on AIRFLOW-342:
---------------------------------------------------------

Commit bdafb12f8dd9e6fcf508b0df869f9cc45bb9593d in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=bdafb12 ]

[AIRFLOW-342] Do not use amqp, rpc as result backend

amqp and rpc (and redis most likely) cannot store
results for tasks
long enough.

Closes #2830 from bolkedebruin/AIRFLOW-342


>  exception in ''airflow scheduler'' : Connection reset by peer
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-342
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-342
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery, scheduler
>    Affects Versions: Airflow 1.7.1.3
>         Environment: OS: Red Hat Enterprise Linux Server 7.2 (Maipo)
> Python: 2.7.5
> Airflow: 1.7.1.3
>            Reporter: Hila Visan
>            Assignee: Hila Visan
>             Fix For: 1.9.1
>
>
> ''airflow scheduler'' command throws an exception when running it. 
> Despite the exception, the workers run the tasks from the queues as expected.
> Error details:
>  
> [2016-06-30 19:00:10,130] {jobs.py:758} ERROR - [Errno 104] Connection reset by peer
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 755, in _execute
>     executor.heartbeat()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/base_executor.py"", line 107, in heartbeat
>     self.sync()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/celery_executor.py"", line 74, in sync
>     state = async.state
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 394, in state
>     return self._get_task_meta()[''status'']
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 339, in _get_task_meta
>     return self._maybe_set_cache(self.backend.get_task_meta(self.id))
>   File ""/usr/lib/python2.7/site-packages/celery/backends/amqp.py"", line 163, in get_task_meta
>     binding.declare()
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 521, in declare
>    self.exchange.declare(nowait)
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 174, in declare
>     nowait=nowait, passive=passive,
>   File ""/usr/lib/python2.7/site-packages/amqp/channel.py"", line 615, in exchange_declare
>     self._send_method((40, 10), args)
>   File ""/usr/lib/python2.7/site-packages/amqp/abstract_channel.py"", line 56, in _send_method
>     self.channel_id, method_sig, args, content,
>   File ""/usr/lib/python2.7/site-packages/amqp/method_framing.py"", line 221, in write_method
>     write_frame(1, channel, payload)
>   File ""/usr/lib/python2.7/site-packages/amqp/transport.py"", line 182, in write_frame
>     frame_type, channel, size, payload, 0xce,
>   File ""/usr/lib64/python2.7/socket.py"", line 224, in meth
>     return getattr(self._sock,name)(*args)
> error: [Errno 104] Connection reset by peer
> [2016-06-30 19:00:10,131] {jobs.py:759} ERROR - Tachycardia!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27482,54,JIRA.12990672.1468912243000.386616.1512465361654@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.12990672.1468912243000@Atlassian.JIRA,,,2017-12-05 01:16:01-08,"[jira] [Resolved] (AIRFLOW-342)  exception in ''airflow scheduler'' :
 Connection reset by peer","
     [ https://issues.apache.org/jira/browse/AIRFLOW-342?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong resolved AIRFLOW-342.
--------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2830
[https://github.com/apache/incubator-airflow/pull/2830]

>  exception in ''airflow scheduler'' : Connection reset by peer
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-342
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-342
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: celery, scheduler
>    Affects Versions: Airflow 1.7.1.3
>         Environment: OS: Red Hat Enterprise Linux Server 7.2 (Maipo)
> Python: 2.7.5
> Airflow: 1.7.1.3
>            Reporter: Hila Visan
>            Assignee: Hila Visan
>             Fix For: 1.9.1
>
>
> ''airflow scheduler'' command throws an exception when running it. 
> Despite the exception, the workers run the tasks from the queues as expected.
> Error details:
>  
> [2016-06-30 19:00:10,130] {jobs.py:758} ERROR - [Errno 104] Connection reset by peer
> Traceback (most recent call last):
>   File ""/usr/lib/python2.7/site-packages/airflow/jobs.py"", line 755, in _execute
>     executor.heartbeat()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/base_executor.py"", line 107, in heartbeat
>     self.sync()
>   File ""/usr/lib/python2.7/site-packages/airflow/executors/celery_executor.py"", line 74, in sync
>     state = async.state
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 394, in state
>     return self._get_task_meta()[''status'']
>   File ""/usr/lib/python2.7/site-packages/celery/result.py"", line 339, in _get_task_meta
>     return self._maybe_set_cache(self.backend.get_task_meta(self.id))
>   File ""/usr/lib/python2.7/site-packages/celery/backends/amqp.py"", line 163, in get_task_meta
>     binding.declare()
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 521, in declare
>    self.exchange.declare(nowait)
>   File ""/usr/lib/python2.7/site-packages/kombu/entity.py"", line 174, in declare
>     nowait=nowait, passive=passive,
>   File ""/usr/lib/python2.7/site-packages/amqp/channel.py"", line 615, in exchange_declare
>     self._send_method((40, 10), args)
>   File ""/usr/lib/python2.7/site-packages/amqp/abstract_channel.py"", line 56, in _send_method
>     self.channel_id, method_sig, args, content,
>   File ""/usr/lib/python2.7/site-packages/amqp/method_framing.py"", line 221, in write_method
>     write_frame(1, channel, payload)
>   File ""/usr/lib/python2.7/site-packages/amqp/transport.py"", line 182, in write_frame
>     frame_type, channel, size, payload, 0xce,
>   File ""/usr/lib64/python2.7/socket.py"", line 224, in meth
>     return getattr(self._sock,name)(*args)
> error: [Errno 104] Connection reset by peer
> [2016-06-30 19:00:10,131] {jobs.py:759} ERROR - Tachycardia!



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27483,54,JIRA.13093082.1502177433000.386700.1512465840414@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13093082.1502177433000@Atlassian.JIRA,,,2017-12-05 01:24:00-08,"[jira] [Commented] (AIRFLOW-1494) backfill job failed because new
 retry comes out when a valid job is running","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1494?page=3Dcom.atlassi=
an.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=3D16=
278264#comment-16278264 ]=20

Bolke de Bruin commented on AIRFLOW-1494:
-----------------------------------------

Please set your broker_transport_options to a visibility timeout longer tha=
n the ETA of your tasks. See linked issue. Option for configuring is target=
ed at Airflow 1.9.1

> backfill job failed because new retry comes out when a valid job is runni=
ng
> -------------------------------------------------------------------------=
--
>
>                 Key: AIRFLOW-1494
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1494
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: backfill
>    Affects Versions: Airflow 1.8
>         Environment: ubuntu
>            Reporter: Xianping lin
>            Priority: Blocker
>             Fix For: 1.9.1
>
>
> I have a spark job, wrapped in a BASH command to run.
> From fail log, i found airflow try to rerun the job while the job is runn=
ing. Then a series strange things happened. My job finally failed. Logs Bel=
ow:
> [2017-08-07 23:24:35,903] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:35,903] {bash_operator.py:94} INFO - 17/08/07 23:24:35 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:36,904] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:36,904] {bash_operator.py:94} INFO - 17/08/07 23:24:36 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:37,905] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:37,904] {bash_operator.py:94} INFO - 17/08/07 23:24:37 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:38,906] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:38,906] {bash_operator.py:94} INFO - 17/08/07 23:24:38 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:38,947] {cli.py:391} INFO - Loading pickle id 148
> [2017-08-07 23:24:39,020] {base_task_runner.py:112} INFO - Running: [''bas=
h'', ''-c'', u''airflow run generated_daily submit_operator 2017-08-05T00:00:00=
 --pickle 148 --job_id 15372 --raw'']
> [2017-08-07 23:24:39,426] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,426] {__init__.py:57} INFO - Using executor CeleryExecutor
> [2017-08-07 23:24:39,495] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,494] {driver.py:120} INFO - Generating grammar tables from /=
usr/lib/python2.7/lib2to3/Grammar.txt
> [2017-08-07 23:24:39,519] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,519] {driver.py:120} INFO - Generating grammar tables from /=
usr/lib/python2.7/lib2to3/PatternGrammar.txt
> [2017-08-07 23:24:39,710] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,710] {cli.py:391} INFO - Loading pickle id 148
> [2017-08-07 23:24:39,772] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,772] {models.py:1120} INFO - Dependencies not met for <TaskI=
nstance: generated_daily.submit_operator 2017-08-05 00:00:00 [running]>, de=
pendency ''Task Instance Not Already Running'' FAILED: Task is already runnin=
g, it started on 2017-08-07 20:55:12.727910.
> [2017-08-07 23:24:39,777] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,777] {models.py:1120} INFO - Dependencies not met for <TaskI=
nstance: generated_daily.submit_operator 2017-08-05 00:00:00 [running]>, de=
pendency ''Task Instance State'' FAILED: Task is in the ''running'' state which=
 is not a valid state for execution. The task must be cleared in order to b=
e run.
> [2017-08-07 23:24:39,907] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,907] {bash_operator.py:94} INFO - 17/08/07 23:24:39 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:40,908] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:40,908] {bash_operator.py:94} INFO - 17/08/07 23:24:40 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:41,909] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:41,909] {bash_operator.py:94} INFO - 17/08/07 23:24:41 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:42,911] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:42,910] {bash_operator.py:94} INFO - 17/08/07 23:24:42 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:43,912] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:43,912] {bash_operator.py:94} INFO - 17/08/07 23:24:43 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:44,120] {jobs.py:2148} WARNING - Recorded pid 116446 is=
 not a descendant of the current pid 33416
> [2017-08-07 23:24:44,914] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:44,913] {bash_operator.py:94} INFO - 17/08/07 23:24:44 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:45,914] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:45,914] {bash_operator.py:94} INFO - 17/08/07 23:24:45 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:46,032] {jobs.py:2155} WARNING - State of this instance=
 has been externally set to up_for_retry. Taking the poison pill. So long.
> [2017-08-07 23:24:46,081] {helpers.py:220} WARNING - Terminating descenda=
nt processes of [''/usr/bin/python /usr/local/bin/airflow run generated_dail=
y submit_operator 2017-08-05T00:00:00 --pickle 148 --job_id 15015 --raw''] P=
ID: 116446
> [2017-08-07 23:24:46,081] {helpers.py:224} WARNING - Terminating descenda=
nt process [''bash'', ''/tmp/airflowtmpCFeIBn/submit_operatorGbO01R''] PID: 116=
496
> [2017-08-07 23:24:46,087] {helpers.py:224} WARNING - Terminating descenda=
nt process [''/bin/sh'', ''/mnt/data/firework-0.2.0/bin/spark-submit'', ''--spar=
k-version'', ''1.6.2'', ''--master'', ''yarn-cluster'', ''--num-executors'', ''128'', =
''--class'', ''com.hulu.reco.metrics.middlelayer.BatchApplication'', ''--conf'', =
''spark.date=3D20170805'', ''--conf'', ''spark.yarn.maxAppAttempts=3D1'', ''--conf=
'', ''spark.driver.memory=3D8g'', ''--conf'', ''spark.executor.memory=3D36g'', ''--=
conf'', ''spark.executor.cores=3D8'', ''--conf'', ''spark.user.javahome.enabled=
=3Dtrue'', ''--conf'', ''spark.user.javahome.path=3D/usr/lib/jvm/hulu-oracle-jd=
k8'', ''--conf'', ''spark.output=3Dhdfs://warehousestore/user/pcdm/generated_pr=
od/20170805'', ''--queue'', ''spark'', ''/home/deploy/middlelayer-1.0-SNAPSHOT-ja=
r-with-dependencies.jar''] PID: 116498
> [2017-08-07 23:24:46,091] {helpers.py:224} WARNING - Terminating descenda=
nt process [''/usr/java/jdk8/jdk1.8.0_92-1//bin/java'', ''-cp'', ''/mnt/data/fir=
ework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db9770=
6/conf/:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b612=
0cd0a431aae3195db97706/assembly/target/scala-2.10/spark-assembly-1.6.2-hado=
op2.6.0-cdh5.7.3-201612201803.jar:/mnt/data/firework-0.2.0/.firework_cache/=
spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucle=
us-rdbms-3.2.9.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2=
/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucleus-core-3.2.10.=
jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0=
a431aae3195db97706/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/mnt/data=
/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db=
97706/hadoop-conf/'', ''org.apache.spark.deploy.SparkSubmit'', ''--master'', ''ya=
rn-cluster'', ''--conf'', ''spark.executor.memory=3D36g'', ''--conf'', ''spark.driv=
er.memory=3D8g'', ''--conf'', ''spark.user.javahome.path=3D/usr/lib/jvm/hulu-or=
acle-jdk8'', ''--conf'', ''spark.output=3Dhdfs://warehousestore/user/pcdm/gener=
ated_prod/20170805'', ''--conf'', ''spark.yarn.maxAppAttempts=3D1'', ''--conf'', ''=
spark.executor.cores=3D8'', ''--conf'', ''spark.date=3D20170805'', ''--conf'', ''sp=
ark.user.javahome.enabled=3Dtrue'', ''--class'', ''com.hulu.reco.metrics.middle=
layer.BatchApplication'', ''--num-executors'', ''128'', ''--queue'', ''spark'', ''/ho=
me/deploy/middlelayer-1.0-SNAPSHOT-jar-with-dependencies.jar''] PID: 116761
> [2017-08-07 23:24:46,096] {helpers.py:231} WARNING - Waiting up to 5s for=
 processes to exit...
> [2017-08-07 23:24:46,099] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,099] {bash_operator.py:94} INFO - 17/08/07 23:24:46 INFO uti=
l.ShutdownHookManager: Shutdown hook called
> [2017-08-07 23:24:46,100] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,100] {bash_operator.py:94} INFO - 17/08/07 23:24:46 INFO uti=
l.ShutdownHookManager: Deleting directory /mnt/data/tmp/spark-b5f3c795-7ea2=
-4ef1-a47c-2507b0747580
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,118] {bash_operator.py:97} INFO - Command exited with return=
 code -15
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,118] {models.py:1417} ERROR - Bash command failed
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: Traceb=
ack (most recent call last):
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in =
run
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask:     re=
sult =3D task_copy.execute(context=3Dcontext)
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/operators/bash_operator.py=
"", line 100, in execute
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask:     ra=
ise AirflowException(""Bash command failed"")
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: Airflo=
wException: Bash command failed
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,120] {models.py:1433} INFO - Marking task as UP_FOR_RETRY
> [2017-08-07 23:24:46,123] {helpers.py:234} WARNING - Done waiting
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,130] {models.py:1462} ERROR - Bash command failed
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: Traceb=
ack (most recent call last):
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/bin/airflow"", line 28, in <module>
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask:     ar=
gs.func(args)
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 422, in =
run
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:     po=
ol=3Dargs.pool,
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/utils/db.py"", line 53, in =
wrapper
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:     re=
sult =3D func(*args, **kwargs)
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in =
run
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:     re=
sult =3D task_copy.execute(context=3Dcontext)
> [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/operators/bash_operator.py=
"", line 100, in execute
> [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask:     ra=
ise AirflowException(""Bash command failed"")
> [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask: airflo=
w.exceptions.AirflowException: Bash command failed
> [2017-08-07 23:24:51,034] {jobs.py:2083} INFO - Task exited with return c=
ode 1
> [2017-08-07 23:33:27,872] {cli.py:391} INFO - Loading pickle id 148
> [2017-08-07 23:33:27,940] {base_task_runner.py:112} INFO - Running: [''bas=
h'', ''-c'', u''airflow run generated_daily submit_operator 2017-08-05T00:00:00=
 --pickle 148 --job_id 15376 --raw'']
> [2017-08-07 23:33:28,302] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,301] {__init__.py:57} INFO - Using executor CeleryExecutor
> [2017-08-07 23:33:28,359] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,358] {driver.py:120} INFO - Generating grammar tables from /=
usr/lib/python2.7/lib2to3/Grammar.txt
> [2017-08-07 23:33:28,378] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,378] {driver.py:120} INFO - Generating grammar tables from /=
usr/lib/python2.7/lib2to3/PatternGrammar.txt
> [2017-08-07 23:33:28,543] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,543] {cli.py:391} INFO - Loading pickle id 148
> [2017-08-07 23:33:28,601] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,601] {models.py:1120} INFO - Dependencies not met for <TaskI=
nstance: generated_daily.submit_operator 2017-08-05 00:00:00 [up_for_retry]=
>, dependency ''Not In Retry Period'' FAILED: Task is not ready for retry yet=
 but will be retried automatically. Current date is 2017-08-07T23:33:28.601=
339 and task will be retried at 2017-08-08T00:24:46.119758.
> [2017-08-07 23:33:32,954] {jobs.py:2083} INFO - Task exited with return c=
ode 0



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27484,54,JIRA.13093082.1502177433000.386710.1512465840492@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13093082.1502177433000@Atlassian.JIRA,,,2017-12-05 01:24:00-08,"[jira] [Resolved] (AIRFLOW-1494) backfill job failed because new
 retry comes out when a valid job is running","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1494?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1494.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

See linked issue.

> backfill job failed because new retry comes out when a valid job is runni=
ng
> -------------------------------------------------------------------------=
--
>
>                 Key: AIRFLOW-1494
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1494
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: backfill
>    Affects Versions: Airflow 1.8
>         Environment: ubuntu
>            Reporter: Xianping lin
>            Priority: Blocker
>             Fix For: 1.9.1
>
>
> I have a spark job, wrapped in a BASH command to run.
> From fail log, i found airflow try to rerun the job while the job is runn=
ing. Then a series strange things happened. My job finally failed. Logs Bel=
ow:
> [2017-08-07 23:24:35,903] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:35,903] {bash_operator.py:94} INFO - 17/08/07 23:24:35 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:36,904] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:36,904] {bash_operator.py:94} INFO - 17/08/07 23:24:36 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:37,905] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:37,904] {bash_operator.py:94} INFO - 17/08/07 23:24:37 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:38,906] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:38,906] {bash_operator.py:94} INFO - 17/08/07 23:24:38 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:38,947] {cli.py:391} INFO - Loading pickle id 148
> [2017-08-07 23:24:39,020] {base_task_runner.py:112} INFO - Running: [''bas=
h'', ''-c'', u''airflow run generated_daily submit_operator 2017-08-05T00:00:00=
 --pickle 148 --job_id 15372 --raw'']
> [2017-08-07 23:24:39,426] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,426] {__init__.py:57} INFO - Using executor CeleryExecutor
> [2017-08-07 23:24:39,495] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,494] {driver.py:120} INFO - Generating grammar tables from /=
usr/lib/python2.7/lib2to3/Grammar.txt
> [2017-08-07 23:24:39,519] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,519] {driver.py:120} INFO - Generating grammar tables from /=
usr/lib/python2.7/lib2to3/PatternGrammar.txt
> [2017-08-07 23:24:39,710] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,710] {cli.py:391} INFO - Loading pickle id 148
> [2017-08-07 23:24:39,772] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,772] {models.py:1120} INFO - Dependencies not met for <TaskI=
nstance: generated_daily.submit_operator 2017-08-05 00:00:00 [running]>, de=
pendency ''Task Instance Not Already Running'' FAILED: Task is already runnin=
g, it started on 2017-08-07 20:55:12.727910.
> [2017-08-07 23:24:39,777] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,777] {models.py:1120} INFO - Dependencies not met for <TaskI=
nstance: generated_daily.submit_operator 2017-08-05 00:00:00 [running]>, de=
pendency ''Task Instance State'' FAILED: Task is in the ''running'' state which=
 is not a valid state for execution. The task must be cleared in order to b=
e run.
> [2017-08-07 23:24:39,907] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:39,907] {bash_operator.py:94} INFO - 17/08/07 23:24:39 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:40,908] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:40,908] {bash_operator.py:94} INFO - 17/08/07 23:24:40 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:41,909] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:41,909] {bash_operator.py:94} INFO - 17/08/07 23:24:41 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:42,911] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:42,910] {bash_operator.py:94} INFO - 17/08/07 23:24:42 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:43,912] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:43,912] {bash_operator.py:94} INFO - 17/08/07 23:24:43 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:44,120] {jobs.py:2148} WARNING - Recorded pid 116446 is=
 not a descendant of the current pid 33416
> [2017-08-07 23:24:44,914] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:44,913] {bash_operator.py:94} INFO - 17/08/07 23:24:44 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:45,914] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:45,914] {bash_operator.py:94} INFO - 17/08/07 23:24:45 INFO yar=
n.Client: Application report for application_1498090868254_911911 (state: R=
UNNING)
> [2017-08-07 23:24:46,032] {jobs.py:2155} WARNING - State of this instance=
 has been externally set to up_for_retry. Taking the poison pill. So long.
> [2017-08-07 23:24:46,081] {helpers.py:220} WARNING - Terminating descenda=
nt processes of [''/usr/bin/python /usr/local/bin/airflow run generated_dail=
y submit_operator 2017-08-05T00:00:00 --pickle 148 --job_id 15015 --raw''] P=
ID: 116446
> [2017-08-07 23:24:46,081] {helpers.py:224} WARNING - Terminating descenda=
nt process [''bash'', ''/tmp/airflowtmpCFeIBn/submit_operatorGbO01R''] PID: 116=
496
> [2017-08-07 23:24:46,087] {helpers.py:224} WARNING - Terminating descenda=
nt process [''/bin/sh'', ''/mnt/data/firework-0.2.0/bin/spark-submit'', ''--spar=
k-version'', ''1.6.2'', ''--master'', ''yarn-cluster'', ''--num-executors'', ''128'', =
''--class'', ''com.hulu.reco.metrics.middlelayer.BatchApplication'', ''--conf'', =
''spark.date=3D20170805'', ''--conf'', ''spark.yarn.maxAppAttempts=3D1'', ''--conf=
'', ''spark.driver.memory=3D8g'', ''--conf'', ''spark.executor.memory=3D36g'', ''--=
conf'', ''spark.executor.cores=3D8'', ''--conf'', ''spark.user.javahome.enabled=
=3Dtrue'', ''--conf'', ''spark.user.javahome.path=3D/usr/lib/jvm/hulu-oracle-jd=
k8'', ''--conf'', ''spark.output=3Dhdfs://warehousestore/user/pcdm/generated_pr=
od/20170805'', ''--queue'', ''spark'', ''/home/deploy/middlelayer-1.0-SNAPSHOT-ja=
r-with-dependencies.jar''] PID: 116498
> [2017-08-07 23:24:46,091] {helpers.py:224} WARNING - Terminating descenda=
nt process [''/usr/java/jdk8/jdk1.8.0_92-1//bin/java'', ''-cp'', ''/mnt/data/fir=
ework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db9770=
6/conf/:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b612=
0cd0a431aae3195db97706/assembly/target/scala-2.10/spark-assembly-1.6.2-hado=
op2.6.0-cdh5.7.3-201612201803.jar:/mnt/data/firework-0.2.0/.firework_cache/=
spark/els/1.6.2/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucle=
us-rdbms-3.2.9.jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2=
/b72e04b6120cd0a431aae3195db97706/lib_managed/jars/datanucleus-core-3.2.10.=
jar:/mnt/data/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0=
a431aae3195db97706/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/mnt/data=
/firework-0.2.0/.firework_cache/spark/els/1.6.2/b72e04b6120cd0a431aae3195db=
97706/hadoop-conf/'', ''org.apache.spark.deploy.SparkSubmit'', ''--master'', ''ya=
rn-cluster'', ''--conf'', ''spark.executor.memory=3D36g'', ''--conf'', ''spark.driv=
er.memory=3D8g'', ''--conf'', ''spark.user.javahome.path=3D/usr/lib/jvm/hulu-or=
acle-jdk8'', ''--conf'', ''spark.output=3Dhdfs://warehousestore/user/pcdm/gener=
ated_prod/20170805'', ''--conf'', ''spark.yarn.maxAppAttempts=3D1'', ''--conf'', ''=
spark.executor.cores=3D8'', ''--conf'', ''spark.date=3D20170805'', ''--conf'', ''sp=
ark.user.javahome.enabled=3Dtrue'', ''--class'', ''com.hulu.reco.metrics.middle=
layer.BatchApplication'', ''--num-executors'', ''128'', ''--queue'', ''spark'', ''/ho=
me/deploy/middlelayer-1.0-SNAPSHOT-jar-with-dependencies.jar''] PID: 116761
> [2017-08-07 23:24:46,096] {helpers.py:231} WARNING - Waiting up to 5s for=
 processes to exit...
> [2017-08-07 23:24:46,099] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,099] {bash_operator.py:94} INFO - 17/08/07 23:24:46 INFO uti=
l.ShutdownHookManager: Shutdown hook called
> [2017-08-07 23:24:46,100] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,100] {bash_operator.py:94} INFO - 17/08/07 23:24:46 INFO uti=
l.ShutdownHookManager: Deleting directory /mnt/data/tmp/spark-b5f3c795-7ea2=
-4ef1-a47c-2507b0747580
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,118] {bash_operator.py:97} INFO - Command exited with return=
 code -15
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,118] {models.py:1417} ERROR - Bash command failed
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: Traceb=
ack (most recent call last):
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in =
run
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask:     re=
sult =3D task_copy.execute(context=3Dcontext)
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/operators/bash_operator.py=
"", line 100, in execute
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask:     ra=
ise AirflowException(""Bash command failed"")
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: Airflo=
wException: Bash command failed
> [2017-08-07 23:24:46,121] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,120] {models.py:1433} INFO - Marking task as UP_FOR_RETRY
> [2017-08-07 23:24:46,123] {helpers.py:234} WARNING - Done waiting
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:24:46,130] {models.py:1462} ERROR - Bash command failed
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask: Traceb=
ack (most recent call last):
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/bin/airflow"", line 28, in <module>
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask:     ar=
gs.func(args)
> [2017-08-07 23:24:46,131] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 422, in =
run
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:     po=
ol=3Dargs.pool,
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/utils/db.py"", line 53, in =
wrapper
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:     re=
sult =3D func(*args, **kwargs)
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/models.py"", line 1374, in =
run
> [2017-08-07 23:24:46,132] {base_task_runner.py:95} INFO - Subtask:     re=
sult =3D task_copy.execute(context=3Dcontext)
> [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask:   File=
 ""/usr/local/lib/python2.7/dist-packages/airflow/operators/bash_operator.py=
"", line 100, in execute
> [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask:     ra=
ise AirflowException(""Bash command failed"")
> [2017-08-07 23:24:46,133] {base_task_runner.py:95} INFO - Subtask: airflo=
w.exceptions.AirflowException: Bash command failed
> [2017-08-07 23:24:51,034] {jobs.py:2083} INFO - Task exited with return c=
ode 1
> [2017-08-07 23:33:27,872] {cli.py:391} INFO - Loading pickle id 148
> [2017-08-07 23:33:27,940] {base_task_runner.py:112} INFO - Running: [''bas=
h'', ''-c'', u''airflow run generated_daily submit_operator 2017-08-05T00:00:00=
 --pickle 148 --job_id 15376 --raw'']
> [2017-08-07 23:33:28,302] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,301] {__init__.py:57} INFO - Using executor CeleryExecutor
> [2017-08-07 23:33:28,359] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,358] {driver.py:120} INFO - Generating grammar tables from /=
usr/lib/python2.7/lib2to3/Grammar.txt
> [2017-08-07 23:33:28,378] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,378] {driver.py:120} INFO - Generating grammar tables from /=
usr/lib/python2.7/lib2to3/PatternGrammar.txt
> [2017-08-07 23:33:28,543] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,543] {cli.py:391} INFO - Loading pickle id 148
> [2017-08-07 23:33:28,601] {base_task_runner.py:95} INFO - Subtask: [2017-=
08-07 23:33:28,601] {models.py:1120} INFO - Dependencies not met for <TaskI=
nstance: generated_daily.submit_operator 2017-08-05 00:00:00 [up_for_retry]=
>, dependency ''Not In Retry Period'' FAILED: Task is not ready for retry yet=
 but will be retried automatically. Current date is 2017-08-07T23:33:28.601=
339 and task will be retried at 2017-08-08T00:24:46.119758.
> [2017-08-07 23:33:32,954] {jobs.py:2083} INFO - Task exited with return c=
ode 0



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27485,54,JIRA.13099086.1504215299000.386715.1512465900327@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13099086.1504215299000@Atlassian.JIRA,,,2017-12-05 01:25:00-08,"[jira] [Resolved] (AIRFLOW-1555) Backfill job gets killed 1 hour
 after starting","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1555?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1555.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Solved by setting visibility timeout longer than ETA of tasks.

> Backfill job gets killed 1 hour after starting
> ----------------------------------------------
>
>                 Key: AIRFLOW-1555
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1555
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: backfill
>    Affects Versions: 1.8.1
>         Environment: Airflow 1.8.1
> Celery 3.1.23 with one coordinator, redis and 3 workers
> Python 3.5.2
> Debian GNU/Linux 8.9 (jessie)
> snakebite uninstalled because it does not work with Python 3.5.2
> MySQL 5.6
>            Reporter: Shreyas Joshi
>             Fix For: 1.9.1
>
>
> *What happens?*
> After running for an hour tasks in a backfill die. The task log shows:
> {code}
> ...
> [2017-08-31 06:48:06,425] {jobs.py:2172} WARNING - Recorded pid 5451 is not a descendant of the current pid 21571
> [2017-08-31 06:48:11,884] {jobs.py:2179} WARNING - State of this instance has been externally set to failed. Taking the poison pill. So long.
> [2017-08-31 06:48:11,892] {helpers.py:220} WARNING - Terminating descendant processes of [<REDACTED>] PID: 5451
> [2017-08-31 06:48:11,892] {helpers.py:224} WARNING - Terminating descendant process [<REDACTED>] PID: 5459
> [2017-08-31 06:48:11,896] {helpers.py:231} WARNING - Waiting up to 5s for processes to exit...
> ...
> {code}
> The backfill log shows:
> {code}
> ...
> [2017-08-31 11:23:44,025] {jobs.py:1729} ERROR - Executor reports task instance <TaskInstance: dag_name.task_name 2017-08-30 02:00:00 [running]> finished (failed) although the task says its running. Was the task killed externally?
> [2017-08-31 11:23:44,025] {models.py:1427} ERROR - Executor reports task instance <TaskInstance: analytics_events.page_views 2017-08-30 02:00:00 [running]> finished (failed) although the task says its running. Was the task killed externally?
> ...
> {code}
> The Celery UI has the following exception, but status shows ""success""
> {code}
> Traceback (most recent call last):
>   File ""/data/airflow-sources/.venv/lib/python3.5/site-packages/airflow/executors/celery_executor.py"", line 56, in execute_command
>     subprocess.check_call(command, shell=True)
>   File ""/usr/share/pyenv/versions/3.5.2/lib/python3.5/subprocess.py"", line 581, in check_call
>     raise CalledProcessError(retcode, cmd)
> subprocess.CalledProcessError: Command ''airflow run dag_name task_name 2017-08-30T02:00:00 --pickle 14 --local'' returned non-zero exit status 1
> During handling of the above exception, another exception occurred:
> Traceback (most recent call last):
>   File ""/data/airflow-sources/.venv/lib/python3.5/site-packages/celery/app/trace.py"", line 240, in trace_task
>     R = retval = fun(*args, **kwargs)
>   File ""/data/airflow-sources/.venv/lib/python3.5/site-packages/celery/app/trace.py"", line 438, in __protected_call__
>     return self.run(*args, **kwargs)
>   File ""/data/airflow-sources/.venv/lib/python3.5/site-packages/airflow/executors/celery_executor.py"", line 59, in execute_command
>     raise AirflowException(''Celery command failed'')
> airflow.exceptions.AirflowException: Celery command failed
> {code}
> The tasks have timeouts explicitly set to 6 hours and SLA set to 5 hours. In the course of debugging this I also set dagrun_timeout to 6 hours. It did not make a difference.
> Here is a thread on [stackoverflow | https://stackoverflow.com/questions/44274381/airflow-long-running-task-in-subdag-marked-as-failed-after-an-hour] that talks about a very similar issue.
> These tasks run fine on our older Airflow 1.7. This is currently blocking our upgrade.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27486,54,JIRA.13065597.1492704317000.386724.1512466020171@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13065597.1492704317000@Atlassian.JIRA,,,2017-12-05 01:27:00-08,"[jira] [Closed] (AIRFLOW-1131) DockerOperator jobs time out and get
 stuck in ""running"" forever","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1131?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin closed AIRFLOW-1131.
-----------------------------------
    Resolution: Invalid

Docker python client issues are outside the scope of Airflow. 

> DockerOperator jobs time out and get stuck in ""running"" forever
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1131
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1131
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0
>         Environment: Python 2.7.12
> git+git://github.com/apache/incubator-airflow.git@35e43f5067f4741640278b765c0e54e4fd45ffa3#egg=airflow[async,password,celery,crypto,postgres,hive,hdfs,jdbc]
>            Reporter: Vitor Baptista
>
> With the following DAG and task:
> {code}
> import os
> from datetime import datetime, timedelta
> from airflow.models import DAG
> from airflow.operators.docker_operator import DockerOperator
> default_args = {
>     ''owner'': ''airflow'',
>     ''depends_on_past'': False,
>     ''start_date'': datetime(2017, 1, 1),
>     ''retries'': 3,
>     ''retry_delay'': timedelta(minutes=10),
> }
> dag = DAG(
>     dag_id=''smoke_test'',
>     default_args=default_args,
>     max_active_runs=1,
>     schedule_interval=''@daily''
> )
> sleep_forever_task = DockerOperator(
>     task_id=''sleep_forever'',
>     dag=dag,
>     image=''alpine:latest'',
>     api_version=os.environ.get(''DOCKER_API_VERSION'', ''1.23''),
>     command=''sleep {}''.format(60 * 60 * 24),
> )
> {code}
> When I run it, this is what I get:
> {code}
> *** Log file isn''t local.
> *** Fetching here: http://589ea17432ec:8793/log/smoke_test/sleep_forever/2017-04-18T00:00:00
> [2017-04-20 11:19:58,258] {models.py:172} INFO - Filling up the DagBag from /usr/local/airflow/dags/smoke_test.py
> [2017-04-20 11:19:58,438] {base_task_runner.py:112} INFO - Running: [''bash'', ''-c'', u''airflow run smoke_test sleep_forever 2017-04-18T00:00:00 --job_id 2537 --raw -sd DAGS_FOLDER/smoke_test.py'']
> [2017-04-20 11:19:58,888] {base_task_runner.py:95} INFO - Subtask: /usr/local/airflow/src/airflow/airflow/configuration.py:128: DeprecationWarning: This method will be removed in future versions.  Use ''parser.read_file()'' instead.
> [2017-04-20 11:19:58,888] {base_task_runner.py:95} INFO - Subtask:   self.readfp(StringIO.StringIO(string))
> [2017-04-20 11:19:59,214] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,214] {__init__.py:56} INFO - Using executor CeleryExecutor
> [2017-04-20 11:19:59,227] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,227] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt
> [2017-04-20 11:19:59,244] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,244] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt
> [2017-04-20 11:19:59,377] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,377] {models.py:172} INFO - Filling up the DagBag from /usr/local/airflow/dags/smoke_test.py
> [2017-04-20 11:19:59,597] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,597] {models.py:1146} INFO - Dependencies all met for <TaskInstance: smoke_test.sleep_forever 2017-04-18 00:00:00 [queued]>
> [2017-04-20 11:19:59,605] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,605] {models.py:1146} INFO - Dependencies all met for <TaskInstance: smoke_test.sleep_forever 2017-04-18 00:00:00 [queued]>
> [2017-04-20 11:19:59,605] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,605] {models.py:1338} INFO - 
> [2017-04-20 11:19:59,606] {base_task_runner.py:95} INFO - Subtask: --------------------------------------------------------------------------------
> [2017-04-20 11:19:59,606] {base_task_runner.py:95} INFO - Subtask: Starting attempt 1 of 4
> [2017-04-20 11:19:59,606] {base_task_runner.py:95} INFO - Subtask: --------------------------------------------------------------------------------
> [2017-04-20 11:19:59,606] {base_task_runner.py:95} INFO - Subtask: 
> [2017-04-20 11:19:59,620] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,620] {models.py:1362} INFO - Executing <Task(DockerOperator): sleep_forever> on 2017-04-18 00:00:00
> [2017-04-20 11:19:59,662] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,661] {docker_operator.py:132} INFO - Starting docker container from image alpine:latest
> [2017-04-20 12:21:25,661] {models.py:172} INFO - Filling up the DagBag from /usr/local/airflow/dags/smoke_test.py
> [2017-04-20 12:21:25,809] {base_task_runner.py:112} INFO - Running: [''bash'', ''-c'', u''airflow run smoke_test sleep_forever 2017-04-18T00:00:00 --job_id 2574 --raw -sd DAGS_FOLDER/smoke_test.py'']
> [2017-04-20 12:21:26,117] {base_task_runner.py:95} INFO - Subtask: /usr/local/airflow/src/airflow/airflow/configuration.py:128: DeprecationWarning: This method will be removed in future versions.  Use ''parser.read_file()'' instead.
> [2017-04-20 12:21:26,117] {base_task_runner.py:95} INFO - Subtask:   self.readfp(StringIO.StringIO(string))
> [2017-04-20 12:21:26,301] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,301] {__init__.py:56} INFO - Using executor CeleryExecutor
> [2017-04-20 12:21:26,310] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,310] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt
> [2017-04-20 12:21:26,324] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,324] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt
> [2017-04-20 12:21:26,426] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,426] {models.py:172} INFO - Filling up the DagBag from /usr/local/airflow/dags/smoke_test.py
> [2017-04-20 12:21:26,564] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,563] {models.py:1140} INFO - Dependencies not met for <TaskInstance: smoke_test.sleep_forever 2017-04-18 00:00:00 [running]>, dependency ''Task Instance State'' FAILED: Task is in the ''running'' state which is not a valid state for execution. The task must be cleared in order to be run.
> [2017-04-20 12:21:26,564] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,564] {models.py:1140} INFO - Dependencies not met for <TaskInstance: smoke_test.sleep_forever 2017-04-18 00:00:00 [running]>, dependency ''Task Instance Not Already Running'' FAILED: Task is already running, it started on 2017-04-20 11:19:59.597425.
> [2017-04-20 12:21:30,821] {jobs.py:2163} WARNING - Recorded pid 113 is not a descendant of the current pid 178
> {code}
> Note that it runs the task normally, and after about 1 hour it tries to re-load the task, running it again, but then fails because the {{subprocess}} started isn''t a child of the current process. After this, the task is still in {{running}} state, never changing to {{failed}}.
> I haven''t tested, but I suspect this doesn''t happen if the task keeps printing something.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27487,54,JIRA.13065597.1492704317000.386741.1512466080110@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13065597.1492704317000@Atlassian.JIRA,,,2017-12-05 01:28:00-08,"[jira] [Commented] (AIRFLOW-1131) DockerOperator jobs time out and
 get stuck in ""running"" forever","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1131?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278270#comment-16278270 ] 

Bolke de Bruin commented on AIRFLOW-1131:
-----------------------------------------

Please see linked issue for configuration options for Celery in case it is a celery issue. Otherwise it is out of scope.

> DockerOperator jobs time out and get stuck in ""running"" forever
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1131
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1131
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: scheduler
>    Affects Versions: 1.9.0
>         Environment: Python 2.7.12
> git+git://github.com/apache/incubator-airflow.git@35e43f5067f4741640278b765c0e54e4fd45ffa3#egg=airflow[async,password,celery,crypto,postgres,hive,hdfs,jdbc]
>            Reporter: Vitor Baptista
>
> With the following DAG and task:
> {code}
> import os
> from datetime import datetime, timedelta
> from airflow.models import DAG
> from airflow.operators.docker_operator import DockerOperator
> default_args = {
>     ''owner'': ''airflow'',
>     ''depends_on_past'': False,
>     ''start_date'': datetime(2017, 1, 1),
>     ''retries'': 3,
>     ''retry_delay'': timedelta(minutes=10),
> }
> dag = DAG(
>     dag_id=''smoke_test'',
>     default_args=default_args,
>     max_active_runs=1,
>     schedule_interval=''@daily''
> )
> sleep_forever_task = DockerOperator(
>     task_id=''sleep_forever'',
>     dag=dag,
>     image=''alpine:latest'',
>     api_version=os.environ.get(''DOCKER_API_VERSION'', ''1.23''),
>     command=''sleep {}''.format(60 * 60 * 24),
> )
> {code}
> When I run it, this is what I get:
> {code}
> *** Log file isn''t local.
> *** Fetching here: http://589ea17432ec:8793/log/smoke_test/sleep_forever/2017-04-18T00:00:00
> [2017-04-20 11:19:58,258] {models.py:172} INFO - Filling up the DagBag from /usr/local/airflow/dags/smoke_test.py
> [2017-04-20 11:19:58,438] {base_task_runner.py:112} INFO - Running: [''bash'', ''-c'', u''airflow run smoke_test sleep_forever 2017-04-18T00:00:00 --job_id 2537 --raw -sd DAGS_FOLDER/smoke_test.py'']
> [2017-04-20 11:19:58,888] {base_task_runner.py:95} INFO - Subtask: /usr/local/airflow/src/airflow/airflow/configuration.py:128: DeprecationWarning: This method will be removed in future versions.  Use ''parser.read_file()'' instead.
> [2017-04-20 11:19:58,888] {base_task_runner.py:95} INFO - Subtask:   self.readfp(StringIO.StringIO(string))
> [2017-04-20 11:19:59,214] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,214] {__init__.py:56} INFO - Using executor CeleryExecutor
> [2017-04-20 11:19:59,227] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,227] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt
> [2017-04-20 11:19:59,244] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,244] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt
> [2017-04-20 11:19:59,377] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,377] {models.py:172} INFO - Filling up the DagBag from /usr/local/airflow/dags/smoke_test.py
> [2017-04-20 11:19:59,597] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,597] {models.py:1146} INFO - Dependencies all met for <TaskInstance: smoke_test.sleep_forever 2017-04-18 00:00:00 [queued]>
> [2017-04-20 11:19:59,605] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,605] {models.py:1146} INFO - Dependencies all met for <TaskInstance: smoke_test.sleep_forever 2017-04-18 00:00:00 [queued]>
> [2017-04-20 11:19:59,605] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,605] {models.py:1338} INFO - 
> [2017-04-20 11:19:59,606] {base_task_runner.py:95} INFO - Subtask: --------------------------------------------------------------------------------
> [2017-04-20 11:19:59,606] {base_task_runner.py:95} INFO - Subtask: Starting attempt 1 of 4
> [2017-04-20 11:19:59,606] {base_task_runner.py:95} INFO - Subtask: --------------------------------------------------------------------------------
> [2017-04-20 11:19:59,606] {base_task_runner.py:95} INFO - Subtask: 
> [2017-04-20 11:19:59,620] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,620] {models.py:1362} INFO - Executing <Task(DockerOperator): sleep_forever> on 2017-04-18 00:00:00
> [2017-04-20 11:19:59,662] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 11:19:59,661] {docker_operator.py:132} INFO - Starting docker container from image alpine:latest
> [2017-04-20 12:21:25,661] {models.py:172} INFO - Filling up the DagBag from /usr/local/airflow/dags/smoke_test.py
> [2017-04-20 12:21:25,809] {base_task_runner.py:112} INFO - Running: [''bash'', ''-c'', u''airflow run smoke_test sleep_forever 2017-04-18T00:00:00 --job_id 2574 --raw -sd DAGS_FOLDER/smoke_test.py'']
> [2017-04-20 12:21:26,117] {base_task_runner.py:95} INFO - Subtask: /usr/local/airflow/src/airflow/airflow/configuration.py:128: DeprecationWarning: This method will be removed in future versions.  Use ''parser.read_file()'' instead.
> [2017-04-20 12:21:26,117] {base_task_runner.py:95} INFO - Subtask:   self.readfp(StringIO.StringIO(string))
> [2017-04-20 12:21:26,301] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,301] {__init__.py:56} INFO - Using executor CeleryExecutor
> [2017-04-20 12:21:26,310] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,310] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/Grammar.txt
> [2017-04-20 12:21:26,324] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,324] {driver.py:120} INFO - Generating grammar tables from /usr/lib/python2.7/lib2to3/PatternGrammar.txt
> [2017-04-20 12:21:26,426] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,426] {models.py:172} INFO - Filling up the DagBag from /usr/local/airflow/dags/smoke_test.py
> [2017-04-20 12:21:26,564] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,563] {models.py:1140} INFO - Dependencies not met for <TaskInstance: smoke_test.sleep_forever 2017-04-18 00:00:00 [running]>, dependency ''Task Instance State'' FAILED: Task is in the ''running'' state which is not a valid state for execution. The task must be cleared in order to be run.
> [2017-04-20 12:21:26,564] {base_task_runner.py:95} INFO - Subtask: [2017-04-20 12:21:26,564] {models.py:1140} INFO - Dependencies not met for <TaskInstance: smoke_test.sleep_forever 2017-04-18 00:00:00 [running]>, dependency ''Task Instance Not Already Running'' FAILED: Task is already running, it started on 2017-04-20 11:19:59.597425.
> [2017-04-20 12:21:30,821] {jobs.py:2163} WARNING - Recorded pid 113 is not a descendant of the current pid 178
> {code}
> Note that it runs the task normally, and after about 1 hour it tries to re-load the task, running it again, but then fails because the {{subprocess}} started isn''t a child of the current process. After this, the task is still in {{running}} state, never changing to {{failed}}.
> I haven''t tested, but I suspect this doesn''t happen if the task keeps printing something.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27488,54,JIRA.13063818.1492095717000.388004.1512478980231@Atlassian.JIRA,1913,Xi Wang (JIRA),JIRA.13063818.1492095717000@Atlassian.JIRA,,,2017-12-05 05:03:00-08,"[jira] [Commented] (AIRFLOW-1110) Error dag_id could not be found
 when dag_id exists","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1110?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278505#comment-16278505 ] 

Xi Wang commented on AIRFLOW-1110:
----------------------------------

I ran into similar issues, check if you have errors from your dag py file, e.g. import error. This may interrupt the scanning process of Airflow in the dag folder which result in some dags are ignored. you can verify it manually by runing airflow list_dags to see if anything missing from the list.

> Error dag_id could not be found when dag_id exists
> --------------------------------------------------
>
>                 Key: AIRFLOW-1110
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1110
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DAG
>    Affects Versions: Airflow 1.8
>         Environment: Airflow 1.8, celery redis
>            Reporter: laomaotou
>
> In my tag, there are four tasks with this structure below:
> [t1, t2] -> [t3, t4]
> When this tag runs, t1 and t2 run successfully, but t3 and t4 are queued, but with ""AirflowException: dag_id could not be found.Either the dag did not exist or it failed to parse."" And then, I retry to run task t3 and t4, there is no error. I don''t know the reason. And this error doesn''t arrive every time, sometime all tasks run successfully.
> Could you help me ?



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27489,54,JIRA.13103258.1505832682000.389289.1512486600147@Atlassian.JIRA,2412,Milan van der Meer (JIRA),JIRA.13103258.1505832682000@Atlassian.JIRA,,,2017-12-05 07:10:00-08,"[jira] [Commented] (AIRFLOW-1623) Clearing task in UI does not
 trigger on_kill method in operator","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1623?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16278696#comment-16278696 ] 

Milan van der Meer commented on AIRFLOW-1623:
---------------------------------------------

This is a big issue for us atm. For example: when submitting a spark job to a cluster, it will not be killed when you clear the job through the UI because of this bug.

> Clearing task in UI does not trigger on_kill method in operator
> ---------------------------------------------------------------
>
>                 Key: AIRFLOW-1623
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1623
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: operators
>    Affects Versions: Airflow 2.0
>            Reporter: Richard Penman
>            Priority: Minor
>             Fix For: Airflow 2.0
>
>
> When a task is cleared in the UI it doesn''t call the [operators on_kill() method|https://github.com/apache/incubator-airflow/blob/b2e1753f5b74ad1b6e0889f7b784ce69623c95ce/airflow/models.py#L2380] to clean up the task. Apparently this is meant to be handled in the [LocalTaskJob.on_kill()|https://github.com/apache/incubator-airflow/blob/b2e1753f5b74ad1b6e0889f7b784ce69623c95ce/airflow/jobs.py#L2512] method, however it is not currently.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27490,54,JIRA.13098871.1504184947000.391503.1512499200583@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13098871.1504184947000@Atlassian.JIRA,,,2017-12-05 10:40:00-08,"[jira] [Commented] (AIRFLOW-1554) Wrong DagFileProcessor  method
 name call","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1554?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16279020#comment-16279020 ] 

ASF subversion and git services commented on AIRFLOW-1554:
----------------------------------------------------------

Commit ff0d75f062a34c267f0902c1d4c7b148ba4b490a in incubator-airflow''s branch refs/heads/master from Paulius
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=ff0d75f ]

[AIRFLOW-1554] Fix wrong DagFileProcessor termination method call

Closes #2821 from
pdambrauskas/fix/wrong_termination_call


> Wrong DagFileProcessor  method name call
> ----------------------------------------
>
>                 Key: AIRFLOW-1554
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1554
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Thiago Rigo
>
> Inside DagFileProcessorManager, on the method set_file_paths(), it tries to call the method stop() from DagFileProcessor, which doesn''t exist. The correct name is terminate().
> This issues causes the scheduler to hang and not take anymore tasks.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27491,54,JIRA.13098871.1504184947000.391507.1512499200610@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13098871.1504184947000@Atlassian.JIRA,,,2017-12-05 10:40:00-08,"[jira] [Commented] (AIRFLOW-1554) Wrong DagFileProcessor  method
 name call","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1554?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16279022#comment-16279022 ] 

ASF subversion and git services commented on AIRFLOW-1554:
----------------------------------------------------------

Commit 1b210337e024b044c9c11f2950a9f3c2368ee47c in incubator-airflow''s branch refs/heads/v1-9-test from Paulius
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=1b21033 ]

[AIRFLOW-1554] Fix wrong DagFileProcessor termination method call

Closes #2821 from
pdambrauskas/fix/wrong_termination_call

(cherry picked from commit ff0d75f062a34c267f0902c1d4c7b148ba4b490a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Wrong DagFileProcessor  method name call
> ----------------------------------------
>
>                 Key: AIRFLOW-1554
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1554
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Thiago Rigo
>
> Inside DagFileProcessorManager, on the method set_file_paths(), it tries to call the method stop() from DagFileProcessor, which doesn''t exist. The correct name is terminate().
> This issues causes the scheduler to hang and not take anymore tasks.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27492,54,JIRA.13098871.1504184947000.391515.1512499260235@Atlassian.JIRA,1275,Bolke de Bruin (JIRA),JIRA.13098871.1504184947000@Atlassian.JIRA,,,2017-12-05 10:41:00-08,"[jira] [Resolved] (AIRFLOW-1554) Wrong DagFileProcessor  method
 name call","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1554?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Bolke de Bruin resolved AIRFLOW-1554.
-------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.0

Issue resolved by pull request #2821
[https://github.com/apache/incubator-airflow/pull/2821]

> Wrong DagFileProcessor  method name call
> ----------------------------------------
>
>                 Key: AIRFLOW-1554
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1554
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Thiago Rigo
>             Fix For: 1.9.0
>
>
> Inside DagFileProcessorManager, on the method set_file_paths(), it tries to call the method stop() from DagFileProcessor, which doesn''t exist. The correct name is terminate().
> This issues causes the scheduler to hang and not take anymore tasks.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27493,54,JIRA.13098871.1504184947000.391512.1512499260212@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13098871.1504184947000@Atlassian.JIRA,,,2017-12-05 10:41:00-08,"[jira] [Commented] (AIRFLOW-1554) Wrong DagFileProcessor  method
 name call","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1554?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16279023#comment-16279023 ] 

ASF subversion and git services commented on AIRFLOW-1554:
----------------------------------------------------------

Commit 44adde47d4dd2ed26c166caee36176af1fc61333 in incubator-airflow''s branch refs/heads/v1-9-stable from Paulius
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=44adde4 ]

[AIRFLOW-1554] Fix wrong DagFileProcessor termination method call

Closes #2821 from
pdambrauskas/fix/wrong_termination_call

(cherry picked from commit ff0d75f062a34c267f0902c1d4c7b148ba4b490a)
Signed-off-by: Bolke de Bruin <bolke@xs4all.nl>


> Wrong DagFileProcessor  method name call
> ----------------------------------------
>
>                 Key: AIRFLOW-1554
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1554
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Thiago Rigo
>             Fix For: 1.9.0
>
>
> Inside DagFileProcessorManager, on the method set_file_paths(), it tries to call the method stop() from DagFileProcessor, which doesn''t exist. The correct name is terminate().
> This issues causes the scheduler to hang and not take anymore tasks.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27494,54,JIRA.13122258.1512158366000.391827.1512501840473@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122258.1512158366000@Atlassian.JIRA,,,2017-12-05 11:24:00-08,"[jira] [Commented] (AIRFLOW-1876) Subtask logs are not easily
 distinguised","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1876?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16279080#comment-16279080 ] 

ASF subversion and git services commented on AIRFLOW-1876:
----------------------------------------------------------

Commit a9ceca5e04d3bdfbd34adc4ef5bf95c5cbf5c954 in incubator-airflow''s branch refs/heads/master from [~wrp]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=a9ceca5 ]

[AIRFLOW-1876] Write subtask id to task log header

Closes #2835 from wrp/subtask-id


> Subtask logs are not easily distinguised
> ----------------------------------------
>
>                 Key: AIRFLOW-1876
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1876
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core
>            Reporter: William Pursell
>            Assignee: William Pursell
>            Priority: Minor
>             Fix For: 1.10.0
>
>   Original Estimate: 0.25h
>  Remaining Estimate: 0.25h
>
> Currently, when the scheduler is outputting all subtask logs to the same stream, it is not easy to distinguish which logs come from which task.  It would be nice if there were some convenient way to filter the logs from a given task.  For example, putting the task id after the word ''Subtask''
> For example:
> diff --git a/airflow/task_runner/base_task_runner.py b/airflow/task_runner/base_task_runner.py                                 
> index bc0edcf3..e40f6ea9 100644                                
> --- a/airflow/task_runner/base_task_runner.py                  
> +++ b/airflow/task_runner/base_task_runner.py                  
> @@ -95,7 +95,11 @@ class BaseTaskRunner(LoggingMixin):         
>                  line = line.decode(''utf-8'')                   
>              if len(line) == 0:                                
>                  break                                         
> -            self.log.info(u''Subtask %%%%s: %%%%s'', self._task_instance, line.rstrip(''\n''))                                          
> +            self.log.info(                                    
> +                u''Subtask %%%%d: %%%%s'',                            
> +                self._task_instance.job_id,                   
> +                line.rstrip(''\n'')                             
> +            )                                                 
>                                                                
>      def run_command(self, run_with, join_args=False):         
>          """""" 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27495,54,JIRA.13122258.1512158366000.391830.1512501840493@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13122258.1512158366000@Atlassian.JIRA,,,2017-12-05 11:24:00-08,"[jira] [Resolved] (AIRFLOW-1876) Subtask logs are not easily
 distinguised","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1876?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1876.
--------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.10.0

> Subtask logs are not easily distinguised
> ----------------------------------------
>
>                 Key: AIRFLOW-1876
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1876
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core
>            Reporter: William Pursell
>            Assignee: William Pursell
>            Priority: Minor
>             Fix For: 1.10.0
>
>   Original Estimate: 0.25h
>  Remaining Estimate: 0.25h
>
> Currently, when the scheduler is outputting all subtask logs to the same stream, it is not easy to distinguish which logs come from which task.  It would be nice if there were some convenient way to filter the logs from a given task.  For example, putting the task id after the word ''Subtask''
> For example:
> diff --git a/airflow/task_runner/base_task_runner.py b/airflow/task_runner/base_task_runner.py                                 
> index bc0edcf3..e40f6ea9 100644                                
> --- a/airflow/task_runner/base_task_runner.py                  
> +++ b/airflow/task_runner/base_task_runner.py                  
> @@ -95,7 +95,11 @@ class BaseTaskRunner(LoggingMixin):         
>                  line = line.decode(''utf-8'')                   
>              if len(line) == 0:                                
>                  break                                         
> -            self.log.info(u''Subtask %%%%s: %%%%s'', self._task_instance, line.rstrip(''\n''))                                          
> +            self.log.info(                                    
> +                u''Subtask %%%%d: %%%%s'',                            
> +                self._task_instance.job_id,                   
> +                line.rstrip(''\n'')                             
> +            )                                                 
>                                                                
>      def run_command(self, run_with, join_args=False):         
>          """""" 



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27496,54,JIRA.13121798.1512004701000.391860.1512501900635@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13121798.1512004701000@Atlassian.JIRA,,,2017-12-05 11:25:00-08,"[jira] [Commented] (AIRFLOW-1869) Logging in gcs_task_handler
 discards too many error messages","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1869?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16279086#comment-16279086 ] 

ASF subversion and git services commented on AIRFLOW-1869:
----------------------------------------------------------

Commit 06b41fbe1bf94ca2013fe164ee275f9fbac92973 in incubator-airflow''s branch refs/heads/master from [~wrp]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=06b41fb ]

[AIRFLOW-1869] Write more error messages into gcs and file logs

Closes #2826 from wrp/gcs-log


> Logging in gcs_task_handler discards too many error messages
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1869
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1869
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core
>            Reporter: William Pursell
>            Assignee: William Pursell
>            Priority: Minor
>             Fix For: Airflow 2.0
>
>
> Many exceptions are caught and effectively discarded in the gcs task log reader.  It makes debugging difficult.  The logs should be more verbose and include the exception strings.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27497,54,JIRA.13121798.1512004701000.391863.1512501960192@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.13121798.1512004701000@Atlassian.JIRA,,,2017-12-05 11:26:00-08,"[jira] [Resolved] (AIRFLOW-1869) Logging in gcs_task_handler
 discards too many error messages","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1869?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Chris Riccomini resolved AIRFLOW-1869.
--------------------------------------
       Resolution: Fixed
    Fix Version/s:     (was: Airflow 2.0)
                   1.10.0

> Logging in gcs_task_handler discards too many error messages
> ------------------------------------------------------------
>
>                 Key: AIRFLOW-1869
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1869
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: core
>            Reporter: William Pursell
>            Assignee: William Pursell
>            Priority: Minor
>             Fix For: 1.10.0
>
>
> Many exceptions are caught and effectively discarded in the gcs task log reader.  It makes debugging difficult.  The logs should be more verbose and include the exception strings.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27498,54,JIRA.13122969.1512502776000.391989.1512502800042@Atlassian.JIRA,2427,Oleg Yamin (JIRA),JIRA.13122969.1512502776000@Atlassian.JIRA,,,2017-12-05 11:40:00-08,"[jira] [Created] (AIRFLOW-1886) Failed jobs are not being counted
 towards max_active_runs_per_dag","Oleg Yamin created AIRFLOW-1886:
-----------------------------------

             Summary: Failed jobs are not being counted towards max_active_runs_per_dag
                 Key: AIRFLOW-1886
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1886
             Project: Apache Airflow
          Issue Type: Bug
          Components: DagRun
    Affects Versions: 1.8.1
            Reporter: Oleg Yamin


Currently, I have setup max_active_runs_per_dag = 2 in airflow.cfg but when a DAG aborts, it will keep submitting next DAG in the queue not counting the current incomplete DAG that is already in the queue. I am using 1.8.1 but i see that the jobs.py in latest version is still not addressing this issue.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
730827,24,154579597857.12692.1528283330032065524.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-12-25 19:46:18-08,"[GitHub] pengzhao-intel edited a comment on issue #13088: make ROIAlign
 support position-sensitive pooling","pengzhao-intel edited a comment on issue #13088: make ROIAlign support position-sensitive pooling
URL: https://github.com/apache/incubator-mxnet/pull/13088#issuecomment-449897970
 
 
   Seems the problem of connection. Please try to retrigger the CI by adding a space/blank line in the code.

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27499,54,JIRA.12963504.1461854580000.392554.1512508500121@Atlassian.JIRA,2269,Barry Hart (JIRA),JIRA.12963504.1461854580000@Atlassian.JIRA,,,2017-12-05 13:15:00-08,[jira] [Commented] (AIRFLOW-15) Remove GCloud from Airflow,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-15?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16279191#comment-16279191 ] 

Barry Hart commented on AIRFLOW-15:
-----------------------------------

Question: This change moved Airflow to using a library that Google no longer recommends (https://github.com/google/google-api-python-client/#google-cloud-platform-apis):
{quote}
If you''re working with Google Cloud Platform APIs such as Datastore or Pub/Sub, consider using the Cloud Client Libraries for Python instead. These are the new and idiomatic Python libraries targeted specifically at Google Cloud Platform Services.
{quote}

Should this decision be revisited and possibly reversed? I am happy to open a new ticket, but wanted to raise the question here first for context.

> Remove GCloud from Airflow
> --------------------------
>
>                 Key: AIRFLOW-15
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-15
>             Project: Apache Airflow
>          Issue Type: Task
>          Components: gcp
>            Reporter: Chris Riccomini
>            Assignee: Chris Riccomini
>              Labels: gcp
>
> After speaking with Google, there was some concern about using the [gcloud-python|https://github.com/GoogleCloudPlatform/gcloud-python] library for Airflow. There are several concerns:
> # It''s not clear (even to people at Google) what this library is, who owns it, etc.
> # It does not support all services (the way [google-api-python-client|https://github.com/google/google-api-python-client] does).
> # There are compatibility issues between google-api-python-client and gcloudpython.
> We currently support both, after libraries depending on which package you you install: {{airfow[gcp_api]}} or {{airflow[gcloud]}}. This ticket is to remove the {{airflow[gcloud]}} packaged, and all associated code.
> The main associated code, afaik, is the use of the {{gcloud}} library in the Google cloud storage hooks/operators--specifically for Google cloud storage Airfow logging.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27500,54,JIRA.13122969.1512502776000.394157.1512521312679@Atlassian.JIRA,1279,Siddharth Anand (JIRA),JIRA.13122969.1512502776000@Atlassian.JIRA,,,2017-12-05 16:48:32-08,"[jira] [Assigned] (AIRFLOW-1886) Failed jobs are not being counted
 towards max_active_runs_per_dag","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1886?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Siddharth Anand reassigned AIRFLOW-1886:
----------------------------------------

    Assignee: Oleg Yamin

> Failed jobs are not being counted towards max_active_runs_per_dag
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1886
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1886
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DagRun
>    Affects Versions: 1.8.1
>            Reporter: Oleg Yamin
>            Assignee: Oleg Yamin
>
> Currently, I have setup max_active_runs_per_dag = 2 in airflow.cfg but when a DAG aborts, it will keep submitting next DAG in the queue not counting the current incomplete DAG that is already in the queue. I am using 1.8.1 but i see that the jobs.py in latest version is still not addressing this issue.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27501,54,JIRA.13122969.1512502776000.394161.1512521312709@Atlassian.JIRA,1279,Siddharth Anand (JIRA),JIRA.13122969.1512502776000@Atlassian.JIRA,,,2017-12-05 16:48:32-08,"[jira] [Updated] (AIRFLOW-1886) Failed jobs are not being counted
 towards max_active_runs_per_dag","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1886?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Siddharth Anand updated AIRFLOW-1886:
-------------------------------------
    Description: # Currently, I have setup max_active_runs_per_dag = 2 in airflow.cfg but when a DAG aborts, it will keep submitting next DAG in the queue not counting the current incomplete DAG that is already in the queue. I am using 1.8.1 but i see that the jobs.py in latest version is still not addressing this issue.  (was: Currently, I have setup max_active_runs_per_dag = 2 in airflow.cfg but when a DAG aborts, it will keep submitting next DAG in the queue not counting the current incomplete DAG that is already in the queue. I am using 1.8.1 but i see that the jobs.py in latest version is still not addressing this issue.)

> Failed jobs are not being counted towards max_active_runs_per_dag
> -----------------------------------------------------------------
>
>                 Key: AIRFLOW-1886
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1886
>             Project: Apache Airflow
>          Issue Type: Bug
>          Components: DagRun
>    Affects Versions: 1.8.1
>            Reporter: Oleg Yamin
>            Assignee: Oleg Yamin
>
> # Currently, I have setup max_active_runs_per_dag = 2 in airflow.cfg but when a DAG aborts, it will keep submitting next DAG in the queue not counting the current incomplete DAG that is already in the queue. I am using 1.8.1 but i see that the jobs.py in latest version is still not addressing this issue.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27502,54,JIRA.13122343.1512206525000.396476.1512550080412@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122343.1512206525000@Atlassian.JIRA,,,2017-12-06 00:48:00-08,[jira] [Commented] (AIRFLOW-1879) Use TaskInstance log in cli.run,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1879?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16279852#comment-16279852 ] 

ASF subversion and git services commented on AIRFLOW-1879:
----------------------------------------------------------

Commit 301ce6b4f0da9d01734dd3a0360bff535a8acad5 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=301ce6b ]

[AIRFLOW-1879] Handle ti log entirely within ti

Previously logging was setup outside a
TaskInstance,
this puts everything inside. Also propery closes
the logging.

Closes #2837 from bolkedebruin/AIRFLOW-1879


> Use TaskInstance log in cli.run
> -------------------------------
>
>                 Key: AIRFLOW-1879
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1879
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.1
>
>
> There is no reason the initalize the log in cli.py for airflow.task. It is fine to handle this by the taskinstance itself



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27503,54,JIRA.13122343.1512206525000.396477.1512550080418@Atlassian.JIRA,1317,ASF subversion and git services (JIRA),JIRA.13122343.1512206525000@Atlassian.JIRA,,,2017-12-06 00:48:00-08,[jira] [Commented] (AIRFLOW-1879) Use TaskInstance log in cli.run,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-1879?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16279853#comment-16279853 ] 

ASF subversion and git services commented on AIRFLOW-1879:
----------------------------------------------------------

Commit 301ce6b4f0da9d01734dd3a0360bff535a8acad5 in incubator-airflow''s branch refs/heads/master from [~bolke]
[ https://git-wip-us.apache.org/repos/asf?p=incubator-airflow.git;h=301ce6b ]

[AIRFLOW-1879] Handle ti log entirely within ti

Previously logging was setup outside a
TaskInstance,
this puts everything inside. Also propery closes
the logging.

Closes #2837 from bolkedebruin/AIRFLOW-1879


> Use TaskInstance log in cli.run
> -------------------------------
>
>                 Key: AIRFLOW-1879
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1879
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.1
>
>
> There is no reason the initalize the log in cli.py for airflow.task. It is fine to handle this by the taskinstance itself



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27504,54,JIRA.13122343.1512206525000.396481.1512550080445@Atlassian.JIRA,1759,Fokko Driesprong (JIRA),JIRA.13122343.1512206525000@Atlassian.JIRA,,,2017-12-06 00:48:00-08,[jira] [Resolved] (AIRFLOW-1879) Use TaskInstance log in cli.run,"
     [ https://issues.apache.org/jira/browse/AIRFLOW-1879?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Fokko Driesprong resolved AIRFLOW-1879.
---------------------------------------
       Resolution: Fixed
    Fix Version/s: 1.9.1

Issue resolved by pull request #2837
[https://github.com/apache/incubator-airflow/pull/2837]

> Use TaskInstance log in cli.run
> -------------------------------
>
>                 Key: AIRFLOW-1879
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1879
>             Project: Apache Airflow
>          Issue Type: Improvement
>            Reporter: Bolke de Bruin
>             Fix For: 1.9.1
>
>
> There is no reason the initalize the log in cli.py for airflow.task. It is fine to handle this by the taskinstance itself



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27505,54,JIRA.13123215.1512573334000.399095.1512573360434@Atlassian.JIRA,2072,=?utf-8?Q?Victor_Villas_B=C3=B4as_Chaves_=28JIRA=29?=,JIRA.13123215.1512573334000@Atlassian.JIRA,,,2017-12-06 07:16:00-08,"[jira] [Created] (AIRFLOW-1887) Misleading variable name in AwsHook
 for the endpoint URL","Victor Villas B=C3=B4as Chaves created AIRFLOW-1887:
--------------------------------------------------

             Summary: Misleading variable name in AwsHook for the endpoint =
URL
                 Key: AIRFLOW-1887
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1887
             Project: Apache Airflow
          Issue Type: Improvement
          Components: hooks
            Reporter: Victor Villas B=C3=B4as Chaves
            Assignee: Victor Villas B=C3=B4as Chaves
            Priority: Trivial


The variable `s3_endpoint_url` has this name for legacy reasons (when the h=
ook dealt only with S3 connections), but actually represents the endpoint U=
RL for whatever AWS resource the connection credentials must refer to.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27506,54,JIRA.13123215.1512573334000.399096.1512573360444@Atlassian.JIRA,2072,=?utf-8?Q?Victor_Villas_B=C3=B4as_Chaves_=28JIRA=29?=,JIRA.13123215.1512573334000@Atlassian.JIRA,,,2017-12-06 07:16:00-08,"[jira] [Work started] (AIRFLOW-1887) Misleading variable name in
 AwsHook for the endpoint URL","
     [ https://issues.apache.org/jira/browse/AIRFLOW-1887?page=3Dcom.atlass=
ian.jira.plugin.system.issuetabpanels:all-tabpanel ]

Work on AIRFLOW-1887 started by Victor Villas B=C3=B4as Chaves.
----------------------------------------------------------
> Misleading variable name in AwsHook for the endpoint URL
> --------------------------------------------------------
>
>                 Key: AIRFLOW-1887
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1887
>             Project: Apache Airflow
>          Issue Type: Improvement
>          Components: hooks
>            Reporter: Victor Villas B=C3=B4as Chaves
>            Assignee: Victor Villas B=C3=B4as Chaves
>            Priority: Trivial
>
> The variable `s3_endpoint_url` has this name for legacy reasons (when the=
 hook dealt only with S3 connections), but actually represents the endpoint=
 URL for whatever AWS resource the connection credentials must refer to.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27507,54,JIRA.13123220.1512574720000.399266.1512574740182@Atlassian.JIRA,2292,Andy Hadjigeorgiou (JIRA),JIRA.13123220.1512574720000@Atlassian.JIRA,,,2017-12-06 07:39:00-08,[jira] [Created] (AIRFLOW-1888) Add AWS Redshift cluster sensor,"Andy Hadjigeorgiou created AIRFLOW-1888:
-------------------------------------------

             Summary: Add AWS Redshift cluster sensor
                 Key: AIRFLOW-1888
                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1888
             Project: Apache Airflow
          Issue Type: Improvement
            Reporter: Andy Hadjigeorgiou
            Assignee: Andy Hadjigeorgiou
            Priority: Minor


With new boto3 usage and the RedshiftHook, we can build additional features to support more common Redshift use cases. A Redshift cluster sensor would solve the common case of checking that a cluster is ''up'' before performing any operations on it.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27508,54,JIRA.12963504.1461854580000.400943.1512584521263@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.12963504.1461854580000@Atlassian.JIRA,,,2017-12-06 10:22:01-08,[jira] [Commented] (AIRFLOW-15) Remove GCloud from Airflow,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-15?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16280614#comment-16280614 ] 

Chris Riccomini commented on AIRFLOW-15:
----------------------------------------

[~barrywhart], to be honest, I am not sure what the right path forward is. I was told specifically by several Google PMs at Google Next last year not to use the idiomatic library. Since then, the message you are pointing to has appeared on the google python client.

The use case is further muddied by the fact that I''m not convinced an idiomatic Python client is actually what we want. The fact that the Google APIs all work the same way in the service binding API makes it pretty easy to abstract over a lot of the mechanics around interacting with Google in a generic way that can be leveraged by all GCP operators. I haven''t looked into whether or not this overhead would increase if we went to an idiomatic library where interacting with GCS might look very different from interacting with Dataflow, etc.

> Remove GCloud from Airflow
> --------------------------
>
>                 Key: AIRFLOW-15
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-15
>             Project: Apache Airflow
>          Issue Type: Task
>          Components: gcp
>            Reporter: Chris Riccomini
>            Assignee: Chris Riccomini
>              Labels: gcp
>
> After speaking with Google, there was some concern about using the [gcloud-python|https://github.com/GoogleCloudPlatform/gcloud-python] library for Airflow. There are several concerns:
> # It''s not clear (even to people at Google) what this library is, who owns it, etc.
> # It does not support all services (the way [google-api-python-client|https://github.com/google/google-api-python-client] does).
> # There are compatibility issues between google-api-python-client and gcloudpython.
> We currently support both, after libraries depending on which package you you install: {{airfow[gcp_api]}} or {{airflow[gcloud]}}. This ticket is to remove the {{airflow[gcloud]}} packaged, and all associated code.
> The main associated code, afaik, is the use of the {{gcloud}} library in the Google cloud storage hooks/operators--specifically for Google cloud storage Airfow logging.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
43856,54,JIRA.13110771.1508440455000.35599.1543299780228@Atlassian.JIRA,2905,jack (JIRA),JIRA.13110771.1508440455000@Atlassian.JIRA,,,2018-11-26 22:23:00-08,"[jira] [Commented] (AIRFLOW-1739) Cleanup naming ambiguity with
 TestDbApiHook test class","
    [ https://issues.apache.org/jira/browse/AIRFLOW-1739?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16699973#comment-16699973 ] 

jack commented on AIRFLOW-1739:
-------------------------------

PR was merged. Ticket can be closed.

> Cleanup naming ambiguity with TestDbApiHook test class
> ------------------------------------------------------
>
>                 Key: AIRFLOW-1739
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-1739
>             Project: Apache Airflow
>          Issue Type: Bug
>            Reporter: Andy Hadjigeorgiou
>            Assignee: Andy Hadjigeorgiou
>            Priority: Trivial
>
> The TestDbApiHook class creates a class whose name is TestDBApiHook - I''m proposing a simple naming change to make sure the two are distinguishable.



--
This message was sent by Atlassian JIRA
(v7.6.3#76005)

",t
43857,54,154330072242.30277.9509515870778375994.gitbox@gitbox.apache.org,3034,GitBox,NULL,,,2018-11-26 22:38:42-08,"[GitHub] codecov-io edited a comment on issue #4156: [AIRFLOW-3314] Changed
 auto inlets feature to work as described","codecov-io edited a comment on issue #4156: [AIRFLOW-3314] Changed auto inlets feature to work as described
URL: https://github.com/apache/incubator-airflow/pull/4156#issuecomment-436867007
 
 
   # [Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/4156?src=pr&el=h1) Report
   > Merging [#4156](https://codecov.io/gh/apache/incubator-airflow/pull/4156?src=pr&el=desc) into [master](https://codecov.io/gh/apache/incubator-airflow/commit/0ac1e639d962f0f2f10846b60dee9818efd0b08f?src=pr&el=desc) will **decrease** coverage by `0.1%%%%`.
   > The diff coverage is `100%%%%`.
   
   [![Impacted file tree graph](https://codecov.io/gh/apache/incubator-airflow/pull/4156/graphs/tree.svg?width=650&token=WdLKlKHOAU&height=150&src=pr)](https://codecov.io/gh/apache/incubator-airflow/pull/4156?src=pr&el=tree)
   
   ```diff
   @@            Coverage Diff             @@
   ##           master    #4156      +/-   ##
   ==========================================
   - Coverage   77.82%%%%   77.71%%%%   -0.11%%%%     
   ==========================================
     Files         201      199       -2     
     Lines       16367    16296      -71     
   ==========================================
   - Hits        12738    12665      -73     
   - Misses       3629     3631       +2
   ```
   
   
   | [Impacted Files](https://codecov.io/gh/apache/incubator-airflow/pull/4156?src=pr&el=tree) | Coverage ?? | |
   |---|---|---|
   | [airflow/lineage/\_\_init\_\_.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy9saW5lYWdlL19faW5pdF9fLnB5) | `96.92%%%% <100%%%%> (+0.31%%%%)` | :arrow_up: |
   | [airflow/hooks/\_\_init\_\_.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy9ob29rcy9fX2luaXRfXy5weQ==) | `33.33%%%% <0%%%%> (-66.67%%%%)` | :arrow_down: |
   | [airflow/macros/\_\_init\_\_.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy9tYWNyb3MvX19pbml0X18ucHk=) | `75%%%% <0%%%%> (-25%%%%)` | :arrow_down: |
   | [airflow/executors/\_\_init\_\_.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy9leGVjdXRvcnMvX19pbml0X18ucHk=) | `55.76%%%% <0%%%%> (-7.7%%%%)` | :arrow_down: |
   | [airflow/\_\_init\_\_.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy9fX2luaXRfXy5weQ==) | `74.28%%%% <0%%%%> (-5.72%%%%)` | :arrow_down: |
   | [airflow/plugins\_manager.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy9wbHVnaW5zX21hbmFnZXIucHk=) | `91.95%%%% <0%%%%> (-1.15%%%%)` | :arrow_down: |
   | [airflow/utils/dag\_processing.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy91dGlscy9kYWdfcHJvY2Vzc2luZy5weQ==) | `57.52%%%% <0%%%%> (-0.33%%%%)` | :arrow_down: |
   | [airflow/configuration.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy9jb25maWd1cmF0aW9uLnB5) | `89.05%%%% <0%%%%> (-0.2%%%%)` | :arrow_down: |
   | [airflow/www\_rbac/security.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy93d3dfcmJhYy9zZWN1cml0eS5weQ==) | `92.61%%%% <0%%%%> (-0.15%%%%)` | :arrow_down: |
   | [airflow/www\_rbac/views.py](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree#diff-YWlyZmxvdy93d3dfcmJhYy92aWV3cy5weQ==) | `72.32%%%% <0%%%%> (-0.09%%%%)` | :arrow_down: |
   | ... and [13 more](https://codecov.io/gh/apache/incubator-airflow/pull/4156/diff?src=pr&el=tree-more) | |
   
   ------
   
   [Continue to review full report at Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/4156?src=pr&el=continue).
   > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
   > `?? = absolute <relative> (impact)`, `?? = not affected`, `? = missing data`
   > Powered by [Codecov](https://codecov.io/gh/apache/incubator-airflow/pull/4156?src=pr&el=footer). Last update [0ac1e63...a0c1508](https://codecov.io/gh/apache/incubator-airflow/pull/4156?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
   

----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


With regards,
Apache Git Services

",t
27509,54,JIRA.12963504.1461854580000.401283.1512585720386@Atlassian.JIRA,2269,Barry Hart (JIRA),JIRA.12963504.1461854580000@Atlassian.JIRA,,,2017-12-06 10:42:00-08,[jira] [Commented] (AIRFLOW-15) Remove GCloud from Airflow,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-15?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16280644#comment-16280644 ] 

Barry Hart commented on AIRFLOW-15:
-----------------------------------

I understand. ""Wait and see"" might be a reasonable strategy. (I.e. until Google clarifies the message). The specific reason for my comment is that one of our DAGs transfers a very large number of files to and from Google Storage. With this number of files, we almost always see some transient 5XX errors from the Google side, so we see some value in the google-cloud-python-library, which has retry logic built in, both [generally|https://github.com/GoogleCloudPlatform/google-cloud-python/blob/master/api_core/google/api_core/retry.py] and [specifically for Google Storage|https://github.com/GoogleCloudPlatform/google-cloud-python/blob/master/storage/google/cloud/storage/blob.py#L84-L91].)

(Although Airflow has its own retry support, I see those as being intended for coarse-grained retries (i.e. when one task does a few things). When one task is transferring thousands of files, it seems useful to retry internal to the task as well (per file).

Let me know what you think. It may be worth creating a ticket about retries to perhaps get input from other users. For now, we can use the google-cloud-python-library directly from our DAGs.

> Remove GCloud from Airflow
> --------------------------
>
>                 Key: AIRFLOW-15
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-15
>             Project: Apache Airflow
>          Issue Type: Task
>          Components: gcp
>            Reporter: Chris Riccomini
>            Assignee: Chris Riccomini
>              Labels: gcp
>
> After speaking with Google, there was some concern about using the [gcloud-python|https://github.com/GoogleCloudPlatform/gcloud-python] library for Airflow. There are several concerns:
> # It''s not clear (even to people at Google) what this library is, who owns it, etc.
> # It does not support all services (the way [google-api-python-client|https://github.com/google/google-api-python-client] does).
> # There are compatibility issues between google-api-python-client and gcloudpython.
> We currently support both, after libraries depending on which package you you install: {{airfow[gcp_api]}} or {{airflow[gcloud]}}. This ticket is to remove the {{airflow[gcloud]}} packaged, and all associated code.
> The main associated code, afaik, is the use of the {{gcloud}} library in the Google cloud storage hooks/operators--specifically for Google cloud storage Airfow logging.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)

",t
27510,54,JIRA.12963504.1461854580000.401328.1512585960414@Atlassian.JIRA,1273,Chris Riccomini (JIRA),JIRA.12963504.1461854580000@Atlassian.JIRA,,,2017-12-06 10:46:00-08,[jira] [Commented] (AIRFLOW-15) Remove GCloud from Airflow,"
    [ https://issues.apache.org/jira/browse/AIRFLOW-15?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=16280649#comment-16280649 ] 

Chris Riccomini commented on AIRFLOW-15:
----------------------------------------

[~fenglu], perhaps you can poke someone on your end to get guidance on the right library to use? What are your thoughts?

> Remove GCloud from Airflow
> --------------------------
>
>                 Key: AIRFLOW-15
>                 URL: https://issues.apache.org/jira/browse/AIRFLOW-15
>             Project: Apache Airflow
>          Issue Type: Task
>          Components: gcp
>            Reporter: Chris Riccomini
>            Assignee: Chris Riccomini
>              Labels: gcp
>
> After speaking with Google, there was some concern about using the [gcloud-python|https://github.com/GoogleCloudPlatform/gcloud-python] library for Airflow. There are several concerns:
> # It''s not clear (even to people at Google) what this library is, who owns it, etc.
> # It does not support all services (the way [google-api-python-client|https://github.com/google/google-api-python-client] does).
> # There are compatibility issues between google-api-python-client and gcloudpython.
> We currently support both, after libraries depending on which package you you install: {{airfow[gcp_api]}} or {{airflow[gcloud]}}. This ticket is to remove the {{airflow[gcloud]}} packaged, and all associated code.
> The main associated code, afaik, is the use of the {{gcloud}} library in the Google cloud storage hooks/operators--specifically for Google cloud storage Airfow logging.



--
This message was sent by Atlassian JIRA
(v6.4.14#64029)
