[{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697018650","html_url":"https://github.com/apache/iceberg/pull/1427#issuecomment-697018650","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1427","id":697018650,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzAxODY1MA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T22:36:19Z","updated_at":"2020-09-22T22:36:19Z","author_association":"CONTRIBUTOR","body":"Thanks @RussellSpitzer for the work and everyone for reviewing!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697018650/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697020400","html_url":"https://github.com/apache/iceberg/pull/1469#issuecomment-697020400","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1469","id":697020400,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzAyMDQwMA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T22:41:31Z","updated_at":"2020-09-22T22:43:32Z","author_association":"CONTRIBUTOR","body":"> For a DELETE using position delete files, I think that this isn't quite correct: \"data files referenced by new deletes must be still present\". The logic for \"no validation for delete files\" applies to this case: if a data file was deleted, then it's okay to delete the row twice. The validation should be \"data files referenced by new deletes must still be present or must be deleted; i.e., cannot be rewritten or overwritten.\"\r\n\r\nWhat about a copy-on-write update that happened concurrently? It could take a file that my delete file references and rewrite it into a new file and keep the record I want to remove. If we allow to commit the delete file that references now non-existing file, there is no way we can guarantee the record we were about to remove is actually removed. That being said, we can still commit our delete file if the data file is removed completely (not rewritten or overwritten).\r\n\r\n> For a DELETE using equality delete files, I'm not sure that snapshot isolation is distinct. If a data file is added concurrently that has a row that is now deleted, then either that commit is first and the row is deleted or the commit is later and it is appended. Either way, the operations are independent. There is no need to validate \"no new potentially matching data files since we read\" because there is not necessarily a read, and the delete applies to the data automatically.\r\n\r\nAgree.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697020400/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697038658","html_url":"https://github.com/apache/iceberg/pull/1469#issuecomment-697038658","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1469","id":697038658,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzAzODY1OA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T23:42:08Z","updated_at":"2020-09-22T23:42:33Z","author_association":"CONTRIBUTOR","body":"> What about a copy-on-write update that happened concurrently?\r\n\r\nThis would result in an overwrite, not a delete commit, right? So we would validate that the data file exists. I'm just saying that if the operation was a pure delete, then we don't need to validate delete files.\r\n\r\n> That being said, we can still commit our delete file if the data file is removed completely (not rewritten or overwritten).\r\n\r\nThat's what I mean.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697038658/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697058010","html_url":"https://github.com/apache/iceberg/pull/1469#issuecomment-697058010","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1469","id":697058010,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzA1ODAxMA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T00:53:00Z","updated_at":"2020-09-23T00:53:00Z","author_association":"CONTRIBUTOR","body":"> I'm just saying that if the operation was a pure delete, then we don't need to validate delete files.\r\n\r\nWe are on the same page here.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697058010/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697135213","html_url":"https://github.com/apache/iceberg/pull/1487#issuecomment-697135213","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1487","id":697135213,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzEzNTIxMw==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T05:01:33Z","updated_at":"2020-09-23T05:01:33Z","author_association":"CONTRIBUTOR","body":"I tried to deal with `date` against hour partition, but no luck on making filter be pushed down (both Spark 2.4 and Spark 3). If someone has an idea to do that (or have an idea to test `date` against hour partition) it would be awesome.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697135213/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697137452","html_url":"https://github.com/apache/iceberg/issues/1486#issuecomment-697137452","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1486","id":697137452,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzEzNzQ1Mg==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T05:10:05Z","updated_at":"2020-09-23T05:10:05Z","author_association":"CONTRIBUTOR","body":"You can do the hack on package private access - create a new class extending VectorizedRowBatchIterator in the same package in your application.\r\n\r\nBut I'm not sure Iceberg project intends to provide utilities other than Iceberg API as public API. I wonder that is not encouraged to be used outside of Iceberg.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697137452/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697143492","html_url":"https://github.com/apache/iceberg/issues/1483#issuecomment-697143492","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1483","id":697143492,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzE0MzQ5Mg==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T05:32:29Z","updated_at":"2020-09-23T05:32:29Z","author_association":"CONTRIBUTOR","body":"I guess you're using Spark 2.4 - the abbreviation is done in Spark, not Iceberg.\r\n\r\nhttps://github.com/apache/spark/blob/e1e94ed4ef45ef81814f1b920bac0afa52ae06a2/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2StringFormat.scala#L61-L86\r\n\r\nSo that's not only about the issue of Iceberg, but an issue of Spark.\r\n\r\nFrom Spark 3 the description looks to be provided from data source, which doesn't look to have such abbreviation, so you may want to check out Spark 3 and try out.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697143492/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697147099","html_url":"https://github.com/apache/iceberg/issues/1422#issuecomment-697147099","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1422","id":697147099,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzE0NzA5OQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T05:45:23Z","updated_at":"2020-09-23T05:45:23Z","author_association":"CONTRIBUTOR","body":"I tend to think that Option 2 would require less substantial changes. We anyway want to have predicate pushdown in metadata tables, fix spec id in `SparkDataFile`, propagate spec id in the `files` metadata table. That being said, I would be interested to know whether any other query engines plan to leverage this too. I know planning in Presto is very different, though.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697147099/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697147571","html_url":"https://github.com/apache/iceberg/pull/1475#issuecomment-697147571","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1475","id":697147571,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzE0NzU3MQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T05:47:05Z","updated_at":"2020-09-23T05:47:05Z","author_association":"CONTRIBUTOR","body":"Let me take a look tomorrow.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697147571/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697226867","html_url":"https://github.com/apache/iceberg/pull/1489#issuecomment-697226867","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1489","id":697226867,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzIyNjg2Nw==","user":{"login":"kbendick","id":9833362,"node_id":"MDQ6VXNlcjk4MzMzNjI=","avatar_url":"https://avatars.githubusercontent.com/u/9833362?v=4","gravatar_id":"","url":"https://api.github.com/users/kbendick","html_url":"https://github.com/kbendick","followers_url":"https://api.github.com/users/kbendick/followers","following_url":"https://api.github.com/users/kbendick/following{/other_user}","gists_url":"https://api.github.com/users/kbendick/gists{/gist_id}","starred_url":"https://api.github.com/users/kbendick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kbendick/subscriptions","organizations_url":"https://api.github.com/users/kbendick/orgs","repos_url":"https://api.github.com/users/kbendick/repos","events_url":"https://api.github.com/users/kbendick/events{/privacy}","received_events_url":"https://api.github.com/users/kbendick/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T08:49:42Z","updated_at":"2020-09-23T08:50:00Z","author_association":"CONTRIBUTOR","body":"cc @RussellSpitzer, my linting and warning-cleaning friend.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697226867/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697256274","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-697256274","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":697256274,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzI1NjI3NA==","user":{"login":"marton-bod","id":19599214,"node_id":"MDQ6VXNlcjE5NTk5MjE0","avatar_url":"https://avatars.githubusercontent.com/u/19599214?v=4","gravatar_id":"","url":"https://api.github.com/users/marton-bod","html_url":"https://github.com/marton-bod","followers_url":"https://api.github.com/users/marton-bod/followers","following_url":"https://api.github.com/users/marton-bod/following{/other_user}","gists_url":"https://api.github.com/users/marton-bod/gists{/gist_id}","starred_url":"https://api.github.com/users/marton-bod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/marton-bod/subscriptions","organizations_url":"https://api.github.com/users/marton-bod/orgs","repos_url":"https://api.github.com/users/marton-bod/repos","events_url":"https://api.github.com/users/marton-bod/events{/privacy}","received_events_url":"https://api.github.com/users/marton-bod/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T09:45:55Z","updated_at":"2020-09-23T09:45:55Z","author_association":"COLLABORATOR","body":"@rdblue @massdosage @rdsr \r\nDo you guys think we can go ahead with this change to enable Hive 3 builds? \r\nLet me know if you have any suggestions, I'm open to discussion and changes. Thank you!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697256274/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697273993","html_url":"https://github.com/apache/iceberg/pull/1490#issuecomment-697273993","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1490","id":697273993,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzI3Mzk5Mw==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T10:22:24Z","updated_at":"2020-09-23T10:22:24Z","author_association":"CONTRIBUTOR","body":"@rdblue @rdsr @cmathiesen @guilload @pvary Please take a look at the docs for Hive read support and let me know what you think. It would be nice to get these into 0.10.0 as it should make it easier for end users to start using this feature.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697273993/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697280721","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-697280721","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":697280721,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzI4MDcyMQ==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T10:37:03Z","updated_at":"2020-09-23T10:37:03Z","author_association":"CONTRIBUTOR","body":"> @rdblue @massdosage @rdsr\r\n> Do you guys think we can go ahead with this change to enable Hive 3 builds?\r\n> Let me know if you have any suggestions, I'm open to discussion and changes. Thank you!\r\n\r\nSince this is optional and from what I can tell works fine with Hive 2 by default I'm OK with it. As I said above, I don't know enough about Gradle to really say whether there is a better way of achieving this so I'll defer to the others.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697280721/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697291549","html_url":"https://github.com/apache/iceberg/pull/1492#issuecomment-697291549","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1492","id":697291549,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzI5MTU0OQ==","user":{"login":"liukun4515","id":7450163,"node_id":"MDQ6VXNlcjc0NTAxNjM=","avatar_url":"https://avatars.githubusercontent.com/u/7450163?v=4","gravatar_id":"","url":"https://api.github.com/users/liukun4515","html_url":"https://github.com/liukun4515","followers_url":"https://api.github.com/users/liukun4515/followers","following_url":"https://api.github.com/users/liukun4515/following{/other_user}","gists_url":"https://api.github.com/users/liukun4515/gists{/gist_id}","starred_url":"https://api.github.com/users/liukun4515/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/liukun4515/subscriptions","organizations_url":"https://api.github.com/users/liukun4515/orgs","repos_url":"https://api.github.com/users/liukun4515/repos","events_url":"https://api.github.com/users/liukun4515/events{/privacy}","received_events_url":"https://api.github.com/users/liukun4515/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T11:02:08Z","updated_at":"2020-09-23T11:02:08Z","author_association":"NONE","body":"@rdblue PTAL","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697291549/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697302448","html_url":"https://github.com/apache/iceberg/pull/1494#issuecomment-697302448","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1494","id":697302448,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzMwMjQ0OA==","user":{"login":"kbendick","id":9833362,"node_id":"MDQ6VXNlcjk4MzMzNjI=","avatar_url":"https://avatars.githubusercontent.com/u/9833362?v=4","gravatar_id":"","url":"https://api.github.com/users/kbendick","html_url":"https://github.com/kbendick","followers_url":"https://api.github.com/users/kbendick/followers","following_url":"https://api.github.com/users/kbendick/following{/other_user}","gists_url":"https://api.github.com/users/kbendick/gists{/gist_id}","starred_url":"https://api.github.com/users/kbendick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kbendick/subscriptions","organizations_url":"https://api.github.com/users/kbendick/orgs","repos_url":"https://api.github.com/users/kbendick/repos","events_url":"https://api.github.com/users/kbendick/events{/privacy}","received_events_url":"https://api.github.com/users/kbendick/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T11:26:31Z","updated_at":"2020-09-23T11:26:31Z","author_association":"CONTRIBUTOR","body":"Hmmm... this should have been labeled as `docs` (as we want all README.md files to be labeled as `docs`). I will open an issue / PR to update the autolabeler config to fix this.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697302448/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697317471","html_url":"https://github.com/apache/iceberg/pull/1495#issuecomment-697317471","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1495","id":697317471,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzMxNzQ3MQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T12:00:59Z","updated_at":"2020-09-23T12:00:59Z","author_association":"CONTRIBUTOR","body":"If you have time, could you please review and / or try it:\r\n@massdosage, @marton-bod, @rdblue, @omalley \r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697317471/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697318015","html_url":"https://github.com/apache/iceberg/pull/1407#issuecomment-697318015","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1407","id":697318015,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzMxODAxNQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T12:02:12Z","updated_at":"2020-09-23T12:02:12Z","author_association":"CONTRIBUTOR","body":"@rdblue: Gentle reminder: Could you please push the PR or raise your further concerns if time allows?\r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697318015/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697340794","html_url":"https://github.com/apache/iceberg/pull/1489#issuecomment-697340794","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1489","id":697340794,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzM0MDc5NA==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T12:46:09Z","updated_at":"2020-09-23T12:46:09Z","author_association":"MEMBER","body":"Wow ... I'm ok with the suppression but this seems like a scary way of handling  equality. Seems like a scary design decision on the MapMaker side?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697340794/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697607994","html_url":"https://github.com/apache/iceberg/issues/1498#issuecomment-697607994","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1498","id":697607994,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzYwNzk5NA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T16:06:43Z","updated_at":"2020-09-23T16:06:43Z","author_association":"CONTRIBUTOR","body":"We've planned [SNAPSHOT and MIGRATE commands ](https://docs.google.com/document/d/1Nf8c16R2hj4lSc-4sQg4oiUUV_F4XqZKth1woEo6TN8/)and @RussellSpitzer is already working on that.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697607994/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697609963","html_url":"https://github.com/apache/iceberg/issues/1498#issuecomment-697609963","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1498","id":697609963,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzYwOTk2Mw==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T16:07:52Z","updated_at":"2020-09-23T16:07:52Z","author_association":"MEMBER","body":"Yep I should have a PR up soon for the backing \"spark actions\" which we'll\nbe able to plug into our new Extensions very easily\n\nOn Wed, Sep 23, 2020 at 11:07 AM Anton Okolnychyi <notifications@github.com>\nwrote:\n\n> We've planned SNAPSHOT and MIGRATE commands\n> <https://docs.google.com/document/d/1Nf8c16R2hj4lSc-4sQg4oiUUV_F4XqZKth1woEo6TN8/>and\n> @RussellSpitzer <https://github.com/RussellSpitzer> is already working on\n> that.\n>\n> —\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/apache/iceberg/issues/1498#issuecomment-697607994>,\n> or unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AADE2YNCW6IXOTYVT3T6GJTSHIMKJANCNFSM4RXFYCWA>\n> .\n>\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697609963/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697755199","html_url":"https://github.com/apache/iceberg/pull/1473#issuecomment-697755199","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1473","id":697755199,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Nzc1NTE5OQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T17:33:18Z","updated_at":"2020-09-23T17:33:18Z","author_association":"CONTRIBUTOR","body":"I've updated this PR but tests will fail until we clarify caching behavior.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697755199/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697780061","html_url":"https://github.com/apache/iceberg/pull/947#issuecomment-697780061","issue_url":"https://api.github.com/repos/apache/iceberg/issues/947","id":697780061,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Nzc4MDA2MQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T17:48:26Z","updated_at":"2020-09-23T17:48:26Z","author_association":"CONTRIBUTOR","body":"I'm closing this because I'm including these changes in the larger spec update.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697780061/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697832400","html_url":"https://github.com/apache/iceberg/pull/1499#issuecomment-697832400","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1499","id":697832400,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NzgzMjQwMA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T18:20:39Z","updated_at":"2020-09-23T18:20:39Z","author_association":"CONTRIBUTOR","body":"FYI @electrum, @openinx. This is the update to document row-level deletes in the spec.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697832400/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697961714","html_url":"https://github.com/apache/iceberg/issues/1422#issuecomment-697961714","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1422","id":697961714,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Nzk2MTcxNA==","user":{"login":"shardulm94","id":6961317,"node_id":"MDQ6VXNlcjY5NjEzMTc=","avatar_url":"https://avatars.githubusercontent.com/u/6961317?v=4","gravatar_id":"","url":"https://api.github.com/users/shardulm94","html_url":"https://github.com/shardulm94","followers_url":"https://api.github.com/users/shardulm94/followers","following_url":"https://api.github.com/users/shardulm94/following{/other_user}","gists_url":"https://api.github.com/users/shardulm94/gists{/gist_id}","starred_url":"https://api.github.com/users/shardulm94/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shardulm94/subscriptions","organizations_url":"https://api.github.com/users/shardulm94/orgs","repos_url":"https://api.github.com/users/shardulm94/repos","events_url":"https://api.github.com/users/shardulm94/events{/privacy}","received_events_url":"https://api.github.com/users/shardulm94/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T20:42:14Z","updated_at":"2020-09-23T20:42:14Z","author_association":"CONTRIBUTOR","body":"@aokolnychyi We have this use case too\r\n> job planning for queries with predicates only on sort key in partitioned tables\r\n\r\nwhere distributed job planning can be helpful. I think one reason why partition predicates are faster is because we have partition upper and lower bounds aggregated at a manifest file level. If we aggregate these metrics for other fields as well do you think that helps your case here?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/697961714/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698017947","html_url":"https://github.com/apache/iceberg/issues/1496#issuecomment-698017947","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1496","id":698017947,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODAxNzk0Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T23:10:43Z","updated_at":"2020-09-23T23:10:43Z","author_association":"CONTRIBUTOR","body":"I think deleting the version hint and renaming a new one in its place is a good idea. I don't think that there is an atomic operation that could do this. If there was, we would use it to maintain a pointer to the version instead of just a hint. We have to delete the file, but at least a rename would ensure that readers don't use the file when it is half written. And if the rename fails, it's okay to give up.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698017947/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698018548","html_url":"https://github.com/apache/iceberg/pull/1495#issuecomment-698018548","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1495","id":698018548,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODAxODU0OA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-23T23:11:58Z","updated_at":"2020-09-23T23:11:58Z","author_association":"CONTRIBUTOR","body":"@pvary, why does this set the Iceberg schema using a table property?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698018548/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698033854","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-698033854","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":698033854,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODAzMzg1NA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T00:02:02Z","updated_at":"2020-09-24T00:02:02Z","author_association":"CONTRIBUTOR","body":"Thanks for working on this, @marton-bod! And sorry for the delay in replying on this. I was initially focused on trying to avoid the problem of needing both hive 2 and hive 3 modules, but I don't see a way around it because the OI interfaces now specify incompatible objects. So I agree that we will need additional modules to handle this.\r\n\r\nBut, I think there are some ways to simplify the changes this introduces. Because the changes needed between 2 and 3 are minor, I think the goal should be to produce a single iceberg-hive-runtime Jar that works in both versions. To do that, we need to build a `DateObjectInspector` for Hive 2 and one for Hive 3, but return the correct one at runtime using reflection. That way, we detect whether to use Hive 2 or 3 (e.g., by checking if `DateWritableV2` exists) and return an OI that matches. The other class would not be loaded, so it would not cause any issues.\r\n\r\nI think that we can achieve this using just one new module, iceberg-hive3, that adds the new object inspectors. The other module could continue to depend on Hive 2.\r\n\r\nI'd like to avoid selecting `versions.props` based on flags, so we would ideally just embed the Hive 3 version in the iceberg-hive3 dependency. That module should also depend on the iceberg-mr module so that it can run the same tests (maybe it can set the test source directory to share with hive2?). This module would have both hive 2 and hive 3 classes in its test classpath, so it would validate that having both doesn't break Hive. And, since Hadoop it always pulled in as a test dependency, it can use Hadoop 3.\r\n\r\nFinally, the iceberg-hive-runtime module would pull in both iceberg-hive and iceberg-hive3 so that all of the classes are in our runtime module.\r\n\r\nI think this would greatly simplify the support:\r\n1. It would add only one new module\r\n2. It would avoid using build flags\r\n\r\nWhat do you think?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698033854/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698040378","html_url":"https://github.com/apache/iceberg/pull/1481#issuecomment-698040378","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1481","id":698040378,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA0MDM3OA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T00:25:23Z","updated_at":"2020-09-24T00:25:23Z","author_association":"CONTRIBUTOR","body":"Thanks, @pvary! Mostly looks good, but I'd like to fix the `protected` method that was removed. People might rely on it since it was in a release.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698040378/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698040761","html_url":"https://github.com/apache/iceberg/pull/1495#issuecomment-698040761","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1495","id":698040761,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA0MDc2MQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T00:26:48Z","updated_at":"2020-09-24T00:26:48Z","author_association":"CONTRIBUTOR","body":"I think I understand why the schema is used, but I'd like to use types from Hive DDL if possible. I'm not sure whether that would need to change in this PR or in #1481.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698040761/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698048010","html_url":"https://github.com/apache/iceberg/pull/1494#issuecomment-698048010","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1494","id":698048010,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA0ODAxMA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T00:54:24Z","updated_at":"2020-09-24T00:54:24Z","author_association":"CONTRIBUTOR","body":"Thanks @kbendick!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698048010/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698048650","html_url":"https://github.com/apache/iceberg/pull/1493#issuecomment-698048650","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1493","id":698048650,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA0ODY1MA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T00:56:42Z","updated_at":"2020-09-24T00:56:42Z","author_association":"CONTRIBUTOR","body":"Does it fix the warning to use a lambda and cast to `Consumer` instead? This could be something like:\r\n\r\n```java\r\nConsumer<String> defaultDelete = (Consumer<String>) file -> io.delete(file);\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698048650/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698054009","html_url":"https://github.com/apache/iceberg/issues/1422#issuecomment-698054009","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1422","id":698054009,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA1NDAwOQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T01:15:05Z","updated_at":"2020-09-24T01:15:05Z","author_association":"CONTRIBUTOR","body":">  I think one reason why partition predicates are faster is because we have partition upper and lower bounds aggregated at a manifest file level. If we aggregate these metrics for other fields as well do you think that helps your case here?\r\n\r\nI am afraid that won't help as a single manifest covers a subset of partitions and we tend to have values for the complete range of sort keys in every partition. We really need to read all manifests to get to the file stats.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698054009/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698054932","html_url":"https://github.com/apache/iceberg/pull/1489#issuecomment-698054932","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1489","id":698054932,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA1NDkzMg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T01:18:26Z","updated_at":"2020-09-24T01:18:26Z","author_association":"CONTRIBUTOR","body":"Thanks, @kbendick!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698054932/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698055822","html_url":"https://github.com/apache/iceberg/pull/1489#issuecomment-698055822","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1489","id":698055822,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA1NTgyMg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T01:21:31Z","updated_at":"2020-09-24T01:21:31Z","author_association":"CONTRIBUTOR","body":"@RussellSpitzer, we did careful validation to make sure the use of intern would be okay here. We want to complete as much as possible without running out of memory, so it works to use the weak key map and intern. And using identity instead of equality is fairly common when using weak keys.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698055822/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698056077","html_url":"https://github.com/apache/iceberg/pull/1477#issuecomment-698056077","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1477","id":698056077,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA1NjA3Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T01:22:24Z","updated_at":"2020-09-24T01:22:24Z","author_association":"CONTRIBUTOR","body":"I'll try to review this tomorrow. Thanks for being patient!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698056077/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698059403","html_url":"https://github.com/apache/iceberg/pull/1489#issuecomment-698059403","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1489","id":698059403,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA1OTQwMw==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T01:33:05Z","updated_at":"2020-09-24T01:33:05Z","author_association":"MEMBER","body":"I'm just a scaredy cat, don't mind me :)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698059403/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698062436","html_url":"https://github.com/apache/iceberg/issues/1498#issuecomment-698062436","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1498","id":698062436,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA2MjQzNg==","user":{"login":"chenjunjiedada","id":3960228,"node_id":"MDQ6VXNlcjM5NjAyMjg=","avatar_url":"https://avatars.githubusercontent.com/u/3960228?v=4","gravatar_id":"","url":"https://api.github.com/users/chenjunjiedada","html_url":"https://github.com/chenjunjiedada","followers_url":"https://api.github.com/users/chenjunjiedada/followers","following_url":"https://api.github.com/users/chenjunjiedada/following{/other_user}","gists_url":"https://api.github.com/users/chenjunjiedada/gists{/gist_id}","starred_url":"https://api.github.com/users/chenjunjiedada/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chenjunjiedada/subscriptions","organizations_url":"https://api.github.com/users/chenjunjiedada/orgs","repos_url":"https://api.github.com/users/chenjunjiedada/repos","events_url":"https://api.github.com/users/chenjunjiedada/events{/privacy}","received_events_url":"https://api.github.com/users/chenjunjiedada/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T01:44:58Z","updated_at":"2020-09-24T01:44:58Z","author_association":"COLLABORATOR","body":"Thanks @aokolnychyi and @RussellSpitzer , Looking forward to this. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698062436/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698063386","html_url":"https://github.com/apache/iceberg/pull/1493#issuecomment-698063386","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1493","id":698063386,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODA2MzM4Ng==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T01:48:22Z","updated_at":"2020-09-24T01:50:50Z","author_association":"MEMBER","body":"I believe the warning is mostly just upset that you save a abstract function class as a field rather than a well defined object with a named method or use just a named function. I believe they would rather it was \r\n\r\n```java\r\nDeleter defaultDelete = new Deleter()\r\n\r\nclass Deleter {\r\n  void doDelete(String) {\r\n  }\r\n}\r\n```\r\nWhere you extend Deleter rather than passing in a lambda. I think this is unnecessary and we should just suppress.\r\n\r\n\r\nThe docs give this example\r\nhttps://errorprone.info/bugpattern/UnnecessaryAnonymousClass\r\nYes\r\n```java\r\nprivate static Bar getBar(Foo foo) {\r\n  return BarService.lookupBar(foo, defaultCredentials());\r\n}\r\n.\r\n\r\nreturn someStream().map(MyClass::getBar)....;\r\n```\r\n\r\nNo\r\n```java\r\nprivate static final Function<Foo, Bar> GET_BAR_FUNCTION =\r\n    new Function<Foo, Bar>() {\r\n      @Override\r\n      public Bar apply(Foo foo) {\r\n        return BarService.lookupBar(foo, defaultCredentials());\r\n      }\r\n    };\r\n\r\nreturn someStream().map(GET_BAR_FUNCTION)....;\r\n```\r\n\r\nSo it's not so much the Lambda, but rather the fact that a function is a field. I think the fact that our field is mutable\r\nmakes this kinda moot.\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698063386/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698106959","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-698106959","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":698106959,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODEwNjk1OQ==","user":{"login":"rdsr","id":129474,"node_id":"MDQ6VXNlcjEyOTQ3NA==","avatar_url":"https://avatars.githubusercontent.com/u/129474?v=4","gravatar_id":"","url":"https://api.github.com/users/rdsr","html_url":"https://github.com/rdsr","followers_url":"https://api.github.com/users/rdsr/followers","following_url":"https://api.github.com/users/rdsr/following{/other_user}","gists_url":"https://api.github.com/users/rdsr/gists{/gist_id}","starred_url":"https://api.github.com/users/rdsr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdsr/subscriptions","organizations_url":"https://api.github.com/users/rdsr/orgs","repos_url":"https://api.github.com/users/rdsr/repos","events_url":"https://api.github.com/users/rdsr/events{/privacy}","received_events_url":"https://api.github.com/users/rdsr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T04:37:51Z","updated_at":"2020-09-24T04:37:51Z","author_association":"CONTRIBUTOR","body":"> Thanks for working on this, @marton-bod! And sorry for the delay in replying on this. I was initially focused on trying to avoid the problem of needing both hive 2 and hive 3 modules, but I don't see a way around it because the OI interfaces now specify incompatible objects. So I agree that we will need additional modules to handle this.\r\n> \r\n> But, I think there are some ways to simplify the changes this introduces. Because the changes needed between 2 and 3 are minor, I think the goal should be to produce a single iceberg-hive-runtime Jar that works in both versions. To do that, we need to build a `DateObjectInspector` for Hive 2 and one for Hive 3, but return the correct one at runtime using reflection. That way, we detect whether to use Hive 2 or 3 (e.g., by checking if `DateWritableV2` exists) and return an OI that matches. The other class would not be loaded, so it would not cause any issues.\r\n> \r\n> I think that we can achieve this using just one new module, iceberg-hive3, that adds the new object inspectors. The other module could continue to depend on Hive 2.\r\n> \r\n> I'd like to avoid selecting `versions.props` based on flags, so we would ideally just embed the Hive 3 version in the iceberg-hive3 dependency. That module should also depend on the iceberg-mr module so that it can run the same tests (maybe it can set the test source directory to share with hive2?). This module would have both hive 2 and hive 3 classes in its test classpath, so it would validate that having both doesn't break Hive. And, since Hadoop it always pulled in as a test dependency, it can use Hadoop 3.\r\n> \r\n> Finally, the iceberg-hive-runtime module would pull in both iceberg-hive and iceberg-hive3 so that all of the classes are in our runtime module.\r\n> \r\n> I think this would greatly simplify the support:\r\n> \r\n> 1. It would add only one new module\r\n> 2. It would avoid using build flags\r\n> \r\n> What do you think?\r\n\r\n+1. This seems like a much cleaner approach if we can get this working!\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698106959/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698127230","html_url":"https://github.com/apache/iceberg/pull/1464#issuecomment-698127230","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1464","id":698127230,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODEyNzIzMA==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T05:48:27Z","updated_at":"2020-09-24T05:48:27Z","author_association":"CONTRIBUTOR","body":"is there requirement for the version of hive?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698127230/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698148650","html_url":"https://github.com/apache/iceberg/pull/1487#issuecomment-698148650","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1487","id":698148650,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODE0ODY1MA==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T06:43:37Z","updated_at":"2020-09-24T06:43:37Z","author_association":"CONTRIBUTOR","body":"One of new tests are failing on Spark 2.4 which didn't fail in IDEA. It looks to be only failing with Gradle.\r\n\r\n`gradle :iceberg-spark2:test`\r\n\r\n```\r\norg.apache.iceberg.spark.source.TestPartitionPruning24 > testPartitionPruningBucketingInteger[0] FAILED\r\n    java.lang.AssertionError: Some of data files in partition range should be read. \r\n\r\nRead files in query: [/tmp/junit2992201386166743058/logs-742071/metadata/snap-6042077398117050700-1-c705c4d6-eee7-4155-8959-c9c87c8ee790.avro, /tmp/junit2992201386166743058/logs-742071/metadata/c705c4d6-eee7-4155-8959-c9c87c8ee790-m0.avro, /tmp/junit2992201386166743058/logs-742071/metadata/version-hint.text, /tmp/junit2992201386166743058/logs-742071/metadata/v2.metadata.json] / \r\n\r\ndata files in partition range: [/tmp/junit2992201386166743058/logs-742071/data/date=2020-02-03/level=INFO/id_bucket=0/message_trunc=info+/timestamp_hour=2020-02-03-00/00005-104-9228c157-fa0d-4a0c-a7f0-0aa69acee184-00001.parquet, /tmp/junit2992201386166743058/logs-742071/data/date=2020-02-02/level=INFO/id_bucket=0/message_trunc=info+/timestamp_hour=2020-02-02-01/00002-101-0fd6786b-fae7-400b-a696-bcbed73dfcde-00001.parquet, /tmp/junit2992201386166743058/logs-742071/data/date=2020-02-04/level=DEBUG/id_bucket=0/message_trunc=debug/timestamp_hour=2020-02-04-01/00007-106-500c178d-079a-4afa-b0d3-4a396c4fb630-00001.parquet]\r\n\r\norg.apache.iceberg.spark.source.TestPartitionPruning24 > testPartitionPruningBucketingInteger[1] FAILED\r\n    java.lang.AssertionError: Some of data files in partition range should be read. \r\n\r\nRead files in query: [/tmp/junit6453382097752467142/logs-387757/metadata/v2.metadata.json, /tmp/junit6453382097752467142/logs-387757/metadata/snap-3808976267395158188-1-411c0913-6e93-43a5-a375-a7e6f7b0a5a8.avro, /tmp/junit6453382097752467142/logs-387757/metadata/version-hint.text, /tmp/junit6453382097752467142/logs-387757/metadata/411c0913-6e93-43a5-a375-a7e6f7b0a5a8-m0.avro] / \r\n\r\ndata files in partition range: [/tmp/junit6453382097752467142/logs-387757/data/date=2020-02-04/level=DEBUG/id_bucket=0/message_trunc=debug/timestamp_hour=2020-02-04-01/00007-217-b5ecf7bf-5207-4470-adf9-58b1571ff7b5-00001.parquet, /tmp/junit6453382097752467142/logs-387757/data/date=2020-02-03/level=INFO/id_bucket=0/message_trunc=info+/timestamp_hour=2020-02-03-00/00005-215-8accdc56-06d0-4fec-9a18-ad10d350ccc8-00001.parquet, /tmp/junit6453382097752467142/logs-387757/data/date=2020-02-02/level=INFO/id_bucket=0/message_trunc=info+/timestamp_hour=2020-02-02-01/00002-212-44a0d2fb-48d3-439a-8818-e800e98b5c7c-00001.parquet]\r\n\r\norg.apache.iceberg.spark.source.TestPartitionPruning24 > testPartitionPruningBucketingInteger[3] FAILED\r\n    java.lang.AssertionError: Some of data files in partition range should be read. \r\n\r\nRead files in query: [/tmp/junit5114414994557432305/logs-606638/metadata/version-hint.text, /tmp/junit5114414994557432305/logs-606638/metadata/snap-7702056315490708928-1-aa5dc06e-b173-4df3-9af0-e6f907ea3c37.avro, /tmp/junit5114414994557432305/logs-606638/metadata/v2.metadata.json, /tmp/junit5114414994557432305/logs-606638/metadata/aa5dc06e-b173-4df3-9af0-e6f907ea3c37-m0.avro] / \r\n\r\ndata files in partition range: [/tmp/junit5114414994557432305/logs-606638/data/date=2020-02-02/level=INFO/id_bucket=0/message_trunc=info+/timestamp_hour=2020-02-02-01/00002-434-39ad40a9-3b76-47ec-820b-5a305edabd79-00001.orc, /tmp/junit5114414994557432305/logs-606638/data/date=2020-02-03/level=INFO/id_bucket=0/message_trunc=info+/timestamp_hour=2020-02-03-00/00005-437-e7545e12-e5f8-40cd-98e7-177969365c5a-00001.orc, /tmp/junit5114414994557432305/logs-606638/data/date=2020-02-04/level=DEBUG/id_bucket=0/message_trunc=debug/timestamp_hour=2020-02-04-01/00007-439-d424d5d2-336f-484c-a499-6478f11f1602-00001.orc]\r\n\r\norg.apache.iceberg.spark.source.TestPartitionPruning24 > testPartitionPruningBucketingInteger[4] FAILED\r\n    java.lang.AssertionError: Some of data files in partition range should be read. \r\n\r\nRead files in query: [/tmp/junit8082590320046964751/logs-144508/metadata/v2.metadata.json, /tmp/junit8082590320046964751/logs-144508/metadata/361a3e56-58b7-4c4c-827b-cc83326d918a-m0.avro, /tmp/junit8082590320046964751/logs-144508/metadata/snap-1068076646929597670-1-361a3e56-58b7-4c4c-827b-cc83326d918a.avro, /tmp/junit8082590320046964751/logs-144508/metadata/version-hint.text] / \r\n\r\ndata files in partition range: [/tmp/junit8082590320046964751/logs-144508/data/date=2020-02-03/level=INFO/id_bucket=0/message_trunc=info+/timestamp_hour=2020-02-03-00/00005-548-4b531da9-09c9-4d16-a851-ed6b5cd35379-00001.orc, /tmp/junit8082590320046964751/logs-144508/data/date=2020-02-04/level=DEBUG/id_bucket=0/message_trunc=debug/timestamp_hour=2020-02-04-01/00007-550-6e838e99-33b0-43b7-b2af-41585e12b4ea-00001.orc, /tmp/junit8082590320046964751/logs-144508/data/date=2020-02-02/level=INFO/id_bucket=0/message_trunc=info+/timestamp_hour=2020-02-02-01/00002-545-8c2a7444-646f-45ea-b082-f4db470db1c3-00001.orc]\r\n```\r\n\r\nThat is not related to the timestamp issue - tracker doesn't see any \"data\" files being read, and query result looks to be (surprisingly) empty. (I modified the test a bit to check whether the query result is empty or not, and it failed on the check.) It's not reproducible in IDEA, very weird.\r\n\r\nDigging on this.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698148650/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698151134","html_url":"https://github.com/apache/iceberg/pull/1495#issuecomment-698151134","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1495","id":698151134,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODE1MTEzNA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T06:49:13Z","updated_at":"2020-09-24T06:49:13Z","author_association":"CONTRIBUTOR","body":"> @pvary, why does this set the Iceberg schema using a table property?\r\n\r\nIt will be quite convoluted to access the Hive DDL columns and types. It will add another layer of complexity which I would like to address in another PR to keep the changes more reviewer friendly 😄  ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698151134/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698158371","html_url":"https://github.com/apache/iceberg/pull/1487#issuecomment-698158371","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1487","id":698158371,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODE1ODM3MQ==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T07:05:58Z","updated_at":"2020-09-24T07:05:58Z","author_association":"CONTRIBUTOR","body":"Never mind. Auto increasing ID mattered, and I fixed the test to not relying on static id values.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698158371/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698174485","html_url":"https://github.com/apache/iceberg/issues/1504#issuecomment-698174485","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1504","id":698174485,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODE3NDQ4NQ==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T07:41:06Z","updated_at":"2020-09-24T07:41:06Z","author_association":"MEMBER","body":"Thanks for reporting this issue, mind to provide the full stacktrace ? I would love to take a look.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698174485/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698183917","html_url":"https://github.com/apache/iceberg/issues/1504#issuecomment-698183917","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1504","id":698183917,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODE4MzkxNw==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T08:00:38Z","updated_at":"2020-09-24T08:00:38Z","author_association":"CONTRIBUTOR","body":"hi, openinx :\r\n\r\nmy spark code is \r\n\r\n`spark.read().format(\"iceberg\").load(\"iceberg_db.iceberg_001\").show();`\r\n\r\nbut I can read the data by presto fine ,so I think the data is ok \r\n\r\nthe full stacktrace is \r\n\r\n`Caused by: java.lang.ClassCastException: org.apache.orc.TypeDescription cannot be cast to org.apache.iceberg.shaded.org.apache.orc.TypeDescription\r\n\tat org.apache.iceberg.orc.OrcIterable.iterator(OrcIterable.java:108)\r\n\tat org.apache.iceberg.spark.source.RowDataReader.open(RowDataReader.java:106)\r\n\tat org.apache.iceberg.spark.source.BaseDataReader.next(BaseDataReader.java:73)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:49)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)`\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698183917/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698263424","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-698263424","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":698263424,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODI2MzQyNA==","user":{"login":"marton-bod","id":19599214,"node_id":"MDQ6VXNlcjE5NTk5MjE0","avatar_url":"https://avatars.githubusercontent.com/u/19599214?v=4","gravatar_id":"","url":"https://api.github.com/users/marton-bod","html_url":"https://github.com/marton-bod","followers_url":"https://api.github.com/users/marton-bod/followers","following_url":"https://api.github.com/users/marton-bod/following{/other_user}","gists_url":"https://api.github.com/users/marton-bod/gists{/gist_id}","starred_url":"https://api.github.com/users/marton-bod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/marton-bod/subscriptions","organizations_url":"https://api.github.com/users/marton-bod/orgs","repos_url":"https://api.github.com/users/marton-bod/repos","events_url":"https://api.github.com/users/marton-bod/events{/privacy}","received_events_url":"https://api.github.com/users/marton-bod/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T10:37:51Z","updated_at":"2020-09-24T10:46:58Z","author_association":"COLLABORATOR","body":"Thanks @rdblue for your comment. I will look into refactoring the solution to use your suggested approach, which I like. \r\n\r\nMy only concern is that because there is a breaking change in the metastore API between Hive2 and 3, there will have to be two `iceberg-hive-metastore` module versions, one for Hive3 (`iceberg-mr-hive3` would use this for the HiveCatalog tests) and one for Hive2 (the rest of the modules would use this). Not sure, but hopefully it's possible to create a second metastore module in Gradle pointing to the same source files, but using different dependency versions.\r\n\r\nThe other things I'm thinking is that if the hive2 specific parts are not factored out from `iceberg-mr`, when `iceberg-mr-hive3` pulls that in as a dependency, but using Hive3, those couple of ObjectInspector classes would not compile","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698263424/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698279480","html_url":"https://github.com/apache/iceberg/pull/1495#issuecomment-698279480","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1495","id":698279480,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODI3OTQ4MA==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T11:15:58Z","updated_at":"2020-09-24T11:15:58Z","author_association":"CONTRIBUTOR","body":"I did a little bit of testing with this on a distributed Hive cluster. Results below.\r\n\r\n1. Tried to create a table with no location set, got error `java.lang.IllegalArgumentException: Table location not set` - that looks correct.\r\n2. Created a table with location set to an HDFS path - this worked and the location folder was created.\r\n3. \"SELECT *\" from this table returned no result - as expected.\r\n4. I then tried an insert which didn't work, I might have the syntax of the insert command wrong?\r\n` insert into foo.iceberg_customers select named_struct(\"customer_id\",\"999\",\"first_name\",\"some first name\");`\r\n`FAILED: SemanticException [Error 10044]: Line 1:12 Cannot insert into target table because column number/types are different 'iceberg_customers': Table insclause-0 has 2 columns, but query has 1 columns.`\r\n5. I then dropped the table - this worked, it was correctly removed from the metastore and the location folder from HDFS.\r\n\r\nIt might be good to have a HiveRunner test that creates a table, does an insert and then reads the values back to check that that all works end to end?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698279480/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698282548","html_url":"https://github.com/apache/iceberg/pull/1495#issuecomment-698282548","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1495","id":698282548,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODI4MjU0OA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T11:22:59Z","updated_at":"2020-09-24T11:22:59Z","author_association":"CONTRIBUTOR","body":"> I did a little bit of testing with this on a distributed Hive cluster. Results below.\r\n\r\nReally appreciate your help here! Thanks!\r\n\r\n> 4. I then tried an insert which didn't work, I might have the syntax of the insert command wrong?\r\n>    ` insert into foo.iceberg_customers select named_struct(\"customer_id\",\"999\",\"first_name\",\"some first name\");`\r\n>    `FAILED: SemanticException [Error 10044]: Line 1:12 Cannot insert into target table because column number/types are different 'iceberg_customers': Table insclause-0 has 2 columns, but query has 1 columns.`\r\n\r\nI would expect this to work (with #1407 also applied)\r\n```sql\r\ninsert into foo.iceberg_customers select 999,\"some first name\";\r\n```\r\nor\r\n```sql\r\ninsert into foo.iceberg_customers values(999,\"some first name\");\r\n```\r\n\r\nTo be honest, I have never used \"named_struct\" before 😄 \r\n\r\n> It might be good to have a HiveRunner test that creates a table, does an insert and then reads the values back to check that that all works end to end?\r\n\r\nThat's the plan, but we need the writer code to get in first 😄 \r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698282548/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698290204","html_url":"https://github.com/apache/iceberg/issues/1504#issuecomment-698290204","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1504","id":698290204,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODI5MDIwNA==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T11:40:40Z","updated_at":"2020-09-24T11:40:40Z","author_association":"MEMBER","body":"I checked the case under my host , seems I did not encounter the problem you described: \r\n\r\n```bash\r\n$ bin/spark-shell \\\r\n    --packages org.apache.iceberg:iceberg-spark-runtime:0.9.1 \\\r\n    --conf spark.sql.warehouse.dir=/Users/openinx/test/hive-warehouse \\\r\n    --conf spark.hadoop.hive.metastore.uris=thrift://localhost:9083\r\n\r\nscala> spark.read.format(\"iceberg\").load(\"iceberg_db.sample\").show()\r\n+---+----+                                                                      \r\n| id|data|\r\n+---+----+\r\n|  1|   a|\r\n+---+----+\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698290204/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698296191","html_url":"https://github.com/apache/iceberg/issues/1504#issuecomment-698296191","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1504","id":698296191,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODI5NjE5MQ==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T11:54:23Z","updated_at":"2020-09-24T11:54:23Z","author_association":"CONTRIBUTOR","body":"Describing all versions would be pretty much helpful. In this case, Spark version, Flink version, Iceberg version(s) if you used different version for Spark and Flink.\r\n\r\nIt would be even better if you don't mind sharing the table schema and how to create Iceberg table, and insert rows.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698296191/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698336718","html_url":"https://github.com/apache/iceberg/issues/1437#issuecomment-698336718","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1437","id":698336718,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODMzNjcxOA==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T13:15:15Z","updated_at":"2020-09-24T13:15:15Z","author_association":"CONTRIBUTOR","body":" I have meet the same issue, for the modification of this issue, I have a few thoughts:\r\n 1. Specify the hive conf dir like flink, and then construct HiveConf through the hive-site.xml in this dir, but in this case, if flink is started in application mode, the cluster will can not load the  hive configuration on the jobmanager of flink, There is maybe no hive-site.xml file on yarn cluster\r\n 2. We construct a HiveConf by the user's parameters. What I tested is that at least two parameters are required to construct HiveConf, hive metastore uri and warehouse,\r\n However, if the user's hive metastore configuration is more complicated, such as authorization verification, etc., then the construction of HiveConf will become very complicated. We need to provide parameters for the user to accept information such as user permissions.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698336718/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698353060","html_url":"https://github.com/apache/iceberg/issues/1504#issuecomment-698353060","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1504","id":698353060,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODM1MzA2MA==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T13:44:05Z","updated_at":"2020-09-25T01:25:37Z","author_association":"CONTRIBUTOR","body":"my spark version is 2.4.5 ,and flink version is 1.11.0, but my hive version is 3.1.2 ,the hive-metastore version of iceberg is 2.3.6,if I use this version ,it will result in jar conflict。\r\n\r\nSo I modified the hive version of iceberg to 3.1.2, and re-compile the jar using the master branch. I don't know if it is caused by this modification. But in the same spark program, there is no problem reading the iceberg table created by java api 。\r\n\r\nAnd Presto has no problem reading the tables created by spark and flink\r\n\r\nI will test it carefully again.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698353060/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698357438","html_url":"https://github.com/apache/iceberg/issues/1502#issuecomment-698357438","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1502","id":698357438,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODM1NzQzOA==","user":{"login":"waterlx","id":13976495,"node_id":"MDQ6VXNlcjEzOTc2NDk1","avatar_url":"https://avatars.githubusercontent.com/u/13976495?v=4","gravatar_id":"","url":"https://api.github.com/users/waterlx","html_url":"https://github.com/waterlx","followers_url":"https://api.github.com/users/waterlx/followers","following_url":"https://api.github.com/users/waterlx/following{/other_user}","gists_url":"https://api.github.com/users/waterlx/gists{/gist_id}","starred_url":"https://api.github.com/users/waterlx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/waterlx/subscriptions","organizations_url":"https://api.github.com/users/waterlx/orgs","repos_url":"https://api.github.com/users/waterlx/repos","events_url":"https://api.github.com/users/waterlx/events{/privacy}","received_events_url":"https://api.github.com/users/waterlx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T13:50:21Z","updated_at":"2020-09-24T13:51:22Z","author_association":"CONTRIBUTOR","body":"@rdblue Could you please share your idea about that? if I understand that correctly, you would like to make HiveClientPool (created by HiveCatalog) unavailable to use right after HiveCatalog is closed or garbage-collected. So HiveClientPool is supposed to have the same life cycle as HiveCatalog?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698357438/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698380788","html_url":"https://github.com/apache/iceberg/pull/1495#issuecomment-698380788","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1495","id":698380788,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODM4MDc4OA==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T14:28:05Z","updated_at":"2020-09-24T14:28:05Z","author_association":"CONTRIBUTOR","body":"> That's the plan, but we need the writer code to get in first smile\r\n\r\nAh yes, of course, I'm losing track of the different pull requests ;)\r\n\r\nThe reason I tried `select named_struct` was I thought that was the only way to match the field names up with their values, I didn't realise one could still do positional inserts using `values`. Anyway, I tried that and got\r\n`FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask`\r\non the Hive shell, looking in the log I got \r\n`2020-09-24T14:19:10,286 ERROR [b4e7cb36-2cbb-42ea-8a22-630b0850c204 main([])]: exec.Task (TezTask.java:execute(230)) - Failed to execute tez graph.\r\njava.lang.UnsupportedOperationException: Writing to an Iceberg table with Hive is not supported`\r\nwhich makes sense. Is there any way to get the exception message to appear in the shell, just that one line would be more user friendly than `return code 1`.\r\n\r\nI then set the execution engine to MR and got \r\n`java.lang.UnsupportedOperationException: Writing to an Iceberg table with Hive is not supported\r\n        at org.apache.iceberg.mr.hive.HiveIcebergOutputFormat.checkOutputSpecs(HiveIcebergOutputFormat.java:37)\r\n`\r\nin the shell. But yes, this is because I don't also have #1407 merged in so that makes sense. If I have some time I'll try again with that. At least the above shows some progress and that the expected code paths are being hit.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698380788/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698405831","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-698405831","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":698405831,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQwNTgzMQ==","user":{"login":"danielcweeks","id":4925077,"node_id":"MDQ6VXNlcjQ5MjUwNzc=","avatar_url":"https://avatars.githubusercontent.com/u/4925077?v=4","gravatar_id":"","url":"https://api.github.com/users/danielcweeks","html_url":"https://github.com/danielcweeks","followers_url":"https://api.github.com/users/danielcweeks/followers","following_url":"https://api.github.com/users/danielcweeks/following{/other_user}","gists_url":"https://api.github.com/users/danielcweeks/gists{/gist_id}","starred_url":"https://api.github.com/users/danielcweeks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielcweeks/subscriptions","organizations_url":"https://api.github.com/users/danielcweeks/orgs","repos_url":"https://api.github.com/users/danielcweeks/repos","events_url":"https://api.github.com/users/danielcweeks/events{/privacy}","received_events_url":"https://api.github.com/users/danielcweeks/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T15:08:04Z","updated_at":"2020-09-24T15:08:04Z","author_association":"CONTRIBUTOR","body":"I would tend to agree with @RussellSpitzer's original point.  I don't feel like cache should be invalidated.  Keep in mind that `createOrReplaceTempView()` is a Dataset operation and though in these examples we are referring to a single table, the query can be arbitrarily complex.  Expiring the caches to reflect the source tables in the more complicated case would result in the cache being more like a materialized view than simply a cache and possibly result in side-effects if you expect the cache to be constant (say using it to update the original table).\r\n\r\nI feel like the cache is more like a temporary table that should reflect the state of the Dataset at the time it was created (though I recognize that cache is not an action, so it may not accurately reflect the table state at the time it's called).","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698405831/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698421717","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-698421717","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":698421717,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQyMTcxNw==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T15:34:31Z","updated_at":"2020-09-24T15:47:27Z","author_association":"CONTRIBUTOR","body":"I see merits in both approaches but the biggest concern for me is that we deviate from the way Spark handles cache for built-in tables (and other v1 tables). I am afraid this will cause a lot of confusion for the users. Moreover, our `SparkSessionCatalog` is not consistent now. It will invalidate cache for non-Iceberg tables only.\r\n\r\n> a temporary table that should reflect the state of the Dataset at the time it was created\r\n\r\nHow Hive or Presto handle temp views? My understanding was that views provide a way to refer to and reuse a logical plan but they are stateless. @danielcweeks, what do you think about case 2? ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698421717/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698423535","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-698423535","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":698423535,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQyMzUzNQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T15:37:38Z","updated_at":"2020-09-24T15:37:38Z","author_association":"CONTRIBUTOR","body":"@dbtsai @dongjoon-hyun @viirya is there a description of how Spark should behave in the cases outlined above? Or is it up to catalogs to interpret that?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698423535/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698425965","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-698425965","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":698425965,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQyNTk2NQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T15:41:47Z","updated_at":"2020-09-24T15:41:47Z","author_association":"CONTRIBUTOR","body":"@danielcweeks, I think I misunderstood your comment after reading it once more. You were referring to case 1 only, right?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698425965/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698436564","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-698436564","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":698436564,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQzNjU2NA==","user":{"login":"danielcweeks","id":4925077,"node_id":"MDQ6VXNlcjQ5MjUwNzc=","avatar_url":"https://avatars.githubusercontent.com/u/4925077?v=4","gravatar_id":"","url":"https://api.github.com/users/danielcweeks","html_url":"https://github.com/danielcweeks","followers_url":"https://api.github.com/users/danielcweeks/followers","following_url":"https://api.github.com/users/danielcweeks/following{/other_user}","gists_url":"https://api.github.com/users/danielcweeks/gists{/gist_id}","starred_url":"https://api.github.com/users/danielcweeks/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/danielcweeks/subscriptions","organizations_url":"https://api.github.com/users/danielcweeks/orgs","repos_url":"https://api.github.com/users/danielcweeks/repos","events_url":"https://api.github.com/users/danielcweeks/events{/privacy}","received_events_url":"https://api.github.com/users/danielcweeks/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T16:00:18Z","updated_at":"2020-09-24T16:00:18Z","author_association":"CONTRIBUTOR","body":"It feels like the biggest issue here is that behavior of cache is not consistent or well-defined (as far as I can tell).  I believe the operation originated from RDDs which were intended to be immutable.  So that would imply that the cache would not change regardless of what happens externally.  The intent was that a cache is fault tolerant and will be reconstituted from the original transforms.  That would imply that Sparks behavior for built in tables is incorrect.\r\n\r\n(You're correct, I was only talking about the first case).  \r\n\r\nPresto and Hive would treat a view as a view and always resolve back to the original table (the only case where this would not be true is in context of a transaction, but that's not implemented).\r\n\r\nThe second case feels more clear to me.  A view should always reflect the current state of the underlying sources, so I would expect what you laid out in the example to be the expected behavior (though it currently doesn't behave that way).\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698436564/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698437297","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698437297","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698437297,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQzNzI5Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T16:01:34Z","updated_at":"2020-09-24T16:01:34Z","author_association":"CONTRIBUTOR","body":"The changes look good. Just a quick sanity check: what happens in Hive if the serde or storage handler class is missing? Will this break metastore operations on the table?\r\n\r\nI remember that a missing class used to break any query that loaded the table, including DROP TABLE. So you couldn't even clean up problems.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698437297/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698439361","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698439361","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698439361,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQzOTM2MQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T16:05:25Z","updated_at":"2020-09-24T16:05:25Z","author_association":"CONTRIBUTOR","body":"> The changes look good. Just a quick sanity check: what happens in Hive if the serde or storage handler class is missing? Will this break metastore operations on the table?\r\n> \r\n> I remember that a missing class used to break any query that loaded the table, including DROP TABLE. So you couldn't even clean up problems.\r\n\r\nThat was my fear as well, but did not have time to test them out yet. It might even cause issues for users who are not using the Hive tables, just want to use HiveCatalog to store the table versions.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698439361/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698443907","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698443907","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698443907,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQ0MzkwNw==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T16:13:28Z","updated_at":"2020-09-24T16:13:28Z","author_association":"CONTRIBUTOR","body":"> That was my fear as well, but did not have time to test them out yet. It might even cause issues for users who are not using the Hive tables, just want to use HiveCatalog to store the table versions.\r\n\r\nYour comment give me a better idea how to test. Changed the serde to an arbitrary string, and got this Exception:\r\n```\r\nFAILED: RuntimeException org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.iceberg.mr.hive.HiveIcebergStorageHandler3\r\n[a3db973d-5b1f-4fde-985b-ad5aa4d09018 Thread-4] ERROR org.apache.hadoop.hive.ql.Driver - FAILED: RuntimeException org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.iceberg.mr.hive.HiveIcebergStorageHandler3\r\njava.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.iceberg.mr.hive.HiveIcebergStorageHandler3\r\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:297)\r\n\tat org.apache.hadoop.hive.ql.metadata.Table.getInputFormatClass(Table.java:307)\r\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:2041)\r\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:1934)\r\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:11080)\r\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11133)\r\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10807)\r\n\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258)\r\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:512)\r\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\r\n\tat org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1295)\r\n\tat org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:204)\r\n\tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:290)\r\n\tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:320)\r\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:530)\r\n\tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:500)\r\n\tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:265)\r\n\tat com.klarna.hiverunner.HiveServerContainer.executeStatement(HiveServerContainer.java:120)\r\n\tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementsWithCommandShellEmulation(HiveShellBase.java:115)\r\n\tat com.klarna.hiverunner.builder.HiveShellBase.executeStatementWithCommandShellEmulation(HiveShellBase.java:109)\r\n\tat com.klarna.hiverunner.builder.HiveShellBase.executeStatement(HiveShellBase.java:99)\r\n\tat org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest.testScanEmptyTable(HiveIcebergStorageHandlerBaseTest.java:119)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat com.klarna.hiverunner.StandaloneHiveRunner.evaluateStatement(StandaloneHiveRunner.java:165)\r\n\tat com.klarna.hiverunner.HiveRunnerRule$HiveRunnerRuleStatement.evaluate(HiveRunnerRule.java:82)\r\n\tat com.klarna.hiverunner.ThrowOnTimeout$1.run(ThrowOnTimeout.java:60)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n```\r\n\r\nIs it ok to require \"iceberg-mr\" classes on the classpath to use HiveCatalog?\r\n\r\nI would be afraid of this requirement until we do not have a Catalog implementation which could serve as a replacement catalog in case we do not have HDFS at hand.\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698443907/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698445928","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698445928","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698445928,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQ0NTkyOA==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T16:17:15Z","updated_at":"2020-09-24T16:17:15Z","author_association":"CONTRIBUTOR","body":"> > The changes look good. Just a quick sanity check: what happens in Hive if the serde or storage handler class is missing? Will this break metastore operations on the table?\r\n> > I remember that a missing class used to break any query that loaded the table, including DROP TABLE. So you couldn't even clean up problems.\r\n> \r\n> That was my fear as well, but did not have time to test them out yet. It might even cause issues for users who are not using the Hive tables, just want to use HiveCatalog to store the table versions.\r\n\r\nI'm pretty sure that if you try to perform any operations on tables with the StorageHandler set on them, and you don't have that class on the classpath, it will fail with an error. I've been in exactly the situation you describe where I couldn't issue a DROP TABLE for a table that was created by a test storage handler that no longer existed.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698445928/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698457408","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-698457408","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":698457408,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQ1NzQwOA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T16:39:07Z","updated_at":"2020-09-24T16:39:07Z","author_association":"CONTRIBUTOR","body":"> It feels like the biggest issue here is that behavior of cache is not consistent or well-defined\r\n\r\nThat's what I think as well. Previously, `InsertIntoDataSourceCommand` always invalidated the cache and data sources did not have an option to change that behavior. Right now, Spark does not do anything automatically for custom catalogs (but does match the old behavior in its catalog) so I am not sure whether the interpretation is now up to the catalog or not.\r\n\r\nAlso, caching of V2 tables is not supported at all now.\r\n\r\n```\r\n// throws CACHE TABLE is only supported with temp views or v1 tables.\r\nspark.sql(\"CACHE TABLE \" + tableName)\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698457408/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698477361","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698477361","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698477361,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQ3NzM2MQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T17:18:40Z","updated_at":"2020-09-24T17:18:40Z","author_association":"CONTRIBUTOR","body":"> > > The changes look good. Just a quick sanity check: what happens in Hive if the serde or storage handler class is missing? Will this break metastore operations on the table?\r\n> > > I remember that a missing class used to break any query that loaded the table, including DROP TABLE. So you couldn't even clean up problems.\r\n> > \r\n> > \r\n> > That was my fear as well, but did not have time to test them out yet. It might even cause issues for users who are not using the Hive tables, just want to use HiveCatalog to store the table versions.\r\n> \r\n> I'm pretty sure that if you try to perform any operations on tables with the StorageHandler set on them, and you don't have that class on the classpath, it will fail with an error. I've been in exactly the situation you describe where I couldn't issue a DROP TABLE for a table that was created by a test storage handler that no longer existed.\r\n\r\nThe tests caught the exact same problem:\r\n```\r\norg.apache.iceberg.spark.sql.TestAlterTable > testSetTableProperties[2] FAILED\r\n    java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.iceberg.mr.hive.HiveIcebergStorageHandler\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:297)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:465)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:424)\r\n        at scala.Option.map(Option.scala:230)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:424)\r\n```\r\n\r\nMy first ideas:\r\n1. Check that the classes are on the classpath before creating the table? - This would only fix the problem if the creator/reader of the table is both have the same things on the classpath.\r\n2. Move the HiveIcebergStorageHandler / HiveIcebergSerDe to their own package and try to shave of the dependencies - Seems like a hard solution and we will eventually fall back to Reflection\r\n3. Go full reflection: Create a reflection based SerDe, StorageHandler which checks the classpath for the actual classes and if successful wraps them?\r\n\r\n@rdblue, @massdosage: What do you think about the 3rd option?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698477361/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698491605","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698491605","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698491605,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQ5MTYwNQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T17:47:17Z","updated_at":"2020-09-24T17:47:17Z","author_association":"CONTRIBUTOR","body":"Would option 3 work? The reflection-based loader would need to be in the Hive classpath at all times. So we would have the same problem with that one right? This is a problem that Hive would need to fix.\r\n\r\nI lean toward the approach of making it optional to set the serde and storage handler, and using either reflection to see if the classes currently exist or a table property to change the behavior. A table property makes sense to me: `engine.hive.enabled=true` could signal that these should be real classes.\r\n\r\nWhat do you think?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698491605/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698497777","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-698497777","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":698497777,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODQ5Nzc3Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T17:59:10Z","updated_at":"2020-09-24T17:59:10Z","author_association":"CONTRIBUTOR","body":"> there is a breaking change in the metastore API between Hive2 and 3\r\n\r\nWhat was the incompatibility? Ideally, we will handle it with reflection to avoid needing a different module.\r\n\r\n> if the hive2 specific parts are not factored out from iceberg-mr, when iceberg-mr-hive3 pulls that in as a dependency, but using Hive3, those couple of ObjectInspector classes would not compile\r\n\r\nThe classes are already compiled. We just have to avoid loading them. So we would use different class names for the inspectors between 2 and 3 and load the correct one using reflection depending on whether the detected Hive version in 2 or 3.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698497777/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698518875","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698518875","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698518875,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODUxODg3NQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T18:40:43Z","updated_at":"2020-09-24T18:40:43Z","author_association":"CONTRIBUTOR","body":"> Would option 3 work? The reflection-based loader would need to be in the Hive classpath at all times. So we would have the same problem with that one right? This is a problem that Hive would need to fix.\r\n\r\nWithout the HiveIcebergStorageHandler, Hive will not even know the columns in the table (some historic version is stored in HMS, but not considered correct and queried again from the StorageHandler every time). So Hive considers these tables corrupt. Not being able to drop them is definitely a problem, but for the other functions it is questionable if we provide any data / possibility to manipulate.\r\n\r\n> I lean toward the approach of making it optional to set the serde and storage handler, and using either reflection to see if the classes currently exist or a table property to change the behavior. A table property makes sense to me: `engine.hive.enabled=true` could signal that these should be real classes.\r\n\r\nConfig still would mean that if we turn that on for a table then we need the HiveIcebergStorageHandler on the classpath for even the Spark processes which are accessing the table.\r\n\r\nIn my opinion this is really a static/dynamic decision. With config we statically set a table to Hive or Spark readable one. With the reflection based solution we allow the accessor to dynamically decide which StorageHandler is needed. But you are absolutely right that if Hive access is required, then something has to be on the classpath.\r\n\r\nWould combining the 2 be an overkill?\r\nLike config if you want or not, and if you want ReflectionStorageHandler would be set. So if enabled the Spark could still access the HiveCatalog without the StorageHandler, and Hive will use HiveIcebergStorageHandler.\r\n\r\nI see the following uses-cases:\r\n1. Users have data on S3 but need some consistent store on the cloud. No HDFS at hand, so they need HiveCatalog, but really uses only Spark\r\n2. Users use Spark and only occasionally Hive. Since they already have Hive at hand, uses HiveCatalog\r\n3. Users use Spark and Hive too. HiveCatalog is an obvious choice\r\n4. Users use Hive but want to use Iceberg for the partitioning and such\r\n\r\nYou might know more about other use-cases to consider...","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698518875/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698533716","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698533716","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698533716,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODUzMzcxNg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T19:09:08Z","updated_at":"2020-09-24T19:09:08Z","author_association":"CONTRIBUTOR","body":"I don't think the reflection option would have any benefit because Hive would still need `ReflectionStorageHandler` in the classpath. If we can guarantee that something is in the classpath, then we should use `HiveIcebergStorageHandler` so everything works.\r\n\r\nThen we have the choice between whether we break Hive or not. I think a config property is the right way to go so that it is up to the user or administrator whether to expect Iceberg in the classpath. I think maybe a Hadoop Configuration option in hive-site.xml is a good idea.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698533716/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698558967","html_url":"https://github.com/apache/iceberg/pull/1487#issuecomment-698558967","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1487","id":698558967,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODU1ODk2Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T20:01:09Z","updated_at":"2020-09-24T20:01:09Z","author_association":"CONTRIBUTOR","body":"@HeartSaVioR, it looks like the purpose of these tests is to validate that predicate pushdown filters partitions correctly. Have you looked at `TestFilteredScan`? That is very similar. Maybe we don't need both?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698558967/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698563166","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698563166","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698563166,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODU2MzE2Ng==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T20:09:50Z","updated_at":"2020-09-24T20:10:48Z","author_association":"CONTRIBUTOR","body":"> I don't think the reflection option would have any benefit because Hive would still need `ReflectionStorageHandler` in the classpath.\r\n\r\nThis is absolutely true for Hive!\r\nI am a little bit concerned about the Spark packages. We might be better of without the iceberg-hive-mr package classes in the iceberg-spark-runtime libraries. The HiveIcebergStorageHandler would be needed only to be present there and would not be used at all, like every other class in the iceberg-hive-mr package if only Spark is used to access the Iceberg tables.\r\n\r\nIf you think that extra few classes is an acceptable compromise to have less complexity, then I would be happy to move forward with the configuration solution.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698563166/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698580233","html_url":"https://github.com/apache/iceberg/pull/1399#issuecomment-698580233","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1399","id":698580233,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODU4MDIzMw==","user":{"login":"edgarRd","id":266670,"node_id":"MDQ6VXNlcjI2NjY3MA==","avatar_url":"https://avatars.githubusercontent.com/u/266670?v=4","gravatar_id":"","url":"https://api.github.com/users/edgarRd","html_url":"https://github.com/edgarRd","followers_url":"https://api.github.com/users/edgarRd/followers","following_url":"https://api.github.com/users/edgarRd/following{/other_user}","gists_url":"https://api.github.com/users/edgarRd/gists{/gist_id}","starred_url":"https://api.github.com/users/edgarRd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/edgarRd/subscriptions","organizations_url":"https://api.github.com/users/edgarRd/orgs","repos_url":"https://api.github.com/users/edgarRd/repos","events_url":"https://api.github.com/users/edgarRd/events{/privacy}","received_events_url":"https://api.github.com/users/edgarRd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T20:47:34Z","updated_at":"2020-09-24T20:47:34Z","author_association":"CONTRIBUTOR","body":"@rdblue I've updated the comment on the default. I was wondering if the relocation of the tests could be done in a follow up PR just to make the diff clear. Thanks!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698580233/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698615970","html_url":"https://github.com/apache/iceberg/pull/1506#issuecomment-698615970","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1506","id":698615970,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODYxNTk3MA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T22:13:28Z","updated_at":"2020-09-24T22:13:28Z","author_association":"CONTRIBUTOR","body":"Thanks @shangxinli! It is nice to have this update since it should make our runtime Jars much smaller.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698615970/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698617667","html_url":"https://github.com/apache/iceberg/pull/1487#issuecomment-698617667","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1487","id":698617667,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODYxNzY2Nw==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T22:18:16Z","updated_at":"2020-09-24T22:18:50Z","author_association":"CONTRIBUTOR","body":"@rdblue \r\n\r\nYes, but the rationalization looks to be a bit different. As I wrote in the PR title, the new test suite is intended to run E2E test and see whether Iceberg and Spark works correctly as we expected. It only uses Iceberg API on creating table and partition spec (because Spark 2.4 doesn't support DDL, probably also a good chance to go through DDL for Spark 3?), and for the remaining part we don't leverage anything on Iceberg internal.\r\n\r\nDue to the difference of rationalization there seems to be some differences on coverage:\r\n\r\n* TestFilteredScan only applies single partition criteria per test, while the new test suite apply all partition criteria on the table. New test suite can additionally check the Iceberg's benefit, partition pruning isn't affected by the order of partition criteria.\r\n* TestFilteredScan checks the tasks after planning input partitions, which is good to count how many data files are being read (if I understand correctly, and also assume planInputPartitions works as expected), but it doesn't provide the actual data files being read. The new test suite goes with hack to track the files being read, which is a bit more complicated, but is able to check the query doesn't read the data files outside of partition range.\r\n* TestFilteredScan has separate implementations between Spark 2.4 and 3 (I'm not sure whether it should be, or just a miss) where the new test suite simply works between two versions (with slight adjust on filter condition for timestamp filter pushdown).\r\n\r\nSo while there're common parts between twos, there're also different parts (and rationalizations) between twos. I'm not sure which side to consolidate - I'm not sure we want to ensure Spark-Iceberg E2E query works perfectly in the Iceberg project side - that's up to the community's decision.\r\n\r\nIf we would like to simply ensure Iceberg works as expected, probably just need to tweak a bit on TestFilteredScan to have `truncate`, and probably change the table partition spec to statically have all partitions for all tests instead of one partition per test. (My understanding is that tests should still work after the change. Do I understand correctly?) Once the decision has taken in this direction I might move the new test in our side, as I still need to ensure E2E query works nicely.\r\n\r\nOtherwise, probably we just need to add date partition on the new test suite and have a new test on date partition, and probably have tests for between on timestamp partitions to cover the tests on partitioned table in TestFilteredScan. Not sure we'd like to remove TestFilteredScan after enriching the new test suite, as it still has tests on unpartitioned table, and good to check whether the problem comes from Iceberg vs Spark - E2E test would fail on problems from any side.\r\n\r\nPlease let me know what you think. Thanks!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698617667/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698621585","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-698621585","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":698621585,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODYyMTU4NQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T22:30:41Z","updated_at":"2020-09-24T22:30:41Z","author_association":"CONTRIBUTOR","body":"@omalley, I'd be curious to know how Hive handles case 1.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698621585/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698635234","html_url":"https://github.com/apache/iceberg/issues/1502#issuecomment-698635234","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1502","id":698635234,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODYzNTIzNA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-24T23:16:22Z","updated_at":"2020-09-24T23:16:22Z","author_association":"CONTRIBUTOR","body":"Yeah, I think that the lifecycle of the client pool should be the same as the catalog. We want to reuse a client pool to avoid opening too many connections to the Hive MetaStore the catalog is configured to use. The most reasonable place to do that is with the catalog.\r\n\r\nWe could separate them, so the catalog is passed a client pool, but it isn't clear to me what that would accomplish: why would we create identical catalogs that use the same external pool? That would also be difficult to manage because we would have to make sure that the pool's connections are made to the same HMS that the catalog is configured to use.\r\n\r\nIf it doesn't make sense to have an external client pool and we want the client pool to be reused and have a long lifecycle, then it makes sense to keep the catalog around to manage that lifecycle. Sounds like the problem here is that code is creating and discarding a catalog before the tables that were created by it are no longer needed.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698635234/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698662152","html_url":"https://github.com/apache/iceberg/issues/1501#issuecomment-698662152","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1501","id":698662152,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODY2MjE1Mg==","user":{"login":"wypoon","id":3925490,"node_id":"MDQ6VXNlcjM5MjU0OTA=","avatar_url":"https://avatars.githubusercontent.com/u/3925490?v=4","gravatar_id":"","url":"https://api.github.com/users/wypoon","html_url":"https://github.com/wypoon","followers_url":"https://api.github.com/users/wypoon/followers","following_url":"https://api.github.com/users/wypoon/following{/other_user}","gists_url":"https://api.github.com/users/wypoon/gists{/gist_id}","starred_url":"https://api.github.com/users/wypoon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wypoon/subscriptions","organizations_url":"https://api.github.com/users/wypoon/orgs","repos_url":"https://api.github.com/users/wypoon/repos","events_url":"https://api.github.com/users/wypoon/events{/privacy}","received_events_url":"https://api.github.com/users/wypoon/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T00:55:22Z","updated_at":"2020-09-25T00:55:22Z","author_association":"CONTRIBUTOR","body":"I created a PR: https://github.com/apache/iceberg/pull/1508\r\nbut I didn't know how to link it to this issue.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698662152/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698662663","html_url":"https://github.com/apache/iceberg/pull/1508#issuecomment-698662663","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1508","id":698662663,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODY2MjY2Mw==","user":{"login":"wypoon","id":3925490,"node_id":"MDQ6VXNlcjM5MjU0OTA=","avatar_url":"https://avatars.githubusercontent.com/u/3925490?v=4","gravatar_id":"","url":"https://api.github.com/users/wypoon","html_url":"https://github.com/wypoon","followers_url":"https://api.github.com/users/wypoon/followers","following_url":"https://api.github.com/users/wypoon/following{/other_user}","gists_url":"https://api.github.com/users/wypoon/gists{/gist_id}","starred_url":"https://api.github.com/users/wypoon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wypoon/subscriptions","organizations_url":"https://api.github.com/users/wypoon/orgs","repos_url":"https://api.github.com/users/wypoon/repos","events_url":"https://api.github.com/users/wypoon/events{/privacy}","received_events_url":"https://api.github.com/users/wypoon/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T00:57:02Z","updated_at":"2020-09-25T00:57:02Z","author_association":"CONTRIBUTOR","body":"This is for https://github.com/apache/iceberg/issues/1501.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698662663/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698666137","html_url":"https://github.com/apache/iceberg/pull/1346#issuecomment-698666137","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1346","id":698666137,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODY2NjEzNw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T01:10:06Z","updated_at":"2020-09-25T01:10:06Z","author_association":"CONTRIBUTOR","body":"I'm going ahead and merging this because I think the remaining things are minor, but we will probably want to fix them.\r\n\r\nFirst, breaking the tests across two classes could be cleaner. In the future, I'd recommend writing tests and not breaking across a parent/child class until you want to add the other set of tests. That way it is easier to see what is needed and will be clean.\r\n\r\nSecond, I don't understand the benefit of having a separate `ScanOptions` class, besides that it can pull options out of a map. It seems to me that it would be simpler to just have the `FlinkSource.Builder` class and move all of the scan options to that class.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698666137/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698682917","html_url":"https://github.com/apache/iceberg/pull/1346#issuecomment-698682917","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1346","id":698682917,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODY4MjkxNw==","user":{"login":"JingsongLi","id":9601882,"node_id":"MDQ6VXNlcjk2MDE4ODI=","avatar_url":"https://avatars.githubusercontent.com/u/9601882?v=4","gravatar_id":"","url":"https://api.github.com/users/JingsongLi","html_url":"https://github.com/JingsongLi","followers_url":"https://api.github.com/users/JingsongLi/followers","following_url":"https://api.github.com/users/JingsongLi/following{/other_user}","gists_url":"https://api.github.com/users/JingsongLi/gists{/gist_id}","starred_url":"https://api.github.com/users/JingsongLi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JingsongLi/subscriptions","organizations_url":"https://api.github.com/users/JingsongLi/orgs","repos_url":"https://api.github.com/users/JingsongLi/repos","events_url":"https://api.github.com/users/JingsongLi/events{/privacy}","received_events_url":"https://api.github.com/users/JingsongLi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T02:20:04Z","updated_at":"2020-09-25T02:20:04Z","author_association":"CONTRIBUTOR","body":"Thanks @rdblue for the review, I'll address your test comments in next PR (Integrate to SQL).\r\nThe reason for a separate `ScanOptions` is: For SQL layer, `ScanOptions` contains options in `CREATE TABLE ... WITH (options)`, while other parameters of `FlinkSource.Builder` are not. This means that the parameters in `ScanOptions` must be in the form of string.\r\nIf you don't think it is necessary, we can also merge it into the builder.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698682917/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698685259","html_url":"https://github.com/apache/iceberg/issues/1437#issuecomment-698685259","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1437","id":698685259,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODY4NTI1OQ==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T02:30:01Z","updated_at":"2020-09-25T02:30:01Z","author_association":"MEMBER","body":"> but in this case, if flink is started in application mode, the cluster will can not load the hive configuration on the jobmanager of flink, There is maybe no hive-site.xml file on yarn cluster\r\n\r\nThis should not be a problem,  because we would load the hive configuration files at the client side and then construct the `IcebergStreamWriter` and `IcebergFilesCommitter`, those classes will serialize the configurations and send them to job manager,  finally the job manager would deserialize to get the `Configuration`. \r\n\r\nMaking the property-value pairs as arguments  sounds reasonable for testing or demo purpose, but it not suitable for production jobs I think. \r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698685259/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698708665","html_url":"https://github.com/apache/iceberg/issues/1437#issuecomment-698708665","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1437","id":698708665,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODcwODY2NQ==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T04:08:15Z","updated_at":"2020-10-14T01:49:59Z","author_association":"CONTRIBUTOR","body":"hi,@openinx \r\n\r\nFor example, I have a flink jar program, which consumes data from Kafka and writes it to iecberg table. If the jar package contains the local configuration path of hive, I upload the jar to the hdfs directory, and my startup command is as follows:\r\n\r\n```\r\n/home/work/software/flink/bin/flink run-application -p 5 -d -t yarn-application  \\\r\n-yD yarn.provided.lib.dirs=\"hdfs://hadoopcluster/data/flink/libs/\" \\\r\nhdfs://hadoopcluster/data/flink/user-lib/flink-kafka-iceberg.jar  \r\n```\r\n\r\nIn this mode, the user jar is parsed on the jobmanager node of the flink cluster. I think it may not be possible to load the local configuration file.\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698708665/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698853877","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-698853877","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":698853877,"node_id":"MDEyOklzc3VlQ29tbWVudDY5ODg1Mzg3Nw==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T10:29:04Z","updated_at":"2020-09-25T10:29:04Z","author_association":"CONTRIBUTOR","body":"> Then we have the choice between whether we break Hive or not. I think a config property is the right way to go so that it is up to the user or administrator whether to expect Iceberg in the classpath. I think maybe a Hadoop Configuration option in hive-site.xml is a good idea.\r\n\r\nImplemented the config property + hive-site.xml solution.\r\n@rdblue: Could you please check that this is the same solution you were thinking about?\r\nAlso please double check the dependency changes I have added.\r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/698853877/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699056581","html_url":"https://github.com/apache/iceberg/pull/1346#issuecomment-699056581","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1346","id":699056581,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTA1NjU4MQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T17:30:02Z","updated_at":"2020-09-25T17:30:02Z","author_association":"CONTRIBUTOR","body":"Let's take a look at `ScanOptions` in the next PR then. I would prefer to keep user-facing APIs simple, rather than leaking a SQL concern (options come from `WITH`) to users (need to use two builders). Since SQL will most likely use the `fromProperties` method, it may make sense to use a single builder, add `withProperties`, and pass properties from SQL as a map.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699056581/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699058530","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-699058530","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":699058530,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTA1ODUzMA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T17:33:57Z","updated_at":"2020-09-28T22:22:04Z","author_association":"CONTRIBUTOR","body":"Okay, I understand what you're saying about `ReflectionStorageHandler` now (for Spark). But I don't think we need it because the Spark code interacts with the table through the Iceberg library. It won't try to instantiate the storage handler because that's specific to the Hive integration.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699058530/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699064644","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-699064644","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":699064644,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTA2NDY0NA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T17:47:00Z","updated_at":"2020-09-25T17:47:00Z","author_association":"CONTRIBUTOR","body":"The approach looks good to me. Thanks @pvary!\r\n\r\nShould we also add this to table property documentation?\r\n\r\n@massdosage, you might want to have a look as well. This sets the Hive storage handler and serde if a hive-site.xml property is set, or if the table has a configuration option.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699064644/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699067934","html_url":"https://github.com/apache/iceberg/pull/1507#issuecomment-699067934","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1507","id":699067934,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTA2NzkzNA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T17:53:52Z","updated_at":"2020-09-25T17:53:52Z","author_association":"CONTRIBUTOR","body":"Thanks, @electrum! It would also be great to have you review #1499 while you're looking at the spec.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699067934/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699073055","html_url":"https://github.com/apache/iceberg/pull/1508#issuecomment-699073055","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1508","id":699073055,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTA3MzA1NQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T18:03:49Z","updated_at":"2020-09-25T18:03:49Z","author_association":"CONTRIBUTOR","body":"Thanks for working on this, @wypoon! This was just recently pointed out by another contributor as well. We're currently tracking this in #1029, which has a bit more detail about how this should be built.\r\n\r\nWhile I like that your solution found a way to recover the old snapshot, we don't want to need to read old metadata files to recover it. Iceberg should track the schema that was used for each snapshot. That means a few steps are needed:\r\n\r\n1. Add a list, `schemas`, to table metadata that has all of the schemas for known snapshots, like `partition-specs`\r\n2. Add a `schema-id` to each schema so that the schemas can be referenced by id\r\n3. Add a `schema-id` to each snapshot to track the schema that was current when it was created\r\n\r\nThen we would need to add a `Snapshot.schema` method to retrieve the schema of a snapshot directly. Would you like to work on a PR for the first one?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699073055/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699097359","html_url":"https://github.com/apache/iceberg/pull/1508#issuecomment-699097359","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1508","id":699097359,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTA5NzM1OQ==","user":{"login":"wypoon","id":3925490,"node_id":"MDQ6VXNlcjM5MjU0OTA=","avatar_url":"https://avatars.githubusercontent.com/u/3925490?v=4","gravatar_id":"","url":"https://api.github.com/users/wypoon","html_url":"https://github.com/wypoon","followers_url":"https://api.github.com/users/wypoon/followers","following_url":"https://api.github.com/users/wypoon/following{/other_user}","gists_url":"https://api.github.com/users/wypoon/gists{/gist_id}","starred_url":"https://api.github.com/users/wypoon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wypoon/subscriptions","organizations_url":"https://api.github.com/users/wypoon/orgs","repos_url":"https://api.github.com/users/wypoon/repos","events_url":"https://api.github.com/users/wypoon/events{/privacy}","received_events_url":"https://api.github.com/users/wypoon/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T18:54:48Z","updated_at":"2020-09-25T18:54:48Z","author_association":"CONTRIBUTOR","body":"@rdblue, thanks for the feedback and the pointer to #1029.\r\nAbout step 1 in the steps you outlined, would the .metadata.json file then contain a `schema` field with the current schema and a (new) `schemas` field with a list of all the schemas (including the current)?\r\nSure, I'd be happy to work on a PR for this step.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699097359/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699099796","html_url":"https://github.com/apache/iceberg/pull/1508#issuecomment-699099796","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1508","id":699099796,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTA5OTc5Ng==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T18:59:48Z","updated_at":"2020-09-25T18:59:48Z","author_association":"CONTRIBUTOR","body":"Yes, we would add `schemas` to track recent schemas (referenced by any known snapshot) and keep `schema` for now. We would also add `current-schema-id` to track which schema in `schemas` is the current one. Eventually, we will deprecate `schema` and just use the one identified by id.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699099796/reactions","total_count":4,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699111706","html_url":"https://github.com/apache/iceberg/pull/1509#issuecomment-699111706","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1509","id":699111706,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTExMTcwNg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T19:25:01Z","updated_at":"2020-09-25T19:25:01Z","author_association":"CONTRIBUTOR","body":"Mostly looks good. Just a couple of minor issues.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699111706/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699112968","html_url":"https://github.com/apache/iceberg/pull/1481#issuecomment-699112968","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1481","id":699112968,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTExMjk2OA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T19:27:57Z","updated_at":"2020-09-25T19:27:57Z","author_association":"CONTRIBUTOR","body":"Thanks for the fix! I merged this.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699112968/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699116309","html_url":"https://github.com/apache/iceberg/issues/1511#issuecomment-699116309","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1511","id":699116309,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTExNjMwOQ==","user":{"login":"mehtaashish23","id":75326,"node_id":"MDQ6VXNlcjc1MzI2","avatar_url":"https://avatars.githubusercontent.com/u/75326?v=4","gravatar_id":"","url":"https://api.github.com/users/mehtaashish23","html_url":"https://github.com/mehtaashish23","followers_url":"https://api.github.com/users/mehtaashish23/followers","following_url":"https://api.github.com/users/mehtaashish23/following{/other_user}","gists_url":"https://api.github.com/users/mehtaashish23/gists{/gist_id}","starred_url":"https://api.github.com/users/mehtaashish23/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mehtaashish23/subscriptions","organizations_url":"https://api.github.com/users/mehtaashish23/orgs","repos_url":"https://api.github.com/users/mehtaashish23/repos","events_url":"https://api.github.com/users/mehtaashish23/events{/privacy}","received_events_url":"https://api.github.com/users/mehtaashish23/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T19:35:27Z","updated_at":"2020-09-25T19:35:27Z","author_association":"CONTRIBUTOR","body":"Working on the fix. Looks trivial!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699116309/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699121192","html_url":"https://github.com/apache/iceberg/issues/1437#issuecomment-699121192","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1437","id":699121192,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTEyMTE5Mg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T19:46:49Z","updated_at":"2020-09-25T19:46:49Z","author_association":"CONTRIBUTOR","body":"I think it's a good idea to load `hive-site.xml` and respect its defaults. But for cases like a missing warehouse directory for a catalog, I think we should prefer configuring the warehouse on the catalog itself. It doesn't make sense for multiple catalogs to use the same `hive-site.xml` setting for warehouse directory.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699121192/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699135771","html_url":"https://github.com/apache/iceberg/pull/1399#issuecomment-699135771","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1399","id":699135771,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTEzNTc3MQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T20:20:05Z","updated_at":"2020-09-25T20:20:05Z","author_association":"CONTRIBUTOR","body":"@edgarRd, I had another look. The default is still changed so that a table with no mapping with create one. That needs to be reverted before we can commit this.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699135771/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699168313","html_url":"https://github.com/apache/iceberg/pull/1399#issuecomment-699168313","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1399","id":699168313,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTE2ODMxMw==","user":{"login":"edgarRd","id":266670,"node_id":"MDQ6VXNlcjI2NjY3MA==","avatar_url":"https://avatars.githubusercontent.com/u/266670?v=4","gravatar_id":"","url":"https://api.github.com/users/edgarRd","html_url":"https://github.com/edgarRd","followers_url":"https://api.github.com/users/edgarRd/followers","following_url":"https://api.github.com/users/edgarRd/following{/other_user}","gists_url":"https://api.github.com/users/edgarRd/gists{/gist_id}","starred_url":"https://api.github.com/users/edgarRd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/edgarRd/subscriptions","organizations_url":"https://api.github.com/users/edgarRd/orgs","repos_url":"https://api.github.com/users/edgarRd/repos","events_url":"https://api.github.com/users/edgarRd/events{/privacy}","received_events_url":"https://api.github.com/users/edgarRd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T21:38:24Z","updated_at":"2020-09-25T21:38:24Z","author_association":"CONTRIBUTOR","body":"@rdblue Thanks for taking a look!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699168313/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699178200","html_url":"https://github.com/apache/iceberg/pull/1399#issuecomment-699178200","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1399","id":699178200,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTE3ODIwMA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T22:05:31Z","updated_at":"2020-09-25T22:05:31Z","author_association":"CONTRIBUTOR","body":"Thanks for updating! I'll merge this when tests are passing.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699178200/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699200005","html_url":"https://github.com/apache/iceberg/pull/1503#issuecomment-699200005","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1503","id":699200005,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTIwMDAwNQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-25T22:55:14Z","updated_at":"2020-09-25T22:55:14Z","author_association":"CONTRIBUTOR","body":"We've been talking internally about a similar feature. I've been calling it \"tagged snapshots\" as it is similar to git version tagging. Our users would like to be able to tag a particular snapshot and use that tag for processing.\r\n\r\nFor example, a tag like \"2020q1\" could be used for a financial reporting table to label the version that was used for Q1 reporting. Our users want to keep these tags forever.\r\n\r\nIs your use case similar? I'm interested because I think we might want to have first-class support for named snapshots, instead of having a property that selects by equality. What do you think?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699200005/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699242399","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-699242399","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":699242399,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTI0MjM5OQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-26T00:31:47Z","updated_at":"2020-09-26T00:56:44Z","author_association":"CONTRIBUTOR","body":"I agree with most of what's been said about case 1. I would expect the behavior to be a point in time, but that doesn't fit with what Spark actually does for existing tables. And I think it is right to be concerned about a behavior change.\r\n\r\nBut I don't think that case 1 is an area where Iceberg should make a choice. Caching datasets and invalidating those datasets is implemented above the source in Spark, so I think this is a concern that Spark should solve, not Iceberg. If Spark chooses to invalidate cached data after a write, then that's fine. Similarly, if a `REFRESH` also invalidates cached data, that high-level choice is up to Spark (case 3). I don't think it would be a good idea for Iceberg to call into Spark to refresh or invalidate, when Spark is calling to Iceberg to perform an operation in the first place.\r\n\r\nCase 2 is the one that I think we should worry about. A temporary view is basically a named logical plan, like a `Dataset`, so I'll just refer to them both as a \"dataset\".\r\n\r\nThe first question is where a dataset lies on the spectrum between a view (always up to date) and an RDD (effectively immutable). I think that we probably agree here that **a dataset should have view behavior**, because:\r\n\r\n* A temporary view would have view behavior\r\n* Spark currently has no guarantee that a dataset will give the same result because the planner is used. Each addition to a Dataset results in a different logical plan that gets resolved and optimized in isolation. For example, `df = spark.table(\"t\").filterExpr(\"date = '2020-09-25'\")` could have refinements `odds = df.filterExpr(\"category = 'odd')` and `evens = df.filterExpr(\"category = 'even'\")`. There is no guarantee that `odds.union(evens)` has any relationship to `df` because the extra filters require building separate scans at different times.\r\n* Spark's v1 behavior for refresh is based on invalidating cached metadata about a table's partitions and it can't cache what has not been loaded.\r\n* Spark supports refreshing tables, which appears to also refresh existing datasets.\r\n\r\nIf we agree on what the behavior should be, then I think that the current caching catalog is correct. Because it reuses the same `Table` instance, all datasets that have a reference from that catalog are in sync. Changes to that table will cause it to be refreshed, and the `refresh` command from Spark also works.\r\n\r\nWhen not using the caching catalog, then the behavior would be that each dataset is independent and fixed at load time. I don't think that's correct. So we should implement a way for `SparkTable` to check whether it should refresh the Iceberg table. Then we could have the same behavior regardless of caching. And, we should also consider routing IcebergSource queries through a catalog so they are covered by the same mechanism.\r\n\r\nI think that agrees with all of @aokolnychyi's points, except for the last one. I think it is important to have a consistent version for operations like self-joins. I'd rather invest in a Spark catalog refresh system, although I think the suggestion to refresh tables for each operation would work in the short term.\r\n\r\nAlso, I didn't look much into the point about `recacheByPlan`. Implementing `SparkTable.equals` sounds good to me.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699242399/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699450888","html_url":"https://github.com/apache/iceberg/pull/1505#issuecomment-699450888","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1505","id":699450888,"node_id":"MDEyOklzc3VlQ29tbWVudDY5OTQ1MDg4OA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-26T07:40:31Z","updated_at":"2020-09-26T07:40:31Z","author_association":"CONTRIBUTOR","body":"> Okay, I understand what you're saying about `ReflectionStorageHandler` now (for Spark). But I don't think we need it because the Spark code interacts with the table through the Iceberg library. It won't try to instantiate the storage handler because that's specific to the Hive integraiton.\r\n\r\nWe need the build changes, because the spark sql uses Hive code to interact with hive tables. This fact was highlighted by the failures in the tests.\r\n\r\nYou can repro by reverting the `gradle.build` changes and setting the `ENGINE_HIVE_ENABLED_DEFAULT` to `true`. \r\n```\r\n./gradlew :iceberg-spark3:test --tests org.apache.iceberg.spark.sql.TestCreateTable\r\n```\r\n\r\nOne of the exception what I have got is here:\r\n```\r\n    java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.iceberg.mr.hive.HiveIcebergStorageHandler\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:297)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:465)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:424)\r\n        at scala.Option.map(Option.scala:230)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:424)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:227)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:226)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:276)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:422)\r\n        at org.apache.spark.sql.hive.client.HiveClient.getTable(HiveClient.scala:90)\r\n        at org.apache.spark.sql.hive.client.HiveClient.getTable$(HiveClient.scala:89)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:90)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:120)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTable$1(HiveExternalCatalog.scala:719)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:719)\r\n        at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\r\n        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:446)\r\n        at org.apache.spark.sql.execution.command.DropTableCommand.run(ddl.scala:226)\r\n        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n        at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\r\n        at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\r\n        at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3616)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\r\n        at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3614)\r\n        at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\r\n        at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n        at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:606)\r\n        at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\r\n        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:601)\r\n        at org.apache.iceberg.spark.SparkTestBase.sql(SparkTestBase.java:83)\r\n        at org.apache.iceberg.spark.sql.TestCreateTable.dropTestTable(TestCreateTable.java:47)\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/699450888/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]