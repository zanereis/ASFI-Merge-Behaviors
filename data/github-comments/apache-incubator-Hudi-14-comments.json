[{"url":"https://api.github.com/repos/apache/hudi/issues/comments/440573918","html_url":"https://github.com/apache/hudi/issues/254#issuecomment-440573918","issue_url":"https://api.github.com/repos/apache/hudi/issues/254","id":440573918,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MDU3MzkxOA==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-21T08:11:52Z","updated_at":"2018-11-21T08:13:36Z","author_association":"CONTRIBUTOR","body":"it seems there is even less intrusive work around: \r\njust set `spark.hadoop.fs.%s.impl.disable.cache` to `true` in the job\r\nin this case, the call will always create a new fs instead of trying to get from cache and close the extra ones, which causing the trouble\r\n```\r\n  /** Returns the FileSystem for this URI's scheme and authority.  The scheme\r\n   * of the URI determines a configuration property name,\r\n   * <tt>fs.<i>scheme</i>.class</tt> whose value names the FileSystem class.\r\n   * The entire URI is passed to the FileSystem instance's initialize method.\r\n   */\r\n  public static FileSystem get(URI uri, Configuration conf) throws IOException {\r\n    String scheme = uri.getScheme();\r\n    String authority = uri.getAuthority();\r\n\r\n    if (scheme == null && authority == null) {     // use default FS\r\n      return get(conf);\r\n    }\r\n\r\n    if (scheme != null && authority == null) {     // no authority\r\n      URI defaultUri = getDefaultUri(conf);\r\n      if (scheme.equals(defaultUri.getScheme())    // if scheme matches default\r\n          && defaultUri.getAuthority() != null) {  // & default has authority\r\n        return get(defaultUri, conf);              // return default\r\n      }\r\n    }\r\n    \r\n    String disableCacheName = String.format(\"fs.%s.impl.disable.cache\", scheme);\r\n    if (conf.getBoolean(disableCacheName, false)) {\r\n      return createFileSystem(uri, conf);\r\n    }\r\n\r\n    return CACHE.get(uri, conf);\r\n  }\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/440573918/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441167459","html_url":"https://github.com/apache/hudi/issues/227#issuecomment-441167459","issue_url":"https://api.github.com/repos/apache/hudi/issues/227","id":441167459,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MTE2NzQ1OQ==","user":{"login":"louisliu318","id":9956334,"node_id":"MDQ6VXNlcjk5NTYzMzQ=","avatar_url":"https://avatars.githubusercontent.com/u/9956334?v=4","gravatar_id":"","url":"https://api.github.com/users/louisliu318","html_url":"https://github.com/louisliu318","followers_url":"https://api.github.com/users/louisliu318/followers","following_url":"https://api.github.com/users/louisliu318/following{/other_user}","gists_url":"https://api.github.com/users/louisliu318/gists{/gist_id}","starred_url":"https://api.github.com/users/louisliu318/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/louisliu318/subscriptions","organizations_url":"https://api.github.com/users/louisliu318/orgs","repos_url":"https://api.github.com/users/louisliu318/repos","events_url":"https://api.github.com/users/louisliu318/events{/privacy}","received_events_url":"https://api.github.com/users/louisliu318/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-23T07:04:36Z","updated_at":"2018-11-23T07:04:36Z","author_association":"NONE","body":"@n3nash @vinothchandar @littleRa1n What's the cause of this problem? I came across the same error ? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441167459/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441198391","html_url":"https://github.com/apache/hudi/issues/227#issuecomment-441198391","issue_url":"https://api.github.com/repos/apache/hudi/issues/227","id":441198391,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MTE5ODM5MQ==","user":{"login":"louisliu318","id":9956334,"node_id":"MDQ6VXNlcjk5NTYzMzQ=","avatar_url":"https://avatars.githubusercontent.com/u/9956334?v=4","gravatar_id":"","url":"https://api.github.com/users/louisliu318","html_url":"https://github.com/louisliu318","followers_url":"https://api.github.com/users/louisliu318/followers","following_url":"https://api.github.com/users/louisliu318/following{/other_user}","gists_url":"https://api.github.com/users/louisliu318/gists{/gist_id}","starred_url":"https://api.github.com/users/louisliu318/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/louisliu318/subscriptions","organizations_url":"https://api.github.com/users/louisliu318/orgs","repos_url":"https://api.github.com/users/louisliu318/repos","events_url":"https://api.github.com/users/louisliu318/events{/privacy}","received_events_url":"https://api.github.com/users/louisliu318/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-23T10:01:34Z","updated_at":"2018-11-23T10:01:34Z","author_association":"NONE","body":"I changed the core of spark to 1 , the error gone away. May I ask why can not set the core more than 1 ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441198391/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441394698","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-441394698","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":441394698,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MTM5NDY5OA==","user":{"login":"smdahmed","id":6824174,"node_id":"MDQ6VXNlcjY4MjQxNzQ=","avatar_url":"https://avatars.githubusercontent.com/u/6824174?v=4","gravatar_id":"","url":"https://api.github.com/users/smdahmed","html_url":"https://github.com/smdahmed","followers_url":"https://api.github.com/users/smdahmed/followers","following_url":"https://api.github.com/users/smdahmed/following{/other_user}","gists_url":"https://api.github.com/users/smdahmed/gists{/gist_id}","starred_url":"https://api.github.com/users/smdahmed/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smdahmed/subscriptions","organizations_url":"https://api.github.com/users/smdahmed/orgs","repos_url":"https://api.github.com/users/smdahmed/repos","events_url":"https://api.github.com/users/smdahmed/events{/privacy}","received_events_url":"https://api.github.com/users/smdahmed/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-24T20:47:58Z","updated_at":"2018-11-24T20:47:58Z","author_association":"NONE","body":"HI, Can you please add me to the slack group? My email is: smdahmed@gmail.com.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441394698/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441533799","html_url":"https://github.com/apache/hudi/issues/499#issuecomment-441533799","issue_url":"https://api.github.com/repos/apache/hudi/issues/499","id":441533799,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MTUzMzc5OQ==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-26T06:38:47Z","updated_at":"2018-11-26T06:38:47Z","author_association":"CONTRIBUTOR","body":"Seeing that again even with the same parallelism. Will add the logging back (#497) and get more info if I run into it again.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441533799/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441543580","html_url":"https://github.com/apache/hudi/pull/495#issuecomment-441543580","issue_url":"https://api.github.com/repos/apache/hudi/issues/495","id":441543580,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MTU0MzU4MA==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-26T07:32:48Z","updated_at":"2018-11-26T07:33:28Z","author_association":"CONTRIBUTOR","body":"@vinothchandar Addressed some comments, replied to others.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/441543580/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442188590","html_url":"https://github.com/apache/hudi/issues/509#issuecomment-442188590","issue_url":"https://api.github.com/repos/apache/hudi/issues/509","id":442188590,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjE4ODU5MA==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T19:36:30Z","updated_at":"2018-11-27T19:36:30Z","author_association":"CONTRIBUTOR","body":"The root cause is likely to be here, in `com.uber.hoodie.common.table.log.HoodieLogFormatWriter.<init>(HoodieLogFormatWriter.java:73)`, the IOException catching requires the message to be \"Not supported\", however the `com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.append(GoogleHadoopFileSystemBase.java:811)` will throw exception with message \"The append operation is not supported.\"\r\n```\r\n      } catch (IOException ioe) {\r\n        if (ioe.getMessage().equalsIgnoreCase(\"Not supported\")) {\r\n          log.info(\"Append not supported. Opening a new log file..\");\r\n          this.logFile = logFile.rollOver(fs);\r\n          createNewFile();\r\n        } else {\r\n          throw ioe;\r\n        }\r\n      }\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442188590/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442189180","html_url":"https://github.com/apache/hudi/issues/509#issuecomment-442189180","issue_url":"https://api.github.com/repos/apache/hudi/issues/509","id":442189180,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjE4OTE4MA==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T19:38:15Z","updated_at":"2018-11-27T19:38:15Z","author_association":"CONTRIBUTOR","body":"@vinothchandar wondering what was the reason there is such a check and for error message check and please advice on how to deal with this issue, likely to be a quick fix if my hypothesis above is right. Thanks!","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442189180/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442190215","html_url":"https://github.com/apache/hudi/issues/509#issuecomment-442190215","issue_url":"https://api.github.com/repos/apache/hudi/issues/509","id":442190215,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjE5MDIxNQ==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T19:41:20Z","updated_at":"2018-11-27T19:41:20Z","author_association":"CONTRIBUTOR","body":"or maybe we can have something like \r\n```       \r\n...\r\n if (ioe.getMessage().toLowerCase().contains(\"not supported\")) { \r\n...\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442190215/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442196679","html_url":"https://github.com/apache/hudi/issues/509#issuecomment-442196679","issue_url":"https://api.github.com/repos/apache/hudi/issues/509","id":442196679,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjE5NjY3OQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T20:01:00Z","updated_at":"2018-11-27T20:01:00Z","author_association":"MEMBER","body":"but the comparison is ignoring case right? Thats how the message was for s3 :) I did not get this issue on s3.. \r\n\r\nWe need to also understand how it eventually succeeds. \r\n\r\nJust to clarify are you saying \r\n - All micro batches first fail and retry succeeds (or) \r\n - The first micro batch fails and the rest suceeds? \r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442196679/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442201343","html_url":"https://github.com/apache/hudi/issues/509#issuecomment-442201343","issue_url":"https://api.github.com/repos/apache/hudi/issues/509","id":442201343,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIwMTM0Mw==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T20:14:57Z","updated_at":"2018-11-27T20:14:57Z","author_association":"CONTRIBUTOR","body":"tried the above fix in our own applications and it seems the error is gone","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442201343/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442204206","html_url":"https://github.com/apache/hudi/issues/509#issuecomment-442204206","issue_url":"https://api.github.com/repos/apache/hudi/issues/509","id":442204206,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIwNDIwNg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T20:24:27Z","updated_at":"2018-11-27T20:24:27Z","author_association":"MEMBER","body":"```\r\n if (ioe.getMessage().toLowerCase().contains(\"not supported\")) { \r\n```\r\nyeah may be . Let me try to gather the exceptions thrown for this case across hdfs, file://, gcs, s3 and go from there.. \r\n\r\n\r\n- [S3AFileSystem](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java) : `\"Append is not supported \"\r\n        + \"by S3AFileSystem\"`\r\n- [[GoogleHadoopFileSystemBase](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/src/main/java/com/google/cloud/hadoop/fs/gcs/GoogleHadoopFileSystemBase.java#L811)] : `The append operation is not supported.`\r\n\r\n\r\nthe fix should address both. Interestingly, the S3 exception has changed. I wonder if we should just do this on any IOException anyway.. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442204206/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442209337","html_url":"https://github.com/apache/hudi/issues/509#issuecomment-442209337","issue_url":"https://api.github.com/repos/apache/hudi/issues/509","id":442209337,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIwOTMzNw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T20:41:32Z","updated_at":"2018-11-27T20:41:32Z","author_association":"MEMBER","body":"Right fix may be just to special case `s3a` and `gs`, instead of messing with exception messages.. Let me prep a diff based on this and add you","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442209337/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442227814","html_url":"https://github.com/apache/hudi/issues/493#issuecomment-442227814","issue_url":"https://api.github.com/repos/apache/hudi/issues/493","id":442227814,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIyNzgxNA==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T21:41:43Z","updated_at":"2018-11-27T21:43:22Z","author_association":"NONE","body":"I ran into this as well\r\n\r\n```\r\nscala> CompactApInvoicesAll.saveToHudiTable(\r\n     |       deltas,\r\n     |       tableName = \"ap_invoices_all_hudi\",\r\n     |       tablePath = \"hdfs:///tmp/hudi-test/\",\r\n     |       primaryKey = \"invoice_id\",\r\n     |       pkOrderingCol = \"capture_timestamp\",\r\n     |       enableHiveSync = true\r\n     |     )\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".                \r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\ncom.uber.hoodie.hive.HoodieHiveSyncException: Failed in executing SQL CREATE EXTERNAL TABLE  IF NOT EXISTS default.ap_invoices_all_hudi( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `invoice_id` string, `last_update_date` bigint, `last_updated_by` string, `vendor_id` string, `invoice_num` string, `set_of_books_id` string, `invoice_currency_code` string, `payment_currency_code` string, `payment_cross_rate` string, `invoice_amount` string, `vendor_site_id` string, `amount_paid` string, `discount_amount_taken` string, `invoice_date` bigint, `source` string, `invoice_type_lookup_code` string, `description` string, `batch_id` string, `amount_applicable_to_discount` string, `tax_amount` string, `terms_id` string, `terms_date` bigint, `payment_method_lookup_code` string, `pay_group_lookup_code` string, `accts_pay_code_combination_id` string, `payment_status_flag` string, `creation_date` bigint, `created_by` string, `base_amount` string, `vat_code` string, `last_update_login` string, `exclusive_payment_flag` string, `po_header_id` string, `freight_amount` string, `goods_received_date` bigint, `invoice_received_date` bigint, `voucher_num` string, `approved_amount` string, `recurring_payment_id` string, `exchange_rate` string, `exchange_rate_type` string, `exchange_date` bigint, `earliest_settlement_date` bigint, `original_prepayment_amount` string, `doc_sequence_id` string, `doc_sequence_value` string, `doc_category_code` string, `attribute1` string, `attribute2` string, `attribute3` string, `attribute4` string, `attribute5` string, `attribute6` string, `attribute7` string, `attribute8` string, `attribute9` string, `attribute10` string, `attribute11` string, `attribute12` string, `attribute13` string, `attribute14` string, `attribute15` string, `attribute_category` string, `approval_status` string, `approval_description` string, `invoice_distribution_total` string, `posting_status` string, `prepay_flag` string, `authorized_by` string, `cancelled_date` bigint, `cancelled_by` string, `cancelled_amount` string, `temp_cancelled_amount` string, `project_accounting_context` string, `ussgl_transaction_code` string, `ussgl_trx_code_context` string, `project_id` string, `task_id` string, `expenditure_type` string, `expenditure_item_date` bigint, `pa_quantity` string, `expenditure_organization_id` string, `pa_default_dist_ccid` string, `vendor_prepay_amount` string, `payment_amount_total` string, `awt_flag` string, `awt_group_id` string, `reference_1` string, `reference_2` string, `org_id` string, `pre_withholding_amount` string, `global_attribute_category` string, `global_attribute1` string, `global_attribute2` string, `global_attribute3` string, `global_attribute4` string, `global_attribute5` string, `global_attribute6` string, `global_attribute7` string, `global_attribute8` string, `global_attribute9` string, `global_attribute10` string, `global_attribute11` string, `global_attribute12` string, `global_attribute13` string, `global_attribute14` string, `global_attribute15` string, `global_attribute16` string, `global_attribute17` string, `global_attribute18` string, `global_attribute19` string, `global_attribute20` string, `auto_tax_calc_flag` string, `payment_cross_rate_type` string, `payment_cross_rate_date` bigint, `pay_curr_invoice_amount` string, `mrc_base_amount` string, `mrc_exchange_rate` string, `mrc_exchange_rate_type` string, `mrc_exchange_date` string, `mrc_posting_status` string, `paid_on_behalf_employee_id` string, `amt_due_ccard_company` string, `amt_due_employee` string, `gl_date` bigint, `award_id` string, `approval_ready_flag` string, `approval_iteration` string, `wfapproval_status` string, `requester_id` string, `validation_request_id` string, `validated_tax_amount` string, `quick_credit` string, `credited_invoice_id` string, `distribution_set_id` string, `application_id` string, `product_table` string, `reference_key1` string, `reference_key2` string, `reference_key3` string, `reference_key4` string, `reference_key5` string, `total_tax_amount` string, `self_assessed_tax_amount` string, `tax_related_invoice_id` string, `trx_business_category` string, `user_defined_fisc_class` string, `taxation_country` string, `document_sub_type` string, `supplier_tax_invoice_number` string, `supplier_tax_invoice_date` bigint, `supplier_tax_exchange_rate` string, `tax_invoice_recording_date` bigint, `tax_invoice_internal_seq` string, `legal_entity_id` string, `historical_flag` string, `force_revalidation_flag` string, `bank_charge_bearer` string, `remittance_message1` string, `remittance_message2` string, `remittance_message3` string, `unique_remittance_identifier` string, `uri_check_digit` string, `settlement_priority` string, `payment_reason_code` string, `payment_reason_comments` string, `payment_method_code` string, `delivery_channel_code` string, `quick_po_header_id` string, `net_of_retainage_flag` string, `release_amount_net_of_tax` string, `control_amount` string, `party_id` string, `party_site_id` string, `pay_proc_trxn_type_code` string, `payment_function` string, `cust_registration_code` string, `cust_registration_number` string, `port_of_entry_code` string, `external_bank_account_id` string, `vendor_contact_id` string, `internal_contact_email` string, `disc_is_inv_less_tax_flag` string, `exclude_freight_from_discount` string, `pay_awt_group_id` string, `original_invoice_amount` string, `dispute_reason` string, `remit_to_supplier_name` string, `remit_to_supplier_id` string, `remit_to_supplier_site` string, `remit_to_supplier_site_id` string, `relationship_id` string, `capture_timestamp` bigint, `integ_key` string, `op_type` string, `updatedby_user` string) PARTITIONED BY ( String) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'com.uber.hoodie.hadoop.HoodieInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 'hdfs:/tmp/hudi-test'\r\n  at com.uber.hoodie.hive.HoodieHiveClient.updateHiveSQL(HoodieHiveClient.java:457)\r\n  at com.uber.hoodie.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:260)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:129)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:96)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:68)\r\n  at com.uber.hoodie.DefaultSource.syncHive(DefaultSource.scala:272)\r\n  at com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:247)\r\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\r\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\r\n  at CompactApInvoicesAll$.saveToHudiTable(CompactApInvoicesAll.scala:54)\r\n  ... 56 elided\r\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:5717 cannot recognize input near ')' 'ROW' 'FORMAT' in column type\r\n  at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:256)\r\n  at org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:242)\r\n  at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:254)\r\n  at org.apache.commons.dbcp.DelegatingStatement.execute(DelegatingStatement.java:264)\r\n  at org.apache.commons.dbcp.DelegatingStatement.execute(DelegatingStatement.java:264)\r\n  at com.uber.hoodie.hive.HoodieHiveClient.updateHiveSQL(HoodieHiveClient.java:455)\r\n  ... 82 more\r\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: ParseException line 1:5717 cannot recognize input near ')' 'ROW' 'FORMAT' in column type\r\n  at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)\r\n  at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:206)\r\n  at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:290)\r\n  at org.apache.hive.service.cli.operation.Operation.run(Operation.java:320)\r\n  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:530)\r\n  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:517)\r\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  at java.lang.reflect.Method.invoke(Method.java:498)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\r\n  at java.security.AccessController.doPrivileged(Native Method)\r\n  at javax.security.auth.Subject.doAs(Subject.java:422)\r\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1840)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\r\n  at com.sun.proxy.$Proxy41.executeStatementAsync(Unknown Source)\r\n  at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:310)\r\n  at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:530)\r\n  at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1437)\r\n  at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1422)\r\n  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\r\n  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n  at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\r\n  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.parse.ParseException:line 1:5717 cannot recognize input near ')' 'ROW' 'FORMAT' in column type\r\n  at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:211)\r\n  at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77)\r\n  at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70)\r\n  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468)\r\n  at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\r\n  at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1295)\r\n  at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:204)\r\n  ... 27 more\r\n```\r\n\r\n```\r\nCREATE EXTERNAL TABLE  IF NOT EXISTS default.ap_invoices_all_hudi( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, ...`integ_key` string, `op_type` string, `updatedby_user` string) \r\nPARTITIONED BY ( String) \r\nROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' \r\nSTORED AS INPUTFORMAT 'com.uber.hoodie.hadoop.HoodieInputFormat' \r\nOUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' \r\nLOCATION 'hdfs:/tmp/hudi-test'\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442227814/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442229286","html_url":"https://github.com/apache/hudi/issues/506#issuecomment-442229286","issue_url":"https://api.github.com/repos/apache/hudi/issues/506","id":442229286,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIyOTI4Ng==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T21:46:29Z","updated_at":"2018-11-27T21:46:29Z","author_association":"MEMBER","body":">>What would be recommended insert / upsert / finalize partition? shall I have them different or the same?\r\nare you asking how to set shuffle parallelism? Can you point me to the exact configs you are asking about.. \r\n\r\nAt a high level, if you want to reduce runtime, you will bottlenecked by index lookup speed +  time to merge a 128MB file.. This ticket will try to improve first one. Whats the time you see on actual write phase? \r\n\r\nif you have not run into this yet, there is some tuning tips here https://uber.github.io/hudi/configurations.html#tuning \r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442229286/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442229290","html_url":"https://github.com/apache/hudi/issues/493#issuecomment-442229290","issue_url":"https://api.github.com/repos/apache/hudi/issues/493","id":442229290,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIyOTI5MA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T21:46:29Z","updated_at":"2018-11-27T21:46:29Z","author_association":"CONTRIBUTOR","body":"Will look into this tonight.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442229290/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442229869","html_url":"https://github.com/apache/hudi/issues/493#issuecomment-442229869","issue_url":"https://api.github.com/repos/apache/hudi/issues/493","id":442229869,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIyOTg2OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T21:48:15Z","updated_at":"2018-11-27T21:48:15Z","author_association":"CONTRIBUTOR","body":"@AndrewKL : BTW, can you let us know the Hive and Hadoop versions you are using.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442229869/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442230498","html_url":"https://github.com/apache/hudi/issues/254#issuecomment-442230498","issue_url":"https://api.github.com/repos/apache/hudi/issues/254","id":442230498,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIzMDQ5OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T21:50:18Z","updated_at":"2018-11-27T21:50:18Z","author_association":"MEMBER","body":"I was looking for a way to do this programmatically.. Were you able to test this out? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442230498/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442230837","html_url":"https://github.com/apache/hudi/issues/227#issuecomment-442230837","issue_url":"https://api.github.com/repos/apache/hudi/issues/227","id":442230837,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjIzMDgzNw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T21:51:27Z","updated_at":"2018-11-27T21:51:27Z","author_association":"MEMBER","body":"@louisliu318 issue is tracked in #254 . Basically, its a race in how the filesystem object gets closed across threads with cores > 2 . You can hop onto that ticket . ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442230837/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442251254","html_url":"https://github.com/apache/hudi/issues/493#issuecomment-442251254","issue_url":"https://api.github.com/repos/apache/hudi/issues/493","id":442251254,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI1MTI1NA==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T23:00:08Z","updated_at":"2018-11-27T23:01:54Z","author_association":"NONE","body":"Here's some more detailed info\r\naws emr 5.19.0\r\nspark 2.3.1\r\nhadoop 2.8.5\r\napache-hive-2.3.3-amzn-1-src\r\nglue catalogue enabled\r\n\r\n\r\n```\r\nval fs: FileSystem = FileSystem.get(delta.sparkSession.sparkContext.hadoopConfiguration)\r\n\r\n    val writer = delta.write.format(\"com.uber.hoodie\") // specify the hoodie source\r\n      //      .option(\"hoodie.insert.shuffle.parallelism\", \"2\")\r\n      //      .option(\"hoodie.upsert.shuffle.parallelism\", \"2\")  //TODO come up with a sane level of parallelism\r\n      //      .option(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY, tableType)  //default copy on write\r\n      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, primaryKey)\r\n      //.option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, \"partition\")  this data is unpartitioned\r\n      .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, pkOrderingCol)\r\n      .option(HoodieWriteConfig.TABLE_NAME, tableName)\r\n      .option(DataSourceWriteOptions.HIVE_TABLE_OPT_KEY, tableName)\r\n      //      .option(DataSourceWriteOptions.HIVE_URL_OPT_KEY, hiveJdbcUrl)\r\n      //      .option(DataSourceWriteOptions.HIVE_USER_OPT_KEY, hiveUser)\r\n      //      .option(DataSourceWriteOptions.HIVE_PASS_OPT_KEY, hivePass)\r\n      .option(DataSourceWriteOptions.HIVE_SYNC_ENABLED_OPT_KEY, enableHiveSync)//this crashes if enable due to un partitioned data\r\n      .mode(SaveMode.Append)\r\n    writer.save(tablePath)\r\n    val commitTime = HoodieDataSourceHelpers.latestCommit(fs, tablePath)\r\n    println(\"commit at instant time :\" + commitTime)\r\n\r\n    writer.save(tablePath)\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442251254/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442253190","html_url":"https://github.com/apache/hudi/issues/493#issuecomment-442253190","issue_url":"https://api.github.com/repos/apache/hudi/issues/493","id":442253190,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI1MzE5MA==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T23:07:31Z","updated_at":"2018-11-27T23:07:31Z","author_association":"NONE","body":"Are unpartitioned tables supported by hudi?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442253190/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442260898","html_url":"https://github.com/apache/hudi/issues/254#issuecomment-442260898","issue_url":"https://api.github.com/repos/apache/hudi/issues/254","id":442260898,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI2MDg5OA==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T23:40:19Z","updated_at":"2018-11-27T23:40:19Z","author_association":"CONTRIBUTOR","body":"@vinothchandar I have not seen this issue coming up after I added the flag. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442260898/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442261400","html_url":"https://github.com/apache/hudi/issues/254#issuecomment-442261400","issue_url":"https://api.github.com/repos/apache/hudi/issues/254","id":442261400,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI2MTQwMA==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T23:42:36Z","updated_at":"2018-11-27T23:42:36Z","author_association":"CONTRIBUTOR","body":"We can \r\n1. add this flag to hadoop config somewhere in the code\r\n2. synchronize the filesystem creation as mention earlier","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442261400/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442263857","html_url":"https://github.com/apache/hudi/issues/509#issuecomment-442263857","issue_url":"https://api.github.com/repos/apache/hudi/issues/509","id":442263857,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI2Mzg1Nw==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-27T23:53:41Z","updated_at":"2018-11-27T23:53:49Z","author_association":"CONTRIBUTOR","body":"> * All micro batches first fail and retry succeeds (or)\r\n\r\nwhat I observed is this one. Did not drill down too much why we are seeing that behavior though.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442263857/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442277332","html_url":"https://github.com/apache/hudi/pull/511#issuecomment-442277332","issue_url":"https://api.github.com/repos/apache/hudi/issues/511","id":442277332,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI3NzMzMg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T00:57:15Z","updated_at":"2018-11-28T00:57:15Z","author_association":"MEMBER","body":"@leletan can you quickly skim this ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442277332/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442280643","html_url":"https://github.com/apache/hudi/pull/511#issuecomment-442280643","issue_url":"https://api.github.com/repos/apache/hudi/issues/511","id":442280643,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI4MDY0Mw==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T01:14:49Z","updated_at":"2018-11-28T01:14:49Z","author_association":"CONTRIBUTOR","body":"> @leletan can you quickly skim this\r\n\r\nLGTM in general","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442280643/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442284052","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-442284052","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":442284052,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI4NDA1Mg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T01:31:32Z","updated_at":"2018-11-28T01:31:32Z","author_association":"MEMBER","body":"Can you rebase, squash commits into one and see if tests pass? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442284052/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442285339","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-442285339","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":442285339,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjI4NTMzOQ==","user":{"login":"hanleiwang","id":38050458,"node_id":"MDQ6VXNlcjM4MDUwNDU4","avatar_url":"https://avatars.githubusercontent.com/u/38050458?v=4","gravatar_id":"","url":"https://api.github.com/users/hanleiwang","html_url":"https://github.com/hanleiwang","followers_url":"https://api.github.com/users/hanleiwang/followers","following_url":"https://api.github.com/users/hanleiwang/following{/other_user}","gists_url":"https://api.github.com/users/hanleiwang/gists{/gist_id}","starred_url":"https://api.github.com/users/hanleiwang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hanleiwang/subscriptions","organizations_url":"https://api.github.com/users/hanleiwang/orgs","repos_url":"https://api.github.com/users/hanleiwang/repos","events_url":"https://api.github.com/users/hanleiwang/events{/privacy}","received_events_url":"https://api.github.com/users/hanleiwang/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T01:37:47Z","updated_at":"2018-11-28T01:37:47Z","author_association":"NONE","body":"@vinothchandar Hi there, can you please add me into the hudi slack channel? my email is hanlei.wang@vungle.com. Tks.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442285339/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442519061","html_url":"https://github.com/apache/hudi/pull/512#issuecomment-442519061","issue_url":"https://api.github.com/repos/apache/hudi/issues/512","id":442519061,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjUxOTA2MQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T16:49:36Z","updated_at":"2018-11-28T16:49:36Z","author_association":"CONTRIBUTOR","body":"@vinothchandar @n3nash : Please review when you get a chance.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442519061/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442556390","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-442556390","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":442556390,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjU1NjM5MA==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T18:35:40Z","updated_at":"2018-11-28T18:35:40Z","author_association":"CONTRIBUTOR","body":"Sure will do that.\r\n\r\n> Can you rebase, squash commits into one and see if tests pass?\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442556390/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442557597","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-442557597","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":442557597,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjU1NzU5Nw==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T18:39:17Z","updated_at":"2018-11-28T18:39:59Z","author_association":"CONTRIBUTOR","body":"@vinothchandar I have been thinking if it makes sense to add stats tracking about the input / output / duration / error count (like statsd) into the `SparkSqlWriter`, this is especially useful for something like a streaming app. Please advice on this. Thanks!","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442557597/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442559055","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-442559055","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":442559055,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjU1OTA1NQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T18:43:49Z","updated_at":"2018-11-28T18:43:49Z","author_association":"MEMBER","body":"that sounds good to me. We already report some metrics into graphite if you have nt noticed.. but we can do this as a follow on PR? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442559055/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442568888","html_url":"https://github.com/apache/hudi/pull/513#issuecomment-442568888","issue_url":"https://api.github.com/repos/apache/hudi/issues/513","id":442568888,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjU2ODg4OA==","user":{"login":"CLAassistant","id":11571300,"node_id":"MDQ6VXNlcjExNTcxMzAw","avatar_url":"https://avatars.githubusercontent.com/u/11571300?v=4","gravatar_id":"","url":"https://api.github.com/users/CLAassistant","html_url":"https://github.com/CLAassistant","followers_url":"https://api.github.com/users/CLAassistant/followers","following_url":"https://api.github.com/users/CLAassistant/following{/other_user}","gists_url":"https://api.github.com/users/CLAassistant/gists{/gist_id}","starred_url":"https://api.github.com/users/CLAassistant/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CLAassistant/subscriptions","organizations_url":"https://api.github.com/users/CLAassistant/orgs","repos_url":"https://api.github.com/users/CLAassistant/repos","events_url":"https://api.github.com/users/CLAassistant/events{/privacy}","received_events_url":"https://api.github.com/users/CLAassistant/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-28T19:13:40Z","updated_at":"2019-03-12T11:25:37Z","author_association":"NONE","body":"[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/uber/hudi?pullRequest=513) <br/>All committers have signed the CLA.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442568888/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442668911","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-442668911","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":442668911,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjY2ODkxMQ==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-29T01:18:54Z","updated_at":"2018-11-29T01:18:54Z","author_association":"CONTRIBUTOR","body":"> that sounds good to me. We already report some metrics into graphite if you have nt noticed.. but we can do this as a follow on PR?\r\n\r\nSounds good","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442668911/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442735416","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-442735416","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":442735416,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjczNTQxNg==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-29T07:37:31Z","updated_at":"2018-11-29T07:39:34Z","author_association":"CONTRIBUTOR","body":"Made changes to the retry logic due to the new naming of the retry flag - not even retry once upon errors / exceptions when `IGNORE_FAILED_BATCH` is set to true (previously the flag only start to kick in after all retries fail)","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442735416/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442912849","html_url":"https://github.com/apache/hudi/issues/514#issuecomment-442912849","issue_url":"https://api.github.com/repos/apache/hudi/issues/514","id":442912849,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjkxMjg0OQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-29T17:03:19Z","updated_at":"2018-11-29T17:03:19Z","author_association":"MEMBER","body":"+1 from me ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442912849/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442925348","html_url":"https://github.com/apache/hudi/issues/493#issuecomment-442925348","issue_url":"https://api.github.com/repos/apache/hudi/issues/493","id":442925348,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjkyNTM0OA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-29T17:39:08Z","updated_at":"2018-11-29T17:39:08Z","author_association":"CONTRIBUTOR","body":"Sorry for the delay.\r\n\r\n@arw357 :  Thanks for debugging the issue. I have a pending PR ( https://github.com/uber/hudi/pull/515) that fixes it. Please use it if you are planning to try hoodie.\r\n\r\n@AndrewKL : Hoodie is supposed to work with both partitioned and non-partitioned hive tables. There were some minor issues with partition-path handling which this PR (https://github.com/uber/hudi/pull/515) addresses. I have also added support in HoodieJavaApp and in integration-test to test non-partitioned table. Please try it out when you get a chance. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442925348/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442925879","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-442925879","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":442925879,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MjkyNTg3OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-29T17:40:42Z","updated_at":"2018-11-29T17:40:42Z","author_association":"CONTRIBUTOR","body":"@arw357 @AndrewKL This is the PR that addresses https://github.com/uber/hudi/issues/493 Please try this out and let me know if you are ok with this. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442925879/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442948993","html_url":"https://github.com/apache/hudi/issues/493#issuecomment-442948993","issue_url":"https://api.github.com/repos/apache/hudi/issues/493","id":442948993,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0Mjk0ODk5Mw==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-29T18:50:15Z","updated_at":"2018-11-29T18:50:15Z","author_association":"NONE","body":"I'll update my code and give it a shot","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442948993/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442949830","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-442949830","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":442949830,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0Mjk0OTgzMA==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-29T18:52:31Z","updated_at":"2018-11-29T18:52:31Z","author_association":"NONE","body":"First glance the code looks good I will try this code in emr today","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442949830/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442956134","html_url":"https://github.com/apache/hudi/issues/514#issuecomment-442956134","issue_url":"https://api.github.com/repos/apache/hudi/issues/514","id":442956134,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0Mjk1NjEzNA==","user":{"login":"prasannarajaperumal","id":38776710,"node_id":"MDQ6VXNlcjM4Nzc2NzEw","avatar_url":"https://avatars.githubusercontent.com/u/38776710?v=4","gravatar_id":"","url":"https://api.github.com/users/prasannarajaperumal","html_url":"https://github.com/prasannarajaperumal","followers_url":"https://api.github.com/users/prasannarajaperumal/followers","following_url":"https://api.github.com/users/prasannarajaperumal/following{/other_user}","gists_url":"https://api.github.com/users/prasannarajaperumal/gists{/gist_id}","starred_url":"https://api.github.com/users/prasannarajaperumal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/prasannarajaperumal/subscriptions","organizations_url":"https://api.github.com/users/prasannarajaperumal/orgs","repos_url":"https://api.github.com/users/prasannarajaperumal/repos","events_url":"https://api.github.com/users/prasannarajaperumal/events{/privacy}","received_events_url":"https://api.github.com/users/prasannarajaperumal/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-29T19:10:10Z","updated_at":"2018-11-29T19:10:10Z","author_association":"CONTRIBUTOR","body":"+1 from me. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/442956134/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443103360","html_url":"https://github.com/apache/hudi/pull/513#issuecomment-443103360","issue_url":"https://api.github.com/repos/apache/hudi/issues/513","id":443103360,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzEwMzM2MA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-30T06:17:29Z","updated_at":"2018-11-30T06:17:29Z","author_association":"MEMBER","body":"@leletan @hanleiwang would you be able to give this a shot and see if index lookup time comes down? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443103360/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443106542","html_url":"https://github.com/apache/hudi/pull/513#issuecomment-443106542","issue_url":"https://api.github.com/repos/apache/hudi/issues/513","id":443106542,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzEwNjU0Mg==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-30T06:36:17Z","updated_at":"2018-11-30T06:37:09Z","author_association":"CONTRIBUTOR","body":"> @leletan @hanleiwang would you be able to give this a shot and see if index lookup time comes down?\r\n\r\nThanks! Will do. We were using global_bloom index for our use case, wondering if this PR is planning to cover that as well.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443106542/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443307200","html_url":"https://github.com/apache/hudi/issues/514#issuecomment-443307200","issue_url":"https://api.github.com/repos/apache/hudi/issues/514","id":443307200,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzMwNzIwMA==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-30T19:08:48Z","updated_at":"2018-11-30T19:08:48Z","author_association":"CONTRIBUTOR","body":"+1 from me!","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443307200/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443332127","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-443332127","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":443332127,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzMzMjEyNw==","user":{"login":"hindog","id":1185158,"node_id":"MDQ6VXNlcjExODUxNTg=","avatar_url":"https://avatars.githubusercontent.com/u/1185158?v=4","gravatar_id":"","url":"https://api.github.com/users/hindog","html_url":"https://github.com/hindog","followers_url":"https://api.github.com/users/hindog/followers","following_url":"https://api.github.com/users/hindog/following{/other_user}","gists_url":"https://api.github.com/users/hindog/gists{/gist_id}","starred_url":"https://api.github.com/users/hindog/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hindog/subscriptions","organizations_url":"https://api.github.com/users/hindog/orgs","repos_url":"https://api.github.com/users/hindog/repos","events_url":"https://api.github.com/users/hindog/events{/privacy}","received_events_url":"https://api.github.com/users/hindog/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-30T20:37:49Z","updated_at":"2018-11-30T20:37:49Z","author_association":"NONE","body":"Please add me to your slack channel: hindog@gmail.com","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443332127/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443366479","html_url":"https://github.com/apache/hudi/issues/514#issuecomment-443366479","issue_url":"https://api.github.com/repos/apache/hudi/issues/514","id":443366479,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzM2NjQ3OQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-11-30T23:04:42Z","updated_at":"2018-11-30T23:04:42Z","author_association":"MEMBER","body":"Alright.. its settled then! ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443366479/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443574050","html_url":"https://github.com/apache/hudi/issues/517#issuecomment-443574050","issue_url":"https://api.github.com/repos/apache/hudi/issues/517","id":443574050,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzU3NDA1MA==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-03T02:57:29Z","updated_at":"2018-12-03T03:05:28Z","author_association":"CONTRIBUTOR","body":"I checked my code and found that I used a Timestamp Type column to insert hive. Avro 1.7.7 is not supported timestamp and timestamp was only introduced in Avro 1.8.x. \r\nI found that in quickstart.md, timestamp is cast to double. Can I change it to long? because I use two column to check type, it show `1.543804343738E12` and `1543804343738`.\r\nOr is there a way to change Avro to 1.8 in spark 2.2.1","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443574050/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443723227","html_url":"https://github.com/apache/hudi/pull/510#issuecomment-443723227","issue_url":"https://api.github.com/repos/apache/hudi/issues/510","id":443723227,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzcyMzIyNw==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-03T14:12:53Z","updated_at":"2018-12-03T14:12:53Z","author_association":"CONTRIBUTOR","body":"@vinothchandar added to insert as well although hasn't happened for inserts yet.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443723227/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443724383","html_url":"https://github.com/apache/hudi/pull/519#issuecomment-443724383","issue_url":"https://api.github.com/repos/apache/hudi/issues/519","id":443724383,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzcyNDM4Mw==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-03T14:16:25Z","updated_at":"2018-12-03T14:16:25Z","author_association":"CONTRIBUTOR","body":"@vinothchandar This is the old branch","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443724383/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443874219","html_url":"https://github.com/apache/hudi/pull/520#issuecomment-443874219","issue_url":"https://api.github.com/repos/apache/hudi/issues/520","id":443874219,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0Mzg3NDIxOQ==","user":{"login":"CLAassistant","id":11571300,"node_id":"MDQ6VXNlcjExNTcxMzAw","avatar_url":"https://avatars.githubusercontent.com/u/11571300?v=4","gravatar_id":"","url":"https://api.github.com/users/CLAassistant","html_url":"https://github.com/CLAassistant","followers_url":"https://api.github.com/users/CLAassistant/followers","following_url":"https://api.github.com/users/CLAassistant/following{/other_user}","gists_url":"https://api.github.com/users/CLAassistant/gists{/gist_id}","starred_url":"https://api.github.com/users/CLAassistant/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CLAassistant/subscriptions","organizations_url":"https://api.github.com/users/CLAassistant/orgs","repos_url":"https://api.github.com/users/CLAassistant/repos","events_url":"https://api.github.com/users/CLAassistant/events{/privacy}","received_events_url":"https://api.github.com/users/CLAassistant/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-03T21:19:33Z","updated_at":"2018-12-18T11:04:38Z","author_association":"NONE","body":"[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/uber/hudi?pullRequest=520) <br/>All committers have signed the CLA.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443874219/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443898389","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-443898389","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":443898389,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0Mzg5ODM4OQ==","user":{"login":"GaganjotSidhu","id":6842045,"node_id":"MDQ6VXNlcjY4NDIwNDU=","avatar_url":"https://avatars.githubusercontent.com/u/6842045?v=4","gravatar_id":"","url":"https://api.github.com/users/GaganjotSidhu","html_url":"https://github.com/GaganjotSidhu","followers_url":"https://api.github.com/users/GaganjotSidhu/followers","following_url":"https://api.github.com/users/GaganjotSidhu/following{/other_user}","gists_url":"https://api.github.com/users/GaganjotSidhu/gists{/gist_id}","starred_url":"https://api.github.com/users/GaganjotSidhu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/GaganjotSidhu/subscriptions","organizations_url":"https://api.github.com/users/GaganjotSidhu/orgs","repos_url":"https://api.github.com/users/GaganjotSidhu/repos","events_url":"https://api.github.com/users/GaganjotSidhu/events{/privacy}","received_events_url":"https://api.github.com/users/GaganjotSidhu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-03T22:40:59Z","updated_at":"2018-12-03T22:40:59Z","author_association":"NONE","body":"@vinothchandar : Hi, Can you please add me into the hudi slack channel? My email is sherry.sidhu@vungle.com","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443898389/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443924812","html_url":"https://github.com/apache/hudi/pull/386#issuecomment-443924812","issue_url":"https://api.github.com/repos/apache/hudi/issues/386","id":443924812,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzkyNDgxMg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T00:37:09Z","updated_at":"2018-12-04T00:37:09Z","author_association":"MEMBER","body":"Closing due to inactivity","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443924812/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443925053","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-443925053","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":443925053,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzkyNTA1Mw==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T00:38:22Z","updated_at":"2018-12-04T00:38:22Z","author_association":"CONTRIBUTOR","body":"@vinothchandar @bvaradar looks good to me.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443925053/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443933015","html_url":"https://github.com/apache/hudi/pull/495#issuecomment-443933015","issue_url":"https://api.github.com/repos/apache/hudi/issues/495","id":443933015,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0MzkzMzAxNQ==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T01:18:51Z","updated_at":"2018-12-04T01:18:51Z","author_association":"CONTRIBUTOR","body":"Removed Converters and opened up an issue for encoders performance here : https://github.com/uber/hudi/issues/521\r\n@vinothchandar @bvaradar ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/443933015/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444230633","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444230633","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444230633,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDIzMDYzMw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T19:40:13Z","updated_at":"2018-12-04T19:40:13Z","author_association":"MEMBER","body":"@AndrewKL did you get a chance to try?  changes lgtm. love to get some validation before merging in ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444230633/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444232277","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-444232277","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":444232277,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDIzMjI3Nw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T19:45:08Z","updated_at":"2018-12-04T19:45:08Z","author_association":"MEMBER","body":"sg. @leletan we good to merge ? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444232277/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444235129","html_url":"https://github.com/apache/hudi/pull/519#issuecomment-444235129","issue_url":"https://api.github.com/repos/apache/hudi/issues/519","id":444235129,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDIzNTEyOQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T19:54:01Z","updated_at":"2018-12-04T19:54:01Z","author_association":"MEMBER","body":"Key gap here is how we open up the logs to check for the keys efficiently, when there are candidatekeys based on the bloom filter lookup \r\n\r\nTwo high level approaches we can take \r\n\r\n1) Write parquet data & push predicates down (it will be same as what we do with parquet files)\r\n\r\n2) Change log format and write keys in an indexable fashion. \r\n\r\n\r\nP.S: Writing as footers/headers may be trouble some/expensive. But, very easy to do & could work for small number of keys/file scenarios. \r\n\r\n\r\n@n3nash @bvaradar wdyt\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444235129/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444242816","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444242816","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444242816,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDI0MjgxNg==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T20:17:51Z","updated_at":"2018-12-04T20:17:51Z","author_association":"NONE","body":"It seems to be getting further now andis now blowing up due to the Glue metastore(which you cant connect to with jdbc).  I'm going to try disabling the glue catalogue and trying again.\r\n\r\n```\r\nUser class threw exception: com.uber.hoodie.hive.HoodieHiveSyncException: Cannot create hive connection jdbc:hive2://localhost:10000/default\r\nat com.uber.hoodie.hive.HoodieHiveClient.createHiveConnection(HoodieHiveClient.java:479)\r\nat com.uber.hoodie.hive.HoodieHiveClient.<init>(HoodieHiveClient.java:100)\r\nat com.uber.hoodie.hive.HiveSyncTool.<init>(HiveSyncTool.java:61)\r\nat com.uber.hoodie.DefaultSource.syncHive(DefaultSource.scala:301)\r\nat com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:273)\r\nat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\nat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\nat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\nat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\nat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\nat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\nat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\nat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\nat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\nat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\nat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\nat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\nat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\r\nat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\r\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\nat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\r\nat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\nat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\r\nat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\r\nat HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply$mcV$sp(HorizonCompactionTool.scala:98)\r\nat HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply(HorizonCompactionTool.scala:65)\r\nat HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply(HorizonCompactionTool.scala:65)\r\nat HorizonCompactionTool$.stopwatch(HorizonCompactionTool.scala:115)\r\nat HorizonCompactionTool$.saveToHudiTable(HorizonCompactionTool.scala:65)\r\nat HorizonCompactionTool$.main(HorizonCompactionTool.scala:42)\r\nat HorizonCompactionTool.main(HorizonCompactionTool.scala)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721)\r\nCaused by: com.uber.hoodie.org.apache.commons.dbcp.SQLNestedException: Cannot create PoolableConnectionFactory (Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000/default: java.net.ConnectException: Connection refused (Connection refused))\r\nat com.uber.hoodie.org.apache.commons.dbcp.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:1549)\r\nat com.uber.hoodie.org.apache.commons.dbcp.BasicDataSource.createDataSource(BasicDataSource.java:1388)\r\nat com.uber.hoodie.org.apache.commons.dbcp.BasicDataSource.getConnection(BasicDataSource.java:1044)\r\nat com.uber.hoodie.hive.HoodieHiveClient.createHiveConnection(HoodieHiveClient.java:475)\r\n... 35 more\r\nCaused by: java.sql.SQLException: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000/default: java.net.ConnectException: Connection refused (Connection refused)\r\nat com.uber.hoodie.org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:231)\r\nat com.uber.hoodie.org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:176)\r\nat com.uber.hoodie.org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105)\r\nat com.uber.hoodie.org.apache.commons.dbcp.DriverConnectionFactory.createConnection(DriverConnectionFactory.java:38)\r\nat com.uber.hoodie.org.apache.commons.dbcp.PoolableConnectionFactory.makeObject(PoolableConnectionFactory.java:582)\r\nat com.uber.hoodie.org.apache.commons.dbcp.BasicDataSource.validateConnectionFactory(BasicDataSource.java:1556)\r\nat com.uber.hoodie.org.apache.commons.dbcp.BasicDataSource.createPoolableConnectionFactory(BasicDataSource.java:1545)\r\n... 38 more\r\nCaused by: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused (Connection refused)\r\nat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\r\nat org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:266)\r\nat org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)\r\nat com.uber.hoodie.org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:204)\r\n... 44 more\r\nCaused by: java.net.ConnectException: Connection refused (Connection refused)\r\nat java.net.PlainSocketImpl.socketConnect(Native Method)\r\nat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)\r\nat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)\r\nat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)\r\nat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\nat java.net.Socket.connect(Socket.java:589)\r\nat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\r\n... 47 more\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444242816/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444279117","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-444279117","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":444279117,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDI3OTExNw==","user":{"login":"leletan","id":6158502,"node_id":"MDQ6VXNlcjYxNTg1MDI=","avatar_url":"https://avatars.githubusercontent.com/u/6158502?v=4","gravatar_id":"","url":"https://api.github.com/users/leletan","html_url":"https://github.com/leletan","followers_url":"https://api.github.com/users/leletan/followers","following_url":"https://api.github.com/users/leletan/following{/other_user}","gists_url":"https://api.github.com/users/leletan/gists{/gist_id}","starred_url":"https://api.github.com/users/leletan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leletan/subscriptions","organizations_url":"https://api.github.com/users/leletan/orgs","repos_url":"https://api.github.com/users/leletan/repos","events_url":"https://api.github.com/users/leletan/events{/privacy}","received_events_url":"https://api.github.com/users/leletan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T22:12:47Z","updated_at":"2018-12-04T22:12:47Z","author_association":"CONTRIBUTOR","body":"@vinothchandar This code was working fine in Vungle so good to merge.\r\n\r\nI had some more refactoring and enhancements to do on the testing side based on @bvaradar 's comment. Still sorting legal things out here in the new company and it may take a little bit more time. I can do test enhancement in another separate PR once I got the approval.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444279117/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444289483","html_url":"https://github.com/apache/hudi/issues/522#issuecomment-444289483","issue_url":"https://api.github.com/repos/apache/hudi/issues/522","id":444289483,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDI4OTQ4Mw==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T22:48:56Z","updated_at":"2018-12-04T22:48:56Z","author_association":"CONTRIBUTOR","body":"@vinothchandar @bvaradar FYI","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444289483/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444301434","html_url":"https://github.com/apache/hudi/issues/493#issuecomment-444301434","issue_url":"https://api.github.com/repos/apache/hudi/issues/493","id":444301434,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMwMTQzNA==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T23:37:58Z","updated_at":"2018-12-04T23:37:58Z","author_association":"NONE","body":"I'm still in the middle of testing this buti found another minor bug.  Currently the HiveSyncTool requires a partition col.\r\n\r\n>com.uber.hoodie.com.beust.jcommander.ParameterException: The following options are required: [--user], [--partitioned-by], [--jdbc-url], [--pass]","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444301434/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444301753","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444301753","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444301753,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMwMTc1Mw==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-04T23:39:25Z","updated_at":"2018-12-04T23:39:25Z","author_association":"NONE","body":"Arg this is due the the fact that the spark driver wasn't running on the master node >.<\r\n\r\nI'm still in the middle of testing this butI found another minor bug. Currently the HiveSyncTool requires a partition col.\r\n\r\n> com.uber.hoodie.com.beust.jcommander.ParameterException: The following options are required: [--user], [--partitioned-by], [--jdbc-url], [--pass]","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444301753/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444309256","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444309256","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444309256,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMwOTI1Ng==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T00:16:17Z","updated_at":"2018-12-05T00:16:17Z","author_association":"NONE","body":"the sql being generated looks good I need to add the jar in the right spot still though\r\n\r\n```\r\nscala>     HorizonCompactionTool.saveToHudiTable(\r\n     |       spark.read.parquet(\"hdfs:///tmp/ap-invoices-all-inc-spark-shell/\").sample(false,0.5),//deltas,//\r\n     |       tableName = \"ap_invoices_all_spark_shell\",\r\n     |       tablePath = \"hdfs:///hudi-data/data/ap-invoices-all/\",\r\n     |       primaryKey = \"invoice_id\",\r\n     |       pkOrderingCol = \"capture_timestamp\",\r\n     |       enableHiveSync = true,\r\n     |       partitionCol = None,\r\n     |       insertMode = \"append\",\r\n     |       None\r\n     |     )\r\n18/12/05 00:02:20 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".                \r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\ncom.uber.hoodie.hive.HoodieHiveSyncException: Failed in executing SQL CREATE EXTERNAL TABLE  IF NOT EXISTS default.ap_invoices_all_spark_shell( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `invoice_id` string, `last_update_date` bigint, `last_updated_by` string, `vendor_id` string, `invoice_num` string, `set_of_books_id` string, `invoice_currency_code` string, `payment_currency_code` string, `payment_cross_rate` string, `invoice_amount` string, `vendor_site_id` string, `amount_paid` string, `discount_amount_taken` string, `invoice_date` bigint, `source` string, `invoice_type_lookup_code` string, `description` string, `batch_id` string, `amount_applicable_to_discount` string, `tax_amount` string, `terms_id` string, `terms_date` bigint, `payment_method_lookup_code` string, `pay_group_lookup_code` string, `accts_pay_code_combination_id` string, `payment_status_flag` string, `creation_date` bigint, `created_by` string, `base_amount` string, `vat_code` string, `last_update_login` string, `exclusive_payment_flag` string, `po_header_id` string, `freight_amount` string, `goods_received_date` bigint, `invoice_received_date` bigint, `voucher_num` string, `approved_amount` string, `recurring_payment_id` string, `exchange_rate` string, `exchange_rate_type` string, `exchange_date` bigint, `earliest_settlement_date` bigint, `original_prepayment_amount` string, `doc_sequence_id` string, `doc_sequence_value` string, `doc_category_code` string, `attribute1` string, `attribute2` string, `attribute3` string, `attribute4` string, `attribute5` string, `attribute6` string, `attribute7` string, `attribute8` string, `attribute9` string, `attribute10` string, `attribute11` string, `attribute12` string, `attribute13` string, `attribute14` string, `attribute15` string, `attribute_category` string, `approval_status` string, `approval_description` string, `invoice_distribution_total` string, `posting_status` string, `prepay_flag` string, `authorized_by` string, `cancelled_date` bigint, `cancelled_by` string, `cancelled_amount` string, `temp_cancelled_amount` string, `project_accounting_context` string, `ussgl_transaction_code` string, `ussgl_trx_code_context` string, `project_id` string, `task_id` string, `expenditure_type` string, `expenditure_item_date` bigint, `pa_quantity` string, `expenditure_organization_id` string, `pa_default_dist_ccid` string, `vendor_prepay_amount` string, `payment_amount_total` string, `awt_flag` string, `awt_group_id` string, `reference_1` string, `reference_2` string, `org_id` string, `pre_withholding_amount` string, `global_attribute_category` string, `global_attribute1` string, `global_attribute2` string, `global_attribute3` string, `global_attribute4` string, `global_attribute5` string, `global_attribute6` string, `global_attribute7` string, `global_attribute8` string, `global_attribute9` string, `global_attribute10` string, `global_attribute11` string, `global_attribute12` string, `global_attribute13` string, `global_attribute14` string, `global_attribute15` string, `global_attribute16` string, `global_attribute17` string, `global_attribute18` string, `global_attribute19` string, `global_attribute20` string, `auto_tax_calc_flag` string, `payment_cross_rate_type` string, `payment_cross_rate_date` bigint, `pay_curr_invoice_amount` string, `mrc_base_amount` string, `mrc_exchange_rate` string, `mrc_exchange_rate_type` string, `mrc_exchange_date` string, `mrc_posting_status` string, `paid_on_behalf_employee_id` string, `amt_due_ccard_company` string, `amt_due_employee` string, `gl_date` bigint, `award_id` string, `approval_ready_flag` string, `approval_iteration` string, `wfapproval_status` string, `requester_id` string, `validation_request_id` string, `validated_tax_amount` string, `quick_credit` string, `credited_invoice_id` string, `distribution_set_id` string, `application_id` string, `product_table` string, `reference_key1` string, `reference_key2` string, `reference_key3` string, `reference_key4` string, `reference_key5` string, `total_tax_amount` string, `self_assessed_tax_amount` string, `tax_related_invoice_id` string, `trx_business_category` string, `user_defined_fisc_class` string, `taxation_country` string, `document_sub_type` string, `supplier_tax_invoice_number` string, `supplier_tax_invoice_date` bigint, `supplier_tax_exchange_rate` string, `tax_invoice_recording_date` bigint, `tax_invoice_internal_seq` string, `legal_entity_id` string, `historical_flag` string, `force_revalidation_flag` string, `bank_charge_bearer` string, `remittance_message1` string, `remittance_message2` string, `remittance_message3` string, `unique_remittance_identifier` string, `uri_check_digit` string, `settlement_priority` string, `payment_reason_code` string, `payment_reason_comments` string, `payment_method_code` string, `delivery_channel_code` string, `quick_po_header_id` string, `net_of_retainage_flag` string, `release_amount_net_of_tax` string, `control_amount` string, `party_id` string, `party_site_id` string, `pay_proc_trxn_type_code` string, `payment_function` string, `cust_registration_code` string, `cust_registration_number` string, `port_of_entry_code` string, `external_bank_account_id` string, `vendor_contact_id` string, `internal_contact_email` string, `disc_is_inv_less_tax_flag` string, `exclude_freight_from_discount` string, `pay_awt_group_id` string, `original_invoice_amount` string, `dispute_reason` string, `remit_to_supplier_name` string, `remit_to_supplier_id` string, `remit_to_supplier_site` string, `remit_to_supplier_site_id` string, `relationship_id` string, `capture_timestamp` bigint, `integ_key` string, `op_type` string, `updatedby_user` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'com.uber.hoodie.hadoop.HoodieInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 'hdfs:/hudi-data/data/ap-invoices-all'\r\n  at com.uber.hoodie.hive.HoodieHiveClient.updateHiveSQL(HoodieHiveClient.java:459)\r\n  at com.uber.hoodie.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:262)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:129)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:96)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:68)\r\n  at com.uber.hoodie.DefaultSource.syncHive(DefaultSource.scala:301)\r\n  at com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:273)\r\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\r\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\r\n  at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply$mcV$sp(HorizonCompactionTool.scala:98)\r\n  at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply(HorizonCompactionTool.scala:65)\r\n  at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply(HorizonCompactionTool.scala:65)\r\n  at HorizonCompactionTool$.stopwatch(HorizonCompactionTool.scala:115)\r\n  at HorizonCompactionTool$.saveToHudiTable(HorizonCompactionTool.scala:65)\r\n  ... 59 elided\r\nCaused by: com.uber.hoodie.org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Cannot find class 'com.uber.hoodie.hadoop.HoodieInputFormat'\r\n  at com.uber.hoodie.org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:256)\r\n  at com.uber.hoodie.org.apache.hive.jdbc.Utils.verifySuccessWithInfo(Utils.java:242)\r\n  at com.uber.hoodie.org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:254)\r\n  at com.uber.hoodie.org.apache.commons.dbcp.DelegatingStatement.execute(DelegatingStatement.java:264)\r\n  at com.uber.hoodie.org.apache.commons.dbcp.DelegatingStatement.execute(DelegatingStatement.java:264)\r\n  at com.uber.hoodie.hive.HoodieHiveClient.updateHiveSQL(HoodieHiveClient.java:457)\r\n  ... 89 more\r\nCaused by: org.apache.hive.service.cli.HiveSQLException: Error while compiling statement: FAILED: SemanticException Cannot find class 'com.uber.hoodie.hadoop.HoodieInputFormat'\r\n  at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:380)\r\n  at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:206)\r\n  at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:290)\r\n  at org.apache.hive.service.cli.operation.Operation.run(Operation.java:320)\r\n  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:530)\r\n  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:517)\r\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  at java.lang.reflect.Method.invoke(Method.java:498)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\r\n  at java.security.AccessController.doPrivileged(Native Method)\r\n  at javax.security.auth.Subject.doAs(Subject.java:422)\r\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\r\n  at com.sun.proxy.$Proxy35.executeStatementAsync(Unknown Source)\r\n  at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:310)\r\n  at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:530)\r\n  at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1437)\r\n  at org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1422)\r\n  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\r\n  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n  at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\r\n  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.hadoop.hive.ql.parse.SemanticException: Cannot find class 'com.uber.hoodie.hadoop.HoodieInputFormat'\r\n  at org.apache.hadoop.hive.ql.parse.ParseUtils.ensureClassExists(ParseUtils.java:263)\r\n  at org.apache.hadoop.hive.ql.parse.StorageFormat.fillStorageFormat(StorageFormat.java:57)\r\n  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeCreateTable(SemanticAnalyzer.java:11887)\r\n  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genResolvedParseTree(SemanticAnalyzer.java:11020)\r\n  at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:11133)\r\n  at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:286)\r\n  at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:258)\r\n  at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:512)\r\n  at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\r\n  at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1295)\r\n  at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:204)\r\n  ... 27 more\r\nCaused by: java.lang.ClassNotFoundException: com.uber.hoodie.hadoop.HoodieInputFormat\r\n  at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n  at java.lang.Class.forName0(Native Method)\r\n  at java.lang.Class.forName(Class.java:348)\r\n  at org.apache.hadoop.hive.ql.parse.ParseUtils.ensureClassExists(ParseUtils.java:261)\r\n  ... 37 more\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444309256/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444310137","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444310137","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444310137,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMxMDEzNw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T00:20:23Z","updated_at":"2018-12-05T00:20:23Z","author_association":"CONTRIBUTOR","body":"@AndrewKL : Thanks a lot for testing. I have made the partition field optional in HiveSyncTool.\r\n\r\nHiveServer needs to be started with Hive bundle. Please see the example here : https://github.com/uber/hudi/blob/master/docker/hoodie/hadoop/hive_base/startup.sh\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444310137/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444311626","html_url":"https://github.com/apache/hudi/issues/516#issuecomment-444311626","issue_url":"https://api.github.com/repos/apache/hudi/issues/516","id":444311626,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMxMTYyNg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T00:28:25Z","updated_at":"2018-12-05T00:28:25Z","author_association":"MEMBER","body":"@bvaradar can you please chime in here? Sounds fair to me. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444311626/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444311896","html_url":"https://github.com/apache/hudi/issues/517#issuecomment-444311896","issue_url":"https://api.github.com/repos/apache/hudi/issues/517","id":444311896,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMxMTg5Ng==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T00:29:45Z","updated_at":"2018-12-05T00:29:45Z","author_association":"MEMBER","body":"We can consider upgrading Avro.. @n3nash wdyt? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444311896/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444312087","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-444312087","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":444312087,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMxMjA4Nw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T00:30:40Z","updated_at":"2018-12-05T00:30:40Z","author_association":"MEMBER","body":"All done.. Sorry forgot to write back on ticket.. Please let me in case I missed someone ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444312087/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444312549","html_url":"https://github.com/apache/hudi/pull/486#issuecomment-444312549","issue_url":"https://api.github.com/repos/apache/hudi/issues/486","id":444312549,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMxMjU0OQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T00:33:08Z","updated_at":"2018-12-05T00:33:08Z","author_association":"MEMBER","body":"sg. merged. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444312549/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444313386","html_url":"https://github.com/apache/hudi/issues/516#issuecomment-444313386","issue_url":"https://api.github.com/repos/apache/hudi/issues/516","id":444313386,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMxMzM4Ng==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T00:37:53Z","updated_at":"2018-12-05T00:37:53Z","author_association":"CONTRIBUTOR","body":"Thanks for reporting this issue. It makes sense to add these exclusion filters. We should do this for other fat-jars  too (hoodie-hive-bundle, hoodie-hive-bundle, hoodie-utilities). \r\n\r\n@cdmikechen : Would you be interested in creating a PR on this :) ? \r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444313386/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444314643","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444314643","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444314643,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMxNDY0Mw==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T00:44:26Z","updated_at":"2018-12-05T01:04:13Z","author_association":"NONE","body":"Thanks again @bvaradar !\r\n\r\nI've created a simple bootstrap action to configure the emr cluster with the jars\r\n\r\n```\r\n#!/usr/bin/env bash\r\n\r\nsudo aws s3 cp s3://horizon-hudi-dev/jars/hoodie-hive-bundle-0.4.5-SNAPSHOT.jar /usr/lib/hive/auxlib/\r\nsudo aws s3 cp s3://horizon-hudi-dev/jars/xercesImpl-2.9.1.jar /usr/lib/hive/auxlib/\r\n#TODO add to spark jars folder.  this is currently breaking spark-shell\r\n#TODO setup preso\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444314643/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444328732","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444328732","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444328732,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMyODczMg==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T01:53:26Z","updated_at":"2018-12-05T01:54:42Z","author_association":"NONE","body":"It looks like there's one more bug.  The table does show up in metastore though.\r\n```\r\nscala> HorizonCompactionTool.saveToHudiTable(\r\n     |       spark.read.parquet(\"hdfs:///tmp/ap-invoices-all-inc-spark-shell/\").sample(false,0.5),//deltas,//\r\n     |       tableName = \"ap_invoices_all_spark_shell\",\r\n     |       tablePath = \"hdfs:///hudi-data/data/ap-invoices-all/\",\r\n     |       primaryKey = \"invoice_id\",\r\n     |       pkOrderingCol = \"capture_timestamp\",\r\n     |       enableHiveSync = true,\r\n     |       partitionCol = None,\r\n     |       insertMode = \"append\",\r\n     |       None\r\n     |     )\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".                \r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\ncom.uber.hoodie.hive.HoodieHiveSyncException: Failed to sync partitions for table ap_invoices_all_spark_shell\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:169)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:109)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:68)\r\n  at com.uber.hoodie.DefaultSource.syncHive(DefaultSource.scala:301)\r\n  at com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:273)\r\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\r\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\r\n  at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply$mcV$sp(HorizonCompactionTool.scala:98)\r\n  at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply(HorizonCompactionTool.scala:65)\r\n  at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply(HorizonCompactionTool.scala:65)\r\n  at HorizonCompactionTool$.stopwatch(HorizonCompactionTool.scala:115)\r\n  at HorizonCompactionTool$.saveToHudiTable(HorizonCompactionTool.scala:65)\r\n  ... 59 elided\r\nCaused by: java.lang.IllegalArgumentException: Partition path default is not in the form yyyy/mm/dd\r\n  at com.uber.hoodie.hive.SlashEncodedDayPartitionValueExtractor.extractPartitionValuesInPath(SlashEncodedDayPartitionValueExtractor.java:47)\r\n  at com.uber.hoodie.hive.HoodieHiveClient.getPartitionEvents(HoodieHiveClient.java:216)\r\n  at com.uber.hoodie.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:160)\r\n  ... 87 more\r\n\r\n``\r\n\r\n```\r\nscala> sql(\"SHOW TABLES\").show(false)\r\n+--------+---------------------------+-----------+\r\n|database|tableName                  |isTemporary|\r\n+--------+---------------------------+-----------+\r\n|default |ap_invoices_all_inc        |false      |\r\n|default |ap_invoices_all_spark_shell|false      |\r\n+--------+---------------------------+-----------+\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444328732/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444329295","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444329295","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444329295,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMyOTI5NQ==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T01:56:15Z","updated_at":"2018-12-05T02:17:47Z","author_association":"NONE","body":"Edit: This bug was due to copying the table files from S3 to hdfs vis the s3-dist-cp tool.  S3 creates special place holder files to indicate the presence of a folder since folders do not actually exist in S3.\r\n\r\nand one more exception while trying to read the table\r\n\r\n```\r\nscala> sql(\"SELECT count(*) FROM ap_invoices_all_spark_shell\").show\r\n18/12/05 01:55:00 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 12810, ip-172-31-29-24.ec2.internal, executor 322): java.io.IOException: Could not read footer for file: FileStatus{path=hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$; isDirectory=false; length=0; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:526)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:513)\r\n\tat scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)\r\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)\r\n\tat scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n\tat scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n\tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)\r\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)\r\n\tat scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)\r\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)\r\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n\tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\r\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\r\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\r\nCaused by: java.lang.RuntimeException: hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$ is not a Parquet file (too small)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:466)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519)\r\n\t... 32 more\r\n\r\n18/12/05 01:55:01 ERROR TaskSetManager: Task 0 in stage 33.0 failed 4 times; aborting job\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 4 times, most recent failure: Lost task 0.3 in stage 33.0 (TID 12813, ip-172-31-29-24.ec2.internal, executor 367): java.io.IOException: Could not read footer for file: FileStatus{path=hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$; isDirectory=false; length=0; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:526)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:513)\r\n\tat scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)\r\n\tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)\r\n\tat scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n\tat scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n\tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)\r\n\tat scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)\r\n\tat scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)\r\n\tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)\r\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n\tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n\tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n\tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n\tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\r\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\r\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\r\nCaused by: java.lang.RuntimeException: hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$ is not a Parquet file (too small)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:466)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)\r\n\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519)\r\n\t... 32 more\r\n\r\nDriver stacktrace:\r\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\r\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\r\n  at scala.Option.foreach(Option.scala:257)\r\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\r\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:611)\r\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\r\n  at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$inferIfNeeded(HiveMetastoreCatalog.scala:239)\r\n  at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6$$anonfun$7.apply(HiveMetastoreCatalog.scala:193)\r\n  at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6$$anonfun$7.apply(HiveMetastoreCatalog.scala:192)\r\n  at scala.Option.getOrElse(Option.scala:121)\r\n  at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6.apply(HiveMetastoreCatalog.scala:192)\r\n  at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6.apply(HiveMetastoreCatalog.scala:185)\r\n  at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:54)\r\n  at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:185)\r\n  at org.apache.spark.sql.hive.RelationConversions.org$apache$spark$sql$hive$RelationConversions$$convert(HiveStrategies.scala:195)\r\n  at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:226)\r\n  at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:215)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n  at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:215)\r\n  at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:180)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r\n  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r\n  at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n  at scala.collection.immutable.List.foreach(List.scala:381)\r\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\r\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\r\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n  ... 49 elided\r\nCaused by: java.io.IOException: Could not read footer for file: FileStatus{path=hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$; isDirectory=false; length=0; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}\r\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:526)\r\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:513)\r\n  at scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)\r\n  at scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)\r\n  at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)\r\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n  at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n  at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)\r\n  at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n  at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n  at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n  at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)\r\n  at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)\r\n  at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)\r\n  at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)\r\n  at scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)\r\n  at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)\r\n  at scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)\r\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n  at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n  at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n  at scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)\r\n  at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n  at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n  at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\r\n  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\r\n  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\r\nCaused by: java.lang.RuntimeException: hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$ is not a Parquet file (too small)\r\n  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:466)\r\n  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)\r\n  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)\r\n  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519)\r\n  ... 32 more\r\n\r\n```\r\n\r\nThis is whats in the folder\r\n```\r\nFound 4 items\r\ndrwxr-xr-x   - hadoop hadoop          0 2018-12-05 01:51 /hudi-data/data/ap-invoices-all/.hoodie\r\n-rw-r--r--   2 hadoop hadoop          0 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie_$folder$\r\ndrwxr-xr-x   - hadoop hadoop          0 2018-12-05 01:51 /hudi-data/data/ap-invoices-all/default\r\n-rw-r--r--   2 hadoop hadoop          0 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default_$folder$\r\n```\r\n\r\nthe hoodi-cli doesn't seem to have a problem with it\r\n\r\n```\r\nhoodie:ap_invoices_all_hudi->commits show\r\n    __________________________________________________________________________________________________________________________________________________________________________\r\n    | CommitTime    | Total Bytes Written| Total Files Added| Total Files Updated| Total Partitions Written| Total Records Written| Total Update Records Written| Total Errors|\r\n    |=========================================================================================================================================================================|\r\n    | 20181205014126| 4.0 GB             | 1                | 101                | 1                       | 45028592             | 303144                      | 0           |\r\n    | 20181204202247| 3.9 GB             | 1                | 100                | 1                       | 44149130             | 355931                      | 0           |\r\n    | 20181204200718| 3.9 GB             | 1                | 99                 | 1                       | 43807211             | 341479                      | 0           |\r\n    | 20181204194909| 3.8 GB             | 0                | 99                 | 1                       | 43299425             | 649694                      | 0           |\r\n    | 20181204193733| 3.8 GB             | 1                | 98                 | 1                       | 43299425             | 299883                      | 0           |\r\n...\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444329295/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444334685","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444334685","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444334685,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDMzNDY4NQ==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T02:24:41Z","updated_at":"2018-12-05T02:28:40Z","author_association":"NONE","body":"And possibly one last bug. The table that is created returns 0 records.\r\n\r\n>spark-shell --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\"  --conf \"spark.sql.hive.convertMetastoreParquet=false\"  --jars s3://horizon-hudi-dev/jars/hoodie-spark-bundle-0.4.5-SNAPSHOT.jar,s3://horizon-hudi-dev/jars/horizon-hudi-compaction-v1.jar --packages com.databricks:spark-avro_2.11:4.0.0,xerces:xercesImpl:2.9.1 --executor-memory 2G --driver-memory 4G  --executor-cores 1 \r\n\r\n\r\n```\r\nsql(\"SELECT * FROM ap_invoices_all_spark_shell\").count\r\n\r\n18/12/05 02:21:55 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\r\nres0: Long = 0     \r\n```     ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444334685/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444373547","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444373547","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444373547,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDM3MzU0Nw==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T06:25:13Z","updated_at":"2018-12-05T06:29:00Z","author_association":"CONTRIBUTOR","body":"> Arg this is due the the fact that the spark driver wasn't running on the master node >.<\r\n> \r\n> I'm still in the middle of testing this butI found another minor bug. Currently the HiveSyncTool requires a partition col.\r\n> \r\n> > com.uber.hoodie.com.beust.jcommander.ParameterException: The following options are required: [--user], [--partitioned-by], [--jdbc-url], [--pass]\r\n\r\n@AndrewKL good point.\r\n@bvaradar We may need to fix this in the SyncTool\r\nAlso, I think the HoodieInputFormat may be assuming some hierarchy for partitionpath and then listing out all the files. Since this table is non-partitioned, it may be instantiating the metaclient incorrectly resulting in 0 records,  check this code here : https://github.com/uber/hudi/blob/master/hoodie-hadoop-mr/src/main/java/com/uber/hoodie/hadoop/HoodieInputFormat.java#L209.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444373547/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444416665","html_url":"https://github.com/apache/hudi/issues/524#issuecomment-444416665","issue_url":"https://api.github.com/repos/apache/hudi/issues/524","id":444416665,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDQxNjY2NQ==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T09:24:05Z","updated_at":"2018-12-05T09:26:05Z","author_association":"CONTRIBUTOR","body":"here is struct\r\n```\r\n      val schema = StructType(List(\r\n        StructField(\"upload_id\", IntegerType, nullable = false),\r\n        StructField(\"barcode\", StringType, nullable = false),\r\n        StructField(\"_row_key\", StringType, nullable = false),\r\n        StructField(\"extend_deal_date\", StringType, nullable = true),\r\n        StructField(\"extend_deal_type\", StringType, nullable = true),\r\n        StructField(\"extend_dispatch_uuid\", StringType, nullable = true),\r\n        StructField(\"partition\", StringType, nullable = true))\r\n      )\r\n```\r\nand partition like \r\n```\r\n`partition`='2018/12/05' \r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444416665/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444614468","html_url":"https://github.com/apache/hudi/issues/524#issuecomment-444614468","issue_url":"https://api.github.com/repos/apache/hudi/issues/524","id":444614468,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDYxNDQ2OA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T19:33:08Z","updated_at":"2018-12-05T19:33:08Z","author_association":"CONTRIBUTOR","body":"@cdmikechen: The symptoms seem to indicate that there are no parquet files written. Can you paste the following output\r\n\r\n1. Show commits:\r\n    (a)  Run Hoodie CLI (./hoodie-cli/ hoodie-cli.sh)\r\n    (b)  connect to your dataset connect --path /tmp/cloud2_code/c_upload_code\r\n    (c) commits show\r\n2. list (recursively) all the files/dirs under the base-path (/tmp/cloud2_code/c_upload_code).\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444614468/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444650070","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444650070","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444650070,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDY1MDA3MA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T21:12:17Z","updated_at":"2018-12-05T21:12:17Z","author_association":"CONTRIBUTOR","body":"@AndrewKL :  The defaults for Hoodie Write and Hive-Sync are setup for date-partitioned tables. So, We would need additional configurations to be added. \r\n\r\nhoodie.datasource.hive_sync.partition_extractor_class=com.uber.hoodie.hive.NonPartitionedExtractor\r\nhoodie.datasource.write.keygenerator.class=com.uber.hoodie.NonpartitionedKeyGenerator\r\n\r\nRecreating the dataset with the above settings should resolve all the issues on both write and read sides. The HoodieJavaApp example changes as part of this PR hopefully should give us an example :) \r\n\r\n\r\n\r\n> It looks like there's one more bug. The table does show up in metastore though.\r\n> \r\n> ```\r\n> scala> HorizonCompactionTool.saveToHudiTable(\r\n>      |       spark.read.parquet(\"hdfs:///tmp/ap-invoices-all-inc-spark-shell/\").sample(false,0.5),//deltas,//\r\n>      |       tableName = \"ap_invoices_all_spark_shell\",\r\n>      |       tablePath = \"hdfs:///hudi-data/data/ap-invoices-all/\",\r\n>      |       primaryKey = \"invoice_id\",\r\n>      |       pkOrderingCol = \"capture_timestamp\",\r\n>      |       enableHiveSync = true,\r\n>      |       partitionCol = None,\r\n>      |       insertMode = \"append\",\r\n>      |       None\r\n>      |     )\r\n> SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".                \r\n> SLF4J: Defaulting to no-operation (NOP) logger implementation\r\n> SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\n> com.uber.hoodie.hive.HoodieHiveSyncException: Failed to sync partitions for table ap_invoices_all_spark_shell\r\n>   at com.uber.hoodie.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:169)\r\n>   at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:109)\r\n>   at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:68)\r\n>   at com.uber.hoodie.DefaultSource.syncHive(DefaultSource.scala:301)\r\n>   at com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:273)\r\n>   at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n>   at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n>   at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n>   at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n>   at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n>   at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n>   at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n>   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n>   at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n>   at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n>   at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n>   at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n>   at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\r\n>   at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\r\n>   at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n>   at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\r\n>   at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n>   at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\r\n>   at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\r\n>   at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply$mcV$sp(HorizonCompactionTool.scala:98)\r\n>   at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply(HorizonCompactionTool.scala:65)\r\n>   at HorizonCompactionTool$$anonfun$saveToHudiTable$1.apply(HorizonCompactionTool.scala:65)\r\n>   at HorizonCompactionTool$.stopwatch(HorizonCompactionTool.scala:115)\r\n>   at HorizonCompactionTool$.saveToHudiTable(HorizonCompactionTool.scala:65)\r\n>   ... 59 elided\r\n> Caused by: java.lang.IllegalArgumentException: Partition path default is not in the form yyyy/mm/dd\r\n>   at com.uber.hoodie.hive.SlashEncodedDayPartitionValueExtractor.extractPartitionValuesInPath(SlashEncodedDayPartitionValueExtractor.java:47)\r\n>   at com.uber.hoodie.hive.HoodieHiveClient.getPartitionEvents(HoodieHiveClient.java:216)\r\n>   at com.uber.hoodie.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:160)\r\n>   ... 87 more\r\n> \r\n> ``\r\n> ```\r\n> scala> sql(\"SHOW TABLES\").show(false)\r\n> +--------+---------------------------+-----------+\r\n> |database|tableName |isTemporary|\r\n> +--------+---------------------------+-----------+\r\n> |default |ap_invoices_all_inc |false |\r\n> |default |ap_invoices_all_spark_shell|false |\r\n> +--------+---------------------------+-----------+\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444650070/reactions","total_count":2,"+1":0,"-1":0,"laugh":1,"hooray":1,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444671653","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444671653","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444671653,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDY3MTY1Mw==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T22:13:31Z","updated_at":"2018-12-05T22:13:31Z","author_association":"NONE","body":"@bvaradar Thanks a ton!\r\n\r\nQuick question I'm interested in digging into the source code a bit. If I wanted to add the ability to auto detect partitioned vs un partitioned.  Where should I start looking?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444671653/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444674843","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444674843","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444674843,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDY3NDg0Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T22:24:20Z","updated_at":"2018-12-05T22:24:20Z","author_association":"CONTRIBUTOR","body":"@AndrewKL : Hoodie Source is expected to handle this case by filtering for parquet and delta files. Can you confirm if spark.sql.hive.convertMetastoreParquet is set to false also for this run ?\r\n\r\nBalaji.V\r\n \r\n> Edit: This bug was due to copying the table files from S3 to hdfs vis the s3-dist-cp tool. S3 creates special place holder files to indicate the presence of a folder since folders do not actually exist in S3.\r\n> \r\n> and one more exception while trying to read the table\r\n> \r\n> ```\r\n> scala> sql(\"SELECT count(*) FROM ap_invoices_all_spark_shell\").show\r\n> 18/12/05 01:55:00 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 12810, ip-172-31-29-24.ec2.internal, executor 322): java.io.IOException: Could not read footer for file: FileStatus{path=hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$; isDirectory=false; length=0; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}\r\n> \tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:526)\r\n> \tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:513)\r\n> \tat scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)\r\n> \tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)\r\n> \tat scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n> \tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n> \tat scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n> \tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n> \tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n> \tat scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)\r\n> \tat scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)\r\n> \tat scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)\r\n> \tat scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)\r\n> \tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)\r\n> \tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n> \tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n> \tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n> \tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n> \tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n> \tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\r\n> \tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\r\n> \tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\r\n> Caused by: java.lang.RuntimeException: hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$ is not a Parquet file (too small)\r\n> \tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:466)\r\n> \tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)\r\n> \tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)\r\n> \tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519)\r\n> \t... 32 more\r\n> \r\n> 18/12/05 01:55:01 ERROR TaskSetManager: Task 0 in stage 33.0 failed 4 times; aborting job\r\n> org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 4 times, most recent failure: Lost task 0.3 in stage 33.0 (TID 12813, ip-172-31-29-24.ec2.internal, executor 367): java.io.IOException: Could not read footer for file: FileStatus{path=hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$; isDirectory=false; length=0; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}\r\n> \tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:526)\r\n> \tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:513)\r\n> \tat scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)\r\n> \tat scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)\r\n> \tat scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n> \tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n> \tat scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n> \tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n> \tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n> \tat scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)\r\n> \tat scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)\r\n> \tat scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)\r\n> \tat scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)\r\n> \tat scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)\r\n> \tat scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n> \tat scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n> \tat scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n> \tat scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n> \tat scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n> \tat scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n> \tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n> \tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\r\n> \tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\r\n> \tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\r\n> Caused by: java.lang.RuntimeException: hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$ is not a Parquet file (too small)\r\n> \tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:466)\r\n> \tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)\r\n> \tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)\r\n> \tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519)\r\n> \t... 32 more\r\n> \r\n> Driver stacktrace:\r\n>   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\r\n>   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\r\n>   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\r\n>   at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n>   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n>   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\r\n>   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\r\n>   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\r\n>   at scala.Option.foreach(Option.scala:257)\r\n>   at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\r\n>   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\r\n>   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\r\n>   at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\r\n>   at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n>   at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\r\n>   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n>   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n>   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n>   at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n>   at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\r\n>   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n>   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n>   at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n>   at org.apache.spark.rdd.RDD.collect(RDD.scala:944)\r\n>   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:611)\r\n>   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:241)\r\n>   at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$inferIfNeeded(HiveMetastoreCatalog.scala:239)\r\n>   at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6$$anonfun$7.apply(HiveMetastoreCatalog.scala:193)\r\n>   at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6$$anonfun$7.apply(HiveMetastoreCatalog.scala:192)\r\n>   at scala.Option.getOrElse(Option.scala:121)\r\n>   at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6.apply(HiveMetastoreCatalog.scala:192)\r\n>   at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6.apply(HiveMetastoreCatalog.scala:185)\r\n>   at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:54)\r\n>   at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:185)\r\n>   at org.apache.spark.sql.hive.RelationConversions.org$apache$spark$sql$hive$RelationConversions$$convert(HiveStrategies.scala:195)\r\n>   at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:226)\r\n>   at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:215)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\r\n>   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n>   at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\r\n>   at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:215)\r\n>   at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:180)\r\n>   at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n>   at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n>   at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r\n>   at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r\n>   at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48)\r\n>   at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n>   at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n>   at scala.collection.immutable.List.foreach(List.scala:381)\r\n>   at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n>   at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124)\r\n>   at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118)\r\n>   at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103)\r\n>   at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\r\n>   at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\r\n>   at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\r\n>   at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)\r\n>   at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\r\n>   ... 49 elided\r\n> Caused by: java.io.IOException: Could not read footer for file: FileStatus{path=hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$; isDirectory=false; length=0; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=false}\r\n>   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:526)\r\n>   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:513)\r\n>   at scala.collection.parallel.AugmentedIterableIterator$class.flatmap2combiner(RemainsIterator.scala:132)\r\n>   at scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)\r\n>   at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)\r\n>   at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n>   at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n>   at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n>   at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n>   at scala.collection.parallel.ParIterableLike$FlatMap.tryLeaf(ParIterableLike.scala:1068)\r\n>   at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n>   at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n>   at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n>   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n>   at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)\r\n>   at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)\r\n>   at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)\r\n>   at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)\r\n>   at scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)\r\n>   at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)\r\n>   at scala.collection.parallel.ParIterableLike$ResultMapping.leaf(ParIterableLike.scala:958)\r\n>   at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply$mcV$sp(Tasks.scala:49)\r\n>   at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n>   at scala.collection.parallel.Task$$anonfun$tryLeaf$1.apply(Tasks.scala:48)\r\n>   at scala.collection.parallel.Task$class.tryLeaf(Tasks.scala:51)\r\n>   at scala.collection.parallel.ParIterableLike$ResultMapping.tryLeaf(ParIterableLike.scala:953)\r\n>   at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask$class.compute(Tasks.scala:152)\r\n>   at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:443)\r\n>   at scala.concurrent.forkjoin.RecursiveAction.exec(RecursiveAction.java:160)\r\n>   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\r\n>   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\r\n>   at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\r\n>   at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\r\n> Caused by: java.lang.RuntimeException: hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all/default_$folder$ is not a Parquet file (too small)\r\n>   at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:466)\r\n>   at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:445)\r\n>   at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:421)\r\n>   at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:519)\r\n>   ... 32 more\r\n> ```\r\n> This is whats in the folder\r\n> \r\n> ```\r\n> Found 4 items\r\n> drwxr-xr-x   - hadoop hadoop          0 2018-12-05 01:51 /hudi-data/data/ap-invoices-all/.hoodie\r\n> -rw-r--r--   2 hadoop hadoop          0 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie_$folder$\r\n> drwxr-xr-x   - hadoop hadoop          0 2018-12-05 01:51 /hudi-data/data/ap-invoices-all/default\r\n> -rw-r--r--   2 hadoop hadoop          0 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default_$folder$\r\n> ```\r\n> the hoodi-cli doesn't seem to have a problem with it\r\n> \r\n> ```\r\n> hoodie:ap_invoices_all_hudi->commits show\r\n>     __________________________________________________________________________________________________________________________________________________________________________\r\n>     | CommitTime    | Total Bytes Written| Total Files Added| Total Files Updated| Total Partitions Written| Total Records Written| Total Update Records Written| Total Errors|\r\n>     |=========================================================================================================================================================================|\r\n>     | 20181205014126| 4.0 GB             | 1                | 101                | 1                       | 45028592             | 303144                      | 0           |\r\n>     | 20181204202247| 3.9 GB             | 1                | 100                | 1                       | 44149130             | 355931                      | 0           |\r\n>     | 20181204200718| 3.9 GB             | 1                | 99                 | 1                       | 43807211             | 341479                      | 0           |\r\n>     | 20181204194909| 3.8 GB             | 0                | 99                 | 1                       | 43299425             | 649694                      | 0           |\r\n>     | 20181204193733| 3.8 GB             | 1                | 98                 | 1                       | 43299425             | 299883                      | 0           |\r\n> ...\r\n> ```\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444674843/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444684065","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444684065","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444684065,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDY4NDA2NQ==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T22:58:53Z","updated_at":"2018-12-05T22:58:53Z","author_association":"NONE","body":"@bvaradar \r\n\r\nHere's what I've done\r\n\r\n```\r\nscala> sql(\"SELECT count(*) FROM ap_invoices_all_spark_shell\").show\r\n+--------+                                                                      \r\n|count(1)|\r\n+--------+\r\n|       0|\r\n+--------+\r\n\r\n\r\nscala> spark.conf.set(\"spark.sql.hive.convertMetastoreParquet\",\"false\")\r\n\r\nscala> sql(\"SELECT count(*) FROM ap_invoices_all_spark_shell\").show\r\n+--------+\r\n|count(1)|\r\n+--------+\r\n|       0|\r\n+--------+\r\n\r\n\r\nscala> sql(\"SELECT count(*) FROM ap_invoices_all_spark_shell\").explain\r\n== Physical Plan ==\r\n*(2) HashAggregate(keys=[], functions=[count(1)])\r\n+- Exchange SinglePartition\r\n   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\r\n      +- HiveTableScan HiveTableRelation `default`.`ap_invoices_all_spark_shell`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [_hoodie_commit_time#1257, _hoodie_commit_seqno#1258, _hoodie_record_key#1259, _hoodie_partition_path#1260, _hoodie_file_name#1261, invoice_id#1262, last_update_date#1263L, last_updated_by#1264, vendor_id#1265, invoice_num#1266, set_of_books_id#1267, invoice_currency_code#1268, payment_currency_code#1269, payment_cross_rate#1270, invoice_amount#1271, vendor_site_id#1272, amount_paid#1273, discount_amount_taken#1274, invoice_date#1275L, source#1276, invoice_type_lookup_code#1277, description#1278, batch_id#1279, amount_applicable_to_discount#1280, ... 177 more fields]\r\n\r\n```\r\n\r\nheres the table description\r\n```\r\nscala> sql(\"DESCRIBE EXTENDED ap_invoices_all_spark_shell\").show(500,false)\r\n+-----------------------------+-----------------------------------------------------------------------+-------+\r\n|col_name                     |data_type                                                              |comment|\r\n+-----------------------------+-----------------------------------------------------------------------+-------+\r\n|_hoodie_commit_time          |string                                                                 |null   |\r\n|_hoodie_commit_seqno         |string                                                                 |null   |\r\n|_hoodie_record_key           |string                                                                 |null   |\r\n|_hoodie_partition_path       |string                                                                 |null   |\r\n|_hoodie_file_name            |string                                                                 |null   |\r\n|invoice_id                   |string                                                                 |null   |\r\n|last_update_date             |bigint                                                                 |null   |\r\n|last_updated_by              |string                                                                 |null   |\r\n|vendor_id                    |string                                                                 |null   |\r\n|invoice_num                  |string                                                                 |null   |\r\n|set_of_books_id              |string                                                                 |null   |\r\n|invoice_currency_code        |string                                                                 |null   |\r\n|payment_currency_code        |string                                                                 |null   |\r\n|payment_cross_rate           |string                                                                 |null   |\r\n|invoice_amount               |string                                                                 |null   |\r\n...\r\n|integ_key                    |string                                                                 |null   |\r\n|op_type                      |string                                                                 |null   |\r\n|updatedby_user               |string                                                                 |null   |\r\n|                             |                                                                       |       |\r\n|# Detailed Table Information |                                                                       |       |\r\n|Database                     |default                                                                |       |\r\n|Table                        |ap_invoices_all_spark_shell                                            |       |\r\n|Owner                        |hive                                                                   |       |\r\n|Created Time                 |Wed Dec 05 01:51:54 UTC 2018                                           |       |\r\n|Last Access                  |Thu Jan 01 00:00:00 UTC 1970                                           |       |\r\n|Created By                   |Spark 2.2 or prior                                                     |       |\r\n|Type                         |EXTERNAL                                                               |       |\r\n|Provider                     |hive                                                                   |       |\r\n|Table Properties             |[transient_lastDdlTime=1543974714]                                     |       |\r\n|Statistics                   |97566763105 bytes                                                      |       |\r\n|Location                     |hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all|       |\r\n|Serde Library                |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe            |       |\r\n|InputFormat                  |com.uber.hoodie.hadoop.HoodieInputFormat                               |       |\r\n|OutputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat         |       |\r\n|Storage Properties           |[serialization.format=1]                                               |       |\r\n|Partition Provider           |Catalog                                                                |       |\r\n+-----------------------------+-----------------------------------------------------------------------+-------+\r\n```\r\nstats filesizes seems to be returning nonsense\r\n```\r\nhoodie->connect --path /hudi-data/data/ap-invoices-all\r\nhoodie:ap_invoices_all_hudi->commits show\r\n    __________________________________________________________________________________________________________________________________________________________________________\r\n    | CommitTime    | Total Bytes Written| Total Files Added| Total Files Updated| Total Partitions Written| Total Records Written| Total Update Records Written| Total Errors|\r\n    |=========================================================================================================================================================================|\r\n    | 20181205014126| 4.0 GB             | 1                | 101                | 1                       | 45028592             | 303144                      | 0           |\r\n    | 20181204202247| 3.9 GB             | 1                | 100                | 1                       | 44149130             | 355931                      | 0           |\r\n\r\nhoodie:ap_invoices_all_hudi->stats filesizes \r\n    ________________________________________________________________________\r\n    | CommitTime| Min  | 10th | 50th | avg  | 95th | Max  | NumFiles| StdDev|\r\n    |=======================================================================|\r\n    | ALL       | 0.0 B| 0.0 B| 0.0 B| 0.0 B| 0.0 B| 0.0 B| 0       | 0.0 B |\r\n\r\nhoodie:ap_invoices_all_hudi->stats wa\r\n    __________________________________________________________________________\r\n    | CommitTime    | Total Upserted| Total Written| Write Amplifiation Factor|\r\n    |=========================================================================|\r\n    | 20181130004310| 0             | 1605638      | 0                        |\r\n    | 20181130013214| 89975         | 2496075      | 27.74                    |\r\n    | 20181203224634| 1605638       | 1605638      | 1.00                     |\r\n    | 20181203235506| 36571         | 3649556      | 99.79                    |\r\n    | 20181204001010| 571416        | 4748296      | 8.31                     |\r\n    | 20181204002420| 16327         | 5557172      | 340.37                   |\r\n    | 20181204003913| 91667         | 6551805      | 71.47                    |\r\n    | 20181204005324| 63750         | 6213254      | 97.46                    |\r\n    | 20181204010534| 66048         | 6990020      | 105.83                   |\r\n```\r\nhere's the files in the folder...\r\n\r\n```\r\n[hadoop@ip-172-31-29-14 ~]$ hdfs dfs -ls /hudi-data/data/ap-invoices-all\r\nFound 2 items\r\ndrwxr-xr-x   - hadoop hadoop          0 2018-12-05 01:51 /hudi-data/data/ap-invoices-all/.hoodie\r\ndrwxr-xr-x   - hadoop hadoop          0 2018-12-05 01:51 /hudi-data/data/ap-invoices-all/default\r\n[hadoop@ip-172-31-29-14 ~]$ hdfs dfs -ls /hudi-data/data/ap-invoices-all/default\r\nFound 2303 items\r\n-rw-r--r--   2 hadoop hadoop         93 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/default/.hoodie_partition_metadata\r\n-rw-r--r--   2 hadoop hadoop   53476703 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_18_20181204192130.parquet\r\n-rw-r--r--   2 hadoop hadoop   53475807 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_19_20181204185654.parquet\r\n-rw-r--r--   2 hadoop hadoop   53482655 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_19_20181204200718.parquet\r\n-rw-r--r--   2 hadoop hadoop   53485633 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_20_20181204193733.parquet\r\n-rw-r--r--   2 hadoop hadoop   53483232 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_20_20181204194909.parquet\r\n-rw-r--r--   2 hadoop hadoop   53498986 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_21_20181204202247.parquet\r\n-rw-r--r--   2 hadoop hadoop   53597006 2018-12-05 01:50 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_21_20181205014126.parquet\r\n```\r\n```\r\n[hadoop@ip-172-31-29-14 ~]$ hdfs dfs -ls /hudi-data/data/ap-invoices-all/.hoodie\r\nFound 178 items\r\n-rw-r--r--   2 hadoop hadoop          0 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/.aux_$folder$\r\n-rw-r--r--   2 hadoop hadoop          0 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/.temp_$folder$\r\n-rw-r--r--   2 hadoop hadoop        378 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181129195620.inflight\r\n-rw-r--r--   2 hadoop hadoop        378 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181129200124.inflight\r\n-rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181130004310.clean\r\n-rw-r--r--   2 hadoop hadoop      20033 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181130004310.commit\r\n-rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181130013214.clean\r\n-rw-r--r--   2 hadoop hadoop      21618 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181130013214.commit\r\n-rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/20181203224634.clean\r\n-rw-r--r--   2 hadoop hadoop      20923 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181203224634.commit\r\n-rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/20181203235506.clean\r\n-rw-r--r--   2 hadoop hadoop      24461 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/20181203235506.commit\r\n-rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/20181204001010.clean\r\n-rw-r--r--   2 hadoop hadoop      27268 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181204001010.commit\r\n-rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181204002420.clean\r\n...\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444684065/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444690211","html_url":"https://github.com/apache/hudi/pull/520#issuecomment-444690211","issue_url":"https://api.github.com/repos/apache/hudi/issues/520","id":444690211,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDY5MDIxMQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T23:24:47Z","updated_at":"2018-12-05T23:24:47Z","author_association":"CONTRIBUTOR","body":"@arukavytsia :  Looks like there is some Java checkstyle violation in the PR. Can you also fix this.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444690211/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444691557","html_url":"https://github.com/apache/hudi/pull/520#issuecomment-444691557","issue_url":"https://api.github.com/repos/apache/hudi/issues/520","id":444691557,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDY5MTU1Nw==","user":{"login":"artem0","id":9960628,"node_id":"MDQ6VXNlcjk5NjA2Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/9960628?v=4","gravatar_id":"","url":"https://api.github.com/users/artem0","html_url":"https://github.com/artem0","followers_url":"https://api.github.com/users/artem0/followers","following_url":"https://api.github.com/users/artem0/following{/other_user}","gists_url":"https://api.github.com/users/artem0/gists{/gist_id}","starred_url":"https://api.github.com/users/artem0/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/artem0/subscriptions","organizations_url":"https://api.github.com/users/artem0/orgs","repos_url":"https://api.github.com/users/artem0/repos","events_url":"https://api.github.com/users/artem0/events{/privacy}","received_events_url":"https://api.github.com/users/artem0/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T23:30:48Z","updated_at":"2018-12-05T23:30:48Z","author_association":"CONTRIBUTOR","body":"> @arukavytsia : Looks like there is some Java checkstyle violation in the PR. Can you also fix this.\r\n\r\nIn progress.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444691557/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444695311","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444695311","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444695311,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDY5NTMxMQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-05T23:48:41Z","updated_at":"2018-12-05T23:49:03Z","author_association":"CONTRIBUTOR","body":"> @bvaradar Thanks a ton!\r\n> \r\n> Quick question I'm interested in digging into the source code a bit. If I wanted to add the ability to auto detect partitioned vs un partitioned. Where should I start looking?\r\n\r\n@AndrewKL  Sounds great. For this, I think the best place should be in configuration management. Can you take a look at HoodieSparkSqlWriter.parametersWithWriteDefaults. You can inspect the partition-fields in config (HIVE_PARTITION_FIELDS_OPT_KEY). If it is empty, automatically set PartitionValueExtractor and KeyGenerator (if empty) classes in the config.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444695311/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444699453","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444699453","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444699453,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDY5OTQ1Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T00:09:02Z","updated_at":"2018-12-06T00:09:02Z","author_association":"CONTRIBUTOR","body":"@AndrewKL \r\n\r\nThanks for the detailed output. This was very useful to debug. \r\n\r\nOne possible issue, I can think of is - The Hive table is defined as an unpartitioned table with  location set to hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all but the files are written under sub-directory \"default\".  With  NonpartitionedKeyGenerator configured, the default partition path \"default\" should never be created. I verified this is the case by running  HoodieJavaApp (With --non-partitioned). If you have not already done, Can you start from scratch and  ingest into a brand new table with updated configs ?\r\n\r\n\r\n\r\n\r\n> @bvaradar\r\n> \r\n> Here's what I've done\r\n> \r\n> ```\r\n> scala> sql(\"SELECT count(*) FROM ap_invoices_all_spark_shell\").show\r\n> +--------+                                                                      \r\n> |count(1)|\r\n> +--------+\r\n> |       0|\r\n> +--------+\r\n> \r\n> \r\n> scala> spark.conf.set(\"spark.sql.hive.convertMetastoreParquet\",\"false\")\r\n> \r\n> scala> sql(\"SELECT count(*) FROM ap_invoices_all_spark_shell\").show\r\n> +--------+\r\n> |count(1)|\r\n> +--------+\r\n> |       0|\r\n> +--------+\r\n> \r\n> \r\n> scala> sql(\"SELECT count(*) FROM ap_invoices_all_spark_shell\").explain\r\n> == Physical Plan ==\r\n> *(2) HashAggregate(keys=[], functions=[count(1)])\r\n> +- Exchange SinglePartition\r\n>    +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\r\n>       +- HiveTableScan HiveTableRelation `default`.`ap_invoices_all_spark_shell`, org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe, [_hoodie_commit_time#1257, _hoodie_commit_seqno#1258, _hoodie_record_key#1259, _hoodie_partition_path#1260, _hoodie_file_name#1261, invoice_id#1262, last_update_date#1263L, last_updated_by#1264, vendor_id#1265, invoice_num#1266, set_of_books_id#1267, invoice_currency_code#1268, payment_currency_code#1269, payment_cross_rate#1270, invoice_amount#1271, vendor_site_id#1272, amount_paid#1273, discount_amount_taken#1274, invoice_date#1275L, source#1276, invoice_type_lookup_code#1277, description#1278, batch_id#1279, amount_applicable_to_discount#1280, ... 177 more fields]\r\n> ```\r\n> heres the table description\r\n> \r\n> ```\r\n> scala> sql(\"DESCRIBE EXTENDED ap_invoices_all_spark_shell\").show(500,false)\r\n> +-----------------------------+-----------------------------------------------------------------------+-------+\r\n> |col_name                     |data_type                                                              |comment|\r\n> +-----------------------------+-----------------------------------------------------------------------+-------+\r\n> |_hoodie_commit_time          |string                                                                 |null   |\r\n> |_hoodie_commit_seqno         |string                                                                 |null   |\r\n> |_hoodie_record_key           |string                                                                 |null   |\r\n> |_hoodie_partition_path       |string                                                                 |null   |\r\n> |_hoodie_file_name            |string                                                                 |null   |\r\n> |invoice_id                   |string                                                                 |null   |\r\n> |last_update_date             |bigint                                                                 |null   |\r\n> |last_updated_by              |string                                                                 |null   |\r\n> |vendor_id                    |string                                                                 |null   |\r\n> |invoice_num                  |string                                                                 |null   |\r\n> |set_of_books_id              |string                                                                 |null   |\r\n> |invoice_currency_code        |string                                                                 |null   |\r\n> |payment_currency_code        |string                                                                 |null   |\r\n> |payment_cross_rate           |string                                                                 |null   |\r\n> |invoice_amount               |string                                                                 |null   |\r\n> ...\r\n> |integ_key                    |string                                                                 |null   |\r\n> |op_type                      |string                                                                 |null   |\r\n> |updatedby_user               |string                                                                 |null   |\r\n> |                             |                                                                       |       |\r\n> |# Detailed Table Information |                                                                       |       |\r\n> |Database                     |default                                                                |       |\r\n> |Table                        |ap_invoices_all_spark_shell                                            |       |\r\n> |Owner                        |hive                                                                   |       |\r\n> |Created Time                 |Wed Dec 05 01:51:54 UTC 2018                                           |       |\r\n> |Last Access                  |Thu Jan 01 00:00:00 UTC 1970                                           |       |\r\n> |Created By                   |Spark 2.2 or prior                                                     |       |\r\n> |Type                         |EXTERNAL                                                               |       |\r\n> |Provider                     |hive                                                                   |       |\r\n> |Table Properties             |[transient_lastDdlTime=1543974714]                                     |       |\r\n> |Statistics                   |97566763105 bytes                                                      |       |\r\n> |Location                     |hdfs://ip-172-31-29-14.ec2.internal:8020/hudi-data/data/ap-invoices-all|       |\r\n> |Serde Library                |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe            |       |\r\n> |InputFormat                  |com.uber.hoodie.hadoop.HoodieInputFormat                               |       |\r\n> |OutputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat         |       |\r\n> |Storage Properties           |[serialization.format=1]                                               |       |\r\n> |Partition Provider           |Catalog                                                                |       |\r\n> +-----------------------------+-----------------------------------------------------------------------+-------+\r\n> ```\r\n> stats filesizes seems to be returning nonsense\r\n> \r\n> ```\r\n> hoodie->connect --path /hudi-data/data/ap-invoices-all\r\n> hoodie:ap_invoices_all_hudi->commits show\r\n>     __________________________________________________________________________________________________________________________________________________________________________\r\n>     | CommitTime    | Total Bytes Written| Total Files Added| Total Files Updated| Total Partitions Written| Total Records Written| Total Update Records Written| Total Errors|\r\n>     |=========================================================================================================================================================================|\r\n>     | 20181205014126| 4.0 GB             | 1                | 101                | 1                       | 45028592             | 303144                      | 0           |\r\n>     | 20181204202247| 3.9 GB             | 1                | 100                | 1                       | 44149130             | 355931                      | 0           |\r\n> \r\n> hoodie:ap_invoices_all_hudi->stats filesizes \r\n>     ________________________________________________________________________\r\n>     | CommitTime| Min  | 10th | 50th | avg  | 95th | Max  | NumFiles| StdDev|\r\n>     |=======================================================================|\r\n>     | ALL       | 0.0 B| 0.0 B| 0.0 B| 0.0 B| 0.0 B| 0.0 B| 0       | 0.0 B |\r\n> \r\n> hoodie:ap_invoices_all_hudi->stats wa\r\n>     __________________________________________________________________________\r\n>     | CommitTime    | Total Upserted| Total Written| Write Amplifiation Factor|\r\n>     |=========================================================================|\r\n>     | 20181130004310| 0             | 1605638      | 0                        |\r\n>     | 20181130013214| 89975         | 2496075      | 27.74                    |\r\n>     | 20181203224634| 1605638       | 1605638      | 1.00                     |\r\n>     | 20181203235506| 36571         | 3649556      | 99.79                    |\r\n>     | 20181204001010| 571416        | 4748296      | 8.31                     |\r\n>     | 20181204002420| 16327         | 5557172      | 340.37                   |\r\n>     | 20181204003913| 91667         | 6551805      | 71.47                    |\r\n>     | 20181204005324| 63750         | 6213254      | 97.46                    |\r\n>     | 20181204010534| 66048         | 6990020      | 105.83                   |\r\n> ```\r\n> here's the files in the folder...\r\n> \r\n> ```\r\n> [hadoop@ip-172-31-29-14 ~]$ hdfs dfs -ls /hudi-data/data/ap-invoices-all\r\n> Found 2 items\r\n> drwxr-xr-x   - hadoop hadoop          0 2018-12-05 01:51 /hudi-data/data/ap-invoices-all/.hoodie\r\n> drwxr-xr-x   - hadoop hadoop          0 2018-12-05 01:51 /hudi-data/data/ap-invoices-all/default\r\n> [hadoop@ip-172-31-29-14 ~]$ hdfs dfs -ls /hudi-data/data/ap-invoices-all/default\r\n> Found 2303 items\r\n> -rw-r--r--   2 hadoop hadoop         93 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/default/.hoodie_partition_metadata\r\n> -rw-r--r--   2 hadoop hadoop   53476703 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_18_20181204192130.parquet\r\n> -rw-r--r--   2 hadoop hadoop   53475807 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_19_20181204185654.parquet\r\n> -rw-r--r--   2 hadoop hadoop   53482655 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_19_20181204200718.parquet\r\n> -rw-r--r--   2 hadoop hadoop   53485633 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_20_20181204193733.parquet\r\n> -rw-r--r--   2 hadoop hadoop   53483232 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_20_20181204194909.parquet\r\n> -rw-r--r--   2 hadoop hadoop   53498986 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_21_20181204202247.parquet\r\n> -rw-r--r--   2 hadoop hadoop   53597006 2018-12-05 01:50 /hudi-data/data/ap-invoices-all/default/07576cc8-5cc1-4da7-b96d-06cd0f80e4a6_21_20181205014126.parquet\r\n> ```\r\n> ```\r\n> [hadoop@ip-172-31-29-14 ~]$ hdfs dfs -ls /hudi-data/data/ap-invoices-all/.hoodie\r\n> Found 178 items\r\n> -rw-r--r--   2 hadoop hadoop          0 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/.aux_$folder$\r\n> -rw-r--r--   2 hadoop hadoop          0 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/.temp_$folder$\r\n> -rw-r--r--   2 hadoop hadoop        378 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181129195620.inflight\r\n> -rw-r--r--   2 hadoop hadoop        378 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181129200124.inflight\r\n> -rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181130004310.clean\r\n> -rw-r--r--   2 hadoop hadoop      20033 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181130004310.commit\r\n> -rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181130013214.clean\r\n> -rw-r--r--   2 hadoop hadoop      21618 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181130013214.commit\r\n> -rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/20181203224634.clean\r\n> -rw-r--r--   2 hadoop hadoop      20923 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181203224634.commit\r\n> -rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/20181203235506.clean\r\n> -rw-r--r--   2 hadoop hadoop      24461 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/20181203235506.commit\r\n> -rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:27 /hudi-data/data/ap-invoices-all/.hoodie/20181204001010.clean\r\n> -rw-r--r--   2 hadoop hadoop      27268 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181204001010.commit\r\n> -rw-r--r--   2 hadoop hadoop       1091 2018-12-05 01:26 /hudi-data/data/ap-invoices-all/.hoodie/20181204002420.clean\r\n> ...\r\n> ```\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444699453/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444710581","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-444710581","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":444710581,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDcxMDU4MQ==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T01:07:56Z","updated_at":"2018-12-06T01:07:56Z","author_association":"NONE","body":"out of curiosity would it be possible to autoset these options?  If so I'd love to give a go at creating a commit.\r\n\r\nex:\r\n\r\n```\r\n  /**\r\n    * Add default options for unspecified write options keys.\r\n    *\r\n    * @param parameters\r\n    * @return\r\n    */\r\n  def parametersWithWriteDefaults(parameters: Map[String, String]) = {\r\n    val defaultsMap = new ConcurrentHashMap[String, String](mapAsJavaMap(parameters))\r\n    defaultsMap.putIfAbsent(OPERATION_OPT_KEY, DEFAULT_OPERATION_OPT_VAL)\r\n    defaultsMap.putIfAbsent(STORAGE_TYPE_OPT_KEY, DEFAULT_STORAGE_TYPE_OPT_VAL)\r\n    defaultsMap.putIfAbsent(PRECOMBINE_FIELD_OPT_KEY, DEFAULT_PRECOMBINE_FIELD_OPT_VAL)\r\n    defaultsMap.putIfAbsent(PAYLOAD_CLASS_OPT_KEY, DEFAULT_PAYLOAD_OPT_VAL)\r\n    defaultsMap.putIfAbsent(RECORDKEY_FIELD_OPT_KEY, DEFAULT_RECORDKEY_FIELD_OPT_VAL)\r\n    //TODO set PARTITIONPATH_FIELD_OPT_KEY automatically based on if the table is partition\r\n    defaultsMap.putIfAbsent(PARTITIONPATH_FIELD_OPT_KEY, DEFAULT_PARTITIONPATH_FIELD_OPT_VAL)\r\n    defaultsMap.putIfAbsent(KEYGENERATOR_CLASS_OPT_KEY, DEFAULT_KEYGENERATOR_CLASS_OPT_VAL)\r\n    defaultsMap.putIfAbsent(COMMIT_METADATA_KEYPREFIX_OPT_KEY, DEFAULT_COMMIT_METADATA_KEYPREFIX_OPT_VAL)\r\n    defaultsMap.putIfAbsent(INSERT_DROP_DUPS_OPT_KEY, DEFAULT_INSERT_DROP_DUPS_OPT_VAL)\r\n    defaultsMap.putIfAbsent(HIVE_SYNC_ENABLED_OPT_KEY, DEFAULT_HIVE_SYNC_ENABLED_OPT_VAL)\r\n    defaultsMap.putIfAbsent(HIVE_DATABASE_OPT_KEY, DEFAULT_HIVE_DATABASE_OPT_VAL)\r\n    defaultsMap.putIfAbsent(HIVE_TABLE_OPT_KEY, DEFAULT_HIVE_TABLE_OPT_VAL)\r\n    defaultsMap.putIfAbsent(HIVE_USER_OPT_KEY, DEFAULT_HIVE_USER_OPT_VAL)\r\n    defaultsMap.putIfAbsent(HIVE_PASS_OPT_KEY, DEFAULT_HIVE_PASS_OPT_VAL)\r\n    defaultsMap.putIfAbsent(HIVE_URL_OPT_KEY, DEFAULT_HIVE_URL_OPT_VAL)\r\n    defaultsMap.putIfAbsent(HIVE_PARTITION_FIELDS_OPT_KEY, DEFAULT_HIVE_PARTITION_FIELDS_OPT_VAL)\r\n    //TODO auto set the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY based on if the table is partition\r\n    defaultsMap.putIfAbsent(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY, DEFAULT_HIVE_PARTITION_EXTRACTOR_CLASS_OPT_VAL)\r\n    defaultsMap.putIfAbsent(HIVE_ASSUME_DATE_PARTITION_OPT_KEY, DEFAULT_HIVE_ASSUME_DATE_PARTITION_OPT_VAL)\r\n    mapAsScalaMap(defaultsMap)\r\n  }\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444710581/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444730242","html_url":"https://github.com/apache/hudi/issues/524#issuecomment-444730242","issue_url":"https://api.github.com/repos/apache/hudi/issues/524","id":444730242,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDczMDI0Mg==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T02:53:59Z","updated_at":"2018-12-06T02:55:21Z","author_association":"CONTRIBUTOR","body":"@bvaradar \r\nhoodie-cli\r\n```\r\nbash-4.1$ ./hoodie-cli.sh\r\nclient jar location not set\r\n18/12/06 10:46:23 INFO xml.XmlBeanDefinitionReader: Loading XML bean definitions from URL [jar:file:/home/hdfs/hudi/hoodie-cli/target/hoodie-cli-0.4.5-SNAPSHOT.jar!/META-INF/spring/spring-shell-plugin.xml]\r\n18/12/06 10:46:24 INFO support.GenericApplicationContext: Refreshing org.springframework.context.support.GenericApplicationContext@26be92ad: startup date [Thu Dec 06 10:46:24 CST 2018]; root of context hierarchy\r\n18/12/06 10:46:24 INFO annotation.AutowiredAnnotationBeanPostProcessor: JSR-330 'javax.inject.Inject' annotation found and supported for autowiring\r\nread history file failed. Reason:hoodie-cmd.log (没有那个文件或目录)\r\n============================================\r\n*                                          *\r\n*     _    _                 _ _           *\r\n*    | |  | |               | (_)          *\r\n*    | |__| | ___   ___   __| |_  ___      *\r\n*    |  __  |/ _ \\ / _ \\ / _` | |/ _ \\     *\r\n*    | |  | | (_) | (_) | (_| | |  __/     *\r\n*    |_|  |_|\\___/ \\___/ \\__,_|_|\\___|     *\r\n*                                          *\r\n============================================\r\n\r\nWelcome to Hoodie CLI. Please type help if you are looking for help.\r\nhoodie->\r\nhoodie->connect --path /tmp/cloud2_code/c_upload_code\r\n18/12/06 10:47:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n18/12/06 10:47:46 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from /tmp/cloud2_code/c_upload_code\r\n18/12/06 10:47:46 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://hdp-neunn-cluster/], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1932170904_11, ugi=hdfs (auth:SIMPLE)]]]\r\n18/12/06 10:47:46 INFO table.HoodieTableConfig: Loading dataset properties from /tmp/cloud2_code/c_upload_code/.hoodie/hoodie.properties\r\n18/12/06 10:47:46 INFO table.HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from /tmp/cloud2_code/c_upload_code\r\nMetadata for table c_upload_code loaded\r\nhoodie:c_upload_code->commits show\r\n18/12/06 10:48:05 INFO timeline.HoodieActiveTimeline: Loaded instants [[20181205165142__clean__COMPLETED], [20181205165142__commit__COMPLETED], [20181205165606__clean__COMPLETED], [20181205165606__commit__COMPLETED]]\r\n    __________________________________________________________________________________________________________________________________________________________________________\r\n    | CommitTime    | Total Bytes Written| Total Files Added| Total Files Updated| Total Partitions Written| Total Records Written| Total Update Records Written| Total Errors|\r\n    |=========================================================================================================================================================================|\r\n    | 20181205165606| 424.8 KB           | 1                | 0                  | 1                       | 1                    | 0                           | 0           |\r\n    | 20181205165142| 627.5 MB           | 1500             | 0                  | 1                       | 100000               | 0                           | 0           |\r\n\r\n```\r\nfile list\r\n```\r\n[root@data1 bin]# ./hadoop fs -ls /tmp/cloud2_code/c_upload_code/*\r\nFound 8 items\r\ndrwxr-xr-x   - hdfs hdfs          0 2018-12-05 16:51 /tmp/cloud2_code/c_upload_code/.hoodie/.aux\r\ndrwxr-xr-x   - hdfs hdfs          0 2018-12-05 16:51 /tmp/cloud2_code/c_upload_code/.hoodie/.temp\r\n-rw-r--r--   3 hdfs hdfs       1098 2018-12-05 16:52 /tmp/cloud2_code/c_upload_code/.hoodie/20181205165142.clean\r\n-rw-r--r--   3 hdfs hdfs    2062452 2018-12-05 16:52 /tmp/cloud2_code/c_upload_code/.hoodie/20181205165142.commit\r\n-rw-r--r--   3 hdfs hdfs       1098 2018-12-05 16:56 /tmp/cloud2_code/c_upload_code/.hoodie/20181205165606.clean\r\n-rw-r--r--   3 hdfs hdfs     899035 2018-12-05 16:56 /tmp/cloud2_code/c_upload_code/.hoodie/20181205165606.commit\r\ndrwxr-xr-x   - hdfs hdfs          0 2018-12-05 16:51 /tmp/cloud2_code/c_upload_code/.hoodie/archived\r\n-rw-r--r--   3 hdfs hdfs        178 2018-12-05 16:51 /tmp/cloud2_code/c_upload_code/.hoodie/hoodie.properties\r\nFound 1 items\r\ndrwxr-xr-x   - hdfs hdfs          0 2018-12-05 16:52 /tmp/cloud2_code/c_upload_code/2018/12\r\n```\r\n```\r\n[root@data1 bin]# ./hadoop fs -ls /tmp/cloud2_code/c_upload_code/2018/12/05\r\n-rw-r--r--   3 hdfs hdfs     438557 2018-12-05 16:52 /tmp/cloud2_code/c_upload_code/2018/12/05/ffb61fde-902f-4aa8-b689-4ddf6aa7a4af_37_20181205165142.parquet\r\n-rw-r--r--   3 hdfs hdfs     438724 2018-12-05 16:52 /tmp/cloud2_code/c_upload_code/2018/12/05/ffb834f1-6ee6-4588-bc76-7de3ab8cd093_1419_20181205165142.parquet\r\n-rw-r--r--   3 hdfs hdfs     438463 2018-12-05 16:52 /tmp/cloud2_code/c_upload_code/2018/12/05/ffbe9a3f-478f-4bc1-b3ab-0bdcbc957886_1442_20181205165142.parquet\r\n-rw-r--r--   3 hdfs hdfs     438911 2018-12-05 16:52 /tmp/cloud2_code/c_upload_code/2018/12/05/ffe3c4a1-d775-4b46-95f1-882c456e9fbc_1471_20181205165142.parquet\r\n-rw-r--r--   3 hdfs hdfs     438536 2018-12-05 16:52 /tmp/cloud2_code/c_upload_code/2018/12/05/ffe9d2dd-92f3-4697-9f4d-355b7389a87f_1394_20181205165142.parquet\r\n...\r\n...\r\n```\r\ncreate table\r\n```\r\nCREATE EXTERNAL TABLE c_upload_code(`_row_key`  string,\r\n`upload_id` int,\r\n`barcode` string,\r\n`extend_deal_date` string,\r\n`extend_deal_type` string,\r\n`extend_dispatch_uuid` string)\r\nPARTITIONED BY (`partition` string)\r\nROW FORMAT SERDE\r\n   'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\r\nSTORED AS INPUTFORMAT\r\n   'com.uber.hoodie.hadoop.HoodieInputFormat'\r\nOUTPUTFORMAT\r\n   'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'\r\nLOCATION\r\n   'hdfs:///tmp/cloud2_code/c_upload_code';\r\nALTER TABLE `c_upload_code` ADD IF NOT EXISTS PARTITION (`partition`='2018/12/05') LOCATION 'hdfs:///tmp/cloud2_code/c_upload_code/2018/12/05';\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444730242/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444741002","html_url":"https://github.com/apache/hudi/issues/517#issuecomment-444741002","issue_url":"https://api.github.com/repos/apache/hudi/issues/517","id":444741002,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDc0MTAwMg==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T04:02:27Z","updated_at":"2018-12-19T08:17:06Z","author_association":"CONTRIBUTOR","body":"@vinothchandar \r\nFind another problem. I replaced avro 1.7.7 to 1.8.2 in spark 2.2.1 and hoodie 0.45, And timestamp can be translated successfully. \r\nBut I found that avro translate timestamp to long(int64) in parquet, not parquet timestamp int96.\r\nI am working on it to find out how to solve this problem. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444741002/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444768442","html_url":"https://github.com/apache/hudi/issues/523#issuecomment-444768442","issue_url":"https://api.github.com/repos/apache/hudi/issues/523","id":444768442,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDc2ODQ0Mg==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T06:57:21Z","updated_at":"2018-12-06T06:57:21Z","author_association":"CONTRIBUTOR","body":"@louisliu318 One way to do this is as follows : \r\n\r\n1. Create a Spark KafkaRDD from the Kafka data. Filter this RDD into multiple different RDD's by grouping change logs of each table in an RDD. You could go over the RDD once and write logic to split that RDD into multiple RDD's per table.\r\n2. For each RDD, generate HoodieRecords, create a new writeClient with a basePath for the corresponding table for that RDD and use Hudi to write out your data to different tables.\r\n```\r\nRDD<GenericRecord> allTables = kafkaSpark.readFromKafkaAsRDD()...\r\nRDD<GenericRecord> table1RDD = allTables.filter(r -> r.contains(\"table1\"));\r\nRDD<GenericRecord> table2RDD = allTables.filter(r -> r.contains(\"table2\")).\r\n..\r\n..\r\n```\r\nAt this point you could convert each RDD to a DF and use Hudi as follows : \r\n\r\n`table1DF = convertToDF(table1RDD);`\r\n\r\n`table1DF.write\r\n      .format(\"com.uber.hoodie\")\r\n      .option(DataSourceWriteOptions.STORAGE_TYPE_OPT_KEY, \r\n      HoodieTableType.COPY_ON_WRITE.name()) \r\n      .option(DataSourceWriteOptions.OPERATION_OPT_KEY, \r\n       DataSourceWriteOptions.BULK_INSERT_OPERATION_OPT_VAL)\r\n      .option(DataSourceWriteOptions.RECORDKEY_FIELD_OPT_KEY, \"somekey\")\r\n      .option(DataSourceWriteOptions.PARTITIONPATH_FIELD_OPT_KEY, \"someparition\")\r\n      .option(DataSourceWriteOptions.PRECOMBINE_FIELD_OPT_KEY, \"somefield\")\r\n      .option(HoodieWriteConfig.TABLE_NAME, \"table1\")\r\n      .mode(SaveMode.Overwrite)\r\n      .save(\"somepath\")`\r\n\r\n.. then for table2 and so on...\r\n...\r\n\r\nAnother way is to create different spark applications for each table and then filter out the records for each table in that Spark Application and perform the insert/upsert using HUDI. Basically, break up the above approach of multiple RDD writing into individual spark jobs. \r\n\r\nAlternatively, if you don't want to use DF, you can always use the JAVA api by using HoodieWriteClient.\r\nHope this helps.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444768442/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444768994","html_url":"https://github.com/apache/hudi/issues/498#issuecomment-444768994","issue_url":"https://api.github.com/repos/apache/hudi/issues/498","id":444768994,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDc2ODk5NA==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T07:00:10Z","updated_at":"2018-12-06T07:00:10Z","author_association":"CONTRIBUTOR","body":"@louisliu318 What's your use case to use deletes in MERGE_ON_READ ? We have a couple of things that needs to be addressed to support true deletes in a MOR table. We will try to fix the deletes in MERGE_ON_READ soon.\r\nIn the meantime, can you try to setup your pipeline and operations using COPY_ON_WRITE table that support deletes right now ? This way you can play around and have some tables running using Hudi.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444768994/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444796578","html_url":"https://github.com/apache/hudi/issues/523#issuecomment-444796578","issue_url":"https://api.github.com/repos/apache/hudi/issues/523","id":444796578,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDc5NjU3OA==","user":{"login":"louisliu318","id":9956334,"node_id":"MDQ6VXNlcjk5NTYzMzQ=","avatar_url":"https://avatars.githubusercontent.com/u/9956334?v=4","gravatar_id":"","url":"https://api.github.com/users/louisliu318","html_url":"https://github.com/louisliu318","followers_url":"https://api.github.com/users/louisliu318/followers","following_url":"https://api.github.com/users/louisliu318/following{/other_user}","gists_url":"https://api.github.com/users/louisliu318/gists{/gist_id}","starred_url":"https://api.github.com/users/louisliu318/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/louisliu318/subscriptions","organizations_url":"https://api.github.com/users/louisliu318/orgs","repos_url":"https://api.github.com/users/louisliu318/repos","events_url":"https://api.github.com/users/louisliu318/events{/privacy}","received_events_url":"https://api.github.com/users/louisliu318/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T08:56:14Z","updated_at":"2018-12-06T08:56:14Z","author_association":"NONE","body":"@n3nash Thanks for your reply.  Right now we are doing it similar to the first way, but one spark streaming application corresponding a few tables(eg ten) to avoid too many rdds in one application.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444796578/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444802867","html_url":"https://github.com/apache/hudi/issues/498#issuecomment-444802867","issue_url":"https://api.github.com/repos/apache/hudi/issues/498","id":444802867,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDgwMjg2Nw==","user":{"login":"louisliu318","id":9956334,"node_id":"MDQ6VXNlcjk5NTYzMzQ=","avatar_url":"https://avatars.githubusercontent.com/u/9956334?v=4","gravatar_id":"","url":"https://api.github.com/users/louisliu318","html_url":"https://github.com/louisliu318","followers_url":"https://api.github.com/users/louisliu318/followers","following_url":"https://api.github.com/users/louisliu318/following{/other_user}","gists_url":"https://api.github.com/users/louisliu318/gists{/gist_id}","starred_url":"https://api.github.com/users/louisliu318/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/louisliu318/subscriptions","organizations_url":"https://api.github.com/users/louisliu318/orgs","repos_url":"https://api.github.com/users/louisliu318/repos","events_url":"https://api.github.com/users/louisliu318/events{/privacy}","received_events_url":"https://api.github.com/users/louisliu318/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T09:15:01Z","updated_at":"2018-12-06T09:15:45Z","author_association":"NONE","body":"@n3nash We want to mirror our production MySql databases to hive in 5 to30 mins latency.  Right now we are doing with COPY_ON_WRITE table with spark streaming.\r\n\r\nOne more question，Is it suitable if I use spark **structured streaming** with 2mins duration on COPY_ON_WRITE table. Thanks~","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444802867/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444842985","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-444842985","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":444842985,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NDg0Mjk4NQ==","user":{"login":"smalltimecharlie","id":17934736,"node_id":"MDQ6VXNlcjE3OTM0NzM2","avatar_url":"https://avatars.githubusercontent.com/u/17934736?v=4","gravatar_id":"","url":"https://api.github.com/users/smalltimecharlie","html_url":"https://github.com/smalltimecharlie","followers_url":"https://api.github.com/users/smalltimecharlie/followers","following_url":"https://api.github.com/users/smalltimecharlie/following{/other_user}","gists_url":"https://api.github.com/users/smalltimecharlie/gists{/gist_id}","starred_url":"https://api.github.com/users/smalltimecharlie/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smalltimecharlie/subscriptions","organizations_url":"https://api.github.com/users/smalltimecharlie/orgs","repos_url":"https://api.github.com/users/smalltimecharlie/repos","events_url":"https://api.github.com/users/smalltimecharlie/events{/privacy}","received_events_url":"https://api.github.com/users/smalltimecharlie/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T11:37:35Z","updated_at":"2018-12-06T11:37:35Z","author_association":"NONE","body":"Hi, \r\n\r\nplease add smalltimecharlie@gmail.com :) \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/444842985/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445071892","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-445071892","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":445071892,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTA3MTg5Mg==","user":{"login":"AndrewKL","id":2659018,"node_id":"MDQ6VXNlcjI2NTkwMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2659018?v=4","gravatar_id":"","url":"https://api.github.com/users/AndrewKL","html_url":"https://github.com/AndrewKL","followers_url":"https://api.github.com/users/AndrewKL/followers","following_url":"https://api.github.com/users/AndrewKL/following{/other_user}","gists_url":"https://api.github.com/users/AndrewKL/gists{/gist_id}","starred_url":"https://api.github.com/users/AndrewKL/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AndrewKL/subscriptions","organizations_url":"https://api.github.com/users/AndrewKL/orgs","repos_url":"https://api.github.com/users/AndrewKL/repos","events_url":"https://api.github.com/users/AndrewKL/events{/privacy}","received_events_url":"https://api.github.com/users/AndrewKL/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T23:47:04Z","updated_at":"2018-12-06T23:47:59Z","author_association":"NONE","body":"So after a bit of hard work I've got it working e2e!  Now to test it with production levels of data.\r\n\r\n```\r\nscala> sql(\"SELECT * FROM ap_invoices_all_manual\").count\r\nres5: Long = 6506722                                                            \r\n\r\n[hadoop@ip-172-31-21-76 hudi]$ ./hoodie-cli/hoodie-cli.sh \r\n...\r\n============================================\r\n*                                          *\r\n*     _    _                 _ _           *\r\n*    | |  | |               | (_)          *\r\n*    | |__| | ___   ___   __| |_  ___      *\r\n*    |  __  |/ _ \\ / _ \\ / _` | |/ _ \\     *\r\n*    | |  | | (_) | (_) | (_| | |  __/     *\r\n*    |_|  |_|\\___/ \\___/ \\__,_|_|\\___|     *\r\n*                                          *\r\n============================================\r\n\r\nWelcome to Hoodie CLI. Please type help if you are looking for help. \r\nhoodie->connect --path hdfs:///hudi-data/ap-invoices-all/\r\n18/12/06 23:45:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n18/12/06 23:45:29 INFO table.HoodieTableMetaClient: Loading HoodieTableMetaClient from hdfs:///hudi-data/ap-invoices-all/\r\n18/12/06 23:45:29 INFO util.FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-172-31-21-76.ec2.internal:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml], FileSystem: [DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-519572413_23, ugi=hadoop (auth:SIMPLE)]]]\r\n18/12/06 23:45:29 INFO table.HoodieTableConfig: Loading dataset properties from hdfs:/hudi-data/ap-invoices-all/.hoodie/hoodie.properties\r\n18/12/06 23:45:29 INFO table.HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from hdfs:///hudi-data/ap-invoices-all/\r\nMetadata for table ap_invoices_all_manual loaded\r\nhoodie:ap_invoices_all_manual->commits show\r\n18/12/06 23:45:32 INFO timeline.HoodieActiveTimeline: Loaded instants [[20181206222337__clean__COMPLETED], [20181206222337__commit__COMPLETED], [20181206224735__clean__COMPLETED], [20181206224735__commit__COMPLETED], [20181206231804__clean__COMPLETED], [20181206231804__commit__COMPLETED], [20181206233158__clean__COMPLETED], [20181206233158__commit__COMPLETED], [==>20181206234530__commit__INFLIGHT]]\r\n    __________________________________________________________________________________________________________________________________________________________________________\r\n    | CommitTime    | Total Bytes Written| Total Files Added| Total Files Updated| Total Partitions Written| Total Records Written| Total Update Records Written| Total Errors|\r\n    |=========================================================================================================================================================================|\r\n    | 20181206233158| 633.1 MB           | 1                | 17                 | 1                       | 6506722              | 130315                      | 0           |\r\n    | 20181206231804| 588.6 MB           | 1                | 16                 | 1                       | 6022267              | 45749                       | 0           |\r\n    | 20181206224735| 502.9 MB           | 8                | 8                  | 1                       | 5139033              | 944674                      | 0           |\r\n    | 20181206222337| 100.7 MB           | 8                | 0                  | 1                       | 944674               | 0                           | 0           |\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445071892/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445073706","html_url":"https://github.com/apache/hudi/pull/515#issuecomment-445073706","issue_url":"https://api.github.com/repos/apache/hudi/issues/515","id":445073706,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTA3MzcwNg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-06T23:56:09Z","updated_at":"2018-12-06T23:56:09Z","author_association":"CONTRIBUTOR","body":"> out of curiosity would it be possible to autoset these options? If so I'd love to give a go at creating a commit.\r\n> \r\n> ex:\r\n> \r\n> ```\r\n>   /**\r\n>     * Add default options for unspecified write options keys.\r\n>     *\r\n>     * @param parameters\r\n>     * @return\r\n>     */\r\n>   def parametersWithWriteDefaults(parameters: Map[String, String]) = {\r\n>     val defaultsMap = new ConcurrentHashMap[String, String](mapAsJavaMap(parameters))\r\n>     defaultsMap.putIfAbsent(OPERATION_OPT_KEY, DEFAULT_OPERATION_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(STORAGE_TYPE_OPT_KEY, DEFAULT_STORAGE_TYPE_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(PRECOMBINE_FIELD_OPT_KEY, DEFAULT_PRECOMBINE_FIELD_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(PAYLOAD_CLASS_OPT_KEY, DEFAULT_PAYLOAD_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(RECORDKEY_FIELD_OPT_KEY, DEFAULT_RECORDKEY_FIELD_OPT_VAL)\r\n>     //TODO set PARTITIONPATH_FIELD_OPT_KEY automatically based on if the table is partition\r\n>     defaultsMap.putIfAbsent(PARTITIONPATH_FIELD_OPT_KEY, DEFAULT_PARTITIONPATH_FIELD_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(KEYGENERATOR_CLASS_OPT_KEY, DEFAULT_KEYGENERATOR_CLASS_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(COMMIT_METADATA_KEYPREFIX_OPT_KEY, DEFAULT_COMMIT_METADATA_KEYPREFIX_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(INSERT_DROP_DUPS_OPT_KEY, DEFAULT_INSERT_DROP_DUPS_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(HIVE_SYNC_ENABLED_OPT_KEY, DEFAULT_HIVE_SYNC_ENABLED_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(HIVE_DATABASE_OPT_KEY, DEFAULT_HIVE_DATABASE_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(HIVE_TABLE_OPT_KEY, DEFAULT_HIVE_TABLE_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(HIVE_USER_OPT_KEY, DEFAULT_HIVE_USER_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(HIVE_PASS_OPT_KEY, DEFAULT_HIVE_PASS_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(HIVE_URL_OPT_KEY, DEFAULT_HIVE_URL_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(HIVE_PARTITION_FIELDS_OPT_KEY, DEFAULT_HIVE_PARTITION_FIELDS_OPT_VAL)\r\n>     //TODO auto set the HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY based on if the table is partition\r\n>     defaultsMap.putIfAbsent(HIVE_PARTITION_EXTRACTOR_CLASS_OPT_KEY, DEFAULT_HIVE_PARTITION_EXTRACTOR_CLASS_OPT_VAL)\r\n>     defaultsMap.putIfAbsent(HIVE_ASSUME_DATE_PARTITION_OPT_KEY, DEFAULT_HIVE_ASSUME_DATE_PARTITION_OPT_VAL)\r\n>     mapAsScalaMap(defaultsMap)\r\n>   }\r\n> ```\r\n\r\n@AndrewKL :  In addition to partitionValueExtractor and KeyGenerator auto detection - I would love to see how we can read hive connectivity configs (Hive URL, database, username and password) directly from hive-site.xml. This way, we can auto enable Hive sync and by default, have hive table name be same as hoodie table name. I **haven't tried finding a way on how to get these configs yet**  but I am guessing Spark is already doing it as part of its integration. So, we can look at Spark source  ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445073706/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445096424","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-445096424","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":445096424,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTA5NjQyNA==","user":{"login":"louisliu318","id":9956334,"node_id":"MDQ6VXNlcjk5NTYzMzQ=","avatar_url":"https://avatars.githubusercontent.com/u/9956334?v=4","gravatar_id":"","url":"https://api.github.com/users/louisliu318","html_url":"https://github.com/louisliu318","followers_url":"https://api.github.com/users/louisliu318/followers","following_url":"https://api.github.com/users/louisliu318/following{/other_user}","gists_url":"https://api.github.com/users/louisliu318/gists{/gist_id}","starred_url":"https://api.github.com/users/louisliu318/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/louisliu318/subscriptions","organizations_url":"https://api.github.com/users/louisliu318/orgs","repos_url":"https://api.github.com/users/louisliu318/repos","events_url":"https://api.github.com/users/louisliu318/events{/privacy}","received_events_url":"https://api.github.com/users/louisliu318/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-07T01:59:39Z","updated_at":"2018-12-07T01:59:39Z","author_association":"NONE","body":"@vinothchandar  Hi, Can you please add me into the hudi slack channel? My email is louisliu318@gmail.com","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445096424/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445347545","html_url":"https://github.com/apache/hudi/issues/525#issuecomment-445347545","issue_url":"https://api.github.com/repos/apache/hudi/issues/525","id":445347545,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTM0NzU0NQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-07T19:56:09Z","updated_at":"2018-12-07T19:56:09Z","author_association":"CONTRIBUTOR","body":"@smalltimecharlie  :  Thanks for filing the issue. Will look into this today  and get back to you.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445347545/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445385612","html_url":"https://github.com/apache/hudi/issues/525#issuecomment-445385612","issue_url":"https://api.github.com/repos/apache/hudi/issues/525","id":445385612,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTM4NTYxMg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-07T22:29:43Z","updated_at":"2018-12-07T22:29:43Z","author_association":"CONTRIBUTOR","body":"\r\nI am able to run it successfully.\r\n\r\nvaradarb-C02SH0P1G8WL:docker varadarb$ ./setup_demo.sh\r\n....\r\nCreating hive-metastore-postgresql\r\nCreating zookeeper\r\nCreating namenode ... done\r\nCreating hivemetastore ...\r\nCreating historyserver ...\r\nCreating historyserver\r\nCreating historyserver ... done\r\nCreating datanode1 ...\r\nCreating hivemetastore ... done\r\nCreating hiveserver ...\r\nCreating hiveserver ... done\r\nCreating sparkmaster ...\r\nCreating sparkmaster ... done\r\nCreating adhoc-2 ...\r\nCreating spark-worker-1 ...\r\nCreating adhoc-1 ...\r\nCreating adhoc-1\r\nCreating adhoc-2\r\nCreating adhoc-1 ... done\r\nCopying spark default config and setting up configs\r\nCopying spark default config and setting up configs\r\nvaradarb-C02SH0P1G8WL:docker varadarb$ echo $?\r\n0\r\n\r\nvaradarb-C02SH0P1G8WL:docker varadarb$  docker exec -it adhoc-1 ls /var/hoodie/ws/docker/build_local_docker_images.sh\r\n\r\nDocker Version : \r\n\r\nvaradarb-C02SH0P1G8WL:docker varadarb$ docker -v\r\nDocker version 17.09.1-ce, build 19e2cf6\r\n\r\nDocker Compose Version:\r\n\r\nvaradarb-C02SH0P1G8WL:docker varadarb$ docker-compose -v\r\ndocker-compose version 1.17.1, build 6d101fb\r\n\r\n\r\n@smalltimecharlie :  \r\nCan you please check if the docker compose versions are same or later than the one I posted. I have seen older versions not parsing the compose file correctly. \r\n\r\nIf you look at the compose configurations,  (docker/compose/docker-compose_hadoop284_hive233_spark231.yml), you can find that we are mounting the workspace to the docker container. The docker image itself does not have this directory. \r\n\r\nThe environment variable \"HUDI_WS\" is setup to your Hoodie workspace automatically by setup_demo.sh. In your case, I think it is not setup correctly causing the mount to fail.\r\n\r\nCouple of scenarios that I can think of\r\n\r\n1. You may not be running the demo script in the expected way. The script works when you \r\n     (a) cd docker\r\n      (b) and run ./setup_demo.sh\r\n\r\n2. The docker directory is somehow not in the Hoodie workspace directory in your machine (possibly moved ?)\r\n\r\nCan you check along these lines and see if you can resolve it ?\r\n\r\nBalaji.V\r\n\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445385612/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445393716","html_url":"https://github.com/apache/hudi/issues/526#issuecomment-445393716","issue_url":"https://api.github.com/repos/apache/hudi/issues/526","id":445393716,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTM5MzcxNg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-07T23:11:23Z","updated_at":"2018-12-07T23:11:23Z","author_association":"CONTRIBUTOR","body":"@cdmikechen : Looking at the other issue, the dataset is configured to be COPY_ON_WRITE. In this mode, all updates and inserts are ingested in columnar (parquet) format. Hence, you are not seeing any log files. \r\n\r\nYou can look at the table type in Hoodie CLI  by running the following commands:\r\n\r\nconnect --path <base-path>\r\ndesc\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445393716/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445413895","html_url":"https://github.com/apache/hudi/issues/498#issuecomment-445413895","issue_url":"https://api.github.com/repos/apache/hudi/issues/498","id":445413895,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTQxMzg5NQ==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-08T00:47:26Z","updated_at":"2018-12-08T00:47:26Z","author_association":"CONTRIBUTOR","body":"@louisliu318 That depends on a few factors : \r\n\r\n1. What is the parallelism you are using to ingest that workload ?\r\n2. What is the output file size (parquet file size) that you intend to write ?\r\n3. Is a large number of small files a concern more or just being able to ingest in 2 mins ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445413895/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445434191","html_url":"https://github.com/apache/hudi/issues/525#issuecomment-445434191","issue_url":"https://api.github.com/repos/apache/hudi/issues/525","id":445434191,"node_id":"MDEyOklzc3VlQ29tbWVudDQ0NTQzNDE5MQ==","user":{"login":"smalltimecharlie","id":17934736,"node_id":"MDQ6VXNlcjE3OTM0NzM2","avatar_url":"https://avatars.githubusercontent.com/u/17934736?v=4","gravatar_id":"","url":"https://api.github.com/users/smalltimecharlie","html_url":"https://github.com/smalltimecharlie","followers_url":"https://api.github.com/users/smalltimecharlie/followers","following_url":"https://api.github.com/users/smalltimecharlie/following{/other_user}","gists_url":"https://api.github.com/users/smalltimecharlie/gists{/gist_id}","starred_url":"https://api.github.com/users/smalltimecharlie/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smalltimecharlie/subscriptions","organizations_url":"https://api.github.com/users/smalltimecharlie/orgs","repos_url":"https://api.github.com/users/smalltimecharlie/repos","events_url":"https://api.github.com/users/smalltimecharlie/events{/privacy}","received_events_url":"https://api.github.com/users/smalltimecharlie/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2018-12-08T05:58:38Z","updated_at":"2018-12-08T05:58:54Z","author_association":"NONE","body":"Hi Balaj, \r\n\r\nThanks for looking into this for me, I did notice that environment variable and figured it might well be that not being set properly for whatever reason. \r\n\r\nChecking my docker versions I do get the same or later as yourself\r\n\r\n```\r\nubuntu@GBC****:~/hudi/docker$ docker-compose -v\r\ndocker-compose version 1.17.1, build unknown\r\nubuntu@GBC****:~/hudi/docker$ docker -v\r\nDocker version 18.09.0, build 4d60db4\r\n```\r\n\r\nI can confirm that I'm running the script exactly as you describe and that the docker directory has not been moved and is under /home/ubuntu/hudi\r\n\r\nOne slight complication might be that I'm not running this on an actual ubuntu machine/vm but on the windows subsystem for linux, have you see anyone try that before/see any issues with that? \r\n\r\nThanks\r\nCharlie","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/445434191/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]