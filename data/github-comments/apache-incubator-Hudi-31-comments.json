[{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541153983","html_url":"https://github.com/apache/hudi/pull/951#issuecomment-541153983","issue_url":"https://api.github.com/repos/apache/hudi/issues/951","id":541153983,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTE1Mzk4Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-11T17:29:12Z","updated_at":"2019-10-11T17:29:12Z","author_association":"MEMBER","body":"man, its weird :) .. Passes locally multiple times.. Will keep digging. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541153983/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541204842","html_url":"https://github.com/apache/hudi/pull/915#issuecomment-541204842","issue_url":"https://api.github.com/repos/apache/hudi/issues/915","id":541204842,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTIwNDg0Mg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-11T20:02:15Z","updated_at":"2019-10-11T20:02:15Z","author_association":"CONTRIBUTOR","body":"@umehrot2 : Trying to understand why this was closed. Is this no longer neeeded ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541204842/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541209562","html_url":"https://github.com/apache/hudi/pull/915#issuecomment-541209562","issue_url":"https://api.github.com/repos/apache/hudi/issues/915","id":541209562,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTIwOTU2Mg==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-11T20:17:53Z","updated_at":"2019-10-11T20:17:53Z","author_association":"CONTRIBUTOR","body":"> @umehrot2 : Trying to understand why this was closed. Is this no longer needed ?\r\n\r\n@bvaradar It was by accident. I saw that there were merge conflicts so I thought I would delete my branch and fix the conflicts and push again. But that closed this PR instead, and now the Reopen functionality does not seem to work either. I might have to create a new PR :(\r\n\r\nBTW your comments make sense, and I will update them. \r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541209562/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541715429","html_url":"https://github.com/apache/hudi/pull/953#issuecomment-541715429","issue_url":"https://api.github.com/repos/apache/hudi/issues/953","id":541715429,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTcxNTQyOQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-14T14:34:13Z","updated_at":"2019-10-14T14:34:13Z","author_association":"MEMBER","body":"Pasting in the -1 for context \r\n\r\nSorry but it’s -1 (binding) due to license issues\r\n\r\nI checked:\r\n- incubating in name\r\n- signature and hash fine\r\n- DISCLAIMER exist (but you might want to consider using the work in progress disclaimer)\r\n- LICENSE file is missing the appendix\r\n- NOTICE is fine\r\n- No unexpected binary file\r\n- Source files have ASF headers\r\n- Can compile from source\r\n\r\nThis file [1] probably incorrectly has an ASF header on it, you may also want to mention this file in LICENSE. It unclear what the license is of the code contained in this file [2] are you sure you have permission to use this and it’s under a compatible license? The code in here from stack overflow don’t have a compatible license [3], see [4] (and [5]).\r\n\r\n\r\n\r\n1. ./hudi-0.5.0-incubating-rc5/hudi-common/src/main/java/org/apache/hudi/common/util/ObjectSizeCalculator.java\r\n2.  ./hudi-0.5.0-incubating-rc5/hudi-common/src/main/java/org/apache/hudi/avro/MercifulJsonConverter.java\r\n3. ./hudi-0.5.0-incubating-rc5/hudi-utilities/src/test/java/org/apache/hudi/utilities/TestHoodieDeltaStreamer.java\r\n4. https://www.apache.org/legal/resolved.html#stackoverflow\r\n5. https://issues.apache.org/jira/browse/LEGAL-471","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541715429/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541770961","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-541770961","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":541770961,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTc3MDk2MQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-14T16:10:38Z","updated_at":"2019-10-14T16:10:38Z","author_association":"MEMBER","body":"@gfn9cho Is this the first write? Looks like it cannot find the registered table. could you share the entire log with other statements also?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541770961/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541780771","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-541780771","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":541780771,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTc4MDc3MQ==","user":{"login":"gfn9cho","id":14010421,"node_id":"MDQ6VXNlcjE0MDEwNDIx","avatar_url":"https://avatars.githubusercontent.com/u/14010421?v=4","gravatar_id":"","url":"https://api.github.com/users/gfn9cho","html_url":"https://github.com/gfn9cho","followers_url":"https://api.github.com/users/gfn9cho/followers","following_url":"https://api.github.com/users/gfn9cho/following{/other_user}","gists_url":"https://api.github.com/users/gfn9cho/gists{/gist_id}","starred_url":"https://api.github.com/users/gfn9cho/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gfn9cho/subscriptions","organizations_url":"https://api.github.com/users/gfn9cho/orgs","repos_url":"https://api.github.com/users/gfn9cho/repos","events_url":"https://api.github.com/users/gfn9cho/events{/privacy}","received_events_url":"https://api.github.com/users/gfn9cho/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-14T16:29:29Z","updated_at":"2019-10-14T16:29:29Z","author_association":"NONE","body":"Yes, this is the first write.\r\nIts creating the hoodie table and I could see the data in S3.\r\nWhen it comes to Hive, its creating the table, but failed to sync the data with the above error.\r\nActually that was the entire log in the spark shell. Below is the one item I missed,\r\n19/10/14 01:30:23 WARN HiveConf: HiveConf of name hive.metastore.client.factory.class does not exist\r\n\r\nI could do spark.sql(\"select * from <hivedb.table>\") successfully with no data in it.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541780771/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541984844","html_url":"https://github.com/apache/hudi/pull/952#issuecomment-541984844","issue_url":"https://api.github.com/repos/apache/hudi/issues/952","id":541984844,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTk4NDg0NA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T00:29:37Z","updated_at":"2019-10-15T00:29:37Z","author_association":"MEMBER","body":"@zhedoubushishi  let us know if you need pointers on what tests to model after etc?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541984844/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541985474","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-541985474","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":541985474,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTk4NTQ3NA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T00:33:18Z","updated_at":"2019-10-15T00:33:18Z","author_association":"MEMBER","body":"There are definitely other lines that are not getting printed.. Have you tried doing a `sc.setLogLevel(\"INFO\") ` to also get the INFO statements.. usually this print the exact SQL being run and we can spot something","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541985474/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541987294","html_url":"https://github.com/apache/hudi/pull/952#issuecomment-541987294","issue_url":"https://api.github.com/repos/apache/hudi/issues/952","id":541987294,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MTk4NzI5NA==","user":{"login":"zhedoubushishi","id":31263084,"node_id":"MDQ6VXNlcjMxMjYzMDg0","avatar_url":"https://avatars.githubusercontent.com/u/31263084?v=4","gravatar_id":"","url":"https://api.github.com/users/zhedoubushishi","html_url":"https://github.com/zhedoubushishi","followers_url":"https://api.github.com/users/zhedoubushishi/followers","following_url":"https://api.github.com/users/zhedoubushishi/following{/other_user}","gists_url":"https://api.github.com/users/zhedoubushishi/gists{/gist_id}","starred_url":"https://api.github.com/users/zhedoubushishi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhedoubushishi/subscriptions","organizations_url":"https://api.github.com/users/zhedoubushishi/orgs","repos_url":"https://api.github.com/users/zhedoubushishi/repos","events_url":"https://api.github.com/users/zhedoubushishi/events{/privacy}","received_events_url":"https://api.github.com/users/zhedoubushishi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T00:43:48Z","updated_at":"2019-10-15T00:43:48Z","author_association":"CONTRIBUTOR","body":"> @zhedoubushishi let us know if you need pointers on what tests to model after etc?\r\n\r\nThank! I was just considering how to add non-partition test cases, do you have any suggestion?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/541987294/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542017798","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-542017798","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":542017798,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjAxNzc5OA==","user":{"login":"gfn9cho","id":14010421,"node_id":"MDQ6VXNlcjE0MDEwNDIx","avatar_url":"https://avatars.githubusercontent.com/u/14010421?v=4","gravatar_id":"","url":"https://api.github.com/users/gfn9cho","html_url":"https://github.com/gfn9cho","followers_url":"https://api.github.com/users/gfn9cho/followers","following_url":"https://api.github.com/users/gfn9cho/following{/other_user}","gists_url":"https://api.github.com/users/gfn9cho/gists{/gist_id}","starred_url":"https://api.github.com/users/gfn9cho/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gfn9cho/subscriptions","organizations_url":"https://api.github.com/users/gfn9cho/orgs","repos_url":"https://api.github.com/users/gfn9cho/repos","events_url":"https://api.github.com/users/gfn9cho/events{/privacy}","received_events_url":"https://api.github.com/users/gfn9cho/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T03:16:49Z","updated_at":"2019-10-15T16:13:25Z","author_association":"NONE","body":"Thanks Vinoth.\r\nLooks like everytime HiveSync tool is executing the create DDL as it is not able to find the table from the metastore.\r\nHere is the trimmed log from the time it finished writing to hoodie table.\r\n\r\n```\r\n19/10/15 03:05:28 INFO TaskSetManager: Finished task 1485.0 in stage 25.0 (TID 12144) in 16 ms on ip-10-63-115-75.corp.stateauto.com (executor 1) (1498/1500)\r\n19/10/15 03:05:28 INFO TaskSetManager: Finished task 1499.0 in stage 25.0 (TID 12147) in 9 ms on ip-10-63-115-75.corp.stateauto.com (executor 1) (1499/1500)\r\n19/10/15 03:05:28 INFO TaskSetManager: Finished task 1487.0 in stage 25.0 (TID 12146) in 10 ms on ip-10-63-115-75.corp.stateauto.com (executor 1) (1500/1500)\r\n19/10/15 03:05:28 INFO YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \r\n19/10/15 03:05:28 INFO DAGScheduler: ShuffleMapStage 25 (mapToPair at HoodieWriteClient.java:461) finished in 1.448 s\r\n19/10/15 03:05:28 INFO DAGScheduler: looking for newly runnable stages\r\n19/10/15 03:05:28 INFO DAGScheduler: running: Set()\r\n19/10/15 03:05:28 INFO DAGScheduler: waiting: Set(ResultStage 26)\r\n19/10/15 03:05:28 INFO DAGScheduler: failed: Set()\r\n19/10/15 03:05:28 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[55] at filter at HoodieSparkSqlWriter.scala:145), which has no missing parents\r\n19/10/15 03:05:28 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 156.3 KB, free 911.2 MB)\r\n19/10/15 03:05:28 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 58.4 KB, free 911.1 MB)\r\n19/10/15 03:05:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ip-10-63-114-58.corp.stateauto.com:43403 (size: 58.4 KB, free: 912.1 MB)\r\n19/10/15 03:05:28 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1201\r\n19/10/15 03:05:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[55] at filter at HoodieSparkSqlWriter.scala:145) (first 15 tasks are for partitions Vector(0))\r\n19/10/15 03:05:28 INFO YarnScheduler: Adding task set 26.0 with 1 tasks\r\n19/10/15 03:05:28 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 12148, ip-10-63-114-115.corp.stateauto.com, executor 2, partition 0, PROCESS_LOCAL, 7674 bytes)\r\n19/10/15 03:05:28 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ip-10-63-114-115.corp.stateauto.com:36209 (size: 58.4 KB, free: 1458.3 MB)\r\n19/10/15 03:05:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.63.114.115:42194\r\n[Stage 26:>                                                         (0 + 1) / 1]19/10/15 03:05:31 INFO BlockManagerInfo: Added rdd_54_0 in memory on ip-10-63-114-115.corp.stateauto.com:36209 (size: 300.0 B, free: 1458.3 MB)\r\n19/10/15 03:05:31 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 12148) in 2940 ms on ip-10-63-114-115.corp.stateauto.com (executor 2) (1/1)\r\n19/10/15 03:05:31 INFO YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \r\n19/10/15 03:05:31 INFO DAGScheduler: ResultStage 26 (count at HoodieSparkSqlWriter.scala:145) finished in 2.957 s\r\n19/10/15 03:05:31 INFO DAGScheduler: Job 7 finished: count at HoodieSparkSqlWriter.scala:145, took 4.414884 s\r\n19/10/15 03:05:31 INFO HoodieSparkSqlWriter$: No errors. Proceeding to commit the write.\r\n19/10/15 03:05:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:31 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:31 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:31 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:31 INFO HoodieWriteClient: Commiting 20191015030518\r\n19/10/15 03:05:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:31 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:31 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:31 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:31 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:31 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@4145ffec\r\n19/10/15 03:05:31 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY\r\n19/10/15 03:05:31 INFO FileSystemViewManager: Creating in-memory based Table View\r\n19/10/15 03:05:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:31 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:31 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:31 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:31 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:31 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@9d5efdd\r\n19/10/15 03:05:32 INFO SparkContext: Starting job: collect at HoodieWriteClient.java:492\r\n19/10/15 03:05:32 INFO DAGScheduler: Got job 8 (collect at HoodieWriteClient.java:492) with 1 output partitions\r\n19/10/15 03:05:32 INFO DAGScheduler: Final stage: ResultStage 33 (collect at HoodieWriteClient.java:492)\r\n19/10/15 03:05:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\r\n19/10/15 03:05:32 INFO DAGScheduler: Missing parents: List()\r\n19/10/15 03:05:32 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[56] at map at HoodieWriteClient.java:492), which has no missing parents\r\n19/10/15 03:05:32 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 156.5 KB, free 911.4 MB)\r\n19/10/15 03:05:32 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 58.5 KB, free 911.4 MB)\r\n19/10/15 03:05:32 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ip-10-63-114-58.corp.stateauto.com:43403 (size: 58.5 KB, free: 912.1 MB)\r\n19/10/15 03:05:32 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1201\r\n19/10/15 03:05:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[56] at map at HoodieWriteClient.java:492) (first 15 tasks are for partitions Vector(0))\r\n19/10/15 03:05:32 INFO YarnScheduler: Adding task set 33.0 with 1 tasks\r\n19/10/15 03:05:32 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 12149, ip-10-63-114-115.corp.stateauto.com, executor 2, partition 0, PROCESS_LOCAL, 7674 bytes)\r\n19/10/15 03:05:32 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ip-10-63-114-115.corp.stateauto.com:36209 (size: 58.5 KB, free: 1458.4 MB)\r\n19/10/15 03:05:32 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 12149) in 67 ms on ip-10-63-114-115.corp.stateauto.com (executor 2) (1/1)\r\n19/10/15 03:05:32 INFO YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \r\n19/10/15 03:05:32 INFO DAGScheduler: ResultStage 33 (collect at HoodieWriteClient.java:492) finished in 0.086 s\r\n19/10/15 03:05:32 INFO DAGScheduler: Job 8 finished: collect at HoodieWriteClient.java:492, took 0.089019 s\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:32 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:32 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@28576562\r\n19/10/15 03:05:32 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY\r\n19/10/15 03:05:32 INFO FileSystemViewManager: Creating in-memory based Table View\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:32 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:32 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@1c7740da\r\n19/10/15 03:05:32 INFO HoodieTable: Removing marker directory=s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/.temp/20191015030518\r\n19/10/15 03:05:32 INFO HoodieActiveTimeline: Marking instant complete [==>20191015030518__commit__INFLIGHT]\r\n19/10/15 03:05:32 INFO MultipartUploadOutputStream: close closed:false s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.inflight\r\n19/10/15 03:05:32 INFO S3NativeFileSystem: rename s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.inflight s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.commit\r\n19/10/15 03:05:32 INFO HoodieActiveTimeline: Completed [==>20191015030518__commit__INFLIGHT]\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:32 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:32 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@24f2a2b6\r\n19/10/15 03:05:32 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY\r\n19/10/15 03:05:32 INFO FileSystemViewManager: Creating in-memory based Table View\r\n19/10/15 03:05:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:32 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:33 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:33 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:33 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:33 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:33 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@2005e23d\r\n19/10/15 03:05:33 INFO HoodieCommitArchiveLog: No Instants to archive\r\n19/10/15 03:05:33 INFO HoodieWriteClient: Auto cleaning is enabled. Running cleaner now\r\n19/10/15 03:05:33 INFO HoodieWriteClient: Cleaner started\r\n19/10/15 03:05:33 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:33 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:33 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:33 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:33 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:33 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:33 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@2d6fce1c\r\n19/10/15 03:05:33 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY\r\n19/10/15 03:05:33 INFO FileSystemViewManager: Creating in-memory based Table View\r\n19/10/15 03:05:33 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:33 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:33 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:33 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:33 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:33 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:33 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@35b68b7a\r\n19/10/15 03:05:33 INFO HoodieCopyOnWriteTable: Partitions to clean up : [2018-07-01], with policy KEEP_LATEST_COMMITS\r\n19/10/15 03:05:33 INFO HoodieCopyOnWriteTable: Using cleanerParallelism: 1\r\n19/10/15 03:05:33 INFO SparkContext: Starting job: collect at HoodieCopyOnWriteTable.java:396\r\n19/10/15 03:05:33 INFO DAGScheduler: Registering RDD 59 (repartition at HoodieCopyOnWriteTable.java:392)\r\n19/10/15 03:05:33 INFO DAGScheduler: Registering RDD 63 (mapPartitionsToPair at HoodieCopyOnWriteTable.java:393)\r\n19/10/15 03:05:33 INFO DAGScheduler: Got job 9 (collect at HoodieCopyOnWriteTable.java:396) with 1 output partitions\r\n19/10/15 03:05:33 INFO DAGScheduler: Final stage: ResultStage 36 (collect at HoodieCopyOnWriteTable.java:396)\r\n19/10/15 03:05:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)\r\n19/10/15 03:05:33 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 35)\r\n19/10/15 03:05:33 INFO DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[59] at repartition at HoodieCopyOnWriteTable.java:392), which has no missing parents\r\n19/10/15 03:05:33 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 154.0 KB, free 911.2 MB)\r\n19/10/15 03:05:33 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 57.3 KB, free 911.1 MB)\r\n19/10/15 03:05:33 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ip-10-63-114-58.corp.stateauto.com:43403 (size: 57.3 KB, free: 912.1 MB)\r\n19/10/15 03:05:33 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1201\r\n19/10/15 03:05:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[59] at repartition at HoodieCopyOnWriteTable.java:392) (first 15 tasks are for partitions Vector(0))\r\n19/10/15 03:05:33 INFO YarnScheduler: Adding task set 34.0 with 1 tasks\r\n19/10/15 03:05:33 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 12150, ip-10-63-114-114.corp.stateauto.com, executor 4, partition 0, PROCESS_LOCAL, 7734 bytes)\r\n19/10/15 03:05:33 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ip-10-63-114-114.corp.stateauto.com:42747 (size: 57.3 KB, free: 1458.5 MB)\r\n[Stage 34:>                                                         (0 + 1) / 1]19/10/15 03:05:35 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 12150) in 2233 ms on ip-10-63-114-114.corp.stateauto.com (executor 4) (1/1)\r\n19/10/15 03:05:35 INFO YarnScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \r\n19/10/15 03:05:35 INFO DAGScheduler: ShuffleMapStage 34 (repartition at HoodieCopyOnWriteTable.java:392) finished in 2.252 s\r\n19/10/15 03:05:35 INFO DAGScheduler: looking for newly runnable stages\r\n19/10/15 03:05:35 INFO DAGScheduler: running: Set()\r\n19/10/15 03:05:35 INFO DAGScheduler: waiting: Set(ShuffleMapStage 35, ResultStage 36)\r\n19/10/15 03:05:35 INFO DAGScheduler: failed: Set()\r\n19/10/15 03:05:35 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[63] at mapPartitionsToPair at HoodieCopyOnWriteTable.java:393), which has no missing parents\r\n19/10/15 03:05:35 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 154.6 KB, free 911.0 MB)\r\n19/10/15 03:05:35 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 57.4 KB, free 910.9 MB)\r\n19/10/15 03:05:35 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on ip-10-63-114-58.corp.stateauto.com:43403 (size: 57.4 KB, free: 912.0 MB)\r\n19/10/15 03:05:35 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1201\r\n19/10/15 03:05:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[63] at mapPartitionsToPair at HoodieCopyOnWriteTable.java:393) (first 15 tasks are for partitions Vector(0))\r\n19/10/15 03:05:35 INFO YarnScheduler: Adding task set 35.0 with 1 tasks\r\n19/10/15 03:05:35 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 12151, ip-10-63-114-114.corp.stateauto.com, executor 4, partition 0, PROCESS_LOCAL, 7939 bytes)\r\n19/10/15 03:05:35 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on ip-10-63-114-114.corp.stateauto.com:42747 (size: 57.4 KB, free: 1458.5 MB)\r\n19/10/15 03:05:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.63.114.114:46592\r\n19/10/15 03:05:35 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 12151) in 95 ms on ip-10-63-114-114.corp.stateauto.com (executor 4) (1/1)\r\n19/10/15 03:05:35 INFO YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \r\n19/10/15 03:05:35 INFO DAGScheduler: ShuffleMapStage 35 (mapPartitionsToPair at HoodieCopyOnWriteTable.java:393) finished in 0.114 s\r\n19/10/15 03:05:35 INFO DAGScheduler: looking for newly runnable stages\r\n19/10/15 03:05:35 INFO DAGScheduler: running: Set()\r\n19/10/15 03:05:35 INFO DAGScheduler: waiting: Set(ResultStage 36)\r\n19/10/15 03:05:35 INFO DAGScheduler: failed: Set()\r\n19/10/15 03:05:35 INFO DAGScheduler: Submitting ResultStage 36 (ShuffledRDD[64] at reduceByKey at HoodieCopyOnWriteTable.java:393), which has no missing parents\r\n19/10/15 03:05:35 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 4.6 KB, free 910.9 MB)\r\n19/10/15 03:05:35 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.6 KB, free 910.9 MB)\r\n19/10/15 03:05:35 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on ip-10-63-114-58.corp.stateauto.com:43403 (size: 2.6 KB, free: 912.0 MB)\r\n19/10/15 03:05:35 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1201\r\n19/10/15 03:05:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[64] at reduceByKey at HoodieCopyOnWriteTable.java:393) (first 15 tasks are for partitions Vector(0))\r\n19/10/15 03:05:35 INFO YarnScheduler: Adding task set 36.0 with 1 tasks\r\n19/10/15 03:05:35 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 12152, ip-10-63-114-115.corp.stateauto.com, executor 2, partition 0, PROCESS_LOCAL, 7674 bytes)\r\n19/10/15 03:05:35 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on ip-10-63-114-115.corp.stateauto.com:36209 (size: 2.6 KB, free: 1458.4 MB)\r\n19/10/15 03:05:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.63.114.115:42194\r\n19/10/15 03:05:35 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 12152) in 12 ms on ip-10-63-114-115.corp.stateauto.com (executor 2) (1/1)\r\n19/10/15 03:05:35 INFO YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \r\n19/10/15 03:05:35 INFO DAGScheduler: ResultStage 36 (collect at HoodieCopyOnWriteTable.java:396) finished in 0.018 s\r\n19/10/15 03:05:35 INFO DAGScheduler: Job 9 finished: collect at HoodieCopyOnWriteTable.java:396, took 2.390622 s\r\n19/10/15 03:05:35 INFO FileSystemViewManager: Creating InMemory based view for basePath s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:35 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:35 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:35 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:35 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:35 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@4d705b45\r\n19/10/15 03:05:35 INFO HoodieWriteClient: Cleaned 0 files\r\n19/10/15 03:05:35 INFO HoodieActiveTimeline: Marking instant complete [==>20191015030518__clean__INFLIGHT]\r\n19/10/15 03:05:36 INFO MultipartUploadOutputStream: close closed:false s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.clean.inflight\r\n19/10/15 03:05:36 INFO HoodieActiveTimeline: Created a new file in meta path: s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.clean.inflight\r\n19/10/15 03:05:36 INFO MultipartUploadOutputStream: close closed:false s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.clean.inflight\r\n19/10/15 03:05:36 INFO S3NativeFileSystem: rename s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.clean.inflight s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.clean\r\n19/10/15 03:05:36 INFO HoodieActiveTimeline: Completed [==>20191015030518__clean__INFLIGHT]\r\n19/10/15 03:05:36 INFO HoodieWriteClient: Marked clean started on 20191015030518 as complete\r\n19/10/15 03:05:36 INFO HoodieWriteClient: Committed 20191015030518\r\n19/10/15 03:05:36 INFO HoodieSparkSqlWriter$: Commit 20191015030518 successful!\r\n19/10/15 03:05:36 INFO HoodieSparkSqlWriter$: Syncing to Hive Metastore (URL: jdbc:hive2://ip-10-63-114-58.corp.stateauto.com:10000)\r\n19/10/15 03:05:36 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, __spark_hadoop_conf__.xml, file:/etc/spark/conf.dist/hive-site.xml], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:36 INFO HiveConf: Found configuration file file:/etc/spark/conf.dist/hive-site.xml\r\n19/10/15 03:05:36 WARN HiveConf: HiveConf of name hive.metastore.client.factory.class does not exist\r\n19/10/15 03:05:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:36 INFO FSUtils: Hadoop Configuration: fs.defaultFS: [hdfs://ip-10-63-114-58.corp.stateauto.com:8020], Config:[Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, emrfs-site.xml, {yarn.ipc.rpc.class=org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC, mapreduce.job.maxtaskfailures.per.tracker=3, yarn.client.max-cached-nodemanagers-proxies=0, mapreduce.job.speculative.retry-after-speculate=15000, ha.health-monitor.connect-retry-interval.ms=1000, yarn.resourcemanager.work-preserving-recovery.enabled=true, mapreduce.reduce.markreset.buffer.percent=0.0, dfs.datanode.data.dir=/mnt/hdfs, mapreduce.jobhistory.max-age-ms=604800000, mapreduce.job.ubertask.enable=false, yarn.nodemanager.log-aggregation.compression-type=none, hive.metastore.connect.retries=15, mapreduce.job.complete.cancel.delegation.tokens=true, yarn.app.mapreduce.am.jhs.backup-dir=file:///var/log/hadoop-mapreduce/history, mapreduce.jobhistory.datestring.cache.size=200000, hadoop.security.kms.client.authentication.retry-count=1, hadoop.ssl.enabled.protocols=TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2, yarn.resourcemanager.scheduler.address=ip-10-63-114-58.corp.stateauto.com:8030, hadoop.http.cross-origin.enabled=false, yarn.resourcemanager.proxy-user-privileges.enabled=false, mapreduce.reduce.shuffle.fetch.retry.enabled=${yarn.nodemanager.recovery.enabled}, io.mapfile.bloom.error.rate=0.005, yarn.nodemanager.resourcemanager.minimum.version=NONE, yarn.resourcemanager.nodemanagers.heartbeat-interval-ms=1000, hadoop.http.cross-origin.allowed-headers=X-Requested-With,Content-Type,Accept,Origin, yarn.nodemanager.delete.debug-delay-sec=0, hadoop.proxyuser.hue.hosts=*, yarn.scheduler.maximum-allocation-vcores=128, yarn.timeline-service.address=${yarn.timeline-service.hostname}:10200, hadoop.job.history.user.location=none, ipc.maximum.response.length=134217728, yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb=0, mapreduce.job.hdfs-servers=${fs.defaultFS}, mapreduce.task.profile.reduce.params=${mapreduce.task.profile.params}, ftp.stream-buffer-size=4096, hadoop.http.cross-origin.allowed-methods=GET,POST,HEAD, fs.s3a.buffer.dir=${hadoop.tmp.dir}/s3a, yarn.client.application-client-protocol.poll-interval-ms=200, yarn.timeline-service.leveldb-timeline-store.path=${hadoop.tmp.dir}/yarn/timeline, mapreduce.job.split.metainfo.maxsize=10000000, fs.s3a.fast.upload.buffer=disk, s3native.bytes-per-checksum=512, mapred.output.direct.EmrFileSystem=true, yarn.client.failover-retries-on-socket-timeouts=0, hadoop.security.sensitive-config-keys=\r\n      secret$\r\n      password$\r\n      ssl.keystore.pass$\r\n      fs.s3.*[Ss]ecret.?[Kk]ey\r\n      fs.azure.account.key.*\r\n      credential$\r\n      oauth.*token$\r\n      hadoop.security.sensitive-config-keys\r\n  , yarn.timeline-service.client.retry-interval-ms=1000, hadoop.http.authentication.type=simple, mapreduce.local.clientfactory.class.name=org.apache.hadoop.mapred.LocalClientFactory, ipc.client.connection.maxidletime=10000, ipc.server.max.connections=0, mapreduce.jobhistory.recovery.store.leveldb.path=${hadoop.tmp.dir}/mapred/history/recoverystore, fs.s3a.multipart.purge.age=86400, yarn.timeline-service.client.best-effort=false, mapreduce.job.ubertask.maxmaps=9, yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage=90.0, mapreduce.ifile.readahead.bytes=4194304, yarn.sharedcache.uploader.server.thread-count=50, mapreduce.jobhistory.admin.address=0.0.0.0:10033, s3.client-write-packet-size=65536, yarn.app.mapreduce.am.resource.cpu-vcores=1, yarn.nodemanager.node-labels.provider.configured-node-partition=CORE, mapreduce.input.lineinputformat.linespermap=1, mapreduce.reduce.shuffle.input.buffer.percent=0.70, hadoop.http.staticuser.user=dr.who, mapreduce.reduce.maxattempts=4, hadoop.security.group.mapping.ldap.search.filter.user=(&(objectClass=user)(sAMAccountName={0})), mapreduce.jobhistory.admin.acl=*, hadoop.workaround.non.threadsafe.getpwuid=true, mapreduce.map.maxattempts=4, yarn.timeline-service.entity-group-fs-store.active-dir=/tmp/entity-file-history/active, yarn.resourcemanager.zk-retry-interval-ms=1000, mapreduce.jobhistory.cleaner.interval-ms=86400000, dfs.permissions.superusergroup=hadoop, yarn.is.minicluster=false, yarn.application.classpath=\r\n        $HADOOP_CONF_DIR,\r\n        $HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,\r\n        $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,\r\n        $HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,\r\n        $HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,\r\n        /usr/lib/hadoop-lzo/lib/*,\r\n        /usr/share/aws/emr/emrfs/conf,\r\n        /usr/share/aws/emr/emrfs/lib/*,\r\n        /usr/share/aws/emr/emrfs/auxlib/*,\r\n        /usr/share/aws/emr/lib/*,\r\n        /usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar,\r\n        /usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar,\r\n        /usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar,\r\n        /usr/lib/spark/yarn/lib/datanucleus-api-jdo.jar,\r\n        /usr/lib/spark/yarn/lib/datanucleus-core.jar,\r\n        /usr/lib/spark/yarn/lib/datanucleus-rdbms.jar,\r\n        /usr/share/aws/emr/cloudwatch-sink/lib/*,\r\n        /usr/share/aws/aws-java-sdk/*\r\n     , fs.s3n.block.size=67108864, hadoop.registry.system.acls=sasl:yarn@, sasl:mapred@, sasl:hdfs@, yarn.nodemanager.node-labels.provider.fetch-timeout-ms=1200000, yarn.sharedcache.store.in-memory.check-period-mins=720, fs.s3a.multiobjectdelete.enable=true, mapreduce.map.skip.proc-count.auto-incr=true, yarn.nodemanager.vmem-check-enabled=true, hadoop.security.authentication=simple, mapreduce.reduce.skip.proc-count.auto-incr=true, mapreduce.reduce.cpu.vcores=1, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.ScriptBasedMapping, fs.s3.sleepTimeSeconds=10, mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem=true, yarn.timeline-service.ttl-ms=604800000, yarn.sharedcache.root-dir=/sharedcache, yarn.resourcemanager.keytab=/etc/krb5.keytab, yarn.resourcemanager.container.liveness-monitor.interval-ms=600000, yarn.node-labels.fs-store.root-dir=/apps/yarn/nodelabels, hadoop.security.group.mapping.ldap.posix.attr.gid.name=gidNumber, yarn.web-proxy.address=ip-10-63-114-58.corp.stateauto.com:20888, yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms=1000, yarn.log-aggregation.enable-local-cleanup=false, yarn.app.mapreduce.client-am.ipc.max-retries-on-timeouts=3, yarn.nodemanager.linux-container-executor.cgroups.hierarchy=/hadoop-yarn, s3.bytes-per-checksum=512, hadoop.ssl.require.client.cert=false, mapreduce.output.fileoutputformat.compress=false, yarn.resourcemanager.node-labels.provider.fetch-interval-ms=1800000, yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled=true, mapreduce.shuffle.max.threads=0, yarn.nodemanager.linux-container-executor.cgroups.delete-timeout-ms=1000, s3native.client-write-packet-size=65536, mapreduce.client.submit.file.replication=10, yarn.app.mapreduce.am.job.committer.commit-window=10000, yarn.nodemanager.sleep-delay-before-sigkill.ms=250, yarn.nodemanager.env-whitelist=JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME, mapreduce.map.speculative=true, mapreduce.job.speculative.slowtaskthreshold=1.0, yarn.nodemanager.linux-container-executor.cgroups.mount=false, mapreduce.tasktracker.http.threads=60, mapreduce.jobhistory.http.policy=HTTP_ONLY, ipc.client.low-latency=false, fs.s3a.paging.maximum=5000, mapreduce.jvm.system-properties-to-log=os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name, hadoop.kerberos.min.seconds.before.relogin=60, yarn.resourcemanager.nodemanager-connect-retries=10, fs.s3.buffer.dir=/mnt/s3, io.native.lib.available=true, mapreduce.jobhistory.done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done, hadoop.registry.zk.retry.interval.ms=1000, mapreduce.job.reducer.unconditional-preempt.delay.sec=300, hadoop.ssl.hostname.verifier=DEFAULT, mapreduce.task.timeout=600000, yarn.resourcemanager.configuration.file-system-based-store=/yarn/conf, yarn.nodemanager.disk-health-checker.interval-ms=120000, adl.feature.ownerandgroup.enableupn=false, dfs.namenode.replication.max-streams-hard-limit=40, hadoop.security.groups.cache.secs=300, mapreduce.input.fileinputformat.split.minsize=0, yarn.minicluster.control-resource-monitoring=false, yarn.resourcemanager.fail-fast=${yarn.fail-fast}, hadoop.proxyuser.hue.groups=*, mapreduce.shuffle.port=13562, hadoop.rpc.protection=authentication, hadoop.proxyuser.hadoop.hosts=*, yarn.timeline-service.recovery.enabled=false, yarn.client.failover-proxy-provider=org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider, ipc.client.tcpnodelay=true, fs.s3.maxRetries=4, mapreduce.jobtracker.address=local, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, hadoop.security.group.mapping.ldap.posix.attr.uid.name=uidNumber, fs.s3bfs.impl=org.apache.hadoop.fs.s3.S3FileSystem, yarn.resourcemanager.nodemanager-graceful-decommission-timeout-secs=3600, yarn.resourcemanager.webapp.address=${yarn.resourcemanager.hostname}:8088, yarn.timeline-service.client.max-retries=30, mapreduce.task.profile.reduces=0-2, yarn.resourcemanager.am.max-attempts=2, dfs.bytes-per-checksum=512, mapreduce.job.end-notification.max.retry.interval=5000, ipc.client.connect.retry.interval=1000, fs.s3a.multipart.size=100M, yarn.app.mapreduce.am.command-opts=-Xmx12288m, yarn.nodemanager.process-kill-wait.ms=2000, yarn.timeline-service.state-store-class=org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore, yarn.timeline-service.client.fd-clean-interval-secs=60, mapreduce.job.speculative.minimum-allowed-tasks=10, hadoop.jetty.logs.serve.aliases=true, mapreduce.reduce.shuffle.fetch.retry.timeout-ms=30000, fs.du.interval=600000, yarn.nodemanager.node-labels.provider.fetch-interval-ms=600000, yarn.sharedcache.admin.address=0.0.0.0:8047, yarn.acl.reservation-enable=false, hadoop.proxyuser.httpfs.groups=hudson,testuser,root,hadoop,jenkins,oozie,hive,httpfs,hue,users, hadoop.security.random.device.file.path=/dev/urandom, mapreduce.task.merge.progress.records=10000, dfs.webhdfs.enabled=true, yarn.nodemanager.container-metrics.period-ms=-1, hadoop.registry.secure=false, hadoop.ssl.client.conf=ssl-client.xml, mapreduce.job.counters.max=120, yarn.nodemanager.localizer.fetch.thread-count=20, io.mapfile.bloom.size=1048576, yarn.nodemanager.localizer.client.thread-count=20, fs.automatic.close=true, mapreduce.task.profile=false, yarn.nodemanager.recovery.compaction-interval-secs=3600, mapreduce.task.combine.progress.records=10000, mapreduce.shuffle.ssl.file.buffer.size=65536, yarn.app.mapreduce.client.job.max-retries=0, fs.swift.impl=org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem, yarn.app.mapreduce.am.container.log.backups=0, dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction=1.0, yarn.minicluster.fixed.ports=false, mapreduce.app-submission.cross-platform=false, yarn.timeline-service.ttl-enable=true, yarn.nodemanager.container-monitor.procfs-tree.smaps-based-rss.enabled=false, yarn.nodemanager.keytab=/etc/krb5.keytab, yarn.nodemanager.log-aggregation.policy.class=org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AllContainerLogAggregationPolicy, yarn.client.application-client-protocol.poll-timeout-ms=-1, yarn.resourcemanager.webapp.ui-actions.enabled=true, yarn.sharedcache.client-server.address=0.0.0.0:8045, yarn.nodemanager.webapp.cross-origin.enabled=false, yarn.nodemanager.runtime.linux.docker.privileged-containers.allowed=false, hadoop.security.instrumentation.requires.admin=false, io.compression.codec.bzip2.library=system-native, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, fs.ftp.host=0.0.0.0, mapreduce.task.exit.timeout=60000, yarn.app.mapreduce.am.containerlauncher.threadpool-initial-size=10, s3.blocksize=67108864, s3native.stream-buffer-size=4096, yarn.nodemanager.resource.memory-mb=122880, mapreduce.task.userlog.limit.kb=0, hadoop.security.crypto.codec.classes.aes.ctr.nopadding=org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec, mapreduce.reduce.speculative=true, yarn.node-labels.fs-store.impl.class=org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore, hadoop.caller.context.max.size=128, dfs.replication=1, yarn.client.failover-retries=0, yarn.nodemanager.resource.cpu-vcores=16, mapreduce.jobhistory.recovery.enable=false, nfs.exports.allowed.hosts=* rw, yarn.sharedcache.checksum.algo.impl=org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl, mapreduce.reduce.shuffle.memory.limit.percent=0.25, file.replication=1, mapreduce.job.reduce.shuffle.consumer.plugin.class=org.apache.hadoop.mapreduce.task.reduce.Shuffle, mapreduce.task.local-fs.write-limit.bytes=-1, yarn.app.mapreduce.am.log.level=INFO, mapreduce.job.jvm.numtasks=20, mapreduce.am.max-attempts=2, mapreduce.shuffle.connection-keep-alive.timeout=5, mapreduce.job.reduces=23, hadoop.security.group.mapping.ldap.connection.timeout.ms=60000, yarn.nodemanager.amrmproxy.client.thread-count=25, yarn.app.mapreduce.am.job.task.listener.thread-count=60, yarn.resourcemanager.store.class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore, s3native.replication=3, mapreduce.tasktracker.reduce.tasks.maximum=3, fs.permissions.umask-mode=022, yarn.resourcemanager.node-ip-cache.expiry-interval-secs=-1, mapreduce.cluster.local.dir=/mnt/mapred, mapreduce.client.output.filter=FAILED, yarn.nodemanager.pmem-check-enabled=true, hadoop.proxyuser.httpfs.hosts=*, ftp.replication=3, hadoop.security.group.mapping.ldap.search.attr.member=member, fs.s3a.max.total.tasks=5, dfs.namenode.replication.work.multiplier.per.iteration=10, yarn.resourcemanager.fs.state-store.num-retries=0, yarn.timeline-service.leveldb-state-store.path=${hadoop.tmp.dir}/yarn/timeline, yarn.resourcemanager.resource-tracker.address=ip-10-63-114-58.corp.stateauto.com:8025, yarn.nodemanager.resource.pcores-vcores-multiplier=1.0, hadoop.security.token.service.use_ip=true, yarn.resourcemanager.scheduler.monitor.enable=false, fs.trash.checkpoint.interval=0, hadoop.registry.zk.retry.times=5, yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size=10000, s3.stream-buffer-size=4096, fs.s3a.connection.maximum=15, hadoop.security.dns.log-slow-lookups.enabled=false, file.client-write-packet-size=65536, hadoop.shell.missing.defaultFs.warning=false, fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, yarn.nodemanager.windows-container.memory-limit.enabled=false, yarn.nodemanager.remote-app-log-dir=/var/log/hadoop-yarn/apps, mapreduce.reduce.shuffle.retry-delay.max.ms=60000, io.map.index.interval=128, mapreduce.admin.user.env=LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native:/usr/lib/hadoop-lzo/lib/native, yarn.nodemanager.container-localizer.java.opts=-Xmx256m, javax.jdo.option.ConnectionUserName=hive, hadoop.ssl.server.conf=ssl-server.xml, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, yarn.minicluster.yarn.nodemanager.resource.memory-mb=4096, yarn.app.mapreduce.client.max-retries=3, yarn.nodemanager.address=${yarn.nodemanager.hostname}:8041, mapreduce.jobhistory.webapp.https.address=0.0.0.0:19890, yarn.resourcemanager.max-log-aggregation-diagnostics-in-memory=10, dfs.datanode.max.transfer.threads=4096, ha.failover-controller.graceful-fence.rpc-timeout.ms=5000, yarn.resourcemanager.delayed.delegation-token.removal-interval-ms=30000, yarn.timeline-service.enabled=false, yarn.app.mapreduce.am.jhs.backup.enabled=true, ipc.maximum.data.length=67108864, mapreduce.job.finish-when-all-reducers-done=false, hadoop.security.key.provider.path=kms://http@ip-10-63-114-58.corp.stateauto.com:9700/kms, hadoop.security.group.mapping.providers.combined=true, yarn.resourcemanager.decommissioning-nodes-watcher.poll-interval-secs=20, hadoop.security.groups.cache.warn.after.ms=5000, hadoop.security.auth_to_local=\r\n      RULE:[1:$1@$0](.*@)s/@.*///L\r\n      RULE:[2:$1@$0](.*@)s/@.*///L\r\n      DEFAULT\r\n    , io.compression.codec.lzo.class=com.hadoop.compression.lzo.LzoCodec, yarn.resourcemanager.fs.state-store.retry-interval-ms=1000, yarn.resourcemanager.zk-acl=world:anyone:rwcda, yarn.nodemanager.resource-monitor.interval-ms=3000, yarn.nodemanager.resource.detect-hardware-capabilities=false, yarn.sharedcache.app-checker.class=org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker, yarn.timeline-service.entity-group-fs-store.retain-seconds=604800, yarn.nodemanager.webapp.https.address=0.0.0.0:8044, yarn.nodemanager.amrmproxy.enable=false, yarn.nodemanager.linux-container-executor.cgroups.delete-delay-ms=20, yarn.resourcemanager.fs.state-store.retry-policy-spec=2000, 500, fs.s3a.fast.upload=false, mapreduce.job.committer.setup.cleanup.needed=true, mapreduce.job.end-notification.retry.attempts=0, yarn.resourcemanager.state-store.max-completed-applications=${yarn.resourcemanager.max-completed-applications}, yarn.scheduler.increment-allocation-mb=32, mapreduce.map.output.compress=true, mapreduce.jobhistory.cleaner.enable=true, dfs.namenode.replication.max-streams=20, mapreduce.job.running.reduce.limit=0, io.seqfile.local.dir=${hadoop.tmp.dir}/io/local, mapreduce.reduce.shuffle.read.timeout=180000, mapreduce.job.queuename=default, dfs.namenode.rpc-address=ip-10-63-114-58.corp.stateauto.com:8020, ipc.client.connect.max.retries=10, yarn.app.mapreduce.am.staging-dir=/tmp/hadoop-yarn/staging, yarn.timeline-service.leveldb-timeline-store.read-cache-size=104857600, yarn.nodemanager.linux-container-executor.resources-handler.class=org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler, yarn.app.mapreduce.client.job.retry-interval=2000, io.file.buffer.size=65536, yarn.resourcemanager.webapp.cross-origin.enabled=false, yarn.resourcemanager.am-rm-tokens.master-key-rolling-interval-secs=86400, yarn.nodemanager.log.deletion-threads-count=4, ha.zookeeper.parent-znode=/hadoop-ha, tfile.io.chunk.size=1048576, yarn.resourcemanager.work-preserving-recovery.scheduling-wait-ms=10000, yarn.timeline-service.keytab=/etc/krb5.keytab, yarn.node-labels.enabled=true, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, yarn.acl.enable=false, hadoop.security.group.mapping.ldap.directory.search.timeout=10000, mapreduce.application.classpath=\r\n    $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,\r\n    $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,\r\n    /usr/lib/hadoop-lzo/lib/*,\r\n    /usr/share/aws/emr/emrfs/conf,\r\n    /usr/share/aws/emr/emrfs/lib/*,\r\n    /usr/share/aws/emr/emrfs/auxlib/*,\r\n    /usr/share/aws/emr/lib/*,\r\n    /usr/share/aws/emr/ddb/lib/emr-ddb-hadoop.jar,\r\n    /usr/share/aws/emr/goodies/lib/emr-hadoop-goodies.jar,\r\n    /usr/share/aws/emr/kinesis/lib/emr-kinesis-hadoop.jar,\r\n    /usr/share/aws/emr/cloudwatch-sink/lib/*,\r\n    /usr/share/aws/aws-java-sdk/*\r\n  , yarn.timeline-service.version=1.0f, mapreduce.job.token.tracking.ids.enabled=false, mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec, mapred.output.direct.NativeS3FileSystem=true, yarn.sharedcache.enabled=false, hadoop.proxyuser.hadoop.groups=*, s3.replication=3, yarn.timeline-service.http-authentication.type=simple, hadoop.registry.zk.root=/registry, tfile.fs.input.buffer.size=262144, ha.failover-controller.graceful-fence.connection.retries=1, net.topology.script.number.args=100, fs.s3n.multipart.uploads.block.size=67108864, yarn.sharedcache.admin.thread-count=1, yarn.nodemanager.recovery.dir=${hadoop.tmp.dir}/yarn-nm-recovery, hadoop.ssl.enabled=false, fs.AbstractFileSystem.ftp.impl=org.apache.hadoop.fs.ftp.FtpFs, yarn.timeline-service.handler-thread-count=10, yarn.nodemanager.container-metrics.unregister-delay-ms=10000, hadoop.caller.context.enabled=false, mapreduce.jobhistory.recovery.store.class=org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService, yarn.nodemanager.log.retain-seconds=10800, yarn.resourcemanager.admin.address=${yarn.resourcemanager.hostname}:8033, yarn.resourcemanager.recovery.enabled=false, yarn.resourcemanager.ha.automatic-failover.zk-base-path=/yarn-leader-election, fs.s3n.impl=com.amazon.ws.emr.hadoop.fs.EmrFileSystem, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.AbstractFileSystem.hdfs.impl=org.apache.hadoop.fs.Hdfs, yarn.resourcemanager.reservation-system.enable=false, mapreduce.job.speculative.speculative-cap-total-tasks=0.01, yarn.timeline-service.generic-application-history.max-applications=10000, yarn.sharedcache.nm.uploader.thread-count=20, yarn.nodemanager.log-container-debug-info.enabled=false, fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A, yarn.resourcemanager.max-completed-applications=10000, hadoop.proxyuser.oozie.groups=*, yarn.nodemanager.log-dirs=/var/log/hadoop-yarn/containers, fs.s3.maxConnections=5000, yarn.resourcemanager.node-removal-untracked.timeout-ms=60000, yarn.nodemanager.linux-container-executor.nonsecure-mode.user-pattern=^[_.A-Za-z0-9][-@_.A-Za-z0-9]{0,255}?[$]?$, dfs.hosts.exclude=/emr/instance-controller/lib/dfs.hosts.exclude, ftp.blocksize=67108864, mapreduce.job.acl-modify-job= , fs.defaultFS=hdfs://ip-10-63-114-58.corp.stateauto.com:8020, hive.metastore.client.factory.class=com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory, yarn.nodemanager.node-labels.resync-interval-ms=120000, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, fs.s3n.multipart.copy.block.size=5368709120, mapreduce.map.java.opts=-Xmx6144m, fs.adl.impl=org.apache.hadoop.fs.adl.AdlFileSystem, fs.adl.oauth2.access.token.provider.type=ClientCredential, yarn.resourcemanager.connect.max-wait.ms=900000, yarn.timeline-service.entity-group-fs-store.scan-interval-seconds=60, hadoop.security.group.mapping.ldap.ssl=false, dfs.namenode.https-address=ip-10-63-114-58.corp.stateauto.com:50470, yarn.nodemanager.aux-services=mapreduce_shuffle,, yarn.intermediate-data-encryption.enable=false, yarn.sharedcache.store.class=org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore, yarn.fail-fast=false, yarn.resourcemanager.admin.client.thread-count=1, hadoop.security.kms.client.encrypted.key.cache.size=500, yarn.app.mapreduce.shuffle.log.separate=true, ipc.client.kill.max=10, hadoop.security.group.mapping.ldap.search.filter.group=(objectClass=group), fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.http.authentication.kerberos.keytab=${user.home}/hadoop.keytab, yarn.client.nodemanager-connect.max-wait-ms=180000, mapreduce.job.map.output.collector.class=org.apache.hadoop.mapred.MapTask$MapOutputBuffer, hadoop.security.uid.cache.secs=14400, mapreduce.map.cpu.vcores=1, yarn.log-aggregation.retain-check-interval-seconds=-1, mapreduce.map.log.level=INFO, mapred.child.java.opts=-Xmx200m, yarn.app.mapreduce.am.hard-kill-timeout-ms=10000, hadoop.registry.zk.session.timeout.ms=60000, mapreduce.job.running.map.limit=0, yarn.sharedcache.store.in-memory.initial-delay-mins=10, yarn.timeline-service.entity-group-fs-store.cleaner-interval-seconds=3600, yarn.sharedcache.client-server.thread-count=50, yarn.nodemanager.local-cache.max-files-per-directory=8192, s3native.blocksize=67108864, dfs.datanode.fsdataset.volume.choosing.policy=org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy, mapreduce.client.completion.pollinterval=5000, fs.s3a.socket.send.buffer=8192, mapreduce.job.maps=48, fs.AbstractFileSystem.swebhdfs.impl=org.apache.hadoop.fs.SWebHdfs, mapreduce.job.acl-view-job= , fs.s3a.readahead.range=64K, yarn.resourcemanager.connect.retry-interval.ms=30000, yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms=300000, fs.s3a.multipart.threshold=2147483647, mapreduce.shuffle.max.connections=0, hadoop.shell.safely.delete.limit.num.files=100, yarn.log-aggregation-enable=true, mapreduce.task.io.sort.factor=48, hadoop.security.dns.log-slow-lookups.threshold.ms=1000, ha.health-monitor.sleep-after-disconnect.ms=1000, ha.zookeeper.session-timeout.ms=10000, yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=true, fs.AbstractFileSystem.s3.impl=org.apache.hadoop.fs.s3.EMRFSDelegate, mapreduce.input.fileinputformat.list-status.num-threads=1, io.skip.checksum.errors=false, yarn.resourcemanager.scheduler.client.thread-count=64, dfs.namenode.safemode.extension=5000, mapreduce.jobhistory.move.thread-count=3, yarn.resourcemanager.zk-state-store.parent-path=/rmstore, yarn.timeline-service.client.fd-retain-secs=300, ipc.client.idlethreshold=4000, yarn.sharedcache.cleaner.initial-delay-mins=10, mapreduce.task.profile.params=-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s, mapreduce.jobhistory.keytab=/etc/security/keytab/jhs.service.keytab, yarn.scheduler.minimum-allocation-mb=32, yarn.resourcemanager.container-tokens.master-key-rolling-interval-secs=86400, mapreduce.reduce.shuffle.fetch.retry.interval-ms=1000, yarn.timeline-service.entity-group-fs-store.app-cache-size=10, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.3f, yarn.dispatcher.exit-on-error=true, fs.s3a.connection.ssl.enabled=true, yarn.node-labels.fs-store.retry-policy-spec=2000, 500, yarn.nodemanager.runtime.linux.docker.capabilities=CHOWN,DAC_OVERRIDE,FSETID,FOWNER,MKNOD,NET_RAW,SETGID,SETUID,SETFCAP,SETPCAP,NET_BIND_SERVICE,SYS_CHROOT,KILL,AUDIT_WRITE, fs.AbstractFileSystem.webhdfs.impl=org.apache.hadoop.fs.WebHdfs, yarn.resourcemanager.scheduler.monitor.policies=org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy, ipc.server.listen.queue.size=128, rpc.metrics.quantile.enable=false, yarn.nodemanager.resource.system-reserved-memory-mb=-1, yarn.nodemanager.log-aggregation.roll-monitoring-interval-seconds=-1, yarn.client.nodemanager-client-async.thread-pool-max-size=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, yarn.resourcemanager.system-metrics-publisher.enabled=true, dfs.namenode.name.dir=/mnt/namenode, yarn.am.liveness-monitor.expiry-interval-ms=600000, yarn.nm.liveness-monitor.expiry-interval-ms=600000, ftp.bytes-per-checksum=512, yarn.sharedcache.nested-level=3, javax.jdo.option.ConnectionPassword=hive, mapreduce.job.emit-timeline-data=false, io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec, mapreduce.map.memory.mb=7680, yarn.client.nodemanager-connect.retry-interval-ms=10000, hadoop.http.cross-origin.max-age=1800, yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size=10000, yarn.scheduler.include-port-in-node-name=false, mapreduce.job.speculative.retry-after-no-speculate=1000, hadoop.registry.zk.connection.timeout.ms=15000, yarn.resourcemanager.address=ip-10-63-114-58.corp.stateauto.com:8032, ipc.client.rpc-timeout.ms=0, mapreduce.task.skip.start.attempts=2, fs.s3a.socket.recv.buffer=8192, yarn.resourcemanager.zk-timeout-ms=10000, yarn.timeline-service.entity-group-fs-store.summary-store=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, hadoop.security.groups.cache.background.reload.threads=3, hadoop.proxyuser.hive.groups=*, yarn.sharedcache.cleaner.resource-sleep-ms=0, yarn.nodemanager.runtime.linux.allowed-runtimes=default, mapreduce.map.skip.maxrecords=0, yarn.resourcemanager.system-metrics-publisher.dispatcher.pool-size=10, dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold=10737418240, mapreduce.jobtracker.system.dir=${hadoop.tmp.dir}/mapred/system, yarn.timeline-service.hostname=ip-10-63-114-58.corp.stateauto.com, hadoop.registry.rm.enabled=false, mapreduce.job.reducer.preempt.delay.sec=0, hadoop.proxyuser.oozie.hosts=*, mapred.output.committer.class=org.apache.hadoop.mapred.DirectFileOutputCommitter, hadoop.security.key.default.bitlength=256, yarn.node-labels.configuration-type=distributed, mapreduce.shuffle.ssl.enabled=false, yarn.nodemanager.vmem-pmem-ratio=5, yarn.nodemanager.container-manager.thread-count=64, hadoop.tmp.dir=/mnt/var/lib/hadoop/tmp, fs.AbstractFileSystem.har.impl=org.apache.hadoop.fs.HarFs, yarn.nodemanager.localizer.cache.target-size-mb=10240, yarn.app.mapreduce.shuffle.log.backups=0, yarn.minicluster.use-rpc=false, yarn.http.policy=HTTP_ONLY, yarn.timeline-service.webapp.https.address=${yarn.timeline-service.hostname}:8190, yarn.resourcemanager.amlauncher.thread-count=50, yarn.log.server.url=http://ip-10-63-114-58.corp.stateauto.com:19888/jobhistory/logs, tfile.fs.output.buffer.size=262144, fs.ftp.host.port=21, mapreduce.task.io.sort.mb=200, hadoop.security.group.mapping.ldap.search.attr.group.name=cn, yarn.nodemanager.amrmproxy.address=0.0.0.0:8048, hadoop.security.group.mapping.ldap.read.timeout.ms=60000, mapreduce.output.fileoutputformat.compress.type=BLOCK, file.bytes-per-checksum=512, mapreduce.job.userlog.retain.hours=48, mapreduce.reduce.java.opts=-Xmx12288m, ha.health-monitor.check-interval.ms=1000, yarn.resourcemanager.delegation.key.update-interval=86400000, yarn.resourcemanager.resource-tracker.client.thread-count=64, mapreduce.reduce.input.buffer.percent=0.0, yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage=false, ha.health-monitor.rpc-timeout.ms=45000, yarn.scheduler.maximum-allocation-mb=122880, yarn.resourcemanager.leveldb-state-store.path=${hadoop.tmp.dir}/yarn/system/rmstore, mapreduce.task.files.preserve.failedtasks=false, yarn.nodemanager.delete.thread-count=4, mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.DefaultCodec, map.sort.class=org.apache.hadoop.util.QuickSort, yarn.nodemanager.resource.count-logical-processors-as-cores=false, mapreduce.jobhistory.jobname.limit=50, mapreduce.job.classloader=false, hadoop.registry.zk.retry.ceiling.ms=60000, io.seqfile.compress.blocksize=1000000, mapreduce.task.profile.maps=0-2, mapreduce.jobtracker.staging.root.dir=${hadoop.tmp.dir}/mapred/staging, yarn.nodemanager.localizer.cache.cleanup.interval-ms=600000, hadoop.proxyuser.hive.hosts=*, hadoop.http.cross-origin.allowed-origins=*, yarn.timeline-service.client.fd-flush-interval-secs=10, hadoop.security.java.secure.random.algorithm=SHA1PRNG, fs.client.resolve.remote.symlinks=true, yarn.resourcemanager.delegation-token-renewer.thread-count=50, mapreduce.shuffle.listen.queue.size=128, yarn.nodemanager.disk-health-checker.min-healthy-disks=0.25, yarn.resourcemanager.nodes.exclude-path=/emr/instance-controller/lib/yarn.nodes.exclude.xml, mapreduce.job.end-notification.retry.interval=1000, mapreduce.jobhistory.loadedjobs.cache.size=5, fs.s3a.fast.upload.active.blocks=4, yarn.nodemanager.local-dirs=/mnt/yarn, mapreduce.task.exit.timeout.check-interval-ms=20000, yarn.timeline-service.webapp.address=${yarn.timeline-service.hostname}:8188, hadoop.registry.jaas.context=Client, mapreduce.jobhistory.address=ip-10-63-114-58.corp.stateauto.com:10020, ipc.server.log.slow.rpc=false, file.blocksize=67108864, yarn.sharedcache.cleaner.period-mins=1440, yarn.timeline-service.entity-group-fs-store.leveldb-cache-read-cache-size=10485760, fs.s3a.block.size=32M, hadoop.security.kms.client.failover.sleep.max.millis=2000, yarn.resourcemanager.metrics.runtime.buckets=60,300,1440, dfs.namenode.http-address=ip-10-63-114-58.corp.stateauto.com:50070, ipc.client.ping=true, yarn.resourcemanager.leveldb-state-store.compaction-interval-secs=3600, yarn.timeline-service.http-cross-origin.enabled=true, yarn.node-labels.am.default-node-label-expression=CORE, yarn.resourcemanager.configuration.provider-class=org.apache.hadoop.yarn.LocalConfigurationProvider, yarn.nodemanager.recovery.enabled=true, yarn.resourcemanager.hostname=10.63.114.58, fs.s3n.multipart.uploads.enabled=true, yarn.nodemanager.disk-health-checker.enable=true, mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem=2, yarn.nodemanager.amrmproxy.interceptor-class.pipeline=org.apache.hadoop.yarn.server.nodemanager.amrmproxy.DefaultRequestInterceptor, ha.failover-controller.cli-check.rpc-timeout.ms=20000, hadoop.proxyuser.presto.hosts=*, ftp.client-write-packet-size=65536, mapreduce.reduce.shuffle.parallelcopies=20, hadoop.caller.context.signature.max.size=40, mapreduce.jobhistory.principal=jhs/_HOST@REALM.TLD, hadoop.http.authentication.simple.anonymous.allowed=true, yarn.log-aggregation.retain-seconds=172800, yarn.resourcemanager.rm.container-allocation.expiry-interval-ms=600000, yarn.nodemanager.windows-container.cpu-limit.enabled=false, yarn.timeline-service.http-authentication.simple.anonymous.allowed=true, hadoop.security.kms.client.failover.sleep.base.millis=100, mapreduce.jobhistory.jhist.format=json, yarn.resourcemanager.reservation-system.planfollower.time-step=1000, mapreduce.job.ubertask.maxreduces=1, fs.s3a.connection.establish.timeout=5000, yarn.nodemanager.health-checker.interval-ms=600000, fs.s3a.multipart.purge=false, hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=2, fs.AbstractFileSystem.adl.impl=org.apache.hadoop.fs.adl.Adl, yarn.timeline-service.store-class=org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore, mapreduce.shuffle.transfer.buffer.size=131072, yarn.resourcemanager.zk-num-retries=1000, yarn.sharedcache.store.in-memory.staleness-period-mins=10080, yarn.nodemanager.webapp.address=${yarn.nodemanager.hostname}:8042, yarn.app.mapreduce.client-am.ipc.max-retries=3, ipc.ping.interval=60000, ha.failover-controller.new-active.rpc-timeout.ms=60000, mapreduce.jobhistory.client.thread-count=10, fs.trash.interval=0, mapreduce.fileoutputcommitter.algorithm.version=1, mapreduce.reduce.skip.maxgroups=0, mapreduce.reduce.memory.mb=15360, yarn.nodemanager.health-checker.script.timeout-ms=1200000, dfs.datanode.du.reserved=536870912, mapreduce.client.progressmonitor.pollinterval=1000, yarn.resourcemanager.delegation.token.renew-interval=86400000, yarn.nodemanager.hostname=0.0.0.0, yarn.resourcemanager.ha.enabled=false, yarn.scheduler.minimum-allocation-vcores=1, yarn.app.mapreduce.am.container.log.limit.kb=0, hadoop.http.authentication.signature.secret.file=${user.home}/hadoop-http-auth-signature-secret, mapreduce.jobhistory.move.interval-ms=180000, yarn.resourcemanager.nm-tokens.master-key-rolling-interval-secs=86400, yarn.nodemanager.container-executor.class=org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor, hadoop.security.authorization=false, yarn.nodemanager.node-labels.provider=config, yarn.nodemanager.localizer.address=${yarn.nodemanager.hostname}:8040, mapreduce.jobhistory.recovery.store.fs.uri=${hadoop.tmp.dir}/mapred/history/recoverystore, hive.metastore.warehouse.dir=hdfs:///user/spark/warehouse, mapreduce.shuffle.connection-keep-alive.enable=false, hadoop.common.configuration.version=0.23.0, yarn.app.mapreduce.task.container.log.backups=0, hadoop.security.groups.negative-cache.secs=30, mapreduce.ifile.readahead=true, hadoop.security.kms.client.timeout=60, yarn.nodemanager.resource.percentage-physical-cpu-limit=100, mapreduce.job.max.split.locations=10, hadoop.registry.zk.quorum=localhost:2181, fs.s3a.threads.keepalivetime=60, fs.s3.impl=com.amazon.ws.emr.hadoop.fs.EmrFileSystem, mapreduce.jobhistory.joblist.cache.size=20000, mapreduce.job.end-notification.max.attempts=5, hadoop.security.groups.cache.background.reload=false, mapreduce.reduce.shuffle.connect.timeout=180000, mapreduce.jobhistory.webapp.address=ip-10-63-114-58.corp.stateauto.com:19888, fs.s3a.connection.timeout=200000, yarn.sharedcache.nm.uploader.replication.factor=10, hadoop.http.authentication.token.validity=36000, ipc.client.connect.max.retries.on.timeouts=5, yarn.timeline-service.client.internal-timers-ttl-secs=420, yarn.nodemanager.docker-container-executor.exec-name=/usr/bin/docker, yarn.app.mapreduce.am.job.committer.cancel-timeout=60000, dfs.ha.fencing.ssh.connect-timeout=30000, mapreduce.reduce.log.level=INFO, mapreduce.reduce.shuffle.merge.percent=0.66, ipc.client.fallback-to-simple-auth-allowed=false, io.serializations=org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization, fs.s3.block.size=67108864, yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user=nobody, hadoop.kerberos.kinit.command=kinit, hadoop.security.kms.client.encrypted.key.cache.expiry=43200000, yarn.resourcemanager.fs.state-store.uri=${hadoop.tmp.dir}/yarn/system/rmstore, yarn.dispatcher.drain-events.timeout=300000, yarn.admin.acl=*, mapreduce.reduce.merge.inmem.threshold=1000, yarn.cluster.max-application-priority=0, net.topology.impl=org.apache.hadoop.net.NetworkTopology, yarn.resourcemanager.ha.automatic-failover.enabled=true, yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler, io.map.index.skip=0, dfs.namenode.handler.count=64, yarn.resourcemanager.webapp.https.address=${yarn.resourcemanager.hostname}:8090, yarn.nodemanager.admin-env=MALLOC_ARENA_MAX=$MALLOC_ARENA_MAX, hadoop.security.crypto.cipher.suite=AES/CTR/NoPadding, mapreduce.task.profile.map.params=${mapreduce.task.profile.params}, hadoop.security.crypto.buffer.size=8192, yarn.nodemanager.aux-services.mapreduce_shuffle.class=org.apache.hadoop.mapred.ShuffleHandler, yarn.nodemanager.container-metrics.enable=false, fs.s3a.path.style.access=false, mapreduce.cluster.acls.enabled=false, yarn.sharedcache.uploader.server.address=0.0.0.0:8046, yarn.log-aggregation-status.time-out.ms=600000, fs.s3a.threads.max=10, fs.har.impl.disable.cache=true, mapreduce.tasktracker.map.tasks.maximum=3, ipc.client.connect.timeout=20000, yarn.nodemanager.remote-app-log-dir-suffix=logs, fs.df.interval=60000, hadoop.util.hash.type=murmur, mapreduce.jobhistory.minicluster.fixed.ports=false, yarn.app.mapreduce.shuffle.log.limit.kb=0, yarn.timeline-service.entity-group-fs-store.done-dir=/tmp/entity-file-history/done/, ha.zookeeper.acl=world:anyone:rwcda, yarn.resourcemanager.delegation.token.max-lifetime=604800000, mapreduce.job.speculative.speculative-cap-running-tasks=0.1, mapreduce.map.sort.spill.percent=0.80, yarn.nodemanager.recovery.supervised=true, file.stream-buffer-size=4096, yarn.resourcemanager.ha.automatic-failover.embedded=true, hive.metastore.uris=thrift://ip-10-63-114-58.corp.stateauto.com:9083, yarn.resourcemanager.nodemanager.minimum.version=NONE, yarn.resourcemanager.history-writer.multi-threaded-dispatcher.pool-size=10, yarn.sharedcache.webapp.address=0.0.0.0:8788, yarn.app.mapreduce.am.resource.mb=15360, mapreduce.framework.name=yarn, mapreduce.job.reduce.slowstart.completedmaps=0.05, yarn.resourcemanager.client.thread-count=64, hadoop.proxyuser.presto.groups=*, mapreduce.cluster.temp.dir=${hadoop.tmp.dir}/mapred/temp, mapreduce.jobhistory.intermediate-done-dir=${yarn.app.mapreduce.am.staging-dir}/history/done_intermediate, fs.s3a.attempts.maximum=20}], FileSystem: [com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c]\r\n19/10/15 03:05:36 INFO HoodieTableConfig: Loading dataset properties from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties\r\n19/10/15 03:05:36 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/hoodie.properties' for reading\r\n19/10/15 03:05:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy\r\n19/10/15 03:05:36 INFO HoodieActiveTimeline: Loaded instants java.util.stream.ReferencePipeline$Head@6b874125\r\n19/10/15 03:05:36 INFO HoodieHiveClient: Creating hive connection jdbc:hive2://ip-10-63-114-58.corp.stateauto.com:10000\r\n19/10/15 03:05:36 INFO Utils: Supplied authorities: ip-10-63-114-58.corp.stateauto.com:10000\r\n19/10/15 03:05:36 INFO Utils: Resolved authority: ip-10-63-114-58.corp.stateauto.com:10000\r\n19/10/15 03:05:36 INFO HiveConnection: Will try to open client transport with JDBC Uri: jdbc:hive2://ip-10-63-114-58.corp.stateauto.com:10000\r\n19/10/15 03:05:37 INFO HoodieHiveClient: Successfully established Hive connection to  jdbc:hive2://ip-10-63-114-58.corp.stateauto.com:10000\r\n19/10/15 03:05:37 INFO metastore: Trying to connect to metastore with URI thrift://ip-10-63-114-58.corp.stateauto.com:9083\r\n19/10/15 03:05:37 INFO metastore: Opened a connection to metastore, current connections: 1\r\n19/10/15 03:05:37 INFO metastore: Connected to metastore.\r\n19/10/15 03:05:37 INFO HiveSyncTool: Trying to sync hoodie table hudi_gwpl_pc_policy with base path s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy of type COPY_ON_WRITE\r\n19/10/15 03:05:37 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/.hoodie/20191015030518.commit' for reading\r\n19/10/15 03:05:37 INFO HoodieHiveClient: Reading schema from s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/2018-07-01/d069dd86-cd5b-44d7-b59a-cffb6afc3b1c-0_0-26-12148_20191015030518.parquet\r\n19/10/15 03:05:37 INFO ContextCleaner: Cleaned accumulator 431\r\n19/10/15 03:05:37 INFO ContextCleaner: Cleaned accumulator 420\r\n19/10/15 03:05:37 INFO ContextCleaner: Cleaned accumulator 401\r\n19/10/15 03:05:37 INFO ContextCleaner: Cleaned accumulator 384\r\n19/10/15 03:05:37 INFO ContextCleaner: Cleaned accumulator 423\r\n19/10/15 03:05:37 INFO ContextCleaner: Cleaned accumulator 462\r\n19/10/15 03:05:37 INFO BlockManagerInfo: Removed broadcast_16_piece0 on ip-10-63-114-58.corp.stateauto.com:43403 in memory (size: 58.5 KB, free: 912.1 MB)\r\n19/10/15 03:05:37 INFO BlockManagerInfo: Removed broadcast_16_piece0 on ip-10-63-114-115.corp.stateauto.com:36209 in memory (size: 58.5 KB, free: 1458.4 MB)\r\n19/10/15 03:05:37 INFO BlockManagerInfo: Removed broadcast_19_piece0 on ip-10-63-114-58.corp.stateauto.com:43403 in memory (size: 2.6 KB, free: 912.1 MB)\r\n19/10/15 03:05:37 INFO BlockManagerInfo: Removed broadcast_19_piece0 on ip-10-63-114-115.corp.stateauto.com:36209 in memory (size: 2.6 KB, free: 1458.4 MB)\r\n19/10/15 03:05:37 INFO ContextCleaner: Cleaned accumulator 400\r\n19/10/15 03:05:37 INFO BlockManagerInfo: Removed broadcast_18_piece0 on ip-10-63-114-58.corp.stateauto.com:43403 in memory (size: 57.4 KB, free: 912.1 MB)\r\n19/10/15 03:05:37 INFO BlockManagerInfo: Removed broadcast_18_piece0 on ip-10-63-114-114.corp.stateauto.com:42747 in memory (size: 57.4 KB, free: 1458.5 MB)\r\n19/10/15 03:05:37 INFO S3NativeFileSystem: Opening 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy/2018-07-01/d069dd86-cd5b-44d7-b59a-cffb6afc3b1c-0_0-26-12148_20191015030518.parquet' for reading\r\n19/10/15 03:05:38 INFO HiveSyncTool: Table hudi_gwpl_pc_policy is not found. Creating it\r\n19/10/15 03:05:38 INFO HoodieHiveClient: Creating table with CREATE EXTERNAL TABLE  IF NOT EXISTS uat_hoodie_staging.hudi_gwpl_pc_policy( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `deleteTime` bigint, `NewProducerCode_Ext` bigint, `DoNotPurge` boolean, `PublicID` string, `PriorPremiums` string, `IssueDate` bigint, `PriorPremiums_cur` int, `MovedPolicySourceAccountID` bigint, `AccountID` bigint, `CreateTime` bigint, `LossHistoryType` int, `ExcludedFromArchive` boolean, `ArchiveState` int, `ArchiveSchemaInfo` bigint, `ArchiveFailureDetailsID` bigint, `PackageRisk` int, `NumPriorLosses` int, `UpdateTime` bigint, `PrimaryLanguage` int, `DoNotArchive` boolean, `ID` bigint, `PrimaryLocale` int, `ProductCode` string, `ExcludeReason` string, `CreateUserID` bigint, `ArchiveFailureID` bigint, `OriginalEffectiveDate` bigint, `BeanVersion` int, `ArchivePartition` bigint, `Retired` bigint, `LossHistoryType_Ext` int, `UpdateUserID` bigint, `PriorTotalIncurred` string, `ArchiveDate` bigint, `PriorTotalIncurred_cur` int, `ProducerCodeOfServiceID` bigint, `UL_BOPEligibility_Ext` boolean, `isDmvReported` boolean, `ClueStatusExt` boolean, `LossHistoryTypeComm_Ext` int, `ClueStatusDetail` bigint, `uniqueId` string, `pctl_archivestate_typecode` string, `pctl_archivestate_name` string, `pctl_archivestate_description` string, `pctl_losshistorytype_typecode2` string, `pctl_losshistorytype_name2` string, `pctl_losshistorytype_description2` string, `pctl_losshistorytype_typecode1` string, `pctl_losshistorytype_name1` string, `pctl_losshistorytype_description1` string, `pctl_losshistorytype_ext_typecode` string, `pctl_losshistorytype_ext_name` string, `pctl_losshistorytype_ext_description` string, `pctl_packagerisk_typecode` string, `pctl_packagerisk_name` string, `pctl_packagerisk_description` string, `pctl_languagetype_typecode` string, `pctl_languagetype_name` string, `pctl_languagetype_description` string, `pctl_localetype_typecode` string, `pctl_localetype_name` string, `pctl_localetype_description` string, `pctl_currency_typecode1` string, `pctl_currency_name1` string, `pctl_currency_description1` string, `pctl_currency_typecode2` string, `pctl_currency_name2` string, `pctl_currency_description2` string, `ingestiondt` string) PARTITIONED BY (`batch` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy'\r\n19/10/15 03:05:38 INFO HoodieHiveClient: Executing SQL CREATE EXTERNAL TABLE  IF NOT EXISTS uat_hoodie_staging.hudi_gwpl_pc_policy( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `deleteTime` bigint, `NewProducerCode_Ext` bigint, `DoNotPurge` boolean, `PublicID` string, `PriorPremiums` string, `IssueDate` bigint, `PriorPremiums_cur` int, `MovedPolicySourceAccountID` bigint, `AccountID` bigint, `CreateTime` bigint, `LossHistoryType` int, `ExcludedFromArchive` boolean, `ArchiveState` int, `ArchiveSchemaInfo` bigint, `ArchiveFailureDetailsID` bigint, `PackageRisk` int, `NumPriorLosses` int, `UpdateTime` bigint, `PrimaryLanguage` int, `DoNotArchive` boolean, `ID` bigint, `PrimaryLocale` int, `ProductCode` string, `ExcludeReason` string, `CreateUserID` bigint, `ArchiveFailureID` bigint, `OriginalEffectiveDate` bigint, `BeanVersion` int, `ArchivePartition` bigint, `Retired` bigint, `LossHistoryType_Ext` int, `UpdateUserID` bigint, `PriorTotalIncurred` string, `ArchiveDate` bigint, `PriorTotalIncurred_cur` int, `ProducerCodeOfServiceID` bigint, `UL_BOPEligibility_Ext` boolean, `isDmvReported` boolean, `ClueStatusExt` boolean, `LossHistoryTypeComm_Ext` int, `ClueStatusDetail` bigint, `uniqueId` string, `pctl_archivestate_typecode` string, `pctl_archivestate_name` string, `pctl_archivestate_description` string, `pctl_losshistorytype_typecode2` string, `pctl_losshistorytype_name2` string, `pctl_losshistorytype_description2` string, `pctl_losshistorytype_typecode1` string, `pctl_losshistorytype_name1` string, `pctl_losshistorytype_description1` string, `pctl_losshistorytype_ext_typecode` string, `pctl_losshistorytype_ext_name` string, `pctl_losshistorytype_ext_description` string, `pctl_packagerisk_typecode` string, `pctl_packagerisk_name` string, `pctl_packagerisk_description` string, `pctl_languagetype_typecode` string, `pctl_languagetype_name` string, `pctl_languagetype_description` string, `pctl_localetype_typecode` string, `pctl_localetype_name` string, `pctl_localetype_description` string, `pctl_currency_typecode1` string, `pctl_currency_name1` string, `pctl_currency_description1` string, `pctl_currency_typecode2` string, `pctl_currency_name2` string, `pctl_currency_description2` string, `ingestiondt` string) PARTITIONED BY (`batch` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy'\r\n19/10/15 03:05:41 INFO HiveSyncTool: Schema sync complete. Syncing partitions for hudi_gwpl_pc_policy\r\n19/10/15 03:05:41 INFO HiveSyncTool: Last commit time synced was found to be null\r\n19/10/15 03:05:41 INFO HoodieHiveClient: Last commit time synced is not known, listing all partitions in s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy,FS :com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c\r\n19/10/15 03:05:41 INFO HiveSyncTool: Storage partitions scan complete. Found 1\r\norg.apache.hudi.hive.HoodieHiveSyncException: Failed to sync partitions for table hudi_gwpl_pc_policy\r\n  at org.apache.hudi.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:172)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:107)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:67)\r\n  at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:235)\r\n  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:169)\r\n  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\r\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\r\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\r\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\r\n  ... 70 elided\r\nCaused by: org.apache.hudi.org.apache.hadoop_hive.metastore.api.NoSuchObjectException: uat_hoodie_staging.hudi_gwpl_pc_policy table not found\r\n  at org.apache.hudi.org.apache.hadoop_hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.read(ThriftHiveMetastore.java)\r\n  at org.apache.hudi.org.apache.hadoop_hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.read(ThriftHiveMetastore.java)\r\n  at org.apache.hudi.org.apache.hadoop_hive.metastore.api.ThriftHiveMetastore$get_partitions_result.read(ThriftHiveMetastore.java)\r\n  at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86)\r\n  at org.apache.hudi.org.apache.hadoop_hive.metastore.api.ThriftHiveMetastore$Client.recv_get_partitions(ThriftHiveMetastore.java:2377)\r\n  at org.apache.hudi.org.apache.hadoop_hive.metastore.api.ThriftHiveMetastore$Client.get_partitions(ThriftHiveMetastore.java:2362)\r\n  at org.apache.hudi.org.apache.hadoop_hive.metastore.HiveMetaStoreClient.listPartitions(HiveMetaStoreClient.java:1162)\r\n  at org.apache.hudi.hive.HoodieHiveClient.scanTablePartitions(HoodieHiveClient.java:240)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:162)\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542017798/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542280295","html_url":"https://github.com/apache/hudi/pull/949#issuecomment-542280295","issue_url":"https://api.github.com/repos/apache/hudi/issues/949","id":542280295,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjI4MDI5NQ==","user":{"login":"garyli1019","id":23007841,"node_id":"MDQ6VXNlcjIzMDA3ODQx","avatar_url":"https://avatars.githubusercontent.com/u/23007841?v=4","gravatar_id":"","url":"https://api.github.com/users/garyli1019","html_url":"https://github.com/garyli1019","followers_url":"https://api.github.com/users/garyli1019/followers","following_url":"https://api.github.com/users/garyli1019/following{/other_user}","gists_url":"https://api.github.com/users/garyli1019/gists{/gist_id}","starred_url":"https://api.github.com/users/garyli1019/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/garyli1019/subscriptions","organizations_url":"https://api.github.com/users/garyli1019/orgs","repos_url":"https://api.github.com/users/garyli1019/repos","events_url":"https://api.github.com/users/garyli1019/events{/privacy}","received_events_url":"https://api.github.com/users/garyli1019/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T15:49:55Z","updated_at":"2019-10-15T15:49:55Z","author_association":"MEMBER","body":"@leesf @vinothchandar Please review. Thanks!","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542280295/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542298488","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-542298488","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":542298488,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjI5ODQ4OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T16:31:09Z","updated_at":"2019-10-15T16:31:09Z","author_association":"MEMBER","body":"What I see is that it created the table,\r\n\r\n```\r\n19/10/15 03:05:38 INFO HiveSyncTool: Table hudi_gwpl_pc_policy is not found. Creating it\r\n19/10/15 03:05:38 INFO HoodieHiveClient: Creating table with CREATE EXTERNAL TABLE  IF NOT EXISTS uat_hoodie_staging.hudi_gwpl_pc_policy( `_hoodie_commit_time` string, `_hoodie_commit_seqno` string, `_hoodie_record_key` string, `_hoodie_partition_path` string, `_hoodie_file_name` string, `deleteTime` bigint, `NewProducerCode_Ext` bigint, `DoNotPurge` boolean, `PublicID` string, `PriorPremiums` string, `IssueDate` bigint, `PriorPremiums_cur` int, `MovedPolicySourceAccountID` bigint, `AccountID` bigint, `CreateTime` bigint, `LossHistoryType` int, `ExcludedFromArchive` boolean, `ArchiveState` int, `ArchiveSchemaInfo` bigint, `ArchiveFailureDetailsID` bigint, `PackageRisk` int, `NumPriorLosses` int, `UpdateTime` bigint, `PrimaryLanguage` int, `DoNotArchive` boolean, `ID` bigint, `PrimaryLocale` int, `ProductCode` string, `ExcludeReason` string, `CreateUserID` bigint, `ArchiveFailureID` bigint, `OriginalEffectiveDate` bigint, `BeanVersion` int, `ArchivePartition` bigint, `Retired` bigint, `LossHistoryType_Ext` int, `UpdateUserID` bigint, `PriorTotalIncurred` string, `ArchiveDate` bigint, `PriorTotalIncurred_cur` int, `ProducerCodeOfServiceID` bigint, `UL_BOPEligibility_Ext` boolean, `isDmvReported` boolean, `ClueStatusExt` boolean, `LossHistoryTypeComm_Ext` int, `ClueStatusDetail` bigint, `uniqueId` string, `pctl_archivestate_typecode` string, `pctl_archivestate_name` string, `pctl_archivestate_description` string, `pctl_losshistorytype_typecode2` string, `pctl_losshistorytype_name2` string, `pctl_losshistorytype_description2` string, `pctl_losshistorytype_typecode1` string, `pctl_losshistorytype_name1` string, `pctl_losshistorytype_description1` string, `pctl_losshistorytype_ext_typecode` string, `pctl_losshistorytype_ext_name` string, `pctl_losshistorytype_ext_description` string, `pctl_packagerisk_typecode` string, `pctl_packagerisk_name` string, `pctl_packagerisk_description` string, `pctl_languagetype_typecode` string, `pctl_languagetype_name` string, `pctl_languagetype_description` string, `pctl_localetype_typecode` string, `pctl_localetype_name` string, `pctl_localetype_description` string, `pctl_currency_typecode1` string, `pctl_currency_name1` string, `pctl_currency_description1` string, `pctl_currency_typecode2` string, `pctl_currency_name2` string, `pctl_currency_description2` string, `ingestiondt` string) PARTITIONED BY (`batch` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hudi.hadoop.HoodieParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 's3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy'\r\n```\r\n\r\n Found the 1 storage partition (i.e from your data on s3) to sync\r\n\r\n```\r\n19/10/15 03:05:41 INFO HiveSyncTool: Schema sync complete. Syncing partitions for hudi_gwpl_pc_policy\r\n19/10/15 03:05:41 INFO HiveSyncTool: Last commit time synced was found to be null\r\n19/10/15 03:05:41 INFO HoodieHiveClient: Last commit time synced is not known, listing all partitions in s3://sa-l3-uat-emr-edl-processed/staging/hoodie/pc_policy,FS :com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem@36c6ad3c\r\n19/10/15 03:05:41 INFO HiveSyncTool: Storage partitions scan complete. Found 1\r\n```\r\n\r\nBut cannot find the table when trying to sync them\r\n\r\n```\r\norg.apache.hudi.hive.HoodieHiveSyncException: Failed to sync partitions for table hudi_gwpl_pc_policy\r\n  at org.apache.hudi.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:172)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:107)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:67)\r\n...\r\nCaused by: org.apache.hudi.org.apache.hadoop_hive.metastore.api.NoSuchObjectException: uat_hoodie_staging.hudi_gwpl_pc_policy table not found\r\n...\r\n at org.apache.hudi.org.apache.hadoop_hive.metastore.HiveMetaStoreClient.listPartitions(HiveMetaStoreClient.java:1162)\r\n  at org.apache.hudi.hive.HoodieHiveClient.scanTablePartitions(HoodieHiveClient.java:240)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:162)\r\n```\r\n\r\nIt almost seems like your metastore is not providing read-after-write consistency? what is the Hive metastore backed by, s3?  I am guessing glue catalog is different from Hive metastore? Could you give it a shot on EMR with Hive metastore?\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542298488/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542298659","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-542298659","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":542298659,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjI5ODY1OQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T16:31:35Z","updated_at":"2019-10-15T16:31:35Z","author_association":"MEMBER","body":"@umehrot2 any ideas, top of your heaD?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542298659/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542304630","html_url":"https://github.com/apache/hudi/pull/949#issuecomment-542304630","issue_url":"https://api.github.com/repos/apache/hudi/issues/949","id":542304630,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjMwNDYzMA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T16:46:45Z","updated_at":"2019-10-15T16:46:45Z","author_association":"MEMBER","body":"@leesf want to take the lead on this one? :) ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542304630/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542318445","html_url":"https://github.com/apache/hudi/pull/952#issuecomment-542318445","issue_url":"https://api.github.com/repos/apache/hudi/issues/952","id":542318445,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjMxODQ0NQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T17:21:53Z","updated_at":"2019-10-15T17:21:53Z","author_association":"MEMBER","body":"For this PR, TestMergeOnTable is a good starting point..  \r\n\r\nWe already have a  `ITTTestHoodieSanity::testRunHoodieJavaAppOnNonPartitionedCOWTable`. we could just add a MOR equivalent there.. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542318445/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542373693","html_url":"https://github.com/apache/hudi/pull/958#issuecomment-542373693","issue_url":"https://api.github.com/repos/apache/hudi/issues/958","id":542373693,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjM3MzY5Mw==","user":{"login":"yihua","id":2497195,"node_id":"MDQ6VXNlcjI0OTcxOTU=","avatar_url":"https://avatars.githubusercontent.com/u/2497195?v=4","gravatar_id":"","url":"https://api.github.com/users/yihua","html_url":"https://github.com/yihua","followers_url":"https://api.github.com/users/yihua/followers","following_url":"https://api.github.com/users/yihua/following{/other_user}","gists_url":"https://api.github.com/users/yihua/gists{/gist_id}","starred_url":"https://api.github.com/users/yihua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yihua/subscriptions","organizations_url":"https://api.github.com/users/yihua/orgs","repos_url":"https://api.github.com/users/yihua/repos","events_url":"https://api.github.com/users/yihua/events{/privacy}","received_events_url":"https://api.github.com/users/yihua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T19:43:30Z","updated_at":"2019-10-15T19:43:30Z","author_association":"CONTRIBUTOR","body":"@leesf @yanghua PTAL when you have time.  Thanks!","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542373693/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542378709","html_url":"https://github.com/apache/hudi/pull/957#issuecomment-542378709","issue_url":"https://api.github.com/repos/apache/hudi/issues/957","id":542378709,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjM3ODcwOQ==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T19:57:12Z","updated_at":"2019-10-15T19:57:12Z","author_association":"CONTRIBUTOR","body":"> Just 1 comment. LGTM otherwise. Should we also doc this somewhere?\r\n\r\nI don't see any build instructions in the `README.md` file. I think we should be adding `build instructions` possibly in a separate PR, and as part of that add a sub-section for building Hudi for AWS EMR. What do you think ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542378709/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542384197","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-542384197","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":542384197,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjM4NDE5Nw==","user":{"login":"gfn9cho","id":14010421,"node_id":"MDQ6VXNlcjE0MDEwNDIx","avatar_url":"https://avatars.githubusercontent.com/u/14010421?v=4","gravatar_id":"","url":"https://api.github.com/users/gfn9cho","html_url":"https://github.com/gfn9cho","followers_url":"https://api.github.com/users/gfn9cho/followers","following_url":"https://api.github.com/users/gfn9cho/following{/other_user}","gists_url":"https://api.github.com/users/gfn9cho/gists{/gist_id}","starred_url":"https://api.github.com/users/gfn9cho/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gfn9cho/subscriptions","organizations_url":"https://api.github.com/users/gfn9cho/orgs","repos_url":"https://api.github.com/users/gfn9cho/repos","events_url":"https://api.github.com/users/gfn9cho/events{/privacy}","received_events_url":"https://api.github.com/users/gfn9cho/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T20:11:50Z","updated_at":"2019-10-15T20:11:50Z","author_association":"NONE","body":"We are using AWS Gluecatalog as the external hive metastore. The regular spark job is able to create, write & read the tables.\r\nI guess, HiveSyncTool is not able to read from the glue catalog. as everytime, we run the job its creating the table again as it is not found. \r\nIs there a chance that HiveSyncTool is overriding the config and thus its not able to see the hive metastore as a external one.\r\n19/10/15 03:05:38 INFO HiveSyncTool: Table hudi_gwpl_pc_policy is not found. Creating it.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542384197/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542396806","html_url":"https://github.com/apache/hudi/pull/956#issuecomment-542396806","issue_url":"https://api.github.com/repos/apache/hudi/issues/956","id":542396806,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjM5NjgwNg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T20:45:07Z","updated_at":"2019-10-15T20:45:07Z","author_association":"MEMBER","body":"Can merge once you repush with nits. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542396806/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542446001","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-542446001","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":542446001,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjQ0NjAwMQ==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-15T23:28:16Z","updated_at":"2019-10-15T23:28:16Z","author_association":"CONTRIBUTOR","body":"@gfn9cho @vinothchandar we are aware of this issue in Hudi. It does not currently work with glue-catalog. We have solved it on our side, and will be pushing a PR soon.\r\n\r\nFor your information, here is explanation:\r\n- Currently Hudi is interacting with Hive through two different ways:\r\n    - Creation of table statement is submitted directly to Hive via JDBC https://github.com/apache/incubator-hudi/blob/master/hudi-hive/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java#L472 . Thus, Hive will internally create the right metastore client (i.e. Glue if `hive.metastore.client.factory.class` is set to `com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory` in hive-site)\r\n    - Whereas partition listing among other things are being done by directly calling hive metastore APIs using hive metastore client: https://github.com/apache/incubator-hudi/blob/master/hudi-hive/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java#L240\r\n    - Now in Hudi code, standard specific implementation of the metastore client (not glue metastore client) is being instantiated: https://github.com/apache/incubator-hudi/blob/master/hudi-hive/src/main/java/org/apache/hudi/hive/HoodieHiveClient.java#L109 .\r\n    - Ideally this instantiation of metastore client should be left to Hive through https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java#L5045 for it to consider other implementations of metastore client that might be configured.\r\n\r\nThat is why your table gets created in Glue metastore, but while reading or scanning partitions it is talking to the local hive metastore where it does not find the table created.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542446001/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542457908","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-542457908","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":542457908,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjQ1NzkwOA==","user":{"login":"gfn9cho","id":14010421,"node_id":"MDQ6VXNlcjE0MDEwNDIx","avatar_url":"https://avatars.githubusercontent.com/u/14010421?v=4","gravatar_id":"","url":"https://api.github.com/users/gfn9cho","html_url":"https://github.com/gfn9cho","followers_url":"https://api.github.com/users/gfn9cho/followers","following_url":"https://api.github.com/users/gfn9cho/following{/other_user}","gists_url":"https://api.github.com/users/gfn9cho/gists{/gist_id}","starred_url":"https://api.github.com/users/gfn9cho/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gfn9cho/subscriptions","organizations_url":"https://api.github.com/users/gfn9cho/orgs","repos_url":"https://api.github.com/users/gfn9cho/repos","events_url":"https://api.github.com/users/gfn9cho/events{/privacy}","received_events_url":"https://api.github.com/users/gfn9cho/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-16T00:24:46Z","updated_at":"2019-10-16T00:24:46Z","author_association":"NONE","body":"@vinothchandar @umehrot2 , Thanks for the support and a great explanation.\r\nI will wait for the updated code to take it forward.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542457908/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542469521","html_url":"https://github.com/apache/hudi/pull/949#issuecomment-542469521","issue_url":"https://api.github.com/repos/apache/hudi/issues/949","id":542469521,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjQ2OTUyMQ==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-16T01:22:16Z","updated_at":"2019-10-16T01:22:16Z","author_association":"CONTRIBUTOR","body":"> @leesf want to take the lead on this one? :)\r\n\r\nSure, i will review when get a chance.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542469521/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542473467","html_url":"https://github.com/apache/hudi/pull/958#issuecomment-542473467","issue_url":"https://api.github.com/repos/apache/hudi/issues/958","id":542473467,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjQ3MzQ2Nw==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-16T01:43:14Z","updated_at":"2019-10-16T01:43:14Z","author_association":"CONTRIBUTOR","body":"Thanks for opening the PR @yihua . The changes look good overall. Only left some minor comments.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542473467/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542510419","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-542510419","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":542510419,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjUxMDQxOQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-16T04:39:59Z","updated_at":"2019-10-16T04:39:59Z","author_association":"MEMBER","body":"Thanks for excellent explanation! much appreciated @umehrot2 ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542510419/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542581805","html_url":"https://github.com/apache/hudi/pull/949#issuecomment-542581805","issue_url":"https://api.github.com/repos/apache/hudi/issues/949","id":542581805,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjU4MTgwNQ==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-16T08:14:44Z","updated_at":"2019-10-16T08:14:44Z","author_association":"CONTRIBUTOR","body":"Thanks for the PR @garyli1019 ! The changes look good overall, only left some minor comments.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542581805/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542604888","html_url":"https://github.com/apache/hudi/pull/952#issuecomment-542604888","issue_url":"https://api.github.com/repos/apache/hudi/issues/952","id":542604888,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MjYwNDg4OA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-16T09:07:22Z","updated_at":"2019-10-16T09:07:22Z","author_association":"CONTRIBUTOR","body":"@zhedoubushishi : This code change looks good. As I am about to cut a release candidate and we think that this bug fix will be a useful one to have in the release, we are going to merge this. Can you please create a new PR for adding the unit-test. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542604888/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542797756","html_url":"https://github.com/apache/hudi/pull/952#issuecomment-542797756","issue_url":"https://api.github.com/repos/apache/hudi/issues/952","id":542797756,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mjc5Nzc1Ng==","user":{"login":"zhedoubushishi","id":31263084,"node_id":"MDQ6VXNlcjMxMjYzMDg0","avatar_url":"https://avatars.githubusercontent.com/u/31263084?v=4","gravatar_id":"","url":"https://api.github.com/users/zhedoubushishi","html_url":"https://github.com/zhedoubushishi","followers_url":"https://api.github.com/users/zhedoubushishi/followers","following_url":"https://api.github.com/users/zhedoubushishi/following{/other_user}","gists_url":"https://api.github.com/users/zhedoubushishi/gists{/gist_id}","starred_url":"https://api.github.com/users/zhedoubushishi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhedoubushishi/subscriptions","organizations_url":"https://api.github.com/users/zhedoubushishi/orgs","repos_url":"https://api.github.com/users/zhedoubushishi/repos","events_url":"https://api.github.com/users/zhedoubushishi/events{/privacy}","received_events_url":"https://api.github.com/users/zhedoubushishi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-16T17:02:13Z","updated_at":"2019-10-16T17:02:13Z","author_association":"CONTRIBUTOR","body":"> @zhedoubushishi : This code change looks good. As I am about to cut a release candidate and we think that this bug fix will be a useful one to have in the release, we are going to merge this. Can you please create a new PR for adding the unit-test.\r\n\r\nGreat! I am working on the unit test with @vinothchandar 's suggestion. Once done I will send another PR.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542797756/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542881000","html_url":"https://github.com/apache/hudi/pull/949#issuecomment-542881000","issue_url":"https://api.github.com/repos/apache/hudi/issues/949","id":542881000,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mjg4MTAwMA==","user":{"login":"garyli1019","id":23007841,"node_id":"MDQ6VXNlcjIzMDA3ODQx","avatar_url":"https://avatars.githubusercontent.com/u/23007841?v=4","gravatar_id":"","url":"https://api.github.com/users/garyli1019","html_url":"https://github.com/garyli1019","followers_url":"https://api.github.com/users/garyli1019/followers","following_url":"https://api.github.com/users/garyli1019/following{/other_user}","gists_url":"https://api.github.com/users/garyli1019/gists{/gist_id}","starred_url":"https://api.github.com/users/garyli1019/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/garyli1019/subscriptions","organizations_url":"https://api.github.com/users/garyli1019/orgs","repos_url":"https://api.github.com/users/garyli1019/repos","events_url":"https://api.github.com/users/garyli1019/events{/privacy}","received_events_url":"https://api.github.com/users/garyli1019/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-16T20:37:05Z","updated_at":"2019-10-16T20:37:05Z","author_association":"MEMBER","body":"@leesf Thanks for the detailed reviewing! Most comments are addressed and left a comment about the assertion. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542881000/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542954214","html_url":"https://github.com/apache/hudi/pull/949#issuecomment-542954214","issue_url":"https://api.github.com/repos/apache/hudi/issues/949","id":542954214,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mjk1NDIxNA==","user":{"login":"garyli1019","id":23007841,"node_id":"MDQ6VXNlcjIzMDA3ODQx","avatar_url":"https://avatars.githubusercontent.com/u/23007841?v=4","gravatar_id":"","url":"https://api.github.com/users/garyli1019","html_url":"https://github.com/garyli1019","followers_url":"https://api.github.com/users/garyli1019/followers","following_url":"https://api.github.com/users/garyli1019/following{/other_user}","gists_url":"https://api.github.com/users/garyli1019/gists{/gist_id}","starred_url":"https://api.github.com/users/garyli1019/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/garyli1019/subscriptions","organizations_url":"https://api.github.com/users/garyli1019/orgs","repos_url":"https://api.github.com/users/garyli1019/repos","events_url":"https://api.github.com/users/garyli1019/events{/privacy}","received_events_url":"https://api.github.com/users/garyli1019/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T01:14:27Z","updated_at":"2019-10-17T01:14:27Z","author_association":"MEMBER","body":"all comments are addressed. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542954214/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542961795","html_url":"https://github.com/apache/hudi/pull/949#issuecomment-542961795","issue_url":"https://api.github.com/repos/apache/hudi/issues/949","id":542961795,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mjk2MTc5NQ==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T01:54:32Z","updated_at":"2019-10-17T01:54:32Z","author_association":"CONTRIBUTOR","body":"Thanks for addressing my comments @garyli1019 ! Looks pretty good. \r\nNow we could merge this PR if there is no other concern. @vinothchandar ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542961795/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542991423","html_url":"https://github.com/apache/hudi/pull/949#issuecomment-542991423","issue_url":"https://api.github.com/repos/apache/hudi/issues/949","id":542991423,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mjk5MTQyMw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T04:11:46Z","updated_at":"2019-10-17T04:11:46Z","author_association":"MEMBER","body":"Great work! lgtm as well /","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542991423/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542997840","html_url":"https://github.com/apache/hudi/pull/958#issuecomment-542997840","issue_url":"https://api.github.com/repos/apache/hudi/issues/958","id":542997840,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mjk5Nzg0MA==","user":{"login":"yihua","id":2497195,"node_id":"MDQ6VXNlcjI0OTcxOTU=","avatar_url":"https://avatars.githubusercontent.com/u/2497195?v=4","gravatar_id":"","url":"https://api.github.com/users/yihua","html_url":"https://github.com/yihua","followers_url":"https://api.github.com/users/yihua/followers","following_url":"https://api.github.com/users/yihua/following{/other_user}","gists_url":"https://api.github.com/users/yihua/gists{/gist_id}","starred_url":"https://api.github.com/users/yihua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yihua/subscriptions","organizations_url":"https://api.github.com/users/yihua/orgs","repos_url":"https://api.github.com/users/yihua/repos","events_url":"https://api.github.com/users/yihua/events{/privacy}","received_events_url":"https://api.github.com/users/yihua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T04:40:11Z","updated_at":"2019-10-17T04:40:11Z","author_association":"CONTRIBUTOR","body":"@leesf Thanks for your meticulous review.  I addressed your comments in my latest commit.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/542997840/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543128944","html_url":"https://github.com/apache/hudi/pull/959#issuecomment-543128944","issue_url":"https://api.github.com/repos/apache/hudi/issues/959","id":543128944,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzEyODk0NA==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T11:23:03Z","updated_at":"2019-10-17T11:23:03Z","author_association":"CONTRIBUTOR","body":"Thanks for the detailed reviewing @yihua ! Updated the PR to address your comments.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543128944/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543150175","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-543150175","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":543150175,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzE1MDE3NQ==","user":{"login":"smdahmed","id":6824174,"node_id":"MDQ6VXNlcjY4MjQxNzQ=","avatar_url":"https://avatars.githubusercontent.com/u/6824174?v=4","gravatar_id":"","url":"https://api.github.com/users/smdahmed","html_url":"https://github.com/smdahmed","followers_url":"https://api.github.com/users/smdahmed/followers","following_url":"https://api.github.com/users/smdahmed/following{/other_user}","gists_url":"https://api.github.com/users/smdahmed/gists{/gist_id}","starred_url":"https://api.github.com/users/smdahmed/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smdahmed/subscriptions","organizations_url":"https://api.github.com/users/smdahmed/orgs","repos_url":"https://api.github.com/users/smdahmed/repos","events_url":"https://api.github.com/users/smdahmed/events{/privacy}","received_events_url":"https://api.github.com/users/smdahmed/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T12:25:57Z","updated_at":"2019-10-17T12:26:14Z","author_association":"NONE","body":"I can confirm that I have been hit by this error. The occurrence of this error is in 2 tables that do not have any similarity in the pattern of data ingestion or sizes of the data they hold. \r\n\r\nAs @n3nash has mentioned there are clean file of size 0 bytes. Deleting the file manually clears the problem to the table (albeit with the first attempt failing and things working from the 2nd attempt onwards). \r\n\r\nI have tried to reproduce the issue without any luck so far. I still continue to pursue to replicate the issue. If anyone else especially @jackwang2 knows what caused this and if he has fixed it in his pipeline, I would be very grateful. Any insights are hugely welcome. \r\n\r\nThe stack trace is as below: \r\n\r\n> Exception in thread \"main\" com.uber.hoodie.exception.HoodieCommitException: Failed to archive commits\r\n> Caused by: java.io.IOException: Not an Avro data file\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543150175/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543239285","html_url":"https://github.com/apache/hudi/pull/958#issuecomment-543239285","issue_url":"https://api.github.com/repos/apache/hudi/issues/958","id":543239285,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzIzOTI4NQ==","user":{"login":"yihua","id":2497195,"node_id":"MDQ6VXNlcjI0OTcxOTU=","avatar_url":"https://avatars.githubusercontent.com/u/2497195?v=4","gravatar_id":"","url":"https://api.github.com/users/yihua","html_url":"https://github.com/yihua","followers_url":"https://api.github.com/users/yihua/followers","following_url":"https://api.github.com/users/yihua/following{/other_user}","gists_url":"https://api.github.com/users/yihua/gists{/gist_id}","starred_url":"https://api.github.com/users/yihua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yihua/subscriptions","organizations_url":"https://api.github.com/users/yihua/orgs","repos_url":"https://api.github.com/users/yihua/repos","events_url":"https://api.github.com/users/yihua/events{/privacy}","received_events_url":"https://api.github.com/users/yihua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T15:50:02Z","updated_at":"2019-10-17T15:50:02Z","author_association":"CONTRIBUTOR","body":"@vinothchandar this PR is ready to be merged.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543239285/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543261798","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-543261798","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":543261798,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzI2MTc5OA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T16:43:53Z","updated_at":"2019-10-17T16:43:53Z","author_association":"CONTRIBUTOR","body":"@smdahmed : Looked at the code to see how this can happen. Not clear how this can happen. Assuming you are using S3, Have you tried setting the consistency guard ( https://hudi.apache.org/configurations.html#withConsistencyCheckEnabled) ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543261798/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543324985","html_url":"https://github.com/apache/hudi/issues/547#issuecomment-543324985","issue_url":"https://api.github.com/repos/apache/hudi/issues/547","id":543324985,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzMyNDk4NQ==","user":{"login":"arw357","id":8440059,"node_id":"MDQ6VXNlcjg0NDAwNTk=","avatar_url":"https://avatars.githubusercontent.com/u/8440059?v=4","gravatar_id":"","url":"https://api.github.com/users/arw357","html_url":"https://github.com/arw357","followers_url":"https://api.github.com/users/arw357/followers","following_url":"https://api.github.com/users/arw357/following{/other_user}","gists_url":"https://api.github.com/users/arw357/gists{/gist_id}","starred_url":"https://api.github.com/users/arw357/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arw357/subscriptions","organizations_url":"https://api.github.com/users/arw357/orgs","repos_url":"https://api.github.com/users/arw357/repos","events_url":"https://api.github.com/users/arw357/events{/privacy}","received_events_url":"https://api.github.com/users/arw357/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T19:25:36Z","updated_at":"2019-10-17T19:25:36Z","author_association":"NONE","body":"@vinothchandar  made this https://github.com/apache/incubator-hudi/pull/960","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543324985/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543333552","html_url":"https://github.com/apache/hudi/issues/547#issuecomment-543333552","issue_url":"https://api.github.com/repos/apache/hudi/issues/547","id":543333552,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzMzMzU1Mg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T19:48:29Z","updated_at":"2019-10-17T19:48:29Z","author_association":"CONTRIBUTOR","body":"@arw357 : ACK. I will review this code.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543333552/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543384278","html_url":"https://github.com/apache/hudi/pull/960#issuecomment-543384278","issue_url":"https://api.github.com/repos/apache/hudi/issues/960","id":543384278,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzM4NDI3OA==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-17T22:16:05Z","updated_at":"2019-10-17T22:16:05Z","author_association":"CONTRIBUTOR","body":"Thanks for opening the PR @arw357 . It would be better if you could file a jira ticket to track this and change the title to [HUDI-$jira-number]  Adding type test for timestamp,date & decimal\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543384278/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543612170","html_url":"https://github.com/apache/hudi/pull/960#issuecomment-543612170","issue_url":"https://api.github.com/repos/apache/hudi/issues/960","id":543612170,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzYxMjE3MA==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-18T08:57:07Z","updated_at":"2019-10-18T08:57:07Z","author_association":"CONTRIBUTOR","body":"@arw357 Also please fix travis errors. one is `DataSourceTest.testCopyOnWriteExtraTypesStorage:103 expected:<[Date]Type> but was:<[Long]Type>`, another is `method generateInsertsStream in class org.apache.hudi.common.HoodieTestDataGenerator cannot be applied to given types`. Thanks","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543612170/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543782446","html_url":"https://github.com/apache/hudi/pull/960#issuecomment-543782446","issue_url":"https://api.github.com/repos/apache/hudi/issues/960","id":543782446,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mzc4MjQ0Ng==","user":{"login":"arw357","id":8440059,"node_id":"MDQ6VXNlcjg0NDAwNTk=","avatar_url":"https://avatars.githubusercontent.com/u/8440059?v=4","gravatar_id":"","url":"https://api.github.com/users/arw357","html_url":"https://github.com/arw357","followers_url":"https://api.github.com/users/arw357/followers","following_url":"https://api.github.com/users/arw357/following{/other_user}","gists_url":"https://api.github.com/users/arw357/gists{/gist_id}","starred_url":"https://api.github.com/users/arw357/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arw357/subscriptions","organizations_url":"https://api.github.com/users/arw357/orgs","repos_url":"https://api.github.com/users/arw357/repos","events_url":"https://api.github.com/users/arw357/events{/privacy}","received_events_url":"https://api.github.com/users/arw357/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-18T14:51:58Z","updated_at":"2019-10-18T14:52:10Z","author_association":"NONE","body":"> @arw357 Also please fix travis errors. one is `DataSourceTest.testCopyOnWriteExtraTypesStorage:103 expected:<[Date]Type> but was:<[Long]Type>`, another is `method generateInsertsStream in class org.apache.hudi.common.HoodieTestDataGenerator cannot be applied to given types`. Thanks\r\n\r\nThe test is supposed to fail since Hudi does not support this yet.  This is to help with the implementation of allowing these other types","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543782446/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543785072","html_url":"https://github.com/apache/hudi/pull/960#issuecomment-543785072","issue_url":"https://api.github.com/repos/apache/hudi/issues/960","id":543785072,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mzc4NTA3Mg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-18T14:58:06Z","updated_at":"2019-10-18T14:58:06Z","author_association":"CONTRIBUTOR","body":"@arw357 : Thanks a lot for the contribution. Looks like the PR has changes only on test-classes. While this is super useful, can you also give it a shot to fix the \"newly added\" failing test. I am not sure if that was the plan from your side. We would need the master to be green with zero test failures and as such this cannot be merged without tests passing.  Let us know how you are planning to proceed.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543785072/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543786796","html_url":"https://github.com/apache/hudi/pull/960#issuecomment-543786796","issue_url":"https://api.github.com/repos/apache/hudi/issues/960","id":543786796,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mzc4Njc5Ng==","user":{"login":"arw357","id":8440059,"node_id":"MDQ6VXNlcjg0NDAwNTk=","avatar_url":"https://avatars.githubusercontent.com/u/8440059?v=4","gravatar_id":"","url":"https://api.github.com/users/arw357","html_url":"https://github.com/arw357","followers_url":"https://api.github.com/users/arw357/followers","following_url":"https://api.github.com/users/arw357/following{/other_user}","gists_url":"https://api.github.com/users/arw357/gists{/gist_id}","starred_url":"https://api.github.com/users/arw357/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arw357/subscriptions","organizations_url":"https://api.github.com/users/arw357/orgs","repos_url":"https://api.github.com/users/arw357/repos","events_url":"https://api.github.com/users/arw357/events{/privacy}","received_events_url":"https://api.github.com/users/arw357/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-18T15:02:00Z","updated_at":"2019-10-18T15:02:00Z","author_association":"NONE","body":"I was planning to do just a test. Due to this message : \r\n[this](https://github.com/apache/incubator-hudi/issues/547#issuecomment-524044375)\r\nI'll have a look for the implementation as well.\r\nShould I close this until then  ?  ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543786796/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543791715","html_url":"https://github.com/apache/hudi/pull/960#issuecomment-543791715","issue_url":"https://api.github.com/repos/apache/hudi/issues/960","id":543791715,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mzc5MTcxNQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-18T15:14:17Z","updated_at":"2019-10-18T15:14:17Z","author_association":"CONTRIBUTOR","body":"@arw357 : Thanks for the context. We have a dormant PR w.r.t upgrading to Spark-2.4. (Please see https://github.com/apache/incubator-hudi/pull/638 ) Would you be interested in resurrecting this PR. This should fit nicely with your tests :) \r\n\r\nBalaji.V","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543791715/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543805046","html_url":"https://github.com/apache/hudi/pull/960#issuecomment-543805046","issue_url":"https://api.github.com/repos/apache/hudi/issues/960","id":543805046,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzgwNTA0Ng==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-18T15:46:52Z","updated_at":"2019-10-18T15:46:52Z","author_association":"MEMBER","body":"@arw357 thanks for the test. Even if you dont have cycles for resurrecting the PR. We can later cherry pick this onto the ultimate PR that will fix this. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543805046/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543810993","html_url":"https://github.com/apache/hudi/pull/962#issuecomment-543810993","issue_url":"https://api.github.com/repos/apache/hudi/issues/962","id":543810993,"node_id":"MDEyOklzc3VlQ29tbWVudDU0MzgxMDk5Mw==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-18T16:01:51Z","updated_at":"2019-10-18T16:01:51Z","author_association":"CONTRIBUTOR","body":"Thanks for opening the PR @taherk77 . I have left some comments.\r\nPS: Please also fix the travis errors. maybe you could run `mvn clean install` locally to avoid some checkstyle errors.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543810993/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543895257","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-543895257","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":543895257,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Mzg5NTI1Nw==","user":{"login":"gfn9cho","id":14010421,"node_id":"MDQ6VXNlcjE0MDEwNDIx","avatar_url":"https://avatars.githubusercontent.com/u/14010421?v=4","gravatar_id":"","url":"https://api.github.com/users/gfn9cho","html_url":"https://github.com/gfn9cho","followers_url":"https://api.github.com/users/gfn9cho/followers","following_url":"https://api.github.com/users/gfn9cho/following{/other_user}","gists_url":"https://api.github.com/users/gfn9cho/gists{/gist_id}","starred_url":"https://api.github.com/users/gfn9cho/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gfn9cho/subscriptions","organizations_url":"https://api.github.com/users/gfn9cho/orgs","repos_url":"https://api.github.com/users/gfn9cho/repos","events_url":"https://api.github.com/users/gfn9cho/events{/privacy}","received_events_url":"https://api.github.com/users/gfn9cho/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-18T19:15:04Z","updated_at":"2019-10-19T18:58:32Z","author_association":"NONE","body":"@umehrot2 @vinothchandar \r\nI did incorporated the changes on my end and I could see the hive table for created with data synced.\r\nBut then, while doing update(Append/Overwrite) to the same table, I am getting the below error, Not sure I am missing something at my end, but wanted to bring it to your notice if its really an issue.\r\n```\r\norg.apache.hudi.hive.HoodieHiveSyncException: Failed to get table schema for hudi_new_gwpl_pc_policy_2\r\n  at org.apache.hudi.hive.HoodieHiveClient.getTableSchema(HoodieHiveClient.java:289)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:144)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:95)\r\n  at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:67)\r\n  at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:235)\r\n  at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:169)\r\n  at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:91)\r\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\r\n  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\r\n  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\r\n  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\r\n  ... 67 elided\r\nCaused by: org.apache.hive.service.cli.HiveSQLException: java.lang.NullPointerException\r\n  at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:256)\r\n  at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:247)\r\n  at org.apache.hive.jdbc.HiveDatabaseMetaData.getColumns(HiveDatabaseMetaData.java:220)\r\n  at org.apache.hudi.hive.HoodieHiveClient.getTableSchema(HoodieHiveClient.java:276)\r\n  ... 94 more\r\nCaused by: org.apache.hive.service.cli.HiveSQLException: java.lang.NullPointerException\r\n  at org.apache.hive.service.cli.operation.GetColumnsOperation.runInternal(GetColumnsOperation.java:213)\r\n  at org.apache.hive.service.cli.operation.Operation.run(Operation.java:320)\r\n  at org.apache.hive.service.cli.session.HiveSessionImpl.getColumns(HiveSessionImpl.java:678)\r\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n  at java.lang.reflect.Method.invoke(Method.java:498)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\r\n  at java.security.AccessController.doPrivileged(Native Method)\r\n  at javax.security.auth.Subject.doAs(Subject.java:422)\r\n  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\r\n  at com.sun.proxy.$Proxy41.getColumns(Unknown Source)\r\n  at org.apache.hive.service.cli.CLIService.getColumns(CLIService.java:385)\r\n  at org.apache.hive.service.cli.thrift.ThriftCLIService.GetColumns(ThriftCLIService.java:622)\r\n  at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetColumns.getResult(TCLIService.java:1557)\r\n  at org.apache.hive.service.rpc.thrift.TCLIService$Processor$GetColumns.getResult(TCLIService.java:1542)\r\n  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)\r\n  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n  at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:56)\r\n  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  at java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.NullPointerException: null\r\n  at org.apache.hive.service.cli.operation.GetColumnsOperation.runInternal(GetColumnsOperation.java:173)\r\n  ... 25 more\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/543895257/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544157071","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-544157071","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":544157071,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDE1NzA3MQ==","user":{"login":"firecast","id":2487532,"node_id":"MDQ6VXNlcjI0ODc1MzI=","avatar_url":"https://avatars.githubusercontent.com/u/2487532?v=4","gravatar_id":"","url":"https://api.github.com/users/firecast","html_url":"https://github.com/firecast","followers_url":"https://api.github.com/users/firecast/followers","following_url":"https://api.github.com/users/firecast/following{/other_user}","gists_url":"https://api.github.com/users/firecast/gists{/gist_id}","starred_url":"https://api.github.com/users/firecast/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/firecast/subscriptions","organizations_url":"https://api.github.com/users/firecast/orgs","repos_url":"https://api.github.com/users/firecast/repos","events_url":"https://api.github.com/users/firecast/events{/privacy}","received_events_url":"https://api.github.com/users/firecast/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-19T15:06:30Z","updated_at":"2019-10-19T15:06:30Z","author_association":"CONTRIBUTOR","body":"I also faced the same issue although I was using a remote hive instance instead of the AWS Glue Data Catalog. A quick fix I did to fix the issue were the following\r\n1. https://github.com/apache/incubator-hudi/blob/ed745dfdbf254bfc2ec6d9c7baed8ccbf571abab/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala#L169 to \r\n```scala\r\nsyncHive(basePath, fs, parameters, sqlContext)\r\n```\r\n2. https://github.com/apache/incubator-hudi/blob/ed745dfdbf254bfc2ec6d9c7baed8ccbf571abab/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala#L231 to \r\n```scala\r\nprivate def syncHive(basePath: Path, fs: FileSystem, parameters: Map[String, String], sqlContext: SQLContext): Boolean = {\r\n```\r\n3. Add the following lines before this line https://github.com/apache/incubator-hudi/blob/ed745dfdbf254bfc2ec6d9c7baed8ccbf571abab/hudi-spark/src/main/scala/org/apache/hudi/HoodieSparkSqlWriter.scala#L235\r\n```scala\r\nval hiveMetastoreURIs = sqlContext.sparkSession.conf.get(ConfVars.METASTOREURIS.varname)\r\nhiveConf.setVar(ConfVars.METASTOREURIS, hiveMetastoreURIs)\r\n```\r\n\r\nWhat this basically does is add the thrift URI you have set while creating the Spark Session to the Hive configuration. A temporary solution if anyone has a similar spark configuration as mine.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544157071/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544162178","html_url":"https://github.com/apache/hudi/pull/943#issuecomment-544162178","issue_url":"https://api.github.com/repos/apache/hudi/issues/943","id":544162178,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDE2MjE3OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-19T15:42:47Z","updated_at":"2019-10-19T15:42:47Z","author_association":"MEMBER","body":"Will close this in favor of the other one.. Cannot repro this locally.. still digging","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544162178/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544171820","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-544171820","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":544171820,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDE3MTgyMA==","user":{"login":"rohithsankepally","id":2110492,"node_id":"MDQ6VXNlcjIxMTA0OTI=","avatar_url":"https://avatars.githubusercontent.com/u/2110492?v=4","gravatar_id":"","url":"https://api.github.com/users/rohithsankepally","html_url":"https://github.com/rohithsankepally","followers_url":"https://api.github.com/users/rohithsankepally/followers","following_url":"https://api.github.com/users/rohithsankepally/following{/other_user}","gists_url":"https://api.github.com/users/rohithsankepally/gists{/gist_id}","starred_url":"https://api.github.com/users/rohithsankepally/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rohithsankepally/subscriptions","organizations_url":"https://api.github.com/users/rohithsankepally/orgs","repos_url":"https://api.github.com/users/rohithsankepally/repos","events_url":"https://api.github.com/users/rohithsankepally/events{/privacy}","received_events_url":"https://api.github.com/users/rohithsankepally/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-19T16:53:37Z","updated_at":"2019-10-19T16:53:37Z","author_association":"NONE","body":"Hi @vinothchandar, please send a slack invite to iamrohith94@gmail.com. TIA","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544171820/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544177414","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-544177414","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":544177414,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDE3NzQxNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-19T17:33:29Z","updated_at":"2019-10-19T17:33:29Z","author_association":"CONTRIBUTOR","body":"Added. Welcome to Hudi community \n\n\nSent from Yahoo Mail for iPhone\n\n\nOn Saturday, October 19, 2019, 9:54 AM, Rohith Reddy <notifications@github.com> wrote:\n\n\nHi @vinothchandar, please send a slack invite to iamrohith94@gmail.com. TIA\n\n—\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or unsubscribe.\n\n\n\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544177414/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544187682","html_url":"https://github.com/apache/hudi/pull/961#issuecomment-544187682","issue_url":"https://api.github.com/repos/apache/hudi/issues/961","id":544187682,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDE4NzY4Mg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-19T18:54:54Z","updated_at":"2019-10-19T18:54:54Z","author_association":"MEMBER","body":"Changes look fine to me. CI seems to have stalled due to inactivity. Wondering if travis is acting up again.. Rekicked the test","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544187682/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544188180","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-544188180","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":544188180,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDE4ODE4MA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-19T19:00:38Z","updated_at":"2019-10-19T19:00:38Z","author_association":"MEMBER","body":"@firecast can we open a JIRA & also open a PR around this? What do you think the root problem here was. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544188180/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544394955","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-544394955","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":544394955,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDM5NDk1NQ==","user":{"login":"firecast","id":2487532,"node_id":"MDQ6VXNlcjI0ODc1MzI=","avatar_url":"https://avatars.githubusercontent.com/u/2487532?v=4","gravatar_id":"","url":"https://api.github.com/users/firecast","html_url":"https://github.com/firecast","followers_url":"https://api.github.com/users/firecast/followers","following_url":"https://api.github.com/users/firecast/following{/other_user}","gists_url":"https://api.github.com/users/firecast/gists{/gist_id}","starred_url":"https://api.github.com/users/firecast/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/firecast/subscriptions","organizations_url":"https://api.github.com/users/firecast/orgs","repos_url":"https://api.github.com/users/firecast/repos","events_url":"https://api.github.com/users/firecast/events{/privacy}","received_events_url":"https://api.github.com/users/firecast/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-21T07:55:36Z","updated_at":"2019-10-21T07:55:36Z","author_association":"CONTRIBUTOR","body":"@vinothchandar Sure. Currently busy with a deployment at my company. I'll create a PR tomorrow and do some more research on it as well.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544394955/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544506120","html_url":"https://github.com/apache/hudi/pull/862#issuecomment-544506120","issue_url":"https://api.github.com/repos/apache/hudi/issues/862","id":544506120,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDUwNjEyMA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-21T13:07:27Z","updated_at":"2019-10-21T13:07:27Z","author_association":"MEMBER","body":"@bvaradar @afilipchik can we make a call on this? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544506120/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544679781","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-544679781","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":544679781,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDY3OTc4MQ==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-21T19:53:50Z","updated_at":"2019-10-21T19:55:36Z","author_association":"CONTRIBUTOR","body":"@gfn9cho @vinothchandar We are aware of this issue as well.\r\n\r\nThis issue happens inside hive code at this line: https://github.com/apache/hive/blob/rel/release-2.3.6/service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java#L173\r\n\r\nThis is because **primaryKeys** is turning up as null, and this is happening because of a bug in our internal glue catalog client, where if the primary key is not defined it returns null instead of an empty list. That breaks this piece of code.\r\n\r\nEMR will be adding Hudi support as an app in its next release, where we will be fixing this bug in the glue client as well. Then your updates will succeed as well. Until then, may be you can try defining a primary key to get around this bug ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544679781/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544710778","html_url":"https://github.com/apache/hudi/pull/965#issuecomment-544710778","issue_url":"https://api.github.com/repos/apache/hudi/issues/965","id":544710778,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NDcxMDc3OA==","user":{"login":"bhasudha","id":2179254,"node_id":"MDQ6VXNlcjIxNzkyNTQ=","avatar_url":"https://avatars.githubusercontent.com/u/2179254?v=4","gravatar_id":"","url":"https://api.github.com/users/bhasudha","html_url":"https://github.com/bhasudha","followers_url":"https://api.github.com/users/bhasudha/followers","following_url":"https://api.github.com/users/bhasudha/following{/other_user}","gists_url":"https://api.github.com/users/bhasudha/gists{/gist_id}","starred_url":"https://api.github.com/users/bhasudha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bhasudha/subscriptions","organizations_url":"https://api.github.com/users/bhasudha/orgs","repos_url":"https://api.github.com/users/bhasudha/repos","events_url":"https://api.github.com/users/bhasudha/events{/privacy}","received_events_url":"https://api.github.com/users/bhasudha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-21T21:14:41Z","updated_at":"2019-10-21T21:14:41Z","author_association":"CONTRIBUTOR","body":"Looks good to me!","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/544710778/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545032525","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-545032525","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":545032525,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTAzMjUyNQ==","user":{"login":"gfn9cho","id":14010421,"node_id":"MDQ6VXNlcjE0MDEwNDIx","avatar_url":"https://avatars.githubusercontent.com/u/14010421?v=4","gravatar_id":"","url":"https://api.github.com/users/gfn9cho","html_url":"https://github.com/gfn9cho","followers_url":"https://api.github.com/users/gfn9cho/followers","following_url":"https://api.github.com/users/gfn9cho/following{/other_user}","gists_url":"https://api.github.com/users/gfn9cho/gists{/gist_id}","starred_url":"https://api.github.com/users/gfn9cho/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gfn9cho/subscriptions","organizations_url":"https://api.github.com/users/gfn9cho/orgs","repos_url":"https://api.github.com/users/gfn9cho/repos","events_url":"https://api.github.com/users/gfn9cho/events{/privacy}","received_events_url":"https://api.github.com/users/gfn9cho/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-22T15:58:22Z","updated_at":"2019-10-22T15:58:54Z","author_association":"NONE","body":"@umehrot2 @vinothchandar , Are you able to add a primary key. I am getting below error,\r\ncreateTableWithConstraints is not supported or addPrimaryKey is not supported. \r\nLooks like it's not yet implemented in AWS Glue catalog as the code in there just throws this exception.\r\nDo we know approximate ETA on when this will be available in EMR.\r\nIt seems a deadlock. On one end, ddl to add constraint is not supported and on the other, its checking for primary key and returning NULL.\r\nPlease let me know if there are any other workaround available.\r\nCan it be handled here to catch exception and return empty list,\r\n`https://github.com/apache/hive/blob/rel/release-2.3.6/service/src/java/org/apache/hive/service/cli/operation/GetColumnsOperation.java#L171`","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545032525/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545139259","html_url":"https://github.com/apache/hudi/pull/961#issuecomment-545139259","issue_url":"https://api.github.com/repos/apache/hudi/issues/961","id":545139259,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTEzOTI1OQ==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-22T20:27:59Z","updated_at":"2019-10-22T20:27:59Z","author_association":"CONTRIBUTOR","body":"@vinothchandar Can you help out with this integration test failure ? I am not sure why it is getting stalled.\r\n\r\nEven on my local setup with master branch, the tests seem to be getting stalled. Not sure how to debug this. I did ssh to the docker container but could not find any logs.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545139259/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545144637","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-545144637","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":545144637,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTE0NDYzNw==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-22T20:40:34Z","updated_at":"2019-10-22T20:40:34Z","author_association":"CONTRIBUTOR","body":"@gfn9cho you are right Glue Catalog does not support Primary Key. Its not actually a problem with Glue service, but its EMR's glue client implementation that returns a null because primary key is not supported. Hive is not able to deal with it correctly.\r\n\r\nAt this point, we cannot just give you something that would make it work. Please not, at this point Hudi is not an officially support application on EMR. It should be supported by mid/end of November, which is when this issue will be fixed in EMRs side as well.\r\n\r\nIf you cannot wait until then, here is one way to unblock yourself:\r\n\r\n- Checkout glue catalog client package which is open sourced, and modify this line to return an empty list instead: https://github.com/awslabs/aws-glue-data-catalog-client-for-apache-hive-metastore/blob/master/aws-glue-datacatalog-hive2-client/src/main/java/com/amazonaws/glue/catalog/metastore/AWSCatalogMetastoreClient.java#L1630\r\n\r\n- SSH to EMR master and replace the jar under `/usr/share/aws/hmclient/lib/`\r\n\r\n- Restart hive-server2 to pick up new library: `sudo stop hive-server2` `sudo start hive-server2`","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545144637/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545246874","html_url":"https://github.com/apache/hudi/pull/951#issuecomment-545246874","issue_url":"https://api.github.com/repos/apache/hudi/issues/951","id":545246874,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTI0Njg3NA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-23T03:17:56Z","updated_at":"2019-10-23T03:17:56Z","author_association":"MEMBER","body":"its mostly weirdness in CI. passes for two rounds now. Will merge and handle if anything comes up. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545246874/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545248108","html_url":"https://github.com/apache/hudi/pull/961#issuecomment-545248108","issue_url":"https://api.github.com/repos/apache/hudi/issues/961","id":545248108,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTI0ODEwOA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-23T03:23:25Z","updated_at":"2019-10-23T03:23:25Z","author_association":"MEMBER","body":"@umehrot2 It passes for me all the time locally.. if it fails, then the docker containers may still be running locally.. You can use `docker/stop_demo.sh` to kill them .. and re-run tests if it helps.. https://travis-ci.org/apache/incubator-hudi/builds used to be spotless.  but seeing some travis weirdness off late.\r\n\r\n let me take a look at the failure itself ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545248108/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545364642","html_url":"https://github.com/apache/hudi/pull/940#issuecomment-545364642","issue_url":"https://api.github.com/repos/apache/hudi/issues/940","id":545364642,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTM2NDY0Mg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-23T09:52:13Z","updated_at":"2019-10-23T09:52:13Z","author_association":"CONTRIBUTOR","body":"Addressed all review comments. Merging this code.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545364642/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545491310","html_url":"https://github.com/apache/hudi/pull/962#issuecomment-545491310","issue_url":"https://api.github.com/repos/apache/hudi/issues/962","id":545491310,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTQ5MTMxMA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-23T15:10:49Z","updated_at":"2019-10-23T15:10:49Z","author_association":"MEMBER","body":"@taherk77 why closing this? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545491310/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545495902","html_url":"https://github.com/apache/hudi/pull/962#issuecomment-545495902","issue_url":"https://api.github.com/repos/apache/hudi/issues/962","id":545495902,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTQ5NTkwMg==","user":{"login":"taherk77","id":29517740,"node_id":"MDQ6VXNlcjI5NTE3NzQw","avatar_url":"https://avatars.githubusercontent.com/u/29517740?v=4","gravatar_id":"","url":"https://api.github.com/users/taherk77","html_url":"https://github.com/taherk77","followers_url":"https://api.github.com/users/taherk77/followers","following_url":"https://api.github.com/users/taherk77/following{/other_user}","gists_url":"https://api.github.com/users/taherk77/gists{/gist_id}","starred_url":"https://api.github.com/users/taherk77/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taherk77/subscriptions","organizations_url":"https://api.github.com/users/taherk77/orgs","repos_url":"https://api.github.com/users/taherk77/repos","events_url":"https://api.github.com/users/taherk77/events{/privacy}","received_events_url":"https://api.github.com/users/taherk77/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-23T15:19:58Z","updated_at":"2019-10-23T15:19:58Z","author_association":"CONTRIBUTOR","body":"> @taherk77 why closing this?\r\n\r\nWhile pushing some new code i messed my git repo, so closed this one. Will reopen again","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545495902/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545548904","html_url":"https://github.com/apache/hudi/pull/968#issuecomment-545548904","issue_url":"https://api.github.com/repos/apache/hudi/issues/968","id":545548904,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTU0ODkwNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-23T17:21:36Z","updated_at":"2019-10-23T17:21:36Z","author_association":"CONTRIBUTOR","body":"@vinothchandar : Ready for review","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545548904/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545631943","html_url":"https://github.com/apache/hudi/issues/954#issuecomment-545631943","issue_url":"https://api.github.com/repos/apache/hudi/issues/954","id":545631943,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTYzMTk0Mw==","user":{"login":"gfn9cho","id":14010421,"node_id":"MDQ6VXNlcjE0MDEwNDIx","avatar_url":"https://avatars.githubusercontent.com/u/14010421?v=4","gravatar_id":"","url":"https://api.github.com/users/gfn9cho","html_url":"https://github.com/gfn9cho","followers_url":"https://api.github.com/users/gfn9cho/followers","following_url":"https://api.github.com/users/gfn9cho/following{/other_user}","gists_url":"https://api.github.com/users/gfn9cho/gists{/gist_id}","starred_url":"https://api.github.com/users/gfn9cho/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gfn9cho/subscriptions","organizations_url":"https://api.github.com/users/gfn9cho/orgs","repos_url":"https://api.github.com/users/gfn9cho/repos","events_url":"https://api.github.com/users/gfn9cho/events{/privacy}","received_events_url":"https://api.github.com/users/gfn9cho/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-23T20:54:43Z","updated_at":"2019-10-23T20:54:43Z","author_association":"NONE","body":"@umehrot2 Thanks much for your inputs.\r\nLooks a bit tricky so as to manage the versions of all the dependencies involved with glue client.\r\nI am working on getting approvals at work to try this on a mock environment and keep this post informed.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545631943/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545731183","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-545731183","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":545731183,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTczMTE4Mw==","user":{"login":"taherk77","id":29517740,"node_id":"MDQ6VXNlcjI5NTE3NzQw","avatar_url":"https://avatars.githubusercontent.com/u/29517740?v=4","gravatar_id":"","url":"https://api.github.com/users/taherk77","html_url":"https://github.com/taherk77","followers_url":"https://api.github.com/users/taherk77/followers","following_url":"https://api.github.com/users/taherk77/following{/other_user}","gists_url":"https://api.github.com/users/taherk77/gists{/gist_id}","starred_url":"https://api.github.com/users/taherk77/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taherk77/subscriptions","organizations_url":"https://api.github.com/users/taherk77/orgs","repos_url":"https://api.github.com/users/taherk77/repos","events_url":"https://api.github.com/users/taherk77/events{/privacy}","received_events_url":"https://api.github.com/users/taherk77/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-24T03:50:23Z","updated_at":"2019-10-24T03:50:23Z","author_association":"CONTRIBUTOR","body":"@vinothchandar @leesf Travis failed on modules which weren't touched by me. Any idea how to restart the travis build","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545731183/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545731705","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-545731705","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":545731705,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTczMTcwNQ==","user":{"login":"taherk77","id":29517740,"node_id":"MDQ6VXNlcjI5NTE3NzQw","avatar_url":"https://avatars.githubusercontent.com/u/29517740?v=4","gravatar_id":"","url":"https://api.github.com/users/taherk77","html_url":"https://github.com/taherk77","followers_url":"https://api.github.com/users/taherk77/followers","following_url":"https://api.github.com/users/taherk77/following{/other_user}","gists_url":"https://api.github.com/users/taherk77/gists{/gist_id}","starred_url":"https://api.github.com/users/taherk77/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taherk77/subscriptions","organizations_url":"https://api.github.com/users/taherk77/orgs","repos_url":"https://api.github.com/users/taherk77/repos","events_url":"https://api.github.com/users/taherk77/events{/privacy}","received_events_url":"https://api.github.com/users/taherk77/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-24T03:53:22Z","updated_at":"2019-10-24T03:53:22Z","author_association":"CONTRIBUTOR","body":"> @vinothchandar @leesf Travis failed on modules which weren't touched by me. Any idea how to restart the travis build\r\n\r\nAlso guys another thing that we need to test and implement here is the continuous pull where user gives an interval and after every interval deltastreamer starts to pull from rdbms until it is terminated by the user. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545731705/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545797804","html_url":"https://github.com/apache/hudi/pull/970#issuecomment-545797804","issue_url":"https://api.github.com/repos/apache/hudi/issues/970","id":545797804,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTc5NzgwNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-24T08:03:55Z","updated_at":"2019-10-24T08:03:55Z","author_association":"CONTRIBUTOR","body":"@vinothchandar : Added a simple release download page.  Please review. Once voting passes and released, we can merge this after checking links","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545797804/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545878745","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-545878745","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":545878745,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NTg3ODc0NQ==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-24T11:40:55Z","updated_at":"2019-10-24T12:13:52Z","author_association":"CONTRIBUTOR","body":"`The forked VM terminated without properly saying goodbye. VM crash or System.exit called occurs again.`, I saw it in my local dev sometimes, and will investigate when get a circle. \r\nNow only committers or PMC could restart the travis build.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/545878745/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546019703","html_url":"https://github.com/apache/hudi/issues/971#issuecomment-546019703","issue_url":"https://api.github.com/repos/apache/hudi/issues/971","id":546019703,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjAxOTcwMw==","user":{"login":"rbhartia","id":2660635,"node_id":"MDQ6VXNlcjI2NjA2MzU=","avatar_url":"https://avatars.githubusercontent.com/u/2660635?v=4","gravatar_id":"","url":"https://api.github.com/users/rbhartia","html_url":"https://github.com/rbhartia","followers_url":"https://api.github.com/users/rbhartia/followers","following_url":"https://api.github.com/users/rbhartia/following{/other_user}","gists_url":"https://api.github.com/users/rbhartia/gists{/gist_id}","starred_url":"https://api.github.com/users/rbhartia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rbhartia/subscriptions","organizations_url":"https://api.github.com/users/rbhartia/orgs","repos_url":"https://api.github.com/users/rbhartia/repos","events_url":"https://api.github.com/users/rbhartia/events{/privacy}","received_events_url":"https://api.github.com/users/rbhartia/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-24T17:25:13Z","updated_at":"2019-10-24T20:42:31Z","author_association":"NONE","body":"Interesting. Setting \"hoodie.parquet.max.file.size\" to 2 * 1000 * 1000 * 1000 ( 2 GB) also works as expected. Seems like this is related to the 2GB limit on partition size in Spark - \r\nhttps://issues.apache.org/jira/browse/SPARK-1476\r\nhttps://issues.apache.org/jira/browse/SPARK-5928","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546019703/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546144234","html_url":"https://github.com/apache/hudi/pull/970#issuecomment-546144234","issue_url":"https://api.github.com/repos/apache/hudi/issues/970","id":546144234,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjE0NDIzNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-24T23:45:10Z","updated_at":"2019-10-24T23:45:10Z","author_association":"CONTRIBUTOR","body":"@leesf @vinothchandar : The links should work now. Please check. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546144234/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546149325","html_url":"https://github.com/apache/hudi/pull/970#issuecomment-546149325","issue_url":"https://api.github.com/repos/apache/hudi/issues/970","id":546149325,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjE0OTMyNQ==","user":{"login":"bhasudha","id":2179254,"node_id":"MDQ6VXNlcjIxNzkyNTQ=","avatar_url":"https://avatars.githubusercontent.com/u/2179254?v=4","gravatar_id":"","url":"https://api.github.com/users/bhasudha","html_url":"https://github.com/bhasudha","followers_url":"https://api.github.com/users/bhasudha/followers","following_url":"https://api.github.com/users/bhasudha/following{/other_user}","gists_url":"https://api.github.com/users/bhasudha/gists{/gist_id}","starred_url":"https://api.github.com/users/bhasudha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bhasudha/subscriptions","organizations_url":"https://api.github.com/users/bhasudha/orgs","repos_url":"https://api.github.com/users/bhasudha/repos","events_url":"https://api.github.com/users/bhasudha/events{/privacy}","received_events_url":"https://api.github.com/users/bhasudha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T00:12:28Z","updated_at":"2019-10-25T00:12:28Z","author_association":"CONTRIBUTOR","body":"@bvaradar the links are working.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546149325/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546156746","html_url":"https://github.com/apache/hudi/pull/970#issuecomment-546156746","issue_url":"https://api.github.com/repos/apache/hudi/issues/970","id":546156746,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjE1Njc0Ng==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T00:52:42Z","updated_at":"2019-10-25T00:52:42Z","author_association":"CONTRIBUTOR","body":"LGTM","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546156746/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546177983","html_url":"https://github.com/apache/hudi/issues/971#issuecomment-546177983","issue_url":"https://api.github.com/repos/apache/hudi/issues/971","id":546177983,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjE3Nzk4Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T02:47:53Z","updated_at":"2019-10-25T02:47:53Z","author_association":"MEMBER","body":"This will go away with 2.4 as well I think. But such a scenario the job usually fails. Let me try reproducing this ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546177983/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546194309","html_url":"https://github.com/apache/hudi/pull/970#issuecomment-546194309","issue_url":"https://api.github.com/repos/apache/hudi/issues/970","id":546194309,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjE5NDMwOQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T04:19:05Z","updated_at":"2019-10-25T04:19:05Z","author_association":"CONTRIBUTOR","body":"@vinothchandar : Addressed review comments. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546194309/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546196958","html_url":"https://github.com/apache/hudi/pull/970#issuecomment-546196958","issue_url":"https://api.github.com/repos/apache/hudi/issues/970","id":546196958,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjE5Njk1OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T04:35:03Z","updated_at":"2019-10-25T04:35:03Z","author_association":"MEMBER","body":"Please go ahead and merge after these small nits","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546196958/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546223497","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-546223497","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":546223497,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjIyMzQ5Nw==","user":{"login":"taherk77","id":29517740,"node_id":"MDQ6VXNlcjI5NTE3NzQw","avatar_url":"https://avatars.githubusercontent.com/u/29517740?v=4","gravatar_id":"","url":"https://api.github.com/users/taherk77","html_url":"https://github.com/taherk77","followers_url":"https://api.github.com/users/taherk77/followers","following_url":"https://api.github.com/users/taherk77/following{/other_user}","gists_url":"https://api.github.com/users/taherk77/gists{/gist_id}","starred_url":"https://api.github.com/users/taherk77/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taherk77/subscriptions","organizations_url":"https://api.github.com/users/taherk77/orgs","repos_url":"https://api.github.com/users/taherk77/repos","events_url":"https://api.github.com/users/taherk77/events{/privacy}","received_events_url":"https://api.github.com/users/taherk77/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T06:40:19Z","updated_at":"2019-10-25T06:40:19Z","author_association":"CONTRIBUTOR","body":"@leesf @vinothchandar can we extract DataFrameReader from a Dataset? Would be really helpful in testing properties if there is a way ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546223497/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546272590","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-546272590","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":546272590,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjI3MjU5MA==","user":{"login":"taherk77","id":29517740,"node_id":"MDQ6VXNlcjI5NTE3NzQw","avatar_url":"https://avatars.githubusercontent.com/u/29517740?v=4","gravatar_id":"","url":"https://api.github.com/users/taherk77","html_url":"https://github.com/taherk77","followers_url":"https://api.github.com/users/taherk77/followers","following_url":"https://api.github.com/users/taherk77/following{/other_user}","gists_url":"https://api.github.com/users/taherk77/gists{/gist_id}","starred_url":"https://api.github.com/users/taherk77/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taherk77/subscriptions","organizations_url":"https://api.github.com/users/taherk77/orgs","repos_url":"https://api.github.com/users/taherk77/repos","events_url":"https://api.github.com/users/taherk77/events{/privacy}","received_events_url":"https://api.github.com/users/taherk77/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T09:10:55Z","updated_at":"2019-10-25T09:10:55Z","author_association":"CONTRIBUTOR","body":"@leesf @vinothchandar Travis failed again on the same module as before. \r\n`[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test (default-test) on project hudi-client: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.19.1:test failed: The forked VM terminated without properly saying goodbye. VM crash or System.exit called?\r\n[ERROR] Command was /bin/sh -c cd /home/travis/build/apache/incubator-hudi/hudi-client && /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java -jar /home/travis/build/apache/incubator-hudi/hudi-client/target/surefire/surefirebooter7257868499234226311.jar /home/travis/build/apache/incubator-hudi/hudi-client/target/surefire/surefire6853623625226259050tmp /home/travis/build/apache/incubator-hudi/hudi-client/target/surefire/surefire_31524552303193339725tmp`","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546272590/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546385234","html_url":"https://github.com/apache/hudi/issues/971#issuecomment-546385234","issue_url":"https://api.github.com/repos/apache/hudi/issues/971","id":546385234,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjM4NTIzNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T14:49:35Z","updated_at":"2019-10-25T14:55:28Z","author_association":"MEMBER","body":"Okay able to repro using even 2.4. Its an integer overflow somewhere in the config passing path. What happens is the workload profile computes negative number of records assigned and thus skips assigning them. Tracking hows it happening still, bit puzzling since the `HoodieWriteConfig` level its all long\r\n\r\nAha. \r\n\r\n```\r\nscala> String.valueOf(3 *1024 * 1024 * 1024)\r\nres1: String = -1073741824\r\n\r\nscala>\r\n```\r\n\r\ncan you try just doing `((Long) (3 *1024 * 1024 * 1024L)).toString();` in Java or `(3 *1024 * 1024 * 1024L).toString` in scala? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546385234/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546387750","html_url":"https://github.com/apache/hudi/issues/971#issuecomment-546387750","issue_url":"https://api.github.com/repos/apache/hudi/issues/971","id":546387750,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjM4Nzc1MA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T14:55:58Z","updated_at":"2019-10-25T14:55:58Z","author_association":"MEMBER","body":"Nonetheless, bounds checking on configs needs to be improved still :)","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546387750/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546390153","html_url":"https://github.com/apache/hudi/issues/971#issuecomment-546390153","issue_url":"https://api.github.com/repos/apache/hudi/issues/971","id":546390153,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjM5MDE1Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T15:01:36Z","updated_at":"2019-10-25T15:01:36Z","author_association":"MEMBER","body":"Once you fix this, all is good\r\n\r\n```\r\n$ ls -lR /tmp/amazon_reviews_hudi/\r\ntotal 0\r\ndrwxr-xr-x  10 vchandar  wheel  320 Oct 25 07:58 US\r\n\r\n/tmp/amazon_reviews_hudi//US:\r\ntotal 3244416\r\n-rw-r--r--  1 vchandar  wheel  547148304 Oct 25 08:00 57e83ba1-036b-41f4-ae84-30ae84bfa689-0_2-9-21_20191025075611.parquet\r\n-rw-r--r--  1 vchandar  wheel  547319837 Oct 25 08:00 cefc04de-d880-4500-aaeb-267688f7f0a8-0_1-9-20_20191025075611.parquet\r\n-rw-r--r--  1 vchandar  wheel  547065620 Oct 25 08:00 ff60519b-6beb-4f48-bb2d-17390cdd0e5e-0_0-9-19_20191025075611.parquet\r\n08:00:57 [amazon-reviews-dataset]$\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546390153/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546458009","html_url":"https://github.com/apache/hudi/issues/971#issuecomment-546458009","issue_url":"https://api.github.com/repos/apache/hudi/issues/971","id":546458009,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjQ1ODAwOQ==","user":{"login":"rbhartia","id":2660635,"node_id":"MDQ6VXNlcjI2NjA2MzU=","avatar_url":"https://avatars.githubusercontent.com/u/2660635?v=4","gravatar_id":"","url":"https://api.github.com/users/rbhartia","html_url":"https://github.com/rbhartia","followers_url":"https://api.github.com/users/rbhartia/followers","following_url":"https://api.github.com/users/rbhartia/following{/other_user}","gists_url":"https://api.github.com/users/rbhartia/gists{/gist_id}","starred_url":"https://api.github.com/users/rbhartia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rbhartia/subscriptions","organizations_url":"https://api.github.com/users/rbhartia/orgs","repos_url":"https://api.github.com/users/rbhartia/repos","events_url":"https://api.github.com/users/rbhartia/events{/privacy}","received_events_url":"https://api.github.com/users/rbhartia/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T18:15:52Z","updated_at":"2019-10-25T18:15:52Z","author_association":"NONE","body":"Thank you, Vinoth! Should have looked into that assumption :( ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546458009/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546458268","html_url":"https://github.com/apache/hudi/issues/971#issuecomment-546458268","issue_url":"https://api.github.com/repos/apache/hudi/issues/971","id":546458268,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjQ1ODI2OA==","user":{"login":"rbhartia","id":2660635,"node_id":"MDQ6VXNlcjI2NjA2MzU=","avatar_url":"https://avatars.githubusercontent.com/u/2660635?v=4","gravatar_id":"","url":"https://api.github.com/users/rbhartia","html_url":"https://github.com/rbhartia","followers_url":"https://api.github.com/users/rbhartia/followers","following_url":"https://api.github.com/users/rbhartia/following{/other_user}","gists_url":"https://api.github.com/users/rbhartia/gists{/gist_id}","starred_url":"https://api.github.com/users/rbhartia/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rbhartia/subscriptions","organizations_url":"https://api.github.com/users/rbhartia/orgs","repos_url":"https://api.github.com/users/rbhartia/repos","events_url":"https://api.github.com/users/rbhartia/events{/privacy}","received_events_url":"https://api.github.com/users/rbhartia/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-25T18:16:32Z","updated_at":"2019-10-25T18:16:32Z","author_association":"NONE","body":"Probably create a JIRA and close this Github issue?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546458268/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546612256","html_url":"https://github.com/apache/hudi/issues/971#issuecomment-546612256","issue_url":"https://api.github.com/repos/apache/hudi/issues/971","id":546612256,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjYxMjI1Ng==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-26T15:24:12Z","updated_at":"2019-10-26T15:24:12Z","author_association":"MEMBER","body":"Np. I did not anticipate that either :) . added to https://issues.apache.org/jira/browse/HUDI-89 . Closing this. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546612256/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546613672","html_url":"https://github.com/apache/hudi/pull/961#issuecomment-546613672","issue_url":"https://api.github.com/repos/apache/hudi/issues/961","id":546613672,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NjYxMzY3Mg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-26T15:40:25Z","updated_at":"2019-10-26T15:40:25Z","author_association":"MEMBER","body":"@umehrot2 did not respond here since there was a mailing list thread on the same. Seems like the jetty threads are not exiting cleanly somehow. and its intermittent? do you see that it always stalls. I or others are not seeing that\r\n\r\n>> I did ssh to the docker container but could not find any logs.\r\nstdout is piped back to the test process, that why. you can use jstack and jmap to get dumps and see whats going on. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546613672/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546760539","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-546760539","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":546760539,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Njc2MDUzOQ==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-28T01:43:43Z","updated_at":"2019-10-28T01:43:43Z","author_association":"CONTRIBUTOR","body":"> @leesf @vinothchandar can we extract DataFrameReader from a Dataset? Would be really helpful in testing properties if there is a way\r\n\r\nAs far as I know, we could easily get Dataset from DataFrameReader, But I didn't find a way to get DataFrameReader from Dataset, let me know if there is.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546760539/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546898779","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-546898779","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":546898779,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Njg5ODc3OQ==","user":{"login":"taherk77","id":29517740,"node_id":"MDQ6VXNlcjI5NTE3NzQw","avatar_url":"https://avatars.githubusercontent.com/u/29517740?v=4","gravatar_id":"","url":"https://api.github.com/users/taherk77","html_url":"https://github.com/taherk77","followers_url":"https://api.github.com/users/taherk77/followers","following_url":"https://api.github.com/users/taherk77/following{/other_user}","gists_url":"https://api.github.com/users/taherk77/gists{/gist_id}","starred_url":"https://api.github.com/users/taherk77/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taherk77/subscriptions","organizations_url":"https://api.github.com/users/taherk77/orgs","repos_url":"https://api.github.com/users/taherk77/repos","events_url":"https://api.github.com/users/taherk77/events{/privacy}","received_events_url":"https://api.github.com/users/taherk77/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-28T11:09:41Z","updated_at":"2019-10-28T11:09:41Z","author_association":"CONTRIBUTOR","body":"@leesf @vinothchandar All changes are addressed and fixed. Travis builds failing because of random VM crashes. What can we do further?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546898779/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546898880","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-546898880","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":546898880,"node_id":"MDEyOklzc3VlQ29tbWVudDU0Njg5ODg4MA==","user":{"login":"taherk77","id":29517740,"node_id":"MDQ6VXNlcjI5NTE3NzQw","avatar_url":"https://avatars.githubusercontent.com/u/29517740?v=4","gravatar_id":"","url":"https://api.github.com/users/taherk77","html_url":"https://github.com/taherk77","followers_url":"https://api.github.com/users/taherk77/followers","following_url":"https://api.github.com/users/taherk77/following{/other_user}","gists_url":"https://api.github.com/users/taherk77/gists{/gist_id}","starred_url":"https://api.github.com/users/taherk77/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taherk77/subscriptions","organizations_url":"https://api.github.com/users/taherk77/orgs","repos_url":"https://api.github.com/users/taherk77/repos","events_url":"https://api.github.com/users/taherk77/events{/privacy}","received_events_url":"https://api.github.com/users/taherk77/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-28T11:10:00Z","updated_at":"2019-10-28T11:10:00Z","author_association":"CONTRIBUTOR","body":"> @leesf @vinothchandar All changes are addressed and fixed. Travis builds failing because of random VM crashes. What can we do further?\r\n\r\nIs this good to go now?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/546898880/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547015423","html_url":"https://github.com/apache/hudi/pull/969#issuecomment-547015423","issue_url":"https://api.github.com/repos/apache/hudi/issues/969","id":547015423,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzAxNTQyMw==","user":{"login":"taherk77","id":29517740,"node_id":"MDQ6VXNlcjI5NTE3NzQw","avatar_url":"https://avatars.githubusercontent.com/u/29517740?v=4","gravatar_id":"","url":"https://api.github.com/users/taherk77","html_url":"https://github.com/taherk77","followers_url":"https://api.github.com/users/taherk77/followers","following_url":"https://api.github.com/users/taherk77/following{/other_user}","gists_url":"https://api.github.com/users/taherk77/gists{/gist_id}","starred_url":"https://api.github.com/users/taherk77/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taherk77/subscriptions","organizations_url":"https://api.github.com/users/taherk77/orgs","repos_url":"https://api.github.com/users/taherk77/repos","events_url":"https://api.github.com/users/taherk77/events{/privacy}","received_events_url":"https://api.github.com/users/taherk77/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-28T15:58:10Z","updated_at":"2019-10-28T15:58:10Z","author_association":"CONTRIBUTOR","body":"@leesf  @vinothchandar  All tests passing guys!!!!!!!! 🥇 ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547015423/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547360105","html_url":"https://github.com/apache/hudi/pull/977#issuecomment-547360105","issue_url":"https://api.github.com/repos/apache/hudi/issues/977","id":547360105,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzM2MDEwNQ==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-29T10:42:58Z","updated_at":"2019-10-29T10:42:58Z","author_association":"CONTRIBUTOR","body":"Thanks for your detail review @garyli1019 . Updated the PR to address your comments.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547360105/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547395996","html_url":"https://github.com/apache/hudi/pull/976#issuecomment-547395996","issue_url":"https://api.github.com/repos/apache/hudi/issues/976","id":547395996,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzM5NTk5Ng==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-29T12:28:40Z","updated_at":"2019-10-29T12:28:40Z","author_association":"MEMBER","body":"@nsivabalan can you rebase against master and see if that helps the test pass ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547395996/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547458994","html_url":"https://github.com/apache/hudi/pull/976#issuecomment-547458994","issue_url":"https://api.github.com/repos/apache/hudi/issues/976","id":547458994,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzQ1ODk5NA==","user":{"login":"nsivabalan","id":513218,"node_id":"MDQ6VXNlcjUxMzIxOA==","avatar_url":"https://avatars.githubusercontent.com/u/513218?v=4","gravatar_id":"","url":"https://api.github.com/users/nsivabalan","html_url":"https://github.com/nsivabalan","followers_url":"https://api.github.com/users/nsivabalan/followers","following_url":"https://api.github.com/users/nsivabalan/following{/other_user}","gists_url":"https://api.github.com/users/nsivabalan/gists{/gist_id}","starred_url":"https://api.github.com/users/nsivabalan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nsivabalan/subscriptions","organizations_url":"https://api.github.com/users/nsivabalan/orgs","repos_url":"https://api.github.com/users/nsivabalan/repos","events_url":"https://api.github.com/users/nsivabalan/events{/privacy}","received_events_url":"https://api.github.com/users/nsivabalan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-29T14:50:27Z","updated_at":"2019-10-29T14:50:27Z","author_association":"CONTRIBUTOR","body":"> @nsivabalan can you rebase against master and see if that helps the test pass\r\n\r\nNope. I didn't import the checkstyle. will fix it. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547458994/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547528039","html_url":"https://github.com/apache/hudi/pull/942#issuecomment-547528039","issue_url":"https://api.github.com/repos/apache/hudi/issues/942","id":547528039,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzUyODAzOQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-29T17:03:50Z","updated_at":"2019-10-29T17:03:50Z","author_association":"CONTRIBUTOR","body":"@vinothchandar : Addressed review comments. Also, re-enabled spotless to fix any linting automatically and disabled spotless again. We can wait for the tests to pass before reviewing again.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547528039/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547675465","html_url":"https://github.com/apache/hudi/pull/972#issuecomment-547675465","issue_url":"https://api.github.com/repos/apache/hudi/issues/972","id":547675465,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzY3NTQ2NQ==","user":{"login":"zhedoubushishi","id":31263084,"node_id":"MDQ6VXNlcjMxMjYzMDg0","avatar_url":"https://avatars.githubusercontent.com/u/31263084?v=4","gravatar_id":"","url":"https://api.github.com/users/zhedoubushishi","html_url":"https://github.com/zhedoubushishi","followers_url":"https://api.github.com/users/zhedoubushishi/followers","following_url":"https://api.github.com/users/zhedoubushishi/following{/other_user}","gists_url":"https://api.github.com/users/zhedoubushishi/gists{/gist_id}","starred_url":"https://api.github.com/users/zhedoubushishi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhedoubushishi/subscriptions","organizations_url":"https://api.github.com/users/zhedoubushishi/orgs","repos_url":"https://api.github.com/users/zhedoubushishi/repos","events_url":"https://api.github.com/users/zhedoubushishi/events{/privacy}","received_events_url":"https://api.github.com/users/zhedoubushishi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-29T23:35:30Z","updated_at":"2019-10-29T23:58:57Z","author_association":"CONTRIBUTOR","body":"> Just the 1 comment.. and you may also want to change the target branch to `master` instead of `release-0.5.0` ?\r\n\r\nSorry I used the wrong branch. Fixed now.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547675465/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547699330","html_url":"https://github.com/apache/hudi/pull/977#issuecomment-547699330","issue_url":"https://api.github.com/repos/apache/hudi/issues/977","id":547699330,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzY5OTMzMA==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-30T01:27:49Z","updated_at":"2019-10-30T01:27:49Z","author_association":"CONTRIBUTOR","body":"Thanks for your detail review @yihua @garyli1019 ! Updated the PR to address your comments.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547699330/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547716257","html_url":"https://github.com/apache/hudi/pull/976#issuecomment-547716257","issue_url":"https://api.github.com/repos/apache/hudi/issues/976","id":547716257,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzcxNjI1Nw==","user":{"login":"nsivabalan","id":513218,"node_id":"MDQ6VXNlcjUxMzIxOA==","avatar_url":"https://avatars.githubusercontent.com/u/513218?v=4","gravatar_id":"","url":"https://api.github.com/users/nsivabalan","html_url":"https://github.com/nsivabalan","followers_url":"https://api.github.com/users/nsivabalan/followers","following_url":"https://api.github.com/users/nsivabalan/following{/other_user}","gists_url":"https://api.github.com/users/nsivabalan/gists{/gist_id}","starred_url":"https://api.github.com/users/nsivabalan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nsivabalan/subscriptions","organizations_url":"https://api.github.com/users/nsivabalan/orgs","repos_url":"https://api.github.com/users/nsivabalan/repos","events_url":"https://api.github.com/users/nsivabalan/events{/privacy}","received_events_url":"https://api.github.com/users/nsivabalan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-30T02:51:10Z","updated_at":"2019-10-30T02:51:10Z","author_association":"CONTRIBUTOR","body":"Should I create a package for bloom.filter in org.apache.hudi.common? and move all BloomFilter related classes to it. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547716257/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547726737","html_url":"https://github.com/apache/hudi/pull/976#issuecomment-547726737","issue_url":"https://api.github.com/repos/apache/hudi/issues/976","id":547726737,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzcyNjczNw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-30T03:44:50Z","updated_at":"2019-10-30T03:44:50Z","author_association":"MEMBER","body":"Consolidating package makes sense.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547726737/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547731457","html_url":"https://github.com/apache/hudi/pull/666#issuecomment-547731457","issue_url":"https://api.github.com/repos/apache/hudi/issues/666","id":547731457,"node_id":"MDEyOklzc3VlQ29tbWVudDU0NzczMTQ1Nw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-10-30T04:10:48Z","updated_at":"2019-10-30T04:10:48Z","author_association":"MEMBER","body":"Closing in favor of #976 ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/547731457/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]