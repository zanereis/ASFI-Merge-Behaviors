[{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2561915227","html_url":"https://github.com/apache/bigtop/pull/1319#issuecomment-2561915227","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1319","id":2561915227,"node_id":"IC_kwDOACDj7M6Ys7lb","user":{"login":"iwasakims","id":1856607,"node_id":"MDQ6VXNlcjE4NTY2MDc=","avatar_url":"https://avatars.githubusercontent.com/u/1856607?v=4","gravatar_id":"","url":"https://api.github.com/users/iwasakims","html_url":"https://github.com/iwasakims","followers_url":"https://api.github.com/users/iwasakims/followers","following_url":"https://api.github.com/users/iwasakims/following{/other_user}","gists_url":"https://api.github.com/users/iwasakims/gists{/gist_id}","starred_url":"https://api.github.com/users/iwasakims/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iwasakims/subscriptions","organizations_url":"https://api.github.com/users/iwasakims/orgs","repos_url":"https://api.github.com/users/iwasakims/repos","events_url":"https://api.github.com/users/iwasakims/events{/privacy}","received_events_url":"https://api.github.com/users/iwasakims/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-12-25T14:40:34Z","updated_at":"2024-12-25T15:01:01Z","author_association":"MEMBER","body":"HADOOP_MAPRED_HOME seems not to be set in the RM/NM launched via systemd. HADOOP_COMMON_HOME, HADOOP_HDFS_HOME and HADOOP_YARN_HOME too. Those variables are defined in `/usr/lib/hadoop/libexec/hadoop-layout.sh`. @masatana ","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2561915227/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2562023040","html_url":"https://github.com/apache/bigtop/pull/1320#issuecomment-2562023040","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1320","id":2562023040,"node_id":"IC_kwDOACDj7M6YtV6A","user":{"login":"sekikn","id":898388,"node_id":"MDQ6VXNlcjg5ODM4OA==","avatar_url":"https://avatars.githubusercontent.com/u/898388?v=4","gravatar_id":"","url":"https://api.github.com/users/sekikn","html_url":"https://github.com/sekikn","followers_url":"https://api.github.com/users/sekikn/followers","following_url":"https://api.github.com/users/sekikn/following{/other_user}","gists_url":"https://api.github.com/users/sekikn/gists{/gist_id}","starred_url":"https://api.github.com/users/sekikn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sekikn/subscriptions","organizations_url":"https://api.github.com/users/sekikn/orgs","repos_url":"https://api.github.com/users/sekikn/repos","events_url":"https://api.github.com/users/sekikn/events{/privacy}","received_events_url":"https://api.github.com/users/sekikn/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-12-25T23:30:27Z","updated_at":"2024-12-25T23:30:27Z","author_association":"CONTRIBUTOR","body":"+1, thanks @iwasakims.","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2562023040/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2563649136","html_url":"https://github.com/apache/bigtop/pull/1319#issuecomment-2563649136","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1319","id":2563649136,"node_id":"IC_kwDOACDj7M6Yzi5w","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-12-27T12:20:04Z","updated_at":"2024-12-27T14:33:02Z","author_association":"CONTRIBUTOR","body":"@iwasakims \r\nThank you for the suggestion. I just realized that the settings described in our [hadoop-layout.sh](https://github.com/apache/bigtop/blob/91cb092db85754df016f704ddba9cff725399fbb/bigtop-packages/src/common/hadoop/hadoop-layout.sh#L15) are defined as local varibles, and are not inherited by processes launched from this shell.\r\nWe should use `export` to convert those variables into environment variables. I'll add another commit to fix it.","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2563649136/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2564194130","html_url":"https://github.com/apache/bigtop/pull/1318#issuecomment-2564194130","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1318","id":2564194130,"node_id":"IC_kwDOACDj7M6Y1n9S","user":{"login":"iwasakims","id":1856607,"node_id":"MDQ6VXNlcjE4NTY2MDc=","avatar_url":"https://avatars.githubusercontent.com/u/1856607?v=4","gravatar_id":"","url":"https://api.github.com/users/iwasakims","html_url":"https://github.com/iwasakims","followers_url":"https://api.github.com/users/iwasakims/followers","following_url":"https://api.github.com/users/iwasakims/following{/other_user}","gists_url":"https://api.github.com/users/iwasakims/gists{/gist_id}","starred_url":"https://api.github.com/users/iwasakims/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iwasakims/subscriptions","organizations_url":"https://api.github.com/users/iwasakims/orgs","repos_url":"https://api.github.com/users/iwasakims/repos","events_url":"https://api.github.com/users/iwasakims/events{/privacy}","received_events_url":"https://api.github.com/users/iwasakims/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-12-28T05:24:58Z","updated_at":"2024-12-28T05:24:58Z","author_association":"MEMBER","body":"@JiaLiangC Are you willing to keep working on this?","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2564194130/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2564196022","html_url":"https://github.com/apache/bigtop/pull/1318#issuecomment-2564196022","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1318","id":2564196022,"node_id":"IC_kwDOACDj7M6Y1oa2","user":{"login":"JiaLiangC","id":18082602,"node_id":"MDQ6VXNlcjE4MDgyNjAy","avatar_url":"https://avatars.githubusercontent.com/u/18082602?v=4","gravatar_id":"","url":"https://api.github.com/users/JiaLiangC","html_url":"https://github.com/JiaLiangC","followers_url":"https://api.github.com/users/JiaLiangC/followers","following_url":"https://api.github.com/users/JiaLiangC/following{/other_user}","gists_url":"https://api.github.com/users/JiaLiangC/gists{/gist_id}","starred_url":"https://api.github.com/users/JiaLiangC/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JiaLiangC/subscriptions","organizations_url":"https://api.github.com/users/JiaLiangC/orgs","repos_url":"https://api.github.com/users/JiaLiangC/repos","events_url":"https://api.github.com/users/JiaLiangC/events{/privacy}","received_events_url":"https://api.github.com/users/JiaLiangC/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-12-28T05:27:20Z","updated_at":"2024-12-28T05:27:20Z","author_association":"CONTRIBUTOR","body":"@iwasakims  Of course, my personal repository was too messy, so I closed it and forked the repository again, which resulted in the PR being closed.\r\n","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2564196022/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2564234386","html_url":"https://github.com/apache/bigtop/pull/1319#issuecomment-2564234386","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1319","id":2564234386,"node_id":"IC_kwDOACDj7M6Y1xyS","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-12-28T06:52:06Z","updated_at":"2024-12-28T06:52:55Z","author_association":"CONTRIBUTOR","body":"@iwasakims \r\nUpdated the PR. It passes smome-tests (hdfs,yarn,mapreduce) on my local (Ubuntu 22.04)\r\nCould you check this again?","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2564234386/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2566032505","html_url":"https://github.com/apache/bigtop/pull/1319#issuecomment-2566032505","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1319","id":2566032505,"node_id":"IC_kwDOACDj7M6Y8ox5","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2024-12-31T00:25:50Z","updated_at":"2024-12-31T00:25:50Z","author_association":"CONTRIBUTOR","body":"@iwasakims Thank you!","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2566032505/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2573378626","html_url":"https://github.com/apache/bigtop/pull/1322#issuecomment-2573378626","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1322","id":2573378626,"node_id":"IC_kwDOACDj7M6ZYqRC","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-01-06T15:44:38Z","updated_at":"2025-01-06T15:44:38Z","author_association":"CONTRIBUTOR","body":"Hmm, provided system unit files don't work on RockyLinux 8 with the error below:\r\n\r\n```\r\nhadoop-hdfs-namenode.service: New main PID 6388 does not belong to service, and PID file is not owned by root. Refusing.\r\n```\r\n\r\nStrangely, it works on Ubuntu 24.04.\r\n\r\nTo workaround this, we can use `GuessMainPID=yes` (which is default) and comment out `PIDFile`. It can guess main java PID correctly on both RockyLinux 8 and Ubuntu 24.04 like below:\r\n\r\n<details>\r\n\r\n```\r\n[root@b2c7d1997649 /]# systemctl cat hadoop-hdfs-namenode\r\n# /usr/lib/systemd/system/hadoop-hdfs-namenode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop NameNode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\n#GuessMainPID=no\r\nRemainAfterExit=no\r\n#PIDFile=/run/hadoop-hdfs/hadoop-hdfs-namenode.pid\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop namenode\r\n[root@b2c7d1997649 /]# systemctl start hadoop-hdfs-namenode\r\n[root@b2c7d1997649 /]# systemctl status hadoop-hdfs-namenode --no-pager\r\n● hadoop-hdfs-namenode.service - Hadoop NameNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-namenode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Mon 2025-01-06 15:38:23 UTC; 3s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 7099 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop namenode (code=exited, status=0/SUCCESS)\r\n  Process: 7164 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode (code=exited, status=0/SUCCESS)\r\n Main PID: 7217 (java)\r\n    Tasks: 71 (limit: 98358)\r\n   Memory: 365.4M\r\n   CGroup: /docker/b2c7d19976495ecf3882759e65bcd1e70f8acac1d917d274b61d83b59e99af3d/system.slice/hadoop-hdfs-namenode.service\r\n           └─7217 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyar…\r\n\r\nJan 06 15:38:21 b2c7d1997649 systemd[1]: hadoop-hdfs-namenode.service: User lookup succeeded: uid=996 gid=993\r\nJan 06 15:38:21 b2c7d1997649 systemd[7164]: hadoop-hdfs-namenode.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode\r\nJan 06 15:38:23 b2c7d1997649 systemd[1]: hadoop-hdfs-namenode.service: Child 7164 belongs to hadoop-hdfs-namenode.service.\r\nJan 06 15:38:23 b2c7d1997649 systemd[1]: hadoop-hdfs-namenode.service: Control process exited, code=exited status=0\r\nJan 06 15:38:23 b2c7d1997649 systemd[1]: hadoop-hdfs-namenode.service: Got final SIGCHLD for state start.\r\nJan 06 15:38:23 b2c7d1997649 systemd[1]: hadoop-hdfs-namenode.service: Main PID guessed: 7217\r\nJan 06 15:38:23 b2c7d1997649 systemd[1]: hadoop-hdfs-namenode.service: Changed start -> running\r\nJan 06 15:38:23 b2c7d1997649 systemd[1]: hadoop-hdfs-namenode.service: Job hadoop-hdfs-namenode.service/start finished, result=done\r\nJan 06 15:38:23 b2c7d1997649 systemd[1]: Started Hadoop NameNode.\r\nJan 06 15:38:23 b2c7d1997649 systemd[1]: hadoop-hdfs-namenode.service: Failed to send unit change signal for hadoop-hdfs-namenode.service: Connection reset by peer\r\n```\r\n</details>\r\n\r\nIf this workaround accepted, we need to apply this to other components...","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2573378626/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2579505184","html_url":"https://github.com/apache/bigtop/pull/1322#issuecomment-2579505184","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1322","id":2579505184,"node_id":"IC_kwDOACDj7M6ZwCAg","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-01-09T09:01:23Z","updated_at":"2025-01-09T09:01:23Z","author_association":"CONTRIBUTOR","body":"RPM test result \r\n\r\n<details>\r\n\r\nBuilding rpm on Rocky 8 (w/ Docker)\r\n```\r\n$ ./gradlew allclean hadoop-pkg-ind repo-ind -POS=rockylinux-8\r\n```\r\n\r\nSmoke tests on Rocky 8 \r\n```\r\n$ cd provisioner/docker\r\n$ ./docker-hadoop.sh --enable-local-repo --disable-gpg-check     --docker-compose-plugin     -C config_rockylinux-8.yaml     -F docker-compose-cgroupv2.yml     --stack hdfs,yarn,mapreduce --smoke-tests hdfs -c 3\r\n\r\n(snip)\r\n\r\nGradle Test Executor 2 finished executing tests.\r\n\r\n> Task :bigtop-tests:smoke-tests:hdfs:test\r\nFinished generating test XML results (0.023 secs) into: /bigtop-home/bigtop-tests/smoke-tests/hdfs/build/test-results/test\r\nGenerating HTML test report...\r\nFinished generating test html results (0.024 secs) into: /bigtop-home/bigtop-tests/smoke-tests/hdfs/build/reports/tests/test\r\nNow testing...\r\n:bigtop-tests:smoke-tests:hdfs:test (Thread[Execution worker for ':' Thread 5,5,main]) completed. Took 9 mins 7.834 secs.\r\n\r\nBUILD SUCCESSFUL in 8m 45s\r\n29 actionable tasks: 8 executed, 21 up-to-date\r\nStopped 1 worker daemon(s).\r\n+ rm -rf buildSrc/build/test-results/binary\r\n+ rm -rf /bigtop-home/.gradle\r\n```\r\n\r\nInstall  additional packages (journalnode,zkfc,dfsrouter,secondarynamenode)\r\n\r\n```\r\n$ ./docker-hadoop.sh -dcp --exec 1 /bin/bash\r\n$ dnf install hadoop-hdfs-journalnode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-hdfs-dfsrouter -y\r\n```\r\n\r\n\r\n Check if systemctl works (`systemctl start` & `systemctl status`)\r\n\r\n```\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do systemctl start hadoop-hdfs-$service_name; done\r\nJob for hadoop-hdfs-zkfc.service failed because the control process exited with error code.\r\nSee \"systemctl status hadoop-hdfs-zkfc.service\" and \"journalctl -xe\" for details.\r\n```\r\n\r\n```\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl status hadoop-hdfs-$service_name; done\r\n\r\n● hadoop-hdfs-namenode.service - Hadoop NameNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-namenode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 02:51:34 UTC; 2h 19min ago\r\n     Docs: https://hadoop.apache.org/\r\n Main PID: 7549 (java)\r\n    Tasks: 73 (limit: 98358)\r\n   Memory: 454.4M\r\n   CGroup: /docker/071de24523e57040eff639e76330f9e4fe9ddb5d3d956a660a52a655ddd7823f/system.slice/hadoop-hdfs-namenode.service\r\n           └─7549 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyar…\r\nJan 09 05:07:31 071de24523e5 systemd[1]: hadoop-hdfs-namenode.service: Unknown serialization key: ref-gid\r\nJan 09 05:07:31 071de24523e5 systemd[1]: hadoop-hdfs-namenode.service: Changed dead -> running\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-namenode.service: Trying to enqueue job hadoop-hdfs-namenode.service/start/replace\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-namenode.service: Installed new job hadoop-hdfs-namenode.service/start as 425\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-namenode.service: Enqueued job hadoop-hdfs-namenode.service/start as 425\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-namenode.service: Job hadoop-hdfs-namenode.service/start finished, result=done\r\nWarning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.\r\n● hadoop-hdfs-datanode.service - Hadoop DataNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-datanode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 02:51:39 UTC; 2h 19min ago\r\n     Docs: https://hadoop.apache.org/\r\n Main PID: 7746 (java)\r\n    Tasks: 76 (limit: 98358)\r\n   Memory: 500.0M\r\n   CGroup: /docker/071de24523e57040eff639e76330f9e4fe9ddb5d3d956a660a52a655ddd7823f/system.slice/hadoop-hdfs-datanode.service\r\n           └─7746 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.…\r\nJan 09 05:07:31 071de24523e5 systemd[1]: hadoop-hdfs-datanode.service: Unknown serialization key: ref-gid\r\nJan 09 05:07:31 071de24523e5 systemd[1]: hadoop-hdfs-datanode.service: Changed dead -> running\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-datanode.service: Trying to enqueue job hadoop-hdfs-datanode.service/start/replace\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-datanode.service: Installed new job hadoop-hdfs-datanode.service/start as 456\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-datanode.service: Enqueued job hadoop-hdfs-datanode.service/start as 456\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-datanode.service: Job hadoop-hdfs-datanode.service/start finished, result=done\r\nWarning: Journal has been rotated since unit was started. Log output is incomplete or unavailable.\r\n● hadoop-hdfs-journalnode.service - Hadoop Journalnode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-journalnode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 05:09:05 UTC; 1min 52s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 31278 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode (code=exited, status=0/SUCCESS)\r\n Main PID: 31329 (java)\r\n    Tasks: 44 (limit: 98358)\r\n   Memory: 164.0M\r\n   CGroup: /docker/071de24523e57040eff639e76330f9e4fe9ddb5d3d956a660a52a655ddd7823f/system.slice/hadoop-hdfs-journalnode.service\r\n           └─31329 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_journalnode -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-journa…\r\nJan 09 05:09:03 071de24523e5 systemd[1]: hadoop-hdfs-journalnode.service: User lookup succeeded: uid=996 gid=993\r\nJan 09 05:09:03 071de24523e5 systemd[31278]: hadoop-hdfs-journalnode.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode\r\nJan 09 05:09:05 071de24523e5 systemd[1]: hadoop-hdfs-journalnode.service: Child 31278 belongs to hadoop-hdfs-journalnode.service.\r\nJan 09 05:09:05 071de24523e5 systemd[1]: hadoop-hdfs-journalnode.service: Control process exited, code=exited status=0\r\nJan 09 05:09:05 071de24523e5 systemd[1]: hadoop-hdfs-journalnode.service: Got final SIGCHLD for state start.\r\nJan 09 05:09:05 071de24523e5 systemd[1]: hadoop-hdfs-journalnode.service: Main PID guessed: 31329\r\nJan 09 05:09:05 071de24523e5 systemd[1]: hadoop-hdfs-journalnode.service: Changed start -> running\r\nJan 09 05:09:05 071de24523e5 systemd[1]: hadoop-hdfs-journalnode.service: Job hadoop-hdfs-journalnode.service/start finished, result=done\r\nJan 09 05:09:05 071de24523e5 systemd[1]: Started Hadoop Journalnode.\r\nJan 09 05:09:05 071de24523e5 systemd[1]: hadoop-hdfs-journalnode.service: Failed to send unit change signal for hadoop-hdfs-journalnode.service: Connection reset by peer\r\n● hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-secondarynamenode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 05:09:07 UTC; 1min 50s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 31372 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode (code=exited, status=0/SUCCESS)\r\n Main PID: 31423 (java)\r\n    Tasks: 37 (limit: 98358)\r\n   Memory: 340.5M\r\n   CGroup: /docker/071de24523e57040eff639e76330f9e4fe9ddb5d3d956a660a52a655ddd7823f/system.slice/hadoop-hdfs-secondarynamenode.service\r\n           └─31423 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_secondarynamenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxre…\r\n\r\nJan 09 05:09:05 071de24523e5 systemd[31372]: hadoop-hdfs-secondarynamenode.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode\r\nJan 09 05:09:05 071de24523e5 hdfs[31372]: WARNING: HADOOP_SECONDARYNAMENODE_OPTS has been replaced by HDFS_SECONDARYNAMENODE_OPTS. Using value of HADOOP_SECONDARYNAMENODE_OPTS.\r\nJan 09 05:09:07 071de24523e5 systemd[1]: hadoop-hdfs-secondarynamenode.service: Child 31372 belongs to hadoop-hdfs-secondarynamenode.service.\r\nJan 09 05:09:07 071de24523e5 systemd[1]: hadoop-hdfs-secondarynamenode.service: Control process exited, code=exited status=0\r\nJan 09 05:09:07 071de24523e5 systemd[1]: hadoop-hdfs-secondarynamenode.service: Got final SIGCHLD for state start.\r\nJan 09 05:09:07 071de24523e5 systemd[1]: hadoop-hdfs-secondarynamenode.service: Main PID guessed: 31423\r\nJan 09 05:09:07 071de24523e5 systemd[1]: hadoop-hdfs-secondarynamenode.service: Changed start -> running\r\nJan 09 05:09:07 071de24523e5 systemd[1]: hadoop-hdfs-secondarynamenode.service: Job hadoop-hdfs-secondarynamenode.service/start finished, result=done\r\nJan 09 05:09:07 071de24523e5 systemd[1]: Started Hadoop Secondary NameNode.\r\nJan 09 05:09:07 071de24523e5 systemd[1]: hadoop-hdfs-secondarynamenode.service: Failed to send unit change signal for hadoop-hdfs-secondarynamenode.service: Connection reset by peer\r\n● hadoop-hdfs-zkfc.service - Hadoop ZKFC\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-zkfc.service; static; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2025-01-09 05:09:09 UTC; 1min 48s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 31467 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc (code=exited, status=1/FAILURE)\r\n\r\nJan 09 05:09:07 071de24523e5 systemd[1]: hadoop-hdfs-zkfc.service: User lookup succeeded: uid=996 gid=993\r\nJan 09 05:09:07 071de24523e5 systemd[31467]: hadoop-hdfs-zkfc.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc\r\nJan 09 05:09:09 071de24523e5 systemd[1]: hadoop-hdfs-zkfc.service: Child 31467 belongs to hadoop-hdfs-zkfc.service.\r\nJan 09 05:09:09 071de24523e5 systemd[1]: hadoop-hdfs-zkfc.service: Control process exited, code=exited status=1\r\nJan 09 05:09:09 071de24523e5 systemd[1]: hadoop-hdfs-zkfc.service: Got final SIGCHLD for state start.\r\nJan 09 05:09:09 071de24523e5 systemd[1]: hadoop-hdfs-zkfc.service: Failed with result 'exit-code'.\r\nJan 09 05:09:09 071de24523e5 systemd[1]: hadoop-hdfs-zkfc.service: Changed start -> failed\r\nJan 09 05:09:09 071de24523e5 systemd[1]: hadoop-hdfs-zkfc.service: Job hadoop-hdfs-zkfc.service/start finished, result=failed\r\nJan 09 05:09:09 071de24523e5 systemd[1]: Failed to start Hadoop ZKFC.\r\nJan 09 05:09:09 071de24523e5 systemd[1]: hadoop-hdfs-zkfc.service: Unit entered failed state.\r\n● hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-dfsrouter.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 05:09:11 UTC; 1min 46s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 31564 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter (code=exited, status=0/SUCCESS)\r\n Main PID: 31615 (java)\r\n    Tasks: 69 (limit: 98358)\r\n   Memory: 294.5M\r\n   CGroup: /docker/071de24523e57040eff639e76330f9e4fe9ddb5d3d956a660a52a655ddd7823f/system.slice/hadoop-hdfs-dfsrouter.service\r\n           └─31615 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_dfsrouter -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-dfsroute…\r\n\r\nJan 09 05:09:09 071de24523e5 systemd[1]: hadoop-hdfs-dfsrouter.service: User lookup succeeded: uid=996 gid=993\r\nJan 09 05:09:09 071de24523e5 systemd[31564]: hadoop-hdfs-dfsrouter.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter\r\nJan 09 05:09:11 071de24523e5 systemd[1]: hadoop-hdfs-dfsrouter.service: Child 31564 belongs to hadoop-hdfs-dfsrouter.service.\r\nJan 09 05:09:11 071de24523e5 systemd[1]: hadoop-hdfs-dfsrouter.service: Control process exited, code=exited status=0\r\nJan 09 05:09:11 071de24523e5 systemd[1]: hadoop-hdfs-dfsrouter.service: Got final SIGCHLD for state start.\r\nJan 09 05:09:11 071de24523e5 systemd[1]: hadoop-hdfs-dfsrouter.service: Main PID guessed: 31615\r\nJan 09 05:09:11 071de24523e5 systemd[1]: hadoop-hdfs-dfsrouter.service: Changed start -> running\r\nJan 09 05:09:11 071de24523e5 systemd[1]: hadoop-hdfs-dfsrouter.service: Job hadoop-hdfs-dfsrouter.service/start finished, result=done\r\nJan 09 05:09:11 071de24523e5 systemd[1]: Started Hadoop dfsrouter.\r\nJan 09 05:09:11 071de24523e5 systemd[1]: hadoop-hdfs-dfsrouter.service: Failed to send unit change signal for hadoop-hdfs-dfsrouter.service: Connection reset by peer\r\n```\r\n\r\nWhile ZKFC failed to start, we can confirm that it can launch via systemd but shutdown immediately because we didn't configure HA settings (see the log below). \r\n```\r\n$  cat /var/log/hadoop-hdfs/hadoop-hdfs-zkfc-071de24523e5.log\r\n\r\n(snip)\r\n\r\n************************************************************/\r\n2025-01-09 05:09:08,670 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: registered UNIX signal handlers for [TERM, HUP, INT]\r\n2025-01-09 05:09:09,034 ERROR org.apache.hadoop.hdfs.tools.DFSZKFailoverController: DFSZKFailOverController exiting due to earlier exception org.apache.hadoop.HadoopIllegalArgumentException: HA is not enabled for this namenode.\r\n2025-01-09 05:09:09,039 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.HadoopIllegalArgumentException: HA is not enabled for this namenode.\r\n2025-01-09 05:09:09,045 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: SHUTDOWN_MSG:\r\n/************************************************************\r\nSHUTDOWN_MSG: Shutting down DFSZKFailoverController at 071de24523e5.bigtop.apache.org/172.19.0.4\r\n************************************************************/\r\n```\r\n\r\nCheck if prepared unit files are used (`systemctl cat`)\r\n\r\n```\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl cat hadoop-hdfs-$service_name; done\r\n\r\n# /usr/lib/systemd/system/hadoop-hdfs-namenode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop NameNode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop namenode\r\n# /usr/lib/systemd/system/hadoop-hdfs-datanode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop DataNode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start datanode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop datanode\r\n# /usr/lib/systemd/system/hadoop-hdfs-journalnode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop Journalnode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop journalnode\r\n# /usr/lib/systemd/system/hadoop-hdfs-secondarynamenode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop Secondary NameNode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop secondarynamenode\r\n# /usr/lib/systemd/system/hadoop-hdfs-zkfc.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop ZKFC\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop zkfc\r\n# /usr/lib/systemd/system/hadoop-hdfs-dfsrouter.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop dfsrouter\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop dfsrouter\r\n```\r\n\r\nCheck if they work after restart (container restart, `systemctl start`)\r\n\r\n```\r\n$ docker restart (container id)\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl start hadoop-hdfs-$service_name; done\r\nJob for hadoop-hdfs-zkfc.service failed because the control process exited with error code.\r\nSee \"systemctl status hadoop-hdfs-zkfc.service\" and \"journalctl -xe\" for details.\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do systemctl status hadoop-hdfs-$service_name; done\r\n\r\n● hadoop-hdfs-namenode.service - Hadoop NameNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-namenode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 08:40:09 UTC; 5min ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 48 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode (code=exited, status=0/SUCCESS)\r\n Main PID: 101 (java)\r\n    Tasks: 73 (limit: 98358)\r\n   Memory: 439.6M\r\n   CGroup: /docker/8dc2c59a2af63935747266a2157e516fee3c344f22909d4a6733ae0247bce98b/system.slice/hadoop-hdfs-namenode.service\r\n           └─101 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyarn>\r\nJan 09 08:40:06 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: User lookup succeeded: uid=996 gid=993\r\nJan 09 08:40:06 8dc2c59a2af6 systemd[48]: hadoop-hdfs-namenode.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Child 48 belongs to hadoop-hdfs-namenode.service.\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Control process exited, code=exited status=0\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Got final SIGCHLD for state start.\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Main PID guessed: 101\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Changed start -> running\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Job hadoop-hdfs-namenode.service/start finished, result=done\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: Started Hadoop NameNode.\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Failed to send unit change signal for hadoop-hdfs-namenode.service: Connection reset by peer\r\n● hadoop-hdfs-datanode.service - Hadoop DataNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-datanode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 08:40:11 UTC; 5min ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 151 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start datanode (code=exited, status=0/SUCCESS)\r\n Main PID: 206 (java)\r\n    Tasks: 65 (limit: 98358)\r\n   Memory: 238.3M\r\n   CGroup: /docker/8dc2c59a2af63935747266a2157e516fee3c344f22909d4a6733ae0247bce98b/system.slice/hadoop-hdfs-datanode.service\r\n           └─206 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.l>\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: User lookup succeeded: uid=996 gid=993\r\nJan 09 08:40:09 8dc2c59a2af6 systemd[151]: hadoop-hdfs-datanode.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start datanode\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Child 151 belongs to hadoop-hdfs-datanode.service.\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Control process exited, code=exited status=0\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Got final SIGCHLD for state start.\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Main PID guessed: 206\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Changed start -> running\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Job hadoop-hdfs-datanode.service/start finished, result=done\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: Started Hadoop DataNode.\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Failed to send unit change signal for hadoop-hdfs-datanode.service: Connection reset by peer\r\n● hadoop-hdfs-journalnode.service - Hadoop Journalnode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-journalnode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 08:40:10 UTC; 5min ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 322 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode (code=exited, status=0/SUCCESS)\r\n Main PID: 373 (java)\r\n    Tasks: 44 (limit: 98358)\r\n   Memory: 163.7M\r\n   CGroup: /docker/8dc2c59a2af63935747266a2157e516fee3c344f22909d4a6733ae0247bce98b/system.slice/hadoop-hdfs-journalnode.service\r\n           └─373 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_journalnode -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-journaln>\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: User lookup succeeded: uid=996 gid=993\r\nJan 09 08:40:11 8dc2c59a2af6 systemd[322]: hadoop-hdfs-journalnode.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Child 322 belongs to hadoop-hdfs-journalnode.service.\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Control process exited, code=exited status=0\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Got final SIGCHLD for state start.\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Main PID guessed: 373\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Changed start -> running\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Job hadoop-hdfs-journalnode.service/start finished, result=done\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[1]: Started Hadoop Journalnode.\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Failed to send unit change signal for hadoop-hdfs-journalnode.service: Connection reset by peer\r\n● hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-secondarynamenode.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 08:40:12 UTC; 5min ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 454 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode (code=exited, status=0/SUCCESS)\r\n Main PID: 505 (java)\r\n    Tasks: 37 (limit: 98358)\r\n   Memory: 340.7M\r\n   CGroup: /docker/8dc2c59a2af63935747266a2157e516fee3c344f22909d4a6733ae0247bce98b/system.slice/hadoop-hdfs-secondarynamenode.service\r\n           └─505 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_secondarynamenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremo>\r\n\r\nJan 09 08:40:10 8dc2c59a2af6 systemd[454]: hadoop-hdfs-secondarynamenode.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode\r\nJan 09 08:40:10 8dc2c59a2af6 hdfs[454]: WARNING: HADOOP_SECONDARYNAMENODE_OPTS has been replaced by HDFS_SECONDARYNAMENODE_OPTS. Using value of HADOOP_SECONDARYNAMENODE_OPTS.\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Child 454 belongs to hadoop-hdfs-secondarynamenode.service.\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Control process exited, code=exited status=0\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Got final SIGCHLD for state start.\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Main PID guessed: 505\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Changed start -> running\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Job hadoop-hdfs-secondarynamenode.service/start finished, result=done\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[1]: Started Hadoop Secondary NameNode.\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Failed to send unit change signal for hadoop-hdfs-secondarynamenode.service: Connection reset by peer\r\n● hadoop-hdfs-zkfc.service - Hadoop ZKFC\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-zkfc.service; static; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2025-01-09 08:40:14 UTC; 5min ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 551 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc (code=exited, status=1/FAILURE)\r\n\r\nJan 09 08:40:12 8dc2c59a2af6 systemd[551]: hadoop-hdfs-zkfc.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc\r\nJan 09 08:40:13 8dc2c59a2af6 hdfs[551]: ERROR: Cannot set priority of zkfc process 602\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Child 551 belongs to hadoop-hdfs-zkfc.service.\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Control process exited, code=exited status=1\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Got final SIGCHLD for state start.\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Failed with result 'exit-code'.\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Changed start -> failed\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Job hadoop-hdfs-zkfc.service/start finished, result=failed\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: Failed to start Hadoop ZKFC.\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Unit entered failed state.\r\n● hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-dfsrouter.service; static; vendor preset: disabled)\r\n   Active: active (running) since Thu 2025-01-09 08:40:17 UTC; 4min 59s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 636 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter (code=exited, status=0/SUCCESS)\r\n Main PID: 687 (java)\r\n    Tasks: 69 (limit: 98358)\r\n   Memory: 296.8M\r\n   CGroup: /docker/8dc2c59a2af63935747266a2157e516fee3c344f22909d4a6733ae0247bce98b/system.slice/hadoop-hdfs-dfsrouter.service\r\n           └─687 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_dfsrouter -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-dfsrouter->\r\n\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: User lookup succeeded: uid=996 gid=993\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[636]: hadoop-hdfs-dfsrouter.service: Executing: /usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter\r\nJan 09 08:40:17 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Child 636 belongs to hadoop-hdfs-dfsrouter.service.\r\nJan 09 08:40:17 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Control process exited, code=exited status=0\r\nJan 09 08:40:17 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Got final SIGCHLD for state start.\r\nJan 09 08:40:17 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Main PID guessed: 687\r\nJan 09 08:40:17 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Changed start -> running\r\nJan 09 08:40:17 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Job hadoop-hdfs-dfsrouter.service/start finished, result=done\r\nJan 09 08:40:17 8dc2c59a2af6 systemd[1]: Started Hadoop dfsrouter.\r\nJan 09 08:40:17 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Failed to send unit change signal for hadoop-hdfs-dfsrouter.service: Connection reset by peer\r\n```\r\n\r\nCheck if we can stop the service (`systemctl stop`)\r\n\r\n```\r\n[root@8dc2c59a2af6 /]# for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl stop hadoop-hdfs-$service_name; done\r\n[root@8dc2c59a2af6 /]# for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl status hadoop-hdfs-$service_name; done\r\n● hadoop-hdfs-namenode.service - Hadoop NameNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-namenode.service; static; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2025-01-09 08:46:57 UTC; 7s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 841 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop namenode (code=exited, status=0/SUCCESS)\r\n  Process: 48 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode (code=exited, status=0/SUCCESS)\r\n Main PID: 101 (code=exited, status=143)\r\n\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Child 101 belongs to hadoop-hdfs-namenode.service.\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Main process exited, code=exited, status=143/n/a\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Child 841 belongs to hadoop-hdfs-namenode.service.\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Control process exited, code=exited status=0\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Got final SIGCHLD for state stop.\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Failed with result 'exit-code'.\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Changed stop -> failed\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Job hadoop-hdfs-namenode.service/stop finished, result=done\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: Stopped Hadoop NameNode.\r\nJan 09 08:46:57 8dc2c59a2af6 systemd[1]: hadoop-hdfs-namenode.service: Unit entered failed state.\r\n● hadoop-hdfs-datanode.service - Hadoop DataNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-datanode.service; static; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2025-01-09 08:46:59 UTC; 5s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 900 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop datanode (code=exited, status=0/SUCCESS)\r\n  Process: 151 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start datanode (code=exited, status=0/SUCCESS)\r\n Main PID: 206 (code=exited, status=143)\r\n\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Child 206 belongs to hadoop-hdfs-datanode.service.\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Main process exited, code=exited, status=143/n/a\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Child 900 belongs to hadoop-hdfs-datanode.service.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Control process exited, code=exited status=0\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Got final SIGCHLD for state stop.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Failed with result 'exit-code'.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Changed stop -> failed\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Job hadoop-hdfs-datanode.service/stop finished, result=done\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: Stopped Hadoop DataNode.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-datanode.service: Unit entered failed state.\r\n● hadoop-hdfs-journalnode.service - Hadoop Journalnode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-journalnode.service; static; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2025-01-09 08:47:00 UTC; 4s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 961 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop journalnode (code=exited, status=0/SUCCESS)\r\n  Process: 322 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode (code=exited, status=0/SUCCESS)\r\n Main PID: 373 (code=exited, status=143)\r\n\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Child 373 belongs to hadoop-hdfs-journalnode.service.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Main process exited, code=exited, status=143/n/a\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Child 961 belongs to hadoop-hdfs-journalnode.service.\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Control process exited, code=exited status=0\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Got final SIGCHLD for state stop.\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Failed with result 'exit-code'.\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Changed stop -> failed\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Job hadoop-hdfs-journalnode.service/stop finished, result=done\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: Stopped Hadoop Journalnode.\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-journalnode.service: Unit entered failed state.\r\n● hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-secondarynamenode.service; static; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2025-01-09 08:46:58 UTC; 6s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 1020 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop secondarynamenode (code=exited, status=0/SUCCESS)\r\n  Process: 454 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode (code=exited, status=0/SUCCESS)\r\n Main PID: 505 (code=exited, status=143)\r\n\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Child 505 belongs to hadoop-hdfs-secondarynamenode.service.\r\nJan 09 08:47:00 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Main process exited, code=exited, status=143/n/a\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Child 1020 belongs to hadoop-hdfs-secondarynamenode.service.\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Control process exited, code=exited status=0\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Got final SIGCHLD for state stop.\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Failed with result 'exit-code'.\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Changed stop -> failed\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Job hadoop-hdfs-secondarynamenode.service/stop finished, result=done\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: Stopped Hadoop Secondary NameNode.\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-secondarynamenode.service: Unit entered failed state.\r\n● hadoop-hdfs-zkfc.service - Hadoop ZKFC\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-zkfc.service; static; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2025-01-09 08:40:14 UTC; 6min ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 551 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc (code=exited, status=1/FAILURE)\r\n\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Got final SIGCHLD for state start.\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Failed with result 'exit-code'.\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Changed start -> failed\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Job hadoop-hdfs-zkfc.service/start finished, result=failed\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: Failed to start Hadoop ZKFC.\r\nJan 09 08:40:14 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Unit entered failed state.\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Trying to enqueue job hadoop-hdfs-zkfc.service/stop/replace\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Installed new job hadoop-hdfs-zkfc.service/stop as 279\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Enqueued job hadoop-hdfs-zkfc.service/stop as 279\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-zkfc.service: Job hadoop-hdfs-zkfc.service/stop finished, result=done\r\n● hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter\r\n   Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-dfsrouter.service; static; vendor preset: disabled)\r\n   Active: failed (Result: exit-code) since Thu 2025-01-09 08:46:59 UTC; 5s ago\r\n     Docs: https://hadoop.apache.org/\r\n  Process: 1083 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop dfsrouter (code=exited, status=0/SUCCESS)\r\n  Process: 636 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter (code=exited, status=0/SUCCESS)\r\n Main PID: 687 (code=exited, status=143)\r\n\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Child 687 belongs to hadoop-hdfs-dfsrouter.service.\r\nJan 09 08:46:58 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Main process exited, code=exited, status=143/n/a\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Child 1083 belongs to hadoop-hdfs-dfsrouter.service.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Control process exited, code=exited status=0\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Got final SIGCHLD for state stop.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Failed with result 'exit-code'.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Changed stop -> failed\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Job hadoop-hdfs-dfsrouter.service/stop finished, result=done\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: Stopped Hadoop dfsrouter.\r\nJan 09 08:46:59 8dc2c59a2af6 systemd[1]: hadoop-hdfs-dfsrouter.service: Unit entered failed state.\r\n```\r\n</details>","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2579505184/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2579516737","html_url":"https://github.com/apache/bigtop/pull/1322#issuecomment-2579516737","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1322","id":2579516737,"node_id":"IC_kwDOACDj7M6ZwE1B","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-01-09T09:04:06Z","updated_at":"2025-01-09T09:04:06Z","author_association":"CONTRIBUTOR","body":"DEB test result\r\n\r\n<details>\r\nBuilding apt on Ubuntu 24.04\r\n\r\n```\r\n$ ./gradlew hadoop-clean bigtop-utils-pkg bigtop-jsvc-pkg bigtop-groovy-pkg hadoop-pkg repo --stacktrace\r\n```\r\n\r\nSmoke tests on Ubuntu 24.04\r\n```\r\n$ cd provisioner/docker\r\n$ ./docker-hadoop.sh --enable-local-repo --disable-gpg-check     --docker-compose-plugin     -C config_ubuntu-24.04.yaml     -F docker-compose-cgroupv2.yml     --stack hdfs,yarn,mapreduce --smoke-tests hdfs -c 3\r\n\r\n(snip)\r\n\r\n\r\n> Task :bigtop-tests:smoke-tests:hdfs:test\r\nFinished generating test XML results (0.018 secs) into: /bigtop-home/bigtop-tests/smoke-tests/hdfs/build/test-results/test\r\nGenerating HTML test report...\r\nFinished generating test html results (0.021 secs) into: /bigtop-home/bigtop-tests/smoke-tests/hdfs/build/reports/tests/test\r\nNow testing...\r\n:bigtop-tests:smoke-tests:hdfs:test (Thread[Execution worker for ':',5,main]) completed. Took 8 mins 23.416 secs.\r\n\r\nBUILD SUCCESSFUL in 8m 58s\r\n29 actionable tasks: 11 executed, 18 up-to-date\r\nStopped 1 worker daemon(s).\r\n+ rm -rf buildSrc/build/test-results/binary\r\n+ rm -rf /bigtop-home/.gradle\r\n```\r\n\r\n\r\nInstall  additional packages  (journalnode, zkfc, dfsrouter, secondarynamenode) \r\n\r\n```\r\n$ ./docker-hadoop.sh -dcp --exec 1 /bin/bash\r\n$ apt install hadoop-hdfs-journalnode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-hdfs-dfsrouter -y\r\n```\r\n\r\n Check if systemctl works (`systemctl start` & `systemctl status`,)\r\n\r\n```\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl start hadoop-hdfs-$service_name; done\r\nJob for hadoop-hdfs-zkfc.service failed because the control process exited with error code.\r\nSee \"systemctl status hadoop-hdfs-zkfc.service\" and \"journalctl -xe\" for details.\r\n```\r\n\r\n```\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl status hadoop-hdfs-$service_name; done\r\n\r\n● hadoop-hdfs-namenode.service - Hadoop NameNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-namenode.service; static)\r\n     Active: active (running) since Wed 2025-01-08 13:57:10 UTC; 22min ago\r\n       Docs: https://hadoop.apache.org/\r\n   Main PID: 5189 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-namenode.service\r\n             └─5189 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-namenode-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-namenode-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.namenode.NameNode\r\n\r\nJan 08 13:57:08 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-namenode.service - Hadoop NameNode...\r\nJan 08 13:57:10 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-namenode.service - Hadoop NameNode.\r\n● hadoop-hdfs-datanode.service - Hadoop DataNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-datanode.service; static)\r\n     Active: active (running) since Wed 2025-01-08 13:58:01 UTC; 21min ago\r\n       Docs: https://hadoop.apache.org/\r\n   Main PID: 6536 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-datanode.service\r\n             └─6536 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-datanode-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-datanode-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.datanode.DataNode\r\n\r\nJan 08 13:57:59 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-datanode.service - Hadoop DataNode...\r\nJan 08 13:58:01 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-datanode.service - Hadoop DataNode.\r\n● hadoop-hdfs-journalnode.service - Hadoop Journalnode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-journalnode.service; static)\r\n     Active: active (running) since Wed 2025-01-08 14:16:41 UTC; 2min 52s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 26960 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode (code=exited, status=0/SUCCESS)\r\n   Main PID: 26999 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-journalnode.service\r\n             └─26999 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_journalnode -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-journalnode-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-journalnode-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.qjournal.server.JournalNode\r\n\r\nJan 08 14:16:39 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-journalnode.service - Hadoop Journalnode...\r\nJan 08 14:16:41 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-journalnode.service - Hadoop Journalnode.\r\n● hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-secondarynamenode.service; static)\r\n     Active: active (running) since Wed 2025-01-08 14:16:44 UTC; 2min 50s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 27053 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode (code=exited, status=0/SUCCESS)\r\n   Main PID: 27092 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-secondarynamenode.service\r\n             └─27092 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_secondarynamenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-secondarynamenode-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-secondarynamenode-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\r\n\r\nJan 08 14:16:42 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode...\r\nJan 08 14:16:42 2e2d226cb5b7 hdfs[27053]: WARNING: HADOOP_SECONDARYNAMENODE_OPTS has been replaced by HDFS_SECONDARYNAMENODE_OPTS. Using value of HADOOP_SECONDARYNAMENODE_OPTS.\r\nJan 08 14:16:44 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode.\r\n× hadoop-hdfs-zkfc.service - Hadoop ZKFC\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-zkfc.service; static)\r\n     Active: failed (Result: exit-code) since Wed 2025-01-08 14:16:46 UTC; 2min 48s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 27139 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc (code=exited, status=1/FAILURE)\r\n\r\nJan 08 14:16:44 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-zkfc.service - Hadoop ZKFC...\r\nJan 08 14:16:45 2e2d226cb5b7 hdfs[27139]: ERROR: Cannot set priority of zkfc process 27178\r\nJan 08 14:16:46 2e2d226cb5b7 systemd[1]: hadoop-hdfs-zkfc.service: Control process exited, code=exited, status=1/FAILURE\r\nJan 08 14:16:46 2e2d226cb5b7 systemd[1]: hadoop-hdfs-zkfc.service: Failed with result 'exit-code'.\r\nJan 08 14:16:46 2e2d226cb5b7 systemd[1]: Failed to start hadoop-hdfs-zkfc.service - Hadoop ZKFC.\r\n● hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-dfsrouter.service; static)\r\n     Active: active (running) since Wed 2025-01-08 14:16:48 UTC; 2min 46s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 27213 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter (code=exited, status=0/SUCCESS)\r\n   Main PID: 27252 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-dfsrouter.service\r\n             └─27252 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_dfsrouter -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-dfsrouter-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-dfsrouter-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.federation.router.DFSRouter\r\n\r\nJan 08 14:16:46 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter...\r\nJan 08 14:16:48 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter.\r\n```\r\n\r\nWhile ZKFC failed to start, we can confirm that it can launch via systemd but shutdown immediately because we didn't configure HA settings (see the log below). \r\n\r\n```\r\n$ cat /var/log/hadoop-hdfs/hadoop-hdfs-zkfc-2e2d226cb5b7.log\r\n\r\n(snip)\r\n\r\n2025-01-08 14:16:44,631 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: registered UNIX signal handlers for [TERM, HUP, INT]\r\n2025-01-08 14:16:44,849 ERROR org.apache.hadoop.hdfs.tools.DFSZKFailoverController: DFSZKFailOverController exiting due to earlier exception org.apache.hadoop.HadoopIllegalArgumentException: HA is not enabled for this namenode.\r\n2025-01-08 14:16:44,851 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.HadoopIllegalArgumentException: HA is not enabled for this namenode.\r\n2025-01-08 14:16:44,853 INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: SHUTDOWN_MSG:\r\n/************************************************************\r\nSHUTDOWN_MSG: Shutting down DFSZKFailoverController at 2e2d226cb5b7.bigtop.apache.org/172.19.0.3\r\n************************************************************/\r\n```\r\n\r\nCheck if prepared unit files are used (`systemctl cat`)\r\n\r\n```\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl cat hadoop-hdfs-$service_name; done\r\n\r\nroot@2e2d226cb5b7:/# for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl cat hadoop-hdfs-$service_name; done\r\n# /usr/lib/systemd/system/hadoop-hdfs-namenode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop NameNode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop namenode\r\n# /usr/lib/systemd/system/hadoop-hdfs-datanode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop DataNode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start datanode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop datanode\r\n# /usr/lib/systemd/system/hadoop-hdfs-journalnode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop Journalnode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop journalnode\r\n# /usr/lib/systemd/system/hadoop-hdfs-secondarynamenode.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop Secondary NameNode\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop secondarynamenode\r\n# /usr/lib/systemd/system/hadoop-hdfs-zkfc.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop ZKFC\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop zkfc\r\n# /usr/lib/systemd/system/hadoop-hdfs-dfsrouter.service\r\n# Licensed to the Apache Software Foundation (ASF) under one or more\r\n# contributor license agreements.  See the NOTICE file distributed with\r\n# this work for additional information regarding copyright ownership.\r\n# The ASF licenses this file to You under the Apache License, Version 2.0\r\n# (the \"License\"); you may not use this file except in compliance with\r\n# the License.  You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n[Unit]\r\nDocumentation=https://hadoop.apache.org/\r\nDescription=Hadoop dfsrouter\r\nBefore=multi-user.target\r\nBefore=graphical.target\r\nAfter=remote-fs.target\r\n\r\n[Service]\r\nUser=hdfs\r\nGroup=hdfs\r\nType=forking\r\nRestart=no\r\nTimeoutSec=5min\r\nIgnoreSIGPIPE=no\r\nKillMode=process\r\nRemainAfterExit=no\r\nSuccessExitStatus=5 6\r\nExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter\r\nExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop dfsrouter\r\n```\r\n\r\nCheck if they work after restart (container restart, `systemctl start`)\r\n\r\n```\r\n$ docker restart\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl start hadoop-hdfs-$service_name; done\r\n$ for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do systemctl status hadoop-hdfs-$service_name; done\r\n\r\n● hadoop-hdfs-namenode.service - Hadoop NameNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-namenode.service; static)\r\n     Active: active (running) since Wed 2025-01-08 14:31:23 UTC; 22s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 149 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode (code=exited, status=0/SUCCESS)\r\n   Main PID: 189 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-namenode.service\r\n             └─189 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-namenode-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-namenode-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.namenode.NameNode\r\n\r\nJan 08 14:31:21 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-namenode.service - Hadoop NameNode...\r\nJan 08 14:31:23 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-namenode.service - Hadoop NameNode.\r\n● hadoop-hdfs-datanode.service - Hadoop DataNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-datanode.service; static)\r\n     Active: active (running) since Wed 2025-01-08 14:31:25 UTC; 20s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 237 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start datanode (code=exited, status=0/SUCCESS)\r\n   Main PID: 278 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-datanode.service\r\n             └─278 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_datanode -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-datanode-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-datanode-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.datanode.DataNode\r\n\r\nJan 08 14:31:23 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-datanode.service - Hadoop DataNode...\r\nJan 08 14:31:25 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-datanode.service - Hadoop DataNode.\r\n● hadoop-hdfs-journalnode.service - Hadoop Journalnode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-journalnode.service; static)\r\n     Active: active (running) since Wed 2025-01-08 14:31:27 UTC; 18s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 397 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode (code=exited, status=0/SUCCESS)\r\n   Main PID: 439 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-journalnode.service\r\n             └─439 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_journalnode -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-journalnode-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-journalnode-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.qjournal.server.JournalNode\r\n\r\nJan 08 14:31:25 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-journalnode.service - Hadoop Journalnode...\r\nJan 08 14:31:27 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-journalnode.service - Hadoop Journalnode.\r\n● hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-secondarynamenode.service; static)\r\n     Active: active (running) since Wed 2025-01-08 14:31:29 UTC; 16s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 520 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode (code=exited, status=0/SUCCESS)\r\n   Main PID: 560 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-secondarynamenode.service\r\n             └─560 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_secondarynamenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-secondarynamenode-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-secondarynamenode-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode\r\n\r\nJan 08 14:31:27 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode...\r\nJan 08 14:31:27 2e2d226cb5b7 hdfs[520]: WARNING: HADOOP_SECONDARYNAMENODE_OPTS has been replaced by HDFS_SECONDARYNAMENODE_OPTS. Using value of HADOOP_SECONDARYNAMENODE_OPTS.\r\nJan 08 14:31:29 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode.\r\n× hadoop-hdfs-zkfc.service - Hadoop ZKFC\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-zkfc.service; static)\r\n     Active: failed (Result: exit-code) since Wed 2025-01-08 14:31:31 UTC; 13s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 607 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc (code=exited, status=1/FAILURE)\r\n\r\nJan 08 14:31:29 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-zkfc.service - Hadoop ZKFC...\r\nJan 08 14:31:30 2e2d226cb5b7 hdfs[607]: ERROR: Cannot set priority of zkfc process 647\r\nJan 08 14:31:31 2e2d226cb5b7 systemd[1]: hadoop-hdfs-zkfc.service: Control process exited, code=exited, status=1/FAILURE\r\nJan 08 14:31:31 2e2d226cb5b7 systemd[1]: hadoop-hdfs-zkfc.service: Failed with result 'exit-code'.\r\nJan 08 14:31:31 2e2d226cb5b7 systemd[1]: Failed to start hadoop-hdfs-zkfc.service - Hadoop ZKFC.\r\n● hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-dfsrouter.service; static)\r\n     Active: active (running) since Wed 2025-01-08 14:31:33 UTC; 11s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 682 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter (code=exited, status=0/SUCCESS)\r\n   Main PID: 722 (java)\r\n     CGroup: /docker/2e2d226cb5b756925802abb1cd7cd84d9002f6b521043c8111411efa5dea836b/system.slice/hadoop-hdfs-dfsrouter.service\r\n             └─722 /usr/lib/jvm/java-1.8.0-openjdk-amd64/bin/java -Dproc_dfsrouter -Djava.net.preferIPv4Stack=true -Dyarn.log.dir=/var/log/hadoop-hdfs -Dyarn.log.file=hadoop-hdfs-dfsrouter-2e2d226cb5b7.log -Dyarn.home.dir=/usr/lib/hadoop-yarn -Dyarn.root.logger=INFO,console -Djava.library.path=//usr/lib/hadoop/lib/native -Dhadoop.log.dir=/var/log/hadoop-hdfs -Dhadoop.log.file=hadoop-hdfs-dfsrouter-2e2d226cb5b7.log -Dhadoop.home.dir=//usr/lib/hadoop -Dhadoop.id.str=hdfs -Dhadoop.root.logger=INFO,RFA -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.hdfs.server.federation.router.DFSRouter\r\n\r\nJan 08 14:31:31 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter...\r\nJan 08 14:31:33 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter.\r\n```\r\n\r\nCheck if we can stop the service (`systemctl stop`)\r\n\r\n```\r\nroot@2e2d226cb5b7:/# ls -lah  /run/hadoop-hdfs\r\ntotal 24K\r\ndrwxr-xr-x  2 hdfs hadoop 160 Jan  8 14:31 .\r\ndrwxr-xr-x 16 root root   380 Jan  8 14:30 ..\r\n-rw-r--r--  1 hdfs hdfs     4 Jan  8 14:31 hadoop-hdfs-datanode.pid\r\n-rw-r--r--  1 hdfs hdfs     4 Jan  8 14:31 hadoop-hdfs-dfsrouter.pid\r\n-rw-r--r--  1 hdfs hdfs     4 Jan  8 14:31 hadoop-hdfs-journalnode.pid\r\n-rw-r--r--  1 hdfs hdfs     4 Jan  8 14:31 hadoop-hdfs-namenode.pid\r\n-rw-r--r--  1 hdfs hdfs     4 Jan  8 14:31 hadoop-hdfs-secondarynamenode.pid\r\n-rw-r--r--  1 hdfs hdfs     4 Jan  8 14:31 hadoop-hdfs-zkfc.pid\r\nroot@2e2d226cb5b7:/# for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl stop hadoop-hdfs-$service_name; done\r\nroot@2e2d226cb5b7:/# for service_name in namenode datanode journalnode secondarynamenode zkfc dfsrouter; do  systemctl status hadoop-hdfs-$service_name; done\r\n× hadoop-hdfs-namenode.service - Hadoop NameNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-namenode.service; static)\r\n     Active: failed (Result: exit-code) since Wed 2025-01-08 14:33:28 UTC; 14s ago\r\n   Duration: 2min 3.941s\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 149 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start namenode (code=exited, status=0/SUCCESS)\r\n    Process: 828 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop namenode (code=exited, status=0/SUCCESS)\r\n   Main PID: 189 (code=exited, status=143)\r\n\r\nJan 08 14:31:21 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-namenode.service - Hadoop NameNode...\r\nJan 08 14:31:23 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-namenode.service - Hadoop NameNode.\r\nJan 08 14:33:27 2e2d226cb5b7 systemd[1]: Stopping hadoop-hdfs-namenode.service - Hadoop NameNode...\r\nJan 08 14:33:27 2e2d226cb5b7 systemd[1]: hadoop-hdfs-namenode.service: Main process exited, code=exited, status=143/n/a\r\nJan 08 14:33:28 2e2d226cb5b7 systemd[1]: hadoop-hdfs-namenode.service: Failed with result 'exit-code'.\r\nJan 08 14:33:28 2e2d226cb5b7 systemd[1]: Stopped hadoop-hdfs-namenode.service - Hadoop NameNode.\r\n× hadoop-hdfs-datanode.service - Hadoop DataNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-datanode.service; static)\r\n     Active: failed (Result: exit-code) since Wed 2025-01-08 14:33:29 UTC; 13s ago\r\n   Duration: 2min 2.940s\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 237 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start datanode (code=exited, status=0/SUCCESS)\r\n    Process: 876 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop datanode (code=exited, status=0/SUCCESS)\r\n   Main PID: 278 (code=exited, status=143)\r\n\r\nJan 08 14:31:23 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-datanode.service - Hadoop DataNode...\r\nJan 08 14:31:25 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-datanode.service - Hadoop DataNode.\r\nJan 08 14:33:28 2e2d226cb5b7 systemd[1]: Stopping hadoop-hdfs-datanode.service - Hadoop DataNode...\r\nJan 08 14:33:28 2e2d226cb5b7 systemd[1]: hadoop-hdfs-datanode.service: Main process exited, code=exited, status=143/n/a\r\nJan 08 14:33:29 2e2d226cb5b7 systemd[1]: hadoop-hdfs-datanode.service: Failed with result 'exit-code'.\r\nJan 08 14:33:29 2e2d226cb5b7 systemd[1]: Stopped hadoop-hdfs-datanode.service - Hadoop DataNode.\r\n× hadoop-hdfs-journalnode.service - Hadoop Journalnode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-journalnode.service; static)\r\n     Active: failed (Result: exit-code) since Wed 2025-01-08 14:33:30 UTC; 12s ago\r\n   Duration: 2min 1.939s\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 397 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start journalnode (code=exited, status=0/SUCCESS)\r\n    Process: 926 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop journalnode (code=exited, status=0/SUCCESS)\r\n   Main PID: 439 (code=exited, status=143)\r\n\r\nJan 08 14:31:25 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-journalnode.service - Hadoop Journalnode...\r\nJan 08 14:31:27 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-journalnode.service - Hadoop Journalnode.\r\nJan 08 14:33:29 2e2d226cb5b7 systemd[1]: Stopping hadoop-hdfs-journalnode.service - Hadoop Journalnode...\r\nJan 08 14:33:29 2e2d226cb5b7 systemd[1]: hadoop-hdfs-journalnode.service: Main process exited, code=exited, status=143/n/a\r\nJan 08 14:33:30 2e2d226cb5b7 systemd[1]: hadoop-hdfs-journalnode.service: Failed with result 'exit-code'.\r\nJan 08 14:33:30 2e2d226cb5b7 systemd[1]: Stopped hadoop-hdfs-journalnode.service - Hadoop Journalnode.\r\n× hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-secondarynamenode.service; static)\r\n     Active: failed (Result: exit-code) since Wed 2025-01-08 14:33:31 UTC; 10s ago\r\n   Duration: 2min 932ms\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 520 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start secondarynamenode (code=exited, status=0/SUCCESS)\r\n    Process: 974 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop secondarynamenode (code=exited, status=0/SUCCESS)\r\n   Main PID: 560 (code=exited, status=143)\r\n\r\nJan 08 14:31:27 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode...\r\nJan 08 14:31:27 2e2d226cb5b7 hdfs[520]: WARNING: HADOOP_SECONDARYNAMENODE_OPTS has been replaced by HDFS_SECONDARYNAMENODE_OPTS. Using value of HADOOP_SECONDARYNAMENODE_OPTS.\r\nJan 08 14:31:29 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode.\r\nJan 08 14:33:30 2e2d226cb5b7 systemd[1]: Stopping hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode...\r\nJan 08 14:33:30 2e2d226cb5b7 hdfs[974]: WARNING: HADOOP_SECONDARYNAMENODE_OPTS has been replaced by HDFS_SECONDARYNAMENODE_OPTS. Using value of HADOOP_SECONDARYNAMENODE_OPTS.\r\nJan 08 14:33:30 2e2d226cb5b7 systemd[1]: hadoop-hdfs-secondarynamenode.service: Main process exited, code=exited, status=143/n/a\r\nJan 08 14:33:31 2e2d226cb5b7 systemd[1]: hadoop-hdfs-secondarynamenode.service: Failed with result 'exit-code'.\r\nJan 08 14:33:31 2e2d226cb5b7 systemd[1]: Stopped hadoop-hdfs-secondarynamenode.service - Hadoop Secondary NameNode.\r\n× hadoop-hdfs-zkfc.service - Hadoop ZKFC\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-zkfc.service; static)\r\n     Active: failed (Result: exit-code) since Wed 2025-01-08 14:31:31 UTC; 2min 10s ago\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 607 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start zkfc (code=exited, status=1/FAILURE)\r\n\r\nJan 08 14:31:29 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-zkfc.service - Hadoop ZKFC...\r\nJan 08 14:31:30 2e2d226cb5b7 hdfs[607]: ERROR: Cannot set priority of zkfc process 647\r\nJan 08 14:31:31 2e2d226cb5b7 systemd[1]: hadoop-hdfs-zkfc.service: Control process exited, code=exited, status=1/FAILURE\r\nJan 08 14:31:31 2e2d226cb5b7 systemd[1]: hadoop-hdfs-zkfc.service: Failed with result 'exit-code'.\r\nJan 08 14:31:31 2e2d226cb5b7 systemd[1]: Failed to start hadoop-hdfs-zkfc.service - Hadoop ZKFC.\r\n× hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter\r\n     Loaded: loaded (/usr/lib/systemd/system/hadoop-hdfs-dfsrouter.service; static)\r\n     Active: failed (Result: exit-code) since Wed 2025-01-08 14:33:32 UTC; 9s ago\r\n   Duration: 1min 57.833s\r\n       Docs: https://hadoop.apache.org/\r\n    Process: 682 ExecStart=/usr/bin/hdfs --config /etc/hadoop/conf --daemon start dfsrouter (code=exited, status=0/SUCCESS)\r\n    Process: 1024 ExecStop=/usr/bin/hdfs --config /etc/hadoop/conf --daemon stop dfsrouter (code=exited, status=0/SUCCESS)\r\n   Main PID: 722 (code=exited, status=143)\r\n\r\nJan 08 14:31:31 2e2d226cb5b7 systemd[1]: Starting hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter...\r\nJan 08 14:31:33 2e2d226cb5b7 systemd[1]: Started hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter.\r\nJan 08 14:33:31 2e2d226cb5b7 systemd[1]: Stopping hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter...\r\nJan 08 14:33:31 2e2d226cb5b7 systemd[1]: hadoop-hdfs-dfsrouter.service: Main process exited, code=exited, status=143/n/a\r\nJan 08 14:33:32 2e2d226cb5b7 systemd[1]: hadoop-hdfs-dfsrouter.service: Failed with result 'exit-code'.\r\nJan 08 14:33:32 2e2d226cb5b7 systemd[1]: Stopped hadoop-hdfs-dfsrouter.service - Hadoop dfsrouter.\r\n```\r\n</details>","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2579516737/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2579534164","html_url":"https://github.com/apache/bigtop/pull/1322#issuecomment-2579534164","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1322","id":2579534164,"node_id":"IC_kwDOACDj7M6ZwJFU","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-01-09T09:10:24Z","updated_at":"2025-01-09T09:10:49Z","author_association":"CONTRIBUTOR","body":"Make this PR as review ready. Could anyone review this?\r\n\r\nThere are some todos that I noticed. After merging this PR, I'll work on these.\r\n* https://github.com/apache/bigtop/pull/1322#issuecomment-2573378626\r\n* We should treat exit code = 143 as normal","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2579534164/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2582327245","html_url":"https://github.com/apache/bigtop/pull/1322#issuecomment-2582327245","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1322","id":2582327245,"node_id":"IC_kwDOACDj7M6Z6y_N","user":{"login":"iwasakims","id":1856607,"node_id":"MDQ6VXNlcjE4NTY2MDc=","avatar_url":"https://avatars.githubusercontent.com/u/1856607?v=4","gravatar_id":"","url":"https://api.github.com/users/iwasakims","html_url":"https://github.com/iwasakims","followers_url":"https://api.github.com/users/iwasakims/followers","following_url":"https://api.github.com/users/iwasakims/following{/other_user}","gists_url":"https://api.github.com/users/iwasakims/gists{/gist_id}","starred_url":"https://api.github.com/users/iwasakims/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iwasakims/subscriptions","organizations_url":"https://api.github.com/users/iwasakims/orgs","repos_url":"https://api.github.com/users/iwasakims/repos","events_url":"https://api.github.com/users/iwasakims/events{/privacy}","received_events_url":"https://api.github.com/users/iwasakims/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-01-10T10:33:45Z","updated_at":"2025-01-10T10:33:45Z","author_association":"MEMBER","body":"> hadoop-hdfs-namenode.service: New main PID 6388 does not belong to service, and PID file is not owned by root. Refusing.\r\n\r\nWe saw the same massage on BIGTOP-3302 (#600) and BIGTOP-3356 (#640). The message implies the failure of check condition added by https://github.com/systemd/systemd/pull/7816 for [CVE-2018-16888](https://nvd.nist.gov/vuln/detail/CVE-2018-16888). su/sudo or /sys/fs/cgroup seems not to be related this time? \r\n\r\n> To workaround this, we can use GuessMainPID=yes (which is default) and comment out PIDFile.\r\n\r\nI'm fine with this workaround if it works.","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2582327245/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2582635176","html_url":"https://github.com/apache/bigtop/pull/1322#issuecomment-2582635176","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1322","id":2582635176,"node_id":"IC_kwDOACDj7M6Z7-Ko","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-01-10T12:47:35Z","updated_at":"2025-01-10T13:49:32Z","author_association":"CONTRIBUTOR","body":"@iwasakims \r\nThanks for the comment. Will check those info.\r\n\r\n> I'm fine with this workaround if it works.\r\n\r\nIt works. Please check the testing log. https://github.com/apache/bigtop/pull/1322#issuecomment-2579505184","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2582635176/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2595835165","html_url":"https://github.com/apache/bigtop/pull/1323#issuecomment-2595835165","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1323","id":2595835165,"node_id":"IC_kwDOACDj7M6auU0d","user":{"login":"iwasakims","id":1856607,"node_id":"MDQ6VXNlcjE4NTY2MDc=","avatar_url":"https://avatars.githubusercontent.com/u/1856607?v=4","gravatar_id":"","url":"https://api.github.com/users/iwasakims","html_url":"https://github.com/iwasakims","followers_url":"https://api.github.com/users/iwasakims/followers","following_url":"https://api.github.com/users/iwasakims/following{/other_user}","gists_url":"https://api.github.com/users/iwasakims/gists{/gist_id}","starred_url":"https://api.github.com/users/iwasakims/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iwasakims/subscriptions","organizations_url":"https://api.github.com/users/iwasakims/orgs","repos_url":"https://api.github.com/users/iwasakims/repos","events_url":"https://api.github.com/users/iwasakims/events{/privacy}","received_events_url":"https://api.github.com/users/iwasakims/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-01-16T14:12:33Z","updated_at":"2025-01-16T14:12:33Z","author_association":"MEMBER","body":"I could not reproduce the issue by docker provisioner (on my ubuntu-24.04 host and rockylinux-8-aarch64 host too). Maybe whether docker.slice is used or not matters.\r\n\r\nyour console output in #1322.:\r\n\r\n```\r\n   CGroup: /docker/b2c7d19976495ecf3882759e65bcd1e70f8acac1d917d274b61d83b59e99af3d/system.slice/hadoop-hdfs-namenode.service\r\n           └─7217 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyar…\r\n```\r\n\r\nmy console output:\r\n\r\n```\r\n   CGroup: /system.slice/hadoop-hdfs-namenode.service\r\n           └─7412 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.432.b06-2.el8.x86_64/bin/java -Dproc_namenode -Djava.net.preferIPv4Stack=true -Dhdfs.audit.logger=INFO,NullAppender -Dcom.sun.management.jmxremote -Dyar...\r\n```","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2595835165/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2597111403","html_url":"https://github.com/apache/bigtop/pull/1323#issuecomment-2597111403","issue_url":"https://api.github.com/repos/apache/bigtop/issues/1323","id":2597111403,"node_id":"IC_kwDOACDj7M6azMZr","user":{"login":"masatana","id":3433428,"node_id":"MDQ6VXNlcjM0MzM0Mjg=","avatar_url":"https://avatars.githubusercontent.com/u/3433428?v=4","gravatar_id":"","url":"https://api.github.com/users/masatana","html_url":"https://github.com/masatana","followers_url":"https://api.github.com/users/masatana/followers","following_url":"https://api.github.com/users/masatana/following{/other_user}","gists_url":"https://api.github.com/users/masatana/gists{/gist_id}","starred_url":"https://api.github.com/users/masatana/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/masatana/subscriptions","organizations_url":"https://api.github.com/users/masatana/orgs","repos_url":"https://api.github.com/users/masatana/repos","events_url":"https://api.github.com/users/masatana/events{/privacy}","received_events_url":"https://api.github.com/users/masatana/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2025-01-16T23:25:25Z","updated_at":"2025-01-16T23:25:25Z","author_association":"CONTRIBUTOR","body":"@iwasakims \r\nThank you for merging. I'm thinking this is a test enviornment issue. Perhaps we should consider using VM as a test instead of Docker. Will check a little more closely.\r\nBut for now, I think we can use this workaround.","reactions":{"url":"https://api.github.com/repos/apache/bigtop/issues/comments/2597111403/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]