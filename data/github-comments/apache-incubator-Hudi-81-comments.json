[{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689300968","html_url":"https://github.com/apache/hudi/pull/2058#issuecomment-689300968","issue_url":"https://api.github.com/repos/apache/hudi/issues/2058","id":689300968,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTMwMDk2OA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T04:52:19Z","updated_at":"2020-09-09T04:52:19Z","author_association":"CONTRIBUTOR","body":"@yanghua : \r\n\r\n```I just want to speed up the local build progress so that I can verify new changes frequently.```\r\n\r\nCan you kindly clarify what you mean by changes here ?  Do you mean hudi code changes ? If you are referring to hudi, we don't have to rebuild docker images to pick up latest hudi code.  The hudi codebase is mounted inside docker containers so that you can use the latest version.\r\n\r\nIf you need to rebuild docker for other reasons, one of the most common case this can happen is when we are upgrading hive, spark, presto versions. In that case the caching wont help.  Let me know if I am missing something. \r\n\r\nRegarding  using SHAs to determine if cached distributions is useful or not, this will pose additional work for someone upgrading the dependencies. Let me know your thoughts.\r\n\r\nThanks,\r\nBalaji.V\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689300968/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689317926","html_url":"https://github.com/apache/hudi/pull/2039#issuecomment-689317926","issue_url":"https://api.github.com/repos/apache/hudi/issues/2039","id":689317926,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTMxNzkyNg==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T05:40:44Z","updated_at":"2020-09-09T05:40:44Z","author_association":"CONTRIBUTOR","body":"@modi95 This LGTM, tried it manually and it works, thanks for taking this over the finish line.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689317926/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689318428","html_url":"https://github.com/apache/hudi/pull/2045#issuecomment-689318428","issue_url":"https://api.github.com/repos/apache/hudi/issues/2045","id":689318428,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTMxODQyOA==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T05:41:55Z","updated_at":"2020-09-09T05:41:55Z","author_association":"CONTRIBUTOR","body":"@nbalajee can you please rebase ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689318428/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689476777","html_url":"https://github.com/apache/hudi/pull/2012#issuecomment-689476777","issue_url":"https://api.github.com/repos/apache/hudi/issues/2012","id":689476777,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTQ3Njc3Nw==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T10:35:31Z","updated_at":"2020-09-09T10:35:31Z","author_association":"CONTRIBUTOR","body":"Lagging a bit, will circle back on this. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689476777/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689553730","html_url":"https://github.com/apache/hudi/issues/2076#issuecomment-689553730","issue_url":"https://api.github.com/repos/apache/hudi/issues/2076","id":689553730,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTU1MzczMA==","user":{"login":"Yogashri12","id":37909597,"node_id":"MDQ6VXNlcjM3OTA5NTk3","avatar_url":"https://avatars.githubusercontent.com/u/37909597?v=4","gravatar_id":"","url":"https://api.github.com/users/Yogashri12","html_url":"https://github.com/Yogashri12","followers_url":"https://api.github.com/users/Yogashri12/followers","following_url":"https://api.github.com/users/Yogashri12/following{/other_user}","gists_url":"https://api.github.com/users/Yogashri12/gists{/gist_id}","starred_url":"https://api.github.com/users/Yogashri12/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Yogashri12/subscriptions","organizations_url":"https://api.github.com/users/Yogashri12/orgs","repos_url":"https://api.github.com/users/Yogashri12/repos","events_url":"https://api.github.com/users/Yogashri12/events{/privacy}","received_events_url":"https://api.github.com/users/Yogashri12/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T13:12:07Z","updated_at":"2020-09-09T13:12:07Z","author_association":"NONE","body":"okie i will try.thank you for the response.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689553730/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689562068","html_url":"https://github.com/apache/hudi/issues/2042#issuecomment-689562068","issue_url":"https://api.github.com/repos/apache/hudi/issues/2042","id":689562068,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTU2MjA2OA==","user":{"login":"sam-wmt","id":67726885,"node_id":"MDQ6VXNlcjY3NzI2ODg1","avatar_url":"https://avatars.githubusercontent.com/u/67726885?v=4","gravatar_id":"","url":"https://api.github.com/users/sam-wmt","html_url":"https://github.com/sam-wmt","followers_url":"https://api.github.com/users/sam-wmt/followers","following_url":"https://api.github.com/users/sam-wmt/following{/other_user}","gists_url":"https://api.github.com/users/sam-wmt/gists{/gist_id}","starred_url":"https://api.github.com/users/sam-wmt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sam-wmt/subscriptions","organizations_url":"https://api.github.com/users/sam-wmt/orgs","repos_url":"https://api.github.com/users/sam-wmt/repos","events_url":"https://api.github.com/users/sam-wmt/events{/privacy}","received_events_url":"https://api.github.com/users/sam-wmt/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T13:26:04Z","updated_at":"2020-09-09T13:26:04Z","author_association":"NONE","body":"This appears to have been caused by an internal change to our Hudi writer which I found in the executor logs:\r\njava.lang.NoSuchMethodException: com.xxx.xxxx.xxxxx.xxxxxx.<init>(org.apache.hudi.common.util.Option)\r\n.\r\nClosing ticket","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689562068/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689722005","html_url":"https://github.com/apache/hudi/pull/1484#issuecomment-689722005","issue_url":"https://api.github.com/repos/apache/hudi/issues/1484","id":689722005,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTcyMjAwNQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T17:54:45Z","updated_at":"2020-09-09T17:54:45Z","author_association":"MEMBER","body":"will do. thanks ! @n3nash can take a pass as well ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689722005/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689812755","html_url":"https://github.com/apache/hudi/pull/2048#issuecomment-689812755","issue_url":"https://api.github.com/repos/apache/hudi/issues/2048","id":689812755,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTgxMjc1NQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T20:46:43Z","updated_at":"2020-09-09T20:46:43Z","author_association":"MEMBER","body":">You suggested to remove HoodieReplaceStat\r\n\r\nI think the suggestion was to simplify HoodieReplaceMetadata such that it only contains the extra information about replaced file groups. and use the HoodieCommitMetadata and its HoodieWriteStat for tracking the new file groups written.\r\nWe could have HoodieReplaceStat to be part of the WriteStatus itself for tracking the additional information about replaced file groups? \r\n\r\nOn cleaning vs archival, it would be good if we can implement this in cleaning. But can that be a follow-on item? Practically speaking, typical deployments don't configure cleaning that low. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689812755/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689884262","html_url":"https://github.com/apache/hudi/pull/2056#issuecomment-689884262","issue_url":"https://api.github.com/repos/apache/hudi/issues/2056","id":689884262,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTg4NDI2Mg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-09T23:52:10Z","updated_at":"2020-09-09T23:52:10Z","author_association":"MEMBER","body":"sorry, long weekend here in the states. will take a look today ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689884262/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689905849","html_url":"https://github.com/apache/hudi/pull/1929#issuecomment-689905849","issue_url":"https://api.github.com/repos/apache/hudi/issues/1929","id":689905849,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTkwNTg0OQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T00:59:46Z","updated_at":"2020-09-10T00:59:46Z","author_association":"MEMBER","body":"@satishkotha Can you please help review? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689905849/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689916526","html_url":"https://github.com/apache/hudi/pull/2058#issuecomment-689916526","issue_url":"https://api.github.com/repos/apache/hudi/issues/2058","id":689916526,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTkxNjUyNg==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T01:30:21Z","updated_at":"2020-09-10T01:30:21Z","author_association":"CONTRIBUTOR","body":"> If you are referring to hudi, we don't have to rebuild docker images to pick up latest hudi code.\r\n\r\nYes\r\n\r\n> The hudi codebase is mounted inside docker containers so that you can use the latest version.\r\n\r\nYou mean if I change the code it would reflect into the hudi on docker immediately? Where can I know the configuration of this mechanism in the project? Sorry, I am not familiar with Docker.\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689916526/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689917712","html_url":"https://github.com/apache/hudi/pull/1704#issuecomment-689917712","issue_url":"https://api.github.com/repos/apache/hudi/issues/1704","id":689917712,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTkxNzcxMg==","user":{"login":"shenh062326","id":9527867,"node_id":"MDQ6VXNlcjk1Mjc4Njc=","avatar_url":"https://avatars.githubusercontent.com/u/9527867?v=4","gravatar_id":"","url":"https://api.github.com/users/shenh062326","html_url":"https://github.com/shenh062326","followers_url":"https://api.github.com/users/shenh062326/followers","following_url":"https://api.github.com/users/shenh062326/following{/other_user}","gists_url":"https://api.github.com/users/shenh062326/gists{/gist_id}","starred_url":"https://api.github.com/users/shenh062326/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shenh062326/subscriptions","organizations_url":"https://api.github.com/users/shenh062326/orgs","repos_url":"https://api.github.com/users/shenh062326/repos","events_url":"https://api.github.com/users/shenh062326/events{/privacy}","received_events_url":"https://api.github.com/users/shenh062326/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T01:34:02Z","updated_at":"2020-09-10T01:34:02Z","author_association":"CONTRIBUTOR","body":"I am working on https://issues.apache.org/jira/browse/HUDI-1058 , it also needs similar modifications. It is better to work on HUDI-1058 after this merge request is merged, but if this MR has not progressed, I can resolve HUDI-1058 first.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689917712/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689961998","html_url":"https://github.com/apache/hudi/pull/1484#issuecomment-689961998","issue_url":"https://api.github.com/repos/apache/hudi/issues/1484","id":689961998,"node_id":"MDEyOklzc3VlQ29tbWVudDY4OTk2MTk5OA==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T03:53:03Z","updated_at":"2020-09-10T03:53:03Z","author_association":"CONTRIBUTOR","body":"Yes, I will do by Friday.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/689961998/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690012667","html_url":"https://github.com/apache/hudi/issues/2068#issuecomment-690012667","issue_url":"https://api.github.com/repos/apache/hudi/issues/2068","id":690012667,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDAxMjY2Nw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T06:14:40Z","updated_at":"2020-09-10T06:14:40Z","author_association":"CONTRIBUTOR","body":"@bradleyhurley : The errors are due to shuffle fetch failures. Increasing executor memory and resources in general helps.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690012667/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690024060","html_url":"https://github.com/apache/hudi/issues/2075#issuecomment-690024060","issue_url":"https://api.github.com/repos/apache/hudi/issues/2075","id":690024060,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDAyNDA2MA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T06:41:25Z","updated_at":"2020-09-10T06:41:25Z","author_association":"CONTRIBUTOR","body":"@rajgowtham24 : This is a known in 0.5.x and was fixed in 0.6.0 version","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690024060/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690086477","html_url":"https://github.com/apache/hudi/pull/1990#issuecomment-690086477","issue_url":"https://api.github.com/repos/apache/hudi/issues/1990","id":690086477,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDA4NjQ3Nw==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T08:41:57Z","updated_at":"2020-09-10T08:41:57Z","author_association":"CONTRIBUTOR","body":"@vinothchandar Do you have any concerns or I can merge this now?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690086477/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690099859","html_url":"https://github.com/apache/hudi/pull/1524#issuecomment-690099859","issue_url":"https://api.github.com/repos/apache/hudi/issues/1524","id":690099859,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDA5OTg1OQ==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T09:07:19Z","updated_at":"2020-09-10T09:07:19Z","author_association":"CONTRIBUTOR","body":"@afilipchik still working on this? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690099859/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690292104","html_url":"https://github.com/apache/hudi/issues/2068#issuecomment-690292104","issue_url":"https://api.github.com/repos/apache/hudi/issues/2068","id":690292104,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDI5MjEwNA==","user":{"login":"bradleyhurley","id":7111357,"node_id":"MDQ6VXNlcjcxMTEzNTc=","avatar_url":"https://avatars.githubusercontent.com/u/7111357?v=4","gravatar_id":"","url":"https://api.github.com/users/bradleyhurley","html_url":"https://github.com/bradleyhurley","followers_url":"https://api.github.com/users/bradleyhurley/followers","following_url":"https://api.github.com/users/bradleyhurley/following{/other_user}","gists_url":"https://api.github.com/users/bradleyhurley/gists{/gist_id}","starred_url":"https://api.github.com/users/bradleyhurley/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bradleyhurley/subscriptions","organizations_url":"https://api.github.com/users/bradleyhurley/orgs","repos_url":"https://api.github.com/users/bradleyhurley/repos","events_url":"https://api.github.com/users/bradleyhurley/events{/privacy}","received_events_url":"https://api.github.com/users/bradleyhurley/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T13:33:33Z","updated_at":"2020-09-10T13:33:33Z","author_association":"NONE","body":"Thanks @bvaradar - I think I have read most of the guides and documentation that I could find. Is there a formula that should drive the number of executors, cores per executor, driver memory, and executor memory?\r\n\r\nWith a properly sized configuration do you have a ballpark of how long you would expect it to take to upsert 100M rows into a Hudi table with 100M existing rows with 99%+ of the data being an insert vs update?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690292104/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690335395","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-690335395","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":690335395,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDMzNTM5NQ==","user":{"login":"wangxianghu","id":49835526,"node_id":"MDQ6VXNlcjQ5ODM1NTI2","avatar_url":"https://avatars.githubusercontent.com/u/49835526?v=4","gravatar_id":"","url":"https://api.github.com/users/wangxianghu","html_url":"https://github.com/wangxianghu","followers_url":"https://api.github.com/users/wangxianghu/followers","following_url":"https://api.github.com/users/wangxianghu/following{/other_user}","gists_url":"https://api.github.com/users/wangxianghu/gists{/gist_id}","starred_url":"https://api.github.com/users/wangxianghu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangxianghu/subscriptions","organizations_url":"https://api.github.com/users/wangxianghu/orgs","repos_url":"https://api.github.com/users/wangxianghu/repos","events_url":"https://api.github.com/users/wangxianghu/events{/privacy}","received_events_url":"https://api.github.com/users/wangxianghu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T14:41:38Z","updated_at":"2020-09-10T15:18:43Z","author_association":"CONTRIBUTOR","body":"@vinothchandar @yanghua @leesf  The ci is green now","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690335395/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690546404","html_url":"https://github.com/apache/hudi/pull/2058#issuecomment-690546404","issue_url":"https://api.github.com/repos/apache/hudi/issues/2058","id":690546404,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDU0NjQwNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T17:35:36Z","updated_at":"2020-09-10T17:35:36Z","author_association":"CONTRIBUTOR","body":"@yanghua : You can look at the docker compose file and https://github.com/apache/hudi/blob/master/docker/setup_demo.sh We mount hudi workspace inside docker to achieve it.\r\n\r\nI guess, we can close this PR then ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690546404/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690551558","html_url":"https://github.com/apache/hudi/pull/2046#issuecomment-690551558","issue_url":"https://api.github.com/repos/apache/hudi/issues/2046","id":690551558,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDU1MTU1OA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T17:38:04Z","updated_at":"2020-09-10T17:38:04Z","author_association":"CONTRIBUTOR","body":"@umehrot2 : Once you add test, please ping me.\r\n\r\nThanks,\r\nBalaji.V","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690551558/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690554395","html_url":"https://github.com/apache/hudi/pull/1524#issuecomment-690554395","issue_url":"https://api.github.com/repos/apache/hudi/issues/1524","id":690554395,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDU1NDM5NQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T17:39:20Z","updated_at":"2020-09-10T17:39:20Z","author_association":"CONTRIBUTOR","body":"@pratyakshsharma : I will help getting this landed.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690554395/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690675022","html_url":"https://github.com/apache/hudi/issues/2065#issuecomment-690675022","issue_url":"https://api.github.com/repos/apache/hudi/issues/2065","id":690675022,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDY3NTAyMg==","user":{"login":"prashanthvg89","id":8704802,"node_id":"MDQ6VXNlcjg3MDQ4MDI=","avatar_url":"https://avatars.githubusercontent.com/u/8704802?v=4","gravatar_id":"","url":"https://api.github.com/users/prashanthvg89","html_url":"https://github.com/prashanthvg89","followers_url":"https://api.github.com/users/prashanthvg89/followers","following_url":"https://api.github.com/users/prashanthvg89/following{/other_user}","gists_url":"https://api.github.com/users/prashanthvg89/gists{/gist_id}","starred_url":"https://api.github.com/users/prashanthvg89/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/prashanthvg89/subscriptions","organizations_url":"https://api.github.com/users/prashanthvg89/orgs","repos_url":"https://api.github.com/users/prashanthvg89/repos","events_url":"https://api.github.com/users/prashanthvg89/events{/privacy}","received_events_url":"https://api.github.com/users/prashanthvg89/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T19:40:37Z","updated_at":"2020-09-10T19:40:37Z","author_association":"NONE","body":"So far it's running good. Usually, it used to fail after 2 days previously and now it's been close to that and no errors. I'll update by end of this week","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690675022/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690711545","html_url":"https://github.com/apache/hudi/pull/2048#issuecomment-690711545","issue_url":"https://api.github.com/repos/apache/hudi/issues/2048","id":690711545,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDcxMTU0NQ==","user":{"login":"satishkotha","id":2992755,"node_id":"MDQ6VXNlcjI5OTI3NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2992755?v=4","gravatar_id":"","url":"https://api.github.com/users/satishkotha","html_url":"https://github.com/satishkotha","followers_url":"https://api.github.com/users/satishkotha/followers","following_url":"https://api.github.com/users/satishkotha/following{/other_user}","gists_url":"https://api.github.com/users/satishkotha/gists{/gist_id}","starred_url":"https://api.github.com/users/satishkotha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishkotha/subscriptions","organizations_url":"https://api.github.com/users/satishkotha/orgs","repos_url":"https://api.github.com/users/satishkotha/repos","events_url":"https://api.github.com/users/satishkotha/events{/privacy}","received_events_url":"https://api.github.com/users/satishkotha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-10T20:34:15Z","updated_at":"2020-09-10T20:34:15Z","author_association":"MEMBER","body":"@vinothchandar As discussed, i added boolean in WriteStatus and removed HoodieReplaceStat. See this [diff](https://github.com/apache/hudi/pull/2048/commits/94b275dbd20ec82ebe568b47bb28447d92ab996f). I committed it as a separate git-sha because this still looks  somewhat awkward IMO. Please take a look and I can revert or reimplement in a different way\r\n\r\nAlso, created https://issues.apache.org/jira/browse/HUDI-1276 for cleaning replaced file during clean.\r\n\r\nI also renamed 'replace' to 'replacecommit' everywhere as you suggested. \r\n\r\nPlease let me know if you have additional comments/suggestions","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690711545/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690838645","html_url":"https://github.com/apache/hudi/pull/2058#issuecomment-690838645","issue_url":"https://api.github.com/repos/apache/hudi/issues/2058","id":690838645,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDgzODY0NQ==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T02:39:50Z","updated_at":"2020-09-11T02:39:50Z","author_association":"CONTRIBUTOR","body":"> @yanghua : You can look at the docker compose file and https://github.com/apache/hudi/blob/master/docker/setup_demo.sh We mount hudi workspace inside docker to achieve it.\r\n> \r\n> I guess, we can close this PR then ?\r\n\r\nGot it. IIUC, you mean:\r\n\r\n```\r\nvolumes:\r\n    - ${HUDI_WS}:/var/hoodie/ws\r\n```\r\n\r\n> I guess, we can close this PR then ?\r\n\r\nYes.\r\n\r\nActually, My colleagues also do not know it. We rarely use docker. IMO, it would be better to describe it into the documentation. wdyt?\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690838645/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690863065","html_url":"https://github.com/apache/hudi/pull/2084#issuecomment-690863065","issue_url":"https://api.github.com/repos/apache/hudi/issues/2084","id":690863065,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDg2MzA2NQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T04:13:59Z","updated_at":"2020-09-11T04:13:59Z","author_association":"CONTRIBUTOR","body":"@nsivabalan : Can you please review this. \r\n\r\nThanks,\r\nBalaji.V","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690863065/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690922454","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-690922454","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":690922454,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDkyMjQ1NA==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T07:18:31Z","updated_at":"2020-09-11T07:18:31Z","author_association":"CONTRIBUTOR","body":"@leesf Thanks for your awesome work. Can you squash these commits for the subsequent review?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690922454/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690925786","html_url":"https://github.com/apache/hudi/pull/2084#issuecomment-690925786","issue_url":"https://api.github.com/repos/apache/hudi/issues/2084","id":690925786,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDkyNTc4Ng==","user":{"login":"tooptoop4","id":33283496,"node_id":"MDQ6VXNlcjMzMjgzNDk2","avatar_url":"https://avatars.githubusercontent.com/u/33283496?v=4","gravatar_id":"","url":"https://api.github.com/users/tooptoop4","html_url":"https://github.com/tooptoop4","followers_url":"https://api.github.com/users/tooptoop4/followers","following_url":"https://api.github.com/users/tooptoop4/following{/other_user}","gists_url":"https://api.github.com/users/tooptoop4/gists{/gist_id}","starred_url":"https://api.github.com/users/tooptoop4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tooptoop4/subscriptions","organizations_url":"https://api.github.com/users/tooptoop4/orgs","repos_url":"https://api.github.com/users/tooptoop4/repos","events_url":"https://api.github.com/users/tooptoop4/events{/privacy}","received_events_url":"https://api.github.com/users/tooptoop4/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T07:25:50Z","updated_at":"2020-09-11T07:25:50Z","author_association":"NONE","body":"I thought https://github.com/apache/hudi/pull/1792 fixed it?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690925786/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690928998","html_url":"https://github.com/apache/hudi/pull/2071#issuecomment-690928998","issue_url":"https://api.github.com/repos/apache/hudi/issues/2071","id":690928998,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDkyODk5OA==","user":{"login":"shenh062326","id":9527867,"node_id":"MDQ6VXNlcjk1Mjc4Njc=","avatar_url":"https://avatars.githubusercontent.com/u/9527867?v=4","gravatar_id":"","url":"https://api.github.com/users/shenh062326","html_url":"https://github.com/shenh062326","followers_url":"https://api.github.com/users/shenh062326/followers","following_url":"https://api.github.com/users/shenh062326/following{/other_user}","gists_url":"https://api.github.com/users/shenh062326/gists{/gist_id}","starred_url":"https://api.github.com/users/shenh062326/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shenh062326/subscriptions","organizations_url":"https://api.github.com/users/shenh062326/orgs","repos_url":"https://api.github.com/users/shenh062326/repos","events_url":"https://api.github.com/users/shenh062326/events{/privacy}","received_events_url":"https://api.github.com/users/shenh062326/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T07:33:28Z","updated_at":"2020-09-11T07:33:28Z","author_association":"CONTRIBUTOR","body":"@n3nash  can you take a look at this MR.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690928998/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690966794","html_url":"https://github.com/apache/hudi/pull/1748#issuecomment-690966794","issue_url":"https://api.github.com/repos/apache/hudi/issues/1748","id":690966794,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MDk2Njc5NA==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T08:56:20Z","updated_at":"2020-09-11T08:56:20Z","author_association":"CONTRIBUTOR","body":"@leesf we no longer use SimpleDateFormat in TimestampBasedKeyGenerator. So the issues linked with usage of SimpleDateFormat are also no longer there. Guess we can close this? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/690966794/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691021969","html_url":"https://github.com/apache/hudi/pull/2085#issuecomment-691021969","issue_url":"https://api.github.com/repos/apache/hudi/issues/2085","id":691021969,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTAyMTk2OQ==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T10:45:02Z","updated_at":"2020-09-11T10:45:02Z","author_association":"CONTRIBUTOR","body":"Changes look good to me. Can you see why the build is failing? @shenh062326 ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691021969/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691043300","html_url":"https://github.com/apache/hudi/pull/1748#issuecomment-691043300","issue_url":"https://api.github.com/repos/apache/hudi/issues/1748","id":691043300,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTA0MzMwMA==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T11:39:24Z","updated_at":"2020-09-11T11:39:24Z","author_association":"CONTRIBUTOR","body":"> @leesf we no longer use SimpleDateFormat in TimestampBasedKeyGenerator. So the issues linked with usage of SimpleDateFormat are also no longer there. Guess we can close this?\r\n\r\nsure, @yui2010 please reopen if you have the issue again.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691043300/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691050799","html_url":"https://github.com/apache/hudi/pull/2078#issuecomment-691050799","issue_url":"https://api.github.com/repos/apache/hudi/issues/2078","id":691050799,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTA1MDc5OQ==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T11:58:06Z","updated_at":"2020-09-11T11:58:06Z","author_association":"CONTRIBUTOR","body":"LGTM, merging.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691050799/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691079649","html_url":"https://github.com/apache/hudi/issues/2075#issuecomment-691079649","issue_url":"https://api.github.com/repos/apache/hudi/issues/2075","id":691079649,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTA3OTY0OQ==","user":{"login":"rajgowtham24","id":68343070,"node_id":"MDQ6VXNlcjY4MzQzMDcw","avatar_url":"https://avatars.githubusercontent.com/u/68343070?v=4","gravatar_id":"","url":"https://api.github.com/users/rajgowtham24","html_url":"https://github.com/rajgowtham24","followers_url":"https://api.github.com/users/rajgowtham24/followers","following_url":"https://api.github.com/users/rajgowtham24/following{/other_user}","gists_url":"https://api.github.com/users/rajgowtham24/gists{/gist_id}","starred_url":"https://api.github.com/users/rajgowtham24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rajgowtham24/subscriptions","organizations_url":"https://api.github.com/users/rajgowtham24/orgs","repos_url":"https://api.github.com/users/rajgowtham24/repos","events_url":"https://api.github.com/users/rajgowtham24/events{/privacy}","received_events_url":"https://api.github.com/users/rajgowtham24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T12:59:07Z","updated_at":"2020-09-11T12:59:07Z","author_association":"NONE","body":"Thanks @bvaradar  will check on that","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691079649/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691200964","html_url":"https://github.com/apache/hudi/pull/2071#issuecomment-691200964","issue_url":"https://api.github.com/repos/apache/hudi/issues/2071","id":691200964,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTIwMDk2NA==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T16:44:25Z","updated_at":"2020-09-11T16:44:25Z","author_association":"CONTRIBUTOR","body":"@shenh062326 The PR looks fine to me, high level, what is the reason for this change ? Is it just to correct timestamp types ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691200964/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691202637","html_url":"https://github.com/apache/hudi/pull/1978#issuecomment-691202637","issue_url":"https://api.github.com/repos/apache/hudi/issues/1978","id":691202637,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTIwMjYzNw==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T16:48:05Z","updated_at":"2020-09-11T16:48:05Z","author_association":"CONTRIBUTOR","body":"@hj2016 Please rebase and fix the tests, we can land after that. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691202637/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691203252","html_url":"https://github.com/apache/hudi/pull/1242#issuecomment-691203252","issue_url":"https://api.github.com/repos/apache/hudi/issues/1242","id":691203252,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTIwMzI1Mg==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T16:49:29Z","updated_at":"2020-09-11T16:49:29Z","author_association":"CONTRIBUTOR","body":"@hddong Extremely sorry, this fell through the crack, please rebase and I will merge this right after.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691203252/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691206241","html_url":"https://github.com/apache/hudi/issues/2062#issuecomment-691206241","issue_url":"https://api.github.com/repos/apache/hudi/issues/2062","id":691206241,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTIwNjI0MQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T16:55:42Z","updated_at":"2020-09-11T16:55:42Z","author_association":"CONTRIBUTOR","body":"@kpurella : I think you should either  try patching the PR I had mentioned or use 0.6.0","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691206241/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691209314","html_url":"https://github.com/apache/hudi/pull/2058#issuecomment-691209314","issue_url":"https://api.github.com/repos/apache/hudi/issues/2058","id":691209314,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTIwOTMxNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T17:02:18Z","updated_at":"2020-09-11T17:02:18Z","author_association":"CONTRIBUTOR","body":"Yes @yanghua  : The docker containers are for mainly for  internal (testing and demo) consumption but I agree we can document it for engineers to know. Would you mind helping document this if possible ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691209314/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691324390","html_url":"https://github.com/apache/hudi/pull/2084#issuecomment-691324390","issue_url":"https://api.github.com/repos/apache/hudi/issues/2084","id":691324390,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTMyNDM5MA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T21:41:43Z","updated_at":"2020-09-11T21:41:43Z","author_association":"CONTRIBUTOR","body":"@tooptoop4 : The issue was still there. I added a testcase (testPreCombineWithDelete) which would fail with existing code but passes with this PR.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691324390/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691327993","html_url":"https://github.com/apache/hudi/pull/2084#issuecomment-691327993","issue_url":"https://api.github.com/repos/apache/hudi/issues/2084","id":691327993,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTMyNzk5Mw==","user":{"login":"nsivabalan","id":513218,"node_id":"MDQ6VXNlcjUxMzIxOA==","avatar_url":"https://avatars.githubusercontent.com/u/513218?v=4","gravatar_id":"","url":"https://api.github.com/users/nsivabalan","html_url":"https://github.com/nsivabalan","followers_url":"https://api.github.com/users/nsivabalan/followers","following_url":"https://api.github.com/users/nsivabalan/following{/other_user}","gists_url":"https://api.github.com/users/nsivabalan/gists{/gist_id}","starred_url":"https://api.github.com/users/nsivabalan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nsivabalan/subscriptions","organizations_url":"https://api.github.com/users/nsivabalan/orgs","repos_url":"https://api.github.com/users/nsivabalan/repos","events_url":"https://api.github.com/users/nsivabalan/events{/privacy}","received_events_url":"https://api.github.com/users/nsivabalan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-11T21:52:26Z","updated_at":"2020-09-11T21:52:26Z","author_association":"CONTRIBUTOR","body":"yeah, https://github.com/apache/hudi/pull/1792 fixed the issue for OverwriteWithLatestAvroPayload and not for AWSDmsAvroPayload. thanks @bvaradar. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691327993/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691379071","html_url":"https://github.com/apache/hudi/pull/1978#issuecomment-691379071","issue_url":"https://api.github.com/repos/apache/hudi/issues/1978","id":691379071,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTM3OTA3MQ==","user":{"login":"hj2016","id":18521084,"node_id":"MDQ6VXNlcjE4NTIxMDg0","avatar_url":"https://avatars.githubusercontent.com/u/18521084?v=4","gravatar_id":"","url":"https://api.github.com/users/hj2016","html_url":"https://github.com/hj2016","followers_url":"https://api.github.com/users/hj2016/followers","following_url":"https://api.github.com/users/hj2016/following{/other_user}","gists_url":"https://api.github.com/users/hj2016/gists{/gist_id}","starred_url":"https://api.github.com/users/hj2016/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hj2016/subscriptions","organizations_url":"https://api.github.com/users/hj2016/orgs","repos_url":"https://api.github.com/users/hj2016/repos","events_url":"https://api.github.com/users/hj2016/events{/privacy}","received_events_url":"https://api.github.com/users/hj2016/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-12T01:45:35Z","updated_at":"2020-09-12T01:45:35Z","author_association":"CONTRIBUTOR","body":"@n3nash The problem nsivabalan said, I have fixed it a few days ago. I don’t know if you are talking about the same problem. If not, can I elaborate on it?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691379071/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691431626","html_url":"https://github.com/apache/hudi/pull/2058#issuecomment-691431626","issue_url":"https://api.github.com/repos/apache/hudi/issues/2058","id":691431626,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTQzMTYyNg==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-12T07:11:47Z","updated_at":"2020-09-12T07:11:47Z","author_association":"CONTRIBUTOR","body":">  Would you mind helping document this if possible ?\r\n\r\nOK, my pleasure.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691431626/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691504599","html_url":"https://github.com/apache/hudi/pull/1242#issuecomment-691504599","issue_url":"https://api.github.com/repos/apache/hudi/issues/1242","id":691504599,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwNDU5OQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-12T15:14:48Z","updated_at":"2020-09-12T15:14:48Z","author_association":"MEMBER","body":"@n3nash if you wish you can also rebase this yourself and push. See https://cwiki.apache.org/confluence/display/HUDI/Resources#Resources-PushingChangesToPRs for a how-to ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691504599/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691505048","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-691505048","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":691505048,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwNTA0OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-12T15:18:51Z","updated_at":"2020-09-12T15:18:51Z","author_association":"MEMBER","body":"@wangxianghu That's awesome. Will resume the Review again this weekend","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691505048/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691505318","html_url":"https://github.com/apache/hudi/pull/1704#issuecomment-691505318","issue_url":"https://api.github.com/repos/apache/hudi/issues/1704","id":691505318,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwNTMxOA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-12T15:21:12Z","updated_at":"2020-09-12T15:21:12Z","author_association":"MEMBER","body":"Looks like this is almost ready . I will rebase, review and try to land this first. \r\n\r\nthanks for flagging @shenh062326 ! ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691505318/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691505393","html_url":"https://github.com/apache/hudi/pull/1978#issuecomment-691505393","issue_url":"https://api.github.com/repos/apache/hudi/issues/1978","id":691505393,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwNTM5Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-12T15:22:00Z","updated_at":"2020-09-12T15:22:00Z","author_association":"MEMBER","body":"@nsivabalan are you good with the test changes like @hj2016 is asking? please clarify ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691505393/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691505608","html_url":"https://github.com/apache/hudi/pull/1990#issuecomment-691505608","issue_url":"https://api.github.com/repos/apache/hudi/issues/1990","id":691505608,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTUwNTYwOA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-12T15:24:01Z","updated_at":"2020-09-12T15:24:01Z","author_association":"MEMBER","body":"@pratyakshsharma please always wait for an explicit LGTM from another committer before merging :) \r\nI will take another pass ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691505608/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691579116","html_url":"https://github.com/apache/hudi/pull/1978#issuecomment-691579116","issue_url":"https://api.github.com/repos/apache/hudi/issues/1978","id":691579116,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTU3OTExNg==","user":{"login":"nsivabalan","id":513218,"node_id":"MDQ6VXNlcjUxMzIxOA==","avatar_url":"https://avatars.githubusercontent.com/u/513218?v=4","gravatar_id":"","url":"https://api.github.com/users/nsivabalan","html_url":"https://github.com/nsivabalan","followers_url":"https://api.github.com/users/nsivabalan/followers","following_url":"https://api.github.com/users/nsivabalan/following{/other_user}","gists_url":"https://api.github.com/users/nsivabalan/gists{/gist_id}","starred_url":"https://api.github.com/users/nsivabalan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nsivabalan/subscriptions","organizations_url":"https://api.github.com/users/nsivabalan/orgs","repos_url":"https://api.github.com/users/nsivabalan/repos","events_url":"https://api.github.com/users/nsivabalan/events{/privacy}","received_events_url":"https://api.github.com/users/nsivabalan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T00:21:05Z","updated_at":"2020-09-13T00:21:05Z","author_association":"CONTRIBUTOR","body":"> @nsivabalan are you good with the test changes like @hj2016 is asking? please clarify\r\n\r\nI did review the commit that addressed my concern and only then approved the PR. @n3nash had some formatting concerns. So left it to him to merge once his feedback is addressed. \r\nSo, the patch is good from my standpoint. \r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691579116/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691584459","html_url":"https://github.com/apache/hudi/pull/2071#issuecomment-691584459","issue_url":"https://api.github.com/repos/apache/hudi/issues/2071","id":691584459,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTU4NDQ1OQ==","user":{"login":"shenh062326","id":9527867,"node_id":"MDQ6VXNlcjk1Mjc4Njc=","avatar_url":"https://avatars.githubusercontent.com/u/9527867?v=4","gravatar_id":"","url":"https://api.github.com/users/shenh062326","html_url":"https://github.com/shenh062326","followers_url":"https://api.github.com/users/shenh062326/followers","following_url":"https://api.github.com/users/shenh062326/following{/other_user}","gists_url":"https://api.github.com/users/shenh062326/gists{/gist_id}","starred_url":"https://api.github.com/users/shenh062326/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shenh062326/subscriptions","organizations_url":"https://api.github.com/users/shenh062326/orgs","repos_url":"https://api.github.com/users/shenh062326/repos","events_url":"https://api.github.com/users/shenh062326/events{/privacy}","received_events_url":"https://api.github.com/users/shenh062326/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T01:02:00Z","updated_at":"2020-09-13T01:02:00Z","author_association":"CONTRIBUTOR","body":"> @shenh062326 The PR looks fine to me, high level, what is the reason for this change ? Is it just to correct timestamp types ?\r\n\r\nThe Description in https://issues.apache.org/jira/browse/HUDI-1143 explain the reason:\r\n\"In order to help `TestBootstrap` succeed, had to turn the field into a regular long. We need to fix this correctly \"","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691584459/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691601872","html_url":"https://github.com/apache/hudi/pull/2087#issuecomment-691601872","issue_url":"https://api.github.com/repos/apache/hudi/issues/2087","id":691601872,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTYwMTg3Mg==","user":{"login":"Kaiux","id":24505361,"node_id":"MDQ6VXNlcjI0NTA1MzYx","avatar_url":"https://avatars.githubusercontent.com/u/24505361?v=4","gravatar_id":"","url":"https://api.github.com/users/Kaiux","html_url":"https://github.com/Kaiux","followers_url":"https://api.github.com/users/Kaiux/followers","following_url":"https://api.github.com/users/Kaiux/following{/other_user}","gists_url":"https://api.github.com/users/Kaiux/gists{/gist_id}","starred_url":"https://api.github.com/users/Kaiux/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Kaiux/subscriptions","organizations_url":"https://api.github.com/users/Kaiux/orgs","repos_url":"https://api.github.com/users/Kaiux/repos","events_url":"https://api.github.com/users/Kaiux/events{/privacy}","received_events_url":"https://api.github.com/users/Kaiux/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T03:21:21Z","updated_at":"2020-09-13T03:21:21Z","author_association":"CONTRIBUTOR","body":"cc @bvaradar ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691601872/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691621101","html_url":"https://github.com/apache/hudi/pull/1990#issuecomment-691621101","issue_url":"https://api.github.com/repos/apache/hudi/issues/1990","id":691621101,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTYyMTEwMQ==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T06:25:25Z","updated_at":"2020-09-13T06:25:25Z","author_association":"CONTRIBUTOR","body":"> @pratyakshsharma please always wait for an explicit LGTM from another committer before merging :)\r\nI will take another pass\r\n\r\nPoint taken. :) ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691621101/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691622152","html_url":"https://github.com/apache/hudi/pull/2085#issuecomment-691622152","issue_url":"https://api.github.com/repos/apache/hudi/issues/2085","id":691622152,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTYyMjE1Mg==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T06:39:52Z","updated_at":"2020-09-13T06:39:52Z","author_association":"CONTRIBUTOR","body":"@vinothchandar Please take a pass. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691622152/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691710534","html_url":"https://github.com/apache/hudi/pull/2048#issuecomment-691710534","issue_url":"https://api.github.com/repos/apache/hudi/issues/2048","id":691710534,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTcxMDUzNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T19:00:18Z","updated_at":"2020-09-13T19:00:18Z","author_association":"MEMBER","body":"@satishkotha can you please resolve all the comments, that you have addressed already. That way we can track whats pending","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691710534/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691739504","html_url":"https://github.com/apache/hudi/issues/1913#issuecomment-691739504","issue_url":"https://api.github.com/repos/apache/hudi/issues/1913","id":691739504,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTczOTUwNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T23:15:04Z","updated_at":"2020-09-13T23:15:04Z","author_association":"CONTRIBUTOR","body":"There is a WIP PR (https://github.com/apache/hudi/pull/2069) which addresses early cleaning of local files that are opened. This is the only close thing that I can suspect anything related to Hudi. Otherwise, this is not a Hudi issue per-se. Closing this ticket as there is a related PR and a Jira on the known issue. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691739504/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691739803","html_url":"https://github.com/apache/hudi/issues/1936#issuecomment-691739803","issue_url":"https://api.github.com/repos/apache/hudi/issues/1936","id":691739803,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTczOTgwMw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T23:17:18Z","updated_at":"2020-09-13T23:17:18Z","author_association":"CONTRIBUTOR","body":"@umehrot2 : when you get a chance please let @harishchanderramesh  of the EMR release date. Closing this ticket. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691739803/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691739964","html_url":"https://github.com/apache/hudi/issues/1939#issuecomment-691739964","issue_url":"https://api.github.com/repos/apache/hudi/issues/1939","id":691739964,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTczOTk2NA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T23:18:41Z","updated_at":"2020-09-13T23:18:41Z","author_association":"CONTRIBUTOR","body":"@RajasekarSribalan : Please reopen if you still have any questions.\r\n\r\nThanks,\r\nBalaji.V","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691739964/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691740550","html_url":"https://github.com/apache/hudi/issues/1943#issuecomment-691740550","issue_url":"https://api.github.com/repos/apache/hudi/issues/1943","id":691740550,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc0MDU1MA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-13T23:23:26Z","updated_at":"2020-09-13T23:23:26Z","author_association":"CONTRIBUTOR","body":"This is a valid issue and is currently assigned for next release with a tracking Jira. Closing the Github issue as we will track this in Jira.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691740550/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691745384","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-691745384","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":691745384,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc0NTM4NA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T00:03:39Z","updated_at":"2020-09-14T00:03:39Z","author_association":"MEMBER","body":"I can help address the remaining feedback. I will push a small diff today/tmrw.\r\nOverall, looks like a reasonable start. \r\n\r\nThe major feedback I still have is the following  \r\n\r\n>would a parallelDo(func, parallelism) method in HoodieEngineContext help us avoid a lot of base/child class duplication of logic like this?\r\n\r\nLot of usages are like `jsc.parallelize(list, parallelism).map(func)` , which all require a base-child class now. I am wondering if its easier to take those usages alone and implement as `engineContext.parallelDo(list, func, parallelism)`. This can be the lowest common denominator across Spark/Flink etc. We can avoid splitting a good chunk of classes if we do this IMO. If this is interesting, and we agree, I can try to quantify. \r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691745384/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691745661","html_url":"https://github.com/apache/hudi/issues/1835#issuecomment-691745661","issue_url":"https://api.github.com/repos/apache/hudi/issues/1835","id":691745661,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc0NTY2MQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T00:05:18Z","updated_at":"2020-09-14T00:05:18Z","author_association":"CONTRIBUTOR","body":"@rajgowtham24 : Closing this issue. Please reopen if you are still having issues.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691745661/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691745743","html_url":"https://github.com/apache/hudi/issues/1954#issuecomment-691745743","issue_url":"https://api.github.com/repos/apache/hudi/issues/1954","id":691745743,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc0NTc0Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T00:05:44Z","updated_at":"2020-09-14T00:05:44Z","author_association":"CONTRIBUTOR","body":"Closing this issue. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691745743/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691771582","html_url":"https://github.com/apache/hudi/issues/1955#issuecomment-691771582","issue_url":"https://api.github.com/repos/apache/hudi/issues/1955","id":691771582,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3MTU4Mg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:12:32Z","updated_at":"2020-09-14T02:12:32Z","author_association":"CONTRIBUTOR","body":"Added Jira for doc update : https://issues.apache.org/jira/browse/HUDI-1279","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691771582/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691772192","html_url":"https://github.com/apache/hudi/issues/1962#issuecomment-691772192","issue_url":"https://api.github.com/repos/apache/hudi/issues/1962","id":691772192,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3MjE5Mg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:15:19Z","updated_at":"2020-09-14T02:15:19Z","author_association":"CONTRIBUTOR","body":"Closing this due to inactivity ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691772192/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691773018","html_url":"https://github.com/apache/hudi/issues/1972#issuecomment-691773018","issue_url":"https://api.github.com/repos/apache/hudi/issues/1972","id":691773018,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3MzAxOA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:18:59Z","updated_at":"2020-09-14T02:18:59Z","author_association":"CONTRIBUTOR","body":"Closing this as dupe","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691773018/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691774166","html_url":"https://github.com/apache/hudi/issues/1982#issuecomment-691774166","issue_url":"https://api.github.com/repos/apache/hudi/issues/1982","id":691774166,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3NDE2Ng==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:23:42Z","updated_at":"2020-09-14T02:23:42Z","author_association":"CONTRIBUTOR","body":"@Ac-Rush : Are you still blocked by this ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691774166/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691777089","html_url":"https://github.com/apache/hudi/issues/1985#issuecomment-691777089","issue_url":"https://api.github.com/repos/apache/hudi/issues/1985","id":691777089,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3NzA4OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:36:03Z","updated_at":"2020-09-14T02:36:03Z","author_association":"CONTRIBUTOR","body":"\r\nAdded https://issues.apache.org/jira/browse/HUDI-1280 for smooth transition to deltastreamer.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691777089/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691777749","html_url":"https://github.com/apache/hudi/issues/2051#issuecomment-691777749","issue_url":"https://api.github.com/repos/apache/hudi/issues/2051","id":691777749,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3Nzc0OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:38:14Z","updated_at":"2020-09-14T02:38:14Z","author_association":"CONTRIBUTOR","body":"Closing this as we have a jira. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691777749/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691778577","html_url":"https://github.com/apache/hudi/issues/2057#issuecomment-691778577","issue_url":"https://api.github.com/repos/apache/hudi/issues/2057","id":691778577,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3ODU3Nw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:41:40Z","updated_at":"2020-09-14T02:41:40Z","author_association":"CONTRIBUTOR","body":"@umehrot2 : Assigning this to you as this is specific to EMR.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691778577/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691778778","html_url":"https://github.com/apache/hudi/issues/2063#issuecomment-691778778","issue_url":"https://api.github.com/repos/apache/hudi/issues/2063","id":691778778,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3ODc3OA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:42:34Z","updated_at":"2020-09-14T02:42:34Z","author_association":"CONTRIBUTOR","body":"@cadl : Did setting the config help ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691778778/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691778939","html_url":"https://github.com/apache/hudi/issues/2065#issuecomment-691778939","issue_url":"https://api.github.com/repos/apache/hudi/issues/2065","id":691778939,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3ODkzOQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:43:13Z","updated_at":"2020-09-14T02:43:13Z","author_association":"CONTRIBUTOR","body":"@prashanthvg89 : Please reopen if you still run into problems.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691778939/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691779127","html_url":"https://github.com/apache/hudi/issues/2066#issuecomment-691779127","issue_url":"https://api.github.com/repos/apache/hudi/issues/2066","id":691779127,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3OTEyNw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:44:04Z","updated_at":"2020-09-14T02:44:04Z","author_association":"CONTRIBUTOR","body":"@modi95 : Can you look at this when you get a chance. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691779127/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691779849","html_url":"https://github.com/apache/hudi/issues/2063#issuecomment-691779849","issue_url":"https://api.github.com/repos/apache/hudi/issues/2063","id":691779849,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc3OTg0OQ==","user":{"login":"cadl","id":1629582,"node_id":"MDQ6VXNlcjE2Mjk1ODI=","avatar_url":"https://avatars.githubusercontent.com/u/1629582?v=4","gravatar_id":"","url":"https://api.github.com/users/cadl","html_url":"https://github.com/cadl","followers_url":"https://api.github.com/users/cadl/followers","following_url":"https://api.github.com/users/cadl/following{/other_user}","gists_url":"https://api.github.com/users/cadl/gists{/gist_id}","starred_url":"https://api.github.com/users/cadl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cadl/subscriptions","organizations_url":"https://api.github.com/users/cadl/orgs","repos_url":"https://api.github.com/users/cadl/repos","events_url":"https://api.github.com/users/cadl/events{/privacy}","received_events_url":"https://api.github.com/users/cadl/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:47:15Z","updated_at":"2020-09-14T02:47:15Z","author_association":"NONE","body":"> @cadl : Did setting the config help ?\r\n\r\n@bvaradar  sorry, I'm a little busy these days. I'll check it this week, the setting looks very helpful. Thanks","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691779849/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691780797","html_url":"https://github.com/apache/hudi/issues/2067#issuecomment-691780797","issue_url":"https://api.github.com/repos/apache/hudi/issues/2067","id":691780797,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTc4MDc5Nw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T02:51:06Z","updated_at":"2020-09-14T02:51:06Z","author_association":"CONTRIBUTOR","body":"Closing this issue. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691780797/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691815280","html_url":"https://github.com/apache/hudi/issues/2020#issuecomment-691815280","issue_url":"https://api.github.com/repos/apache/hudi/issues/2020","id":691815280,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTgxNTI4MA==","user":{"login":"zherenyu831","id":52404525,"node_id":"MDQ6VXNlcjUyNDA0NTI1","avatar_url":"https://avatars.githubusercontent.com/u/52404525?v=4","gravatar_id":"","url":"https://api.github.com/users/zherenyu831","html_url":"https://github.com/zherenyu831","followers_url":"https://api.github.com/users/zherenyu831/followers","following_url":"https://api.github.com/users/zherenyu831/following{/other_user}","gists_url":"https://api.github.com/users/zherenyu831/gists{/gist_id}","starred_url":"https://api.github.com/users/zherenyu831/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zherenyu831/subscriptions","organizations_url":"https://api.github.com/users/zherenyu831/orgs","repos_url":"https://api.github.com/users/zherenyu831/repos","events_url":"https://api.github.com/users/zherenyu831/events{/privacy}","received_events_url":"https://api.github.com/users/zherenyu831/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T05:14:17Z","updated_at":"2020-09-14T05:14:17Z","author_association":"CONTRIBUTOR","body":"@bvaradar \r\nThank you so much, will keeping using hoodie.filesystem.view.incr.timeline.sync.enable=false","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691815280/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691855992","html_url":"https://github.com/apache/hudi/issues/2083#issuecomment-691855992","issue_url":"https://api.github.com/repos/apache/hudi/issues/2083","id":691855992,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MTg1NTk5Mg==","user":{"login":"zherenyu831","id":52404525,"node_id":"MDQ6VXNlcjUyNDA0NTI1","avatar_url":"https://avatars.githubusercontent.com/u/52404525?v=4","gravatar_id":"","url":"https://api.github.com/users/zherenyu831","html_url":"https://github.com/zherenyu831","followers_url":"https://api.github.com/users/zherenyu831/followers","following_url":"https://api.github.com/users/zherenyu831/following{/other_user}","gists_url":"https://api.github.com/users/zherenyu831/gists{/gist_id}","starred_url":"https://api.github.com/users/zherenyu831/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zherenyu831/subscriptions","organizations_url":"https://api.github.com/users/zherenyu831/orgs","repos_url":"https://api.github.com/users/zherenyu831/repos","events_url":"https://api.github.com/users/zherenyu831/events{/privacy}","received_events_url":"https://api.github.com/users/zherenyu831/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T06:54:25Z","updated_at":"2020-09-14T06:54:25Z","author_association":"CONTRIBUTOR","body":"Hi rafaelhbarros\r\nWe are running similar solution as you,\r\njust a suggestion, isn't the parallelism too small for you?\r\n```\r\nhoodie.insert.shuffle.parallelism=10 \r\nhoodie.upsert.shuffle.parallelism=10\r\n```\r\n\r\nwe using 1500 parallelism(default) to handle 2000 upsert tps with 9 cores of c5 instances.\r\n\r\n \r\n ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/691855992/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692027032","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-692027032","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":692027032,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjAyNzAzMg==","user":{"login":"wangxianghu","id":49835526,"node_id":"MDQ6VXNlcjQ5ODM1NTI2","avatar_url":"https://avatars.githubusercontent.com/u/49835526?v=4","gravatar_id":"","url":"https://api.github.com/users/wangxianghu","html_url":"https://github.com/wangxianghu","followers_url":"https://api.github.com/users/wangxianghu/followers","following_url":"https://api.github.com/users/wangxianghu/following{/other_user}","gists_url":"https://api.github.com/users/wangxianghu/gists{/gist_id}","starred_url":"https://api.github.com/users/wangxianghu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangxianghu/subscriptions","organizations_url":"https://api.github.com/users/wangxianghu/orgs","repos_url":"https://api.github.com/users/wangxianghu/repos","events_url":"https://api.github.com/users/wangxianghu/events{/privacy}","received_events_url":"https://api.github.com/users/wangxianghu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T12:43:28Z","updated_at":"2020-09-14T12:43:28Z","author_association":"CONTRIBUTOR","body":"> I can help address the remaining feedback. I will push a small diff today/tmrw.\r\n> Overall, looks like a reasonable start.\r\n> \r\n> The major feedback I still have is the following\r\n> \r\n> > would a parallelDo(func, parallelism) method in HoodieEngineContext help us avoid a lot of base/child class duplication of logic like this?\r\n> \r\n> Lot of usages are like `jsc.parallelize(list, parallelism).map(func)` , which all require a base-child class now. I am wondering if its easier to take those usages alone and implement as `engineContext.parallelDo(list, func, parallelism)`. This can be the lowest common denominator across Spark/Flink etc. We can avoid splitting a good chunk of classes if we do this IMO. If this is interesting, and we agree, I can try to quantify.\r\n\r\n@vinothchandar thanks for your help, I think introducing a method to replace `jsc.parallelize(list, parallelism).map(func)` is a good idea. I'll give a try about this.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692027032/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692035393","html_url":"https://github.com/apache/hudi/pull/1968#issuecomment-692035393","issue_url":"https://api.github.com/repos/apache/hudi/issues/1968","id":692035393,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjAzNTM5Mw==","user":{"login":"liujinhui1994","id":25769285,"node_id":"MDQ6VXNlcjI1NzY5Mjg1","avatar_url":"https://avatars.githubusercontent.com/u/25769285?v=4","gravatar_id":"","url":"https://api.github.com/users/liujinhui1994","html_url":"https://github.com/liujinhui1994","followers_url":"https://api.github.com/users/liujinhui1994/followers","following_url":"https://api.github.com/users/liujinhui1994/following{/other_user}","gists_url":"https://api.github.com/users/liujinhui1994/gists{/gist_id}","starred_url":"https://api.github.com/users/liujinhui1994/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/liujinhui1994/subscriptions","organizations_url":"https://api.github.com/users/liujinhui1994/orgs","repos_url":"https://api.github.com/users/liujinhui1994/repos","events_url":"https://api.github.com/users/liujinhui1994/events{/privacy}","received_events_url":"https://api.github.com/users/liujinhui1994/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T12:59:43Z","updated_at":"2020-09-14T12:59:43Z","author_association":"CONTRIBUTOR","body":"Please help review again,thanks \r\n@yanghua @vinothchandar ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692035393/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692074997","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-692074997","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":692074997,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjA3NDk5Nw==","user":{"login":"wangxianghu","id":49835526,"node_id":"MDQ6VXNlcjQ5ODM1NTI2","avatar_url":"https://avatars.githubusercontent.com/u/49835526?v=4","gravatar_id":"","url":"https://api.github.com/users/wangxianghu","html_url":"https://github.com/wangxianghu","followers_url":"https://api.github.com/users/wangxianghu/followers","following_url":"https://api.github.com/users/wangxianghu/following{/other_user}","gists_url":"https://api.github.com/users/wangxianghu/gists{/gist_id}","starred_url":"https://api.github.com/users/wangxianghu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangxianghu/subscriptions","organizations_url":"https://api.github.com/users/wangxianghu/orgs","repos_url":"https://api.github.com/users/wangxianghu/repos","events_url":"https://api.github.com/users/wangxianghu/events{/privacy}","received_events_url":"https://api.github.com/users/wangxianghu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T14:05:27Z","updated_at":"2020-09-14T14:07:00Z","author_association":"CONTRIBUTOR","body":"> I can help address the remaining feedback. I will push a small diff today/tmrw.\r\n> Overall, looks like a reasonable start.\r\n> \r\n> The major feedback I still have is the following\r\n> \r\n> > would a parallelDo(func, parallelism) method in HoodieEngineContext help us avoid a lot of base/child class duplication of logic like this?\r\n> \r\n> Lot of usages are like `jsc.parallelize(list, parallelism).map(func)` , which all require a base-child class now. I am wondering if its easier to take those usages alone and implement as `engineContext.parallelDo(list, func, parallelism)`. This can be the lowest common denominator across Spark/Flink etc. We can avoid splitting a good chunk of classes if we do this IMO. If this is interesting, and we agree, I can try to quantify.\r\n\r\nHi @vinothchandar, how about this demo?\r\n![image](https://user-images.githubusercontent.com/49835526/93096069-826f7c00-f6d6-11ea-9453-a96bd6ff8157.png)\r\n![image](https://user-images.githubusercontent.com/49835526/93096090-8ac7b700-f6d6-11ea-971b-d6956b016988.png)\r\n![image](https://user-images.githubusercontent.com/49835526/93096113-91eec500-f6d6-11ea-84c3-2712f72530fa.png)\r\n![image](https://user-images.githubusercontent.com/49835526/93096140-9a470000-f6d6-11ea-9685-686c507bf8ad.png)\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692074997/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692170318","html_url":"https://github.com/apache/hudi/issues/2083#issuecomment-692170318","issue_url":"https://api.github.com/repos/apache/hudi/issues/2083","id":692170318,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjE3MDMxOA==","user":{"login":"rnatarajan","id":2908985,"node_id":"MDQ6VXNlcjI5MDg5ODU=","avatar_url":"https://avatars.githubusercontent.com/u/2908985?v=4","gravatar_id":"","url":"https://api.github.com/users/rnatarajan","html_url":"https://github.com/rnatarajan","followers_url":"https://api.github.com/users/rnatarajan/followers","following_url":"https://api.github.com/users/rnatarajan/following{/other_user}","gists_url":"https://api.github.com/users/rnatarajan/gists{/gist_id}","starred_url":"https://api.github.com/users/rnatarajan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rnatarajan/subscriptions","organizations_url":"https://api.github.com/users/rnatarajan/orgs","repos_url":"https://api.github.com/users/rnatarajan/repos","events_url":"https://api.github.com/users/rnatarajan/events{/privacy}","received_events_url":"https://api.github.com/users/rnatarajan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T16:31:00Z","updated_at":"2020-09-14T16:31:00Z","author_association":"NONE","body":"Update on what @rafaelhbarros  has mentioned.\r\n\r\nWith Hudi 0.6.0, Identified a bottleneck in Sort and turned the feature off (\"hoodie.bulkinsert.sort.mode - NONE\").\r\nMatching parallelism with number of cores*executors available give the optimal speed.\r\nIf the cores*executors = 10 and if parallelism is 20, then 10 cores*processors cannot perform real parallelism of 20 and the time taken to process the record becomes more.\r\n\r\nWith Hudi MoR and Bulk Insert + without Sort, with the parameters that @rafaelhbarros has posted was able to achieve about 20K Rows Per second.\r\n\r\nWith Hudi CoW and Insert Mode + Without Sort was able to achieve 15K Rows per second.\r\n\r\nWe are aiming to achieve about 20K Rows per second with similar hardware( --driver-memory 4G    --executor-memory 5G   \r\n --executor-cores 4  --num-executors 6 ).\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692170318/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692196027","html_url":"https://github.com/apache/hudi/pull/1978#issuecomment-692196027","issue_url":"https://api.github.com/repos/apache/hudi/issues/1978","id":692196027,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjE5NjAyNw==","user":{"login":"nsivabalan","id":513218,"node_id":"MDQ6VXNlcjUxMzIxOA==","avatar_url":"https://avatars.githubusercontent.com/u/513218?v=4","gravatar_id":"","url":"https://api.github.com/users/nsivabalan","html_url":"https://github.com/nsivabalan","followers_url":"https://api.github.com/users/nsivabalan/followers","following_url":"https://api.github.com/users/nsivabalan/following{/other_user}","gists_url":"https://api.github.com/users/nsivabalan/gists{/gist_id}","starred_url":"https://api.github.com/users/nsivabalan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nsivabalan/subscriptions","organizations_url":"https://api.github.com/users/nsivabalan/orgs","repos_url":"https://api.github.com/users/nsivabalan/repos","events_url":"https://api.github.com/users/nsivabalan/events{/privacy}","received_events_url":"https://api.github.com/users/nsivabalan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T17:18:19Z","updated_at":"2020-09-14T17:18:19Z","author_association":"CONTRIBUTOR","body":"@n3nash : not sure if you had any issues w/ tests. my feeback is addressed and patch is good from my end. Please land it if you are fine with the patch. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692196027/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692228332","html_url":"https://github.com/apache/hudi/issues/2089#issuecomment-692228332","issue_url":"https://api.github.com/repos/apache/hudi/issues/2089","id":692228332,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjIyODMzMg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T18:19:12Z","updated_at":"2020-09-14T18:19:12Z","author_association":"MEMBER","body":"Mind sharing the full stack trace that spits out \r\n\r\n```\r\nERROR: org.apache.hudi.exception.HoodieIOException: IOException when reading log file\r\n\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692228332/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692240555","html_url":"https://github.com/apache/hudi/issues/2089#issuecomment-692240555","issue_url":"https://api.github.com/repos/apache/hudi/issues/2089","id":692240555,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjI0MDU1NQ==","user":{"login":"harishchanderramesh","id":46951911,"node_id":"MDQ6VXNlcjQ2OTUxOTEx","avatar_url":"https://avatars.githubusercontent.com/u/46951911?v=4","gravatar_id":"","url":"https://api.github.com/users/harishchanderramesh","html_url":"https://github.com/harishchanderramesh","followers_url":"https://api.github.com/users/harishchanderramesh/followers","following_url":"https://api.github.com/users/harishchanderramesh/following{/other_user}","gists_url":"https://api.github.com/users/harishchanderramesh/gists{/gist_id}","starred_url":"https://api.github.com/users/harishchanderramesh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harishchanderramesh/subscriptions","organizations_url":"https://api.github.com/users/harishchanderramesh/orgs","repos_url":"https://api.github.com/users/harishchanderramesh/repos","events_url":"https://api.github.com/users/harishchanderramesh/events{/privacy}","received_events_url":"https://api.github.com/users/harishchanderramesh/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T18:42:27Z","updated_at":"2020-09-14T18:42:27Z","author_association":"NONE","body":"@vinothchandar Thanks for the turn around.\r\n\r\nFull stack trace :\r\n```\r\norg.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n20/09/14 18:37:40 WARN TaskSetManager: Lost task 10.0 in stage 0.0 (TID 10, ip-10-11-4-181.corp.bluejeans.com, executor 3): org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n[Stage 0:>                                                       (1 + 32) / 313]^[[C20/09/14 18:37:54 WARN TaskSetManager: Lost task 13.0 in stage 0.0 (TID 13, ip-10-11-4-181.corp.bluejeans.com, executor 4): org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n20/09/14 18:38:09 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2, ip-10-11-4-86.corp.bluejeans.com, executor 1): org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n[Stage 0:>                                                       (5 + 36) / 313]20/09/14 18:38:23 WARN TaskSetManager: Lost task 25.0 in stage 0.0 (TID 25, ip-10-11-4-181.corp.bluejeans.com, executor 7): org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n[Stage 0:=>                                                      (6 + 40) / 313]20/09/14 18:38:36 WARN TaskSetManager: Lost task 17.0 in stage 0.0 (TID 17, ip-10-11-4-181.corp.bluejeans.com, executor 6): org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n[Stage 0:=>                                                     (11 + 40) / 313]20/09/14 18:38:54 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4, ip-10-11-4-181.corp.bluejeans.com, executor 2): org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n20/09/14 18:38:54 WARN TaskSetManager: Lost task 4.1 in stage 0.0 (TID 71, ip-10-11-4-181.corp.bluejeans.com, executor 2): org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bjnbi-hudi/endpoints/creation_date=2020-09-14/69118cac-0c59-4298-a902-7110200264e8-0_37-2225-410250_20200914181014.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)\r\n\tat org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:297)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)\r\n\tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)\r\n\tat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\r\n\tat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\r\n\tat com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)\r\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\r\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1323)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)\r\n\t... 55 more\r\n\r\n20/09/14 18:38:54 WARN TaskSetManager: Lost task 51.0 in stage 0.0 (TID 70, ip-10-11-4-181.corp.bluejeans.com, executor 2): org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bjnbi-hudi/endpoints/creation_date=2020-09-14/28f6df99-a474-4138-9164-29054c628cbe-0_33-2216-409750_20200914180902.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)\r\n\tat org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:297)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)\r\n\tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)\r\n\tat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\r\n\tat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\r\n\tat com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)\r\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\r\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1323)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)\r\n\t... 55 more\r\n\r\n20/09/14 18:38:54 WARN TaskSetManager: Lost task 52.0 in stage 0.0 (TID 73, ip-10-11-4-181.corp.bluejeans.com, executor 2): org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bjnbi-hudi/endpoints/creation_date=2020-09-14/43d228d8-5c49-4b6c-ac27-5171429c618b-0_22-2225-410235_20200914181014.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)\r\n\tat org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:297)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)\r\n\tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)\r\n\tat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\r\n\tat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\r\n\tat com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)\r\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\r\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1323)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)\r\n\t... 55 more\r\n\r\n20/09/14 18:38:54 WARN TaskSetManager: Lost task 6.1 in stage 0.0 (TID 72, ip-10-11-4-181.corp.bluejeans.com, executor 2): org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bjnbi-hudi/endpoints/creation_date=2020-09-14/d3880b74-c0d9-459c-b3e6-a8d58b0e65c9-0_198-2225-410411_20200914181014.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)\r\n\tat org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:297)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)\r\n\tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)\r\n\tat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\r\n\tat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\r\n\tat com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)\r\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\r\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1323)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)\r\n\t... 55 more\r\n\r\n20/09/14 18:39:12 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3, ip-10-11-4-86.corp.bluejeans.com, executor 1): org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\r\n20/09/14 18:39:12 WARN TaskSetManager: Lost task 6.3 in stage 0.0 (TID 79, ip-10-11-4-86.corp.bluejeans.com, executor 1): org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bjnbi-hudi/endpoints/creation_date=2020-09-14/d3880b74-c0d9-459c-b3e6-a8d58b0e65c9-0_198-2225-410411_20200914181014.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)\r\n\tat org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:297)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)\r\n\tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)\r\n\tat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\r\n\tat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\r\n\tat com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)\r\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\r\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1323)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)\r\n\t... 55 more\r\n\r\n20/09/14 18:39:12 ERROR TaskSetManager: Task 6 in stage 0.0 failed 4 times; aborting job\r\n20/09/14 18:39:12 WARN TaskSetManager: Lost task 3.1 in stage 0.0 (TID 80, ip-10-11-4-86.corp.bluejeans.com, executor 1): org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bjnbi-hudi/endpoints/creation_date=2020-09-14/bbcc2983-c861-49ce-ba52-adb9f07ba151-0_17-2225-410230_20200914181014.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)\r\n\tat org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:297)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)\r\n\tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)\r\n\tat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\r\n\tat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\r\n\tat com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)\r\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\r\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1323)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)\r\n\t... 55 more\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/lib/spark/python/pyspark/sql/dataframe.py\", line 383, in show\r\n    print(self._jdf.showString(n, int(truncate), vertical))\r\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\r\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\r\n    return f(*a, **kw)\r\n  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\r\npy4j.protocol.Py4JJavaError: An error occurred while calling o79.showString.\r\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 0.0 failed 4 times, most recent failure: Lost task 6.3 in stage 0.0 (TID 79, ip-10-11-4-86.corp.bluejeans.com, executor 1): org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bjnbi-hudi/endpoints/creation_date=2020-09-14/d3880b74-c0d9-459c-b3e6-a8d58b0e65c9-0_198-2225-410411_20200914181014.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)\r\n\tat org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:297)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)\r\n\tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)\r\n\tat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\r\n\tat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\r\n\tat com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)\r\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\r\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1323)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)\r\n\t... 55 more\r\n\r\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:146)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:128)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$anonfun$finalPhysicalPlan$1.apply(AdaptiveSparkPlanExec.scala:127)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:777)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.finalPhysicalPlan(AdaptiveSparkPlanExec.scala:127)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:134)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3395)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2552)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2552)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2766)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:255)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:292)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bjnbi-hudi/endpoints/creation_date=2020-09-14/d3880b74-c0d9-459c-b3e6-a8d58b0e65c9-0_198-2225-410411_20200914181014.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond: Unable to execute HTTP request: The target server failed to respond\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:128)\r\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:385)\r\n\tat parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:371)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit(ParquetRecordReaderWrapper.java:252)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:99)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:85)\r\n\tat org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:72)\r\n\tat org.apache.hudi.hadoop.HoodieParquetInputFormat.getRecordReader(HoodieParquetInputFormat.java:297)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:267)\r\n\tat org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:266)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:224)\r\n\tat org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:95)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:141)\r\n\tat org.apache.http.impl.conn.DefaultHttpResponseParser.parseHead(DefaultHttpResponseParser.java:56)\r\n\tat org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:259)\r\n\tat org.apache.http.impl.DefaultBHttpClientConnection.receiveResponseHeader(DefaultBHttpClientConnection.java:163)\r\n\tat org.apache.http.impl.conn.CPoolProxy.receiveResponseHeader(CPoolProxy.java:157)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:273)\r\n\tat com.amazonaws.http.protocol.SdkHttpRequestExecutor.doReceiveResponse(SdkHttpRequestExecutor.java:82)\r\n\tat org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)\r\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:272)\r\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)\r\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\r\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\r\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1323)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1139)\r\n\t... 55 more\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692240555/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692257956","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-692257956","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":692257956,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjI1Nzk1Ng==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-14T19:15:14Z","updated_at":"2020-09-14T19:15:14Z","author_association":"MEMBER","body":"That sounds promising. We cannot easily fix `T,R,O` though right at the class level. it will be different in each setting, right?  \r\n\r\nWe can use generic methods though I think and what I had in mind was something little different\r\nif the method signature can be the following \r\n\r\n```\r\npublic abstract <I,O> List<O> parallelDo(List<I> data, Function<I, O> func, int parallelism); \r\n```\r\n\r\nand for the Spark the implementation \r\n\r\n```\r\npublic <I,O> List<O> parallelDo(List<I> data, Function<I, O> func, int parallelism) {\r\n   return jsc.parallelize(data, parallelism).map(func).collectAsList();\r\n}\r\n```\r\n\r\nand the invocation \r\n\r\n```\r\nengineContext.parallelDo(Arrays.asList(1,2,3), (a) -> {\r\n      return 0;\r\n    }, 3);\r\n\r\n```\r\n\r\nCan you a small Flink implementation as well and confirm this approach will work for Flink as well. Thats another key thing to answer, before we finalize the approach. \r\n\r\n\r\nNote that \r\n\r\na) some of the code that uses `rdd.mapToPair()` etc before collecting, have to be rewritten using Java Streams. i.e \r\n\r\n`engineContext.parallelDo().map()` instead of `jsc.parallelize().map().mapToPair()` currently.  \r\n\r\nb) Also we should limit the use of parallelDo() to only cases where there are no grouping/aggregations involved. for e.g if we are doing `jsc.parallelize(files, ..).map(record).reduceByKey(..)`, collecting without reducing will lead to OOMs. We can preserve what you are doing currently, for such cases.  \r\n\r\nBut the parallelDo should help up reduce the amount of code broken up (and thus reduce the effort for the flink engine). \r\n\r\n\r\n@wangxianghu I am happy to shepherd this PR through from this point as well. lmk \r\n\r\ncc @yanghua @leesf \r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692257956/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692389993","html_url":"https://github.com/apache/hudi/pull/2064#issuecomment-692389993","issue_url":"https://api.github.com/repos/apache/hudi/issues/2064","id":692389993,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjM4OTk5Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T00:33:08Z","updated_at":"2020-09-15T00:33:08Z","author_association":"MEMBER","body":"@prashantwason please let @umehrot2 know if we can start testing this. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692389993/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692436132","html_url":"https://github.com/apache/hudi/pull/2090#issuecomment-692436132","issue_url":"https://api.github.com/repos/apache/hudi/issues/2090","id":692436132,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjQzNjEzMg==","user":{"login":"liujinhui1994","id":25769285,"node_id":"MDQ6VXNlcjI1NzY5Mjg1","avatar_url":"https://avatars.githubusercontent.com/u/25769285?v=4","gravatar_id":"","url":"https://api.github.com/users/liujinhui1994","html_url":"https://github.com/liujinhui1994","followers_url":"https://api.github.com/users/liujinhui1994/followers","following_url":"https://api.github.com/users/liujinhui1994/following{/other_user}","gists_url":"https://api.github.com/users/liujinhui1994/gists{/gist_id}","starred_url":"https://api.github.com/users/liujinhui1994/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/liujinhui1994/subscriptions","organizations_url":"https://api.github.com/users/liujinhui1994/orgs","repos_url":"https://api.github.com/users/liujinhui1994/repos","events_url":"https://api.github.com/users/liujinhui1994/events{/privacy}","received_events_url":"https://api.github.com/users/liujinhui1994/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T03:11:55Z","updated_at":"2020-09-15T03:11:55Z","author_association":"CONTRIBUTOR","body":"What do you think？\r\n@vinothchandar @yanghua ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692436132/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692485806","html_url":"https://github.com/apache/hudi/issues/2086#issuecomment-692485806","issue_url":"https://api.github.com/repos/apache/hudi/issues/2086","id":692485806,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjQ4NTgwNg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T06:03:40Z","updated_at":"2020-09-15T06:03:40Z","author_association":"CONTRIBUTOR","body":"@rajgowtham24 : Please take a look at https://github.com/apache/hudi/issues/1977#issuecomment-678558692 (cc @umehrot2 )","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692485806/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692486055","html_url":"https://github.com/apache/hudi/issues/2083#issuecomment-692486055","issue_url":"https://api.github.com/repos/apache/hudi/issues/2083","id":692486055,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjQ4NjA1NQ==","user":{"login":"rnatarajan","id":2908985,"node_id":"MDQ6VXNlcjI5MDg5ODU=","avatar_url":"https://avatars.githubusercontent.com/u/2908985?v=4","gravatar_id":"","url":"https://api.github.com/users/rnatarajan","html_url":"https://github.com/rnatarajan","followers_url":"https://api.github.com/users/rnatarajan/followers","following_url":"https://api.github.com/users/rnatarajan/following{/other_user}","gists_url":"https://api.github.com/users/rnatarajan/gists{/gist_id}","starred_url":"https://api.github.com/users/rnatarajan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rnatarajan/subscriptions","organizations_url":"https://api.github.com/users/rnatarajan/orgs","repos_url":"https://api.github.com/users/rnatarajan/repos","events_url":"https://api.github.com/users/rnatarajan/events{/privacy}","received_events_url":"https://api.github.com/users/rnatarajan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T06:04:13Z","updated_at":"2020-09-15T06:04:13Z","author_association":"NONE","body":"Update on this: \r\n\r\nFound the bottleneck as [countByKey](https://github.com/apache/hudi/blob/master/hudi-client/src/main/java/org/apache/hudi/table/WorkloadProfile.java#L73)\r\n\r\nWe were reading data from Kafka(spread across 20 partitions)\r\nWe tested with hoodie.datasource.write.partitionpath.field as \"\" or \"<somefield>\"\r\n\r\nIn both cases, records read from Kafka across all partitioned(For a batch) was shuffled performing countByKey.\r\nThis caused a major throughput drop.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692486055/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692487548","html_url":"https://github.com/apache/hudi/issues/2089#issuecomment-692487548","issue_url":"https://api.github.com/repos/apache/hudi/issues/2089","id":692487548,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjQ4NzU0OA==","user":{"login":"harishchanderramesh","id":46951911,"node_id":"MDQ6VXNlcjQ2OTUxOTEx","avatar_url":"https://avatars.githubusercontent.com/u/46951911?v=4","gravatar_id":"","url":"https://api.github.com/users/harishchanderramesh","html_url":"https://github.com/harishchanderramesh","followers_url":"https://api.github.com/users/harishchanderramesh/followers","following_url":"https://api.github.com/users/harishchanderramesh/following{/other_user}","gists_url":"https://api.github.com/users/harishchanderramesh/gists{/gist_id}","starred_url":"https://api.github.com/users/harishchanderramesh/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harishchanderramesh/subscriptions","organizations_url":"https://api.github.com/users/harishchanderramesh/orgs","repos_url":"https://api.github.com/users/harishchanderramesh/repos","events_url":"https://api.github.com/users/harishchanderramesh/events{/privacy}","received_events_url":"https://api.github.com/users/harishchanderramesh/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T06:08:02Z","updated_at":"2020-09-15T06:08:02Z","author_association":"NONE","body":"After sometime, reading the same table on hive shell also returns the same error.\r\n```\r\nhive > select callguid,meeting_uuid,count(*) from endpoints_rt where creation_date>='2020-09-14' group by callguid,meeting_uuid having count(*)>1;\r\nQuery ID = hadoop_20200915060546_8bede2c7-d7ec-4cde-b7d8-616fc53a4053\r\nTotal jobs = 1\r\nLaunching Job 1 out of 1\r\nTez session was closed. Reopening...\r\nSession re-established.\r\nStatus: Running (Executing on YARN cluster with App id application_1592910106194_134023)\r\n\r\n----------------------------------------------------------------------------------------------\r\n        VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED  \r\n----------------------------------------------------------------------------------------------\r\nMap 1 .          container       RUNNING      8          1        0        7      15       0  \r\nReducer 2        container        INITED      2          0        0        2       0       0  \r\n----------------------------------------------------------------------------------------------\r\nVERTICES: 00/02  [==>>------------------------] 10%   ELAPSED TIME: 39.10 s    \r\n----------------------------------------------------------------------------------------------\r\nStatus: Failed\r\nVertex failed, vertexName=Map 1, vertexId=vertex_1592910106194_134023_1_00, diagnostics=[Task failed, taskId=task_1592910106194_134023_1_00_000007, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1592910106194_134023_1_00_000007_0:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)\r\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)\r\n\t... 14 more\r\nCaused by: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:379)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n\t... 25 more\r\nCaused by: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:376)\r\n\t... 26 more\r\n], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1592910106194_134023_1_00_000007_1:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)\r\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)\r\n\t... 14 more\r\nCaused by: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:379)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n\t... 25 more\r\nCaused by: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:376)\r\n\t... 26 more\r\n], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1592910106194_134023_1_00_000007_2:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)\r\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)\r\n\t... 14 more\r\nCaused by: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:379)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n\t... 25 more\r\nCaused by: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:376)\r\n\t... 26 more\r\n], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1592910106194_134023_1_00_000007_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)\r\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)\r\n\t... 14 more\r\nCaused by: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:379)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n\t... 25 more\r\nCaused by: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:376)\r\n\t... 26 more\r\n]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:6, Vertex vertex_1592910106194_134023_1_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]\r\nVertex killed, vertexName=Reducer 2, vertexId=vertex_1592910106194_134023_1_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:2, Vertex vertex_1592910106194_134023_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]\r\nDAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1\r\nFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1592910106194_134023_1_00, diagnostics=[Task failed, taskId=task_1592910106194_134023_1_00_000007, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1592910106194_134023_1_00_000007_0:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)\r\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)\r\n\t... 14 more\r\nCaused by: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:379)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n\t... 25 more\r\nCaused by: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:376)\r\n\t... 26 more\r\n], TaskAttempt 1 failed, info=[Error: Error while running task ( failure ) : attempt_1592910106194_134023_1_00_000007_1:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)\r\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)\r\n\t... 14 more\r\nCaused by: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:379)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n\t... 25 more\r\nCaused by: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:376)\r\n\t... 26 more\r\n], TaskAttempt 2 failed, info=[Error: Error while running task ( failure ) : attempt_1592910106194_134023_1_00_000007_2:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)\r\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)\r\n\t... 14 more\r\nCaused by: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:379)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n\t... 25 more\r\nCaused by: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:376)\r\n\t... 26 more\r\n], TaskAttempt 3 failed, info=[Error: Error while running task ( failure ) : attempt_1592910106194_134023_1_00_000007_3:java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:211)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:168)\r\n\tat org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61)\r\n\tat org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37)\r\n\tat org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36)\r\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.RuntimeException: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:206)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.<init>(TezGroupedSplitsInputFormat.java:145)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getRecordReader(TezGroupedSplitsInputFormat.java:111)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157)\r\n\tat org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703)\r\n\tat org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150)\r\n\tat org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171)\r\n\tat org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:184)\r\n\t... 14 more\r\nCaused by: java.io.IOException: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97)\r\n\tat org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:379)\r\n\tat org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat$TezGroupedSplitsRecordReader.initNextRecordReader(TezGroupedSplitsInputFormat.java:203)\r\n\t... 25 more\r\nCaused by: org.apache.hudi.exception.HoodieIOException: IOException when reading log file \r\n\tat org.apache.hudi.common.table.log.AbstractHoodieLogRecordScanner.scan(AbstractHoodieLogRecordScanner.java:244)\r\n\tat org.apache.hudi.common.table.log.HoodieMergedLogRecordScanner.<init>(HoodieMergedLogRecordScanner.java:81)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.getMergedLogRecordScanner(RealtimeCompactedRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.RealtimeCompactedRecordReader.<init>(RealtimeCompactedRecordReader.java:52)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.constructRecordReader(HoodieRealtimeRecordReader.java:69)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader.<init>(HoodieRealtimeRecordReader.java:47)\r\n\tat org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat.getRecordReader(HoodieParquetRealtimeInputFormat.java:253)\r\n\tat org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:376)\r\n\t... 26 more\r\n]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:6, Vertex vertex_1592910106194_134023_1_00 [Map 1] killed/failed due to:OWN_TASK_FAILURE]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1592910106194_134023_1_01, diagnostics=[Vertex received Kill while in RUNNING state., Vertex did not succeed due to OTHER_VERTEX_FAILURE, failedTasks:0 killedTasks:2, Vertex vertex_1592910106194_134023_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692487548/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692501087","html_url":"https://github.com/apache/hudi/issues/2089#issuecomment-692501087","issue_url":"https://api.github.com/repos/apache/hudi/issues/2089","id":692501087,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjUwMTA4Nw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T06:40:13Z","updated_at":"2020-09-15T06:40:13Z","author_association":"CONTRIBUTOR","body":"@harishchanderramesh : There should have error log message starting with \"Got exception when reading log file\" which should have given the root-cause. \r\n\r\nBut from the context  that you pasted, I do see this : \r\n\r\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: The target server failed to respond\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1201)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1147)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:796)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:764)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:738)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:698)\r\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:680)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:544)\r\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:524)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5054)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5000)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1335)\r\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1309)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\r\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\r\n\t... 42 more\r\nCaused by: org.apache.http.NoHttpResponseException: The target server failed to respond\r\n\r\n\r\nThis looks like server side issue on the S3 side. Are you seeing the same exception consistently ? \r\n\r\n@umehrot2 : Any idea what is happening. ?\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692501087/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692524861","html_url":"https://github.com/apache/hudi/issues/2083#issuecomment-692524861","issue_url":"https://api.github.com/repos/apache/hudi/issues/2083","id":692524861,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjUyNDg2MQ==","user":{"login":"tooptoop4","id":33283496,"node_id":"MDQ6VXNlcjMzMjgzNDk2","avatar_url":"https://avatars.githubusercontent.com/u/33283496?v=4","gravatar_id":"","url":"https://api.github.com/users/tooptoop4","html_url":"https://github.com/tooptoop4","followers_url":"https://api.github.com/users/tooptoop4/followers","following_url":"https://api.github.com/users/tooptoop4/following{/other_user}","gists_url":"https://api.github.com/users/tooptoop4/gists{/gist_id}","starred_url":"https://api.github.com/users/tooptoop4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tooptoop4/subscriptions","organizations_url":"https://api.github.com/users/tooptoop4/orgs","repos_url":"https://api.github.com/users/tooptoop4/repos","events_url":"https://api.github.com/users/tooptoop4/events{/privacy}","received_events_url":"https://api.github.com/users/tooptoop4/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T07:30:49Z","updated_at":"2020-09-15T07:30:49Z","author_association":"NONE","body":"did u fix?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692524861/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692580624","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-692580624","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":692580624,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjU4MDYyNA==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T09:08:33Z","updated_at":"2020-09-15T09:08:33Z","author_association":"CONTRIBUTOR","body":"+1 to turn to function API fashion and standard / reduce some API's behavior. We are verifying and discussing with @wangxianghu offline.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692580624/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692604118","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-692604118","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":692604118,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjYwNDExOA==","user":{"login":"wangxianghu","id":49835526,"node_id":"MDQ6VXNlcjQ5ODM1NTI2","avatar_url":"https://avatars.githubusercontent.com/u/49835526?v=4","gravatar_id":"","url":"https://api.github.com/users/wangxianghu","html_url":"https://github.com/wangxianghu","followers_url":"https://api.github.com/users/wangxianghu/followers","following_url":"https://api.github.com/users/wangxianghu/following{/other_user}","gists_url":"https://api.github.com/users/wangxianghu/gists{/gist_id}","starred_url":"https://api.github.com/users/wangxianghu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangxianghu/subscriptions","organizations_url":"https://api.github.com/users/wangxianghu/orgs","repos_url":"https://api.github.com/users/wangxianghu/repos","events_url":"https://api.github.com/users/wangxianghu/events{/privacy}","received_events_url":"https://api.github.com/users/wangxianghu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T09:46:59Z","updated_at":"2020-09-15T10:21:10Z","author_association":"CONTRIBUTOR","body":"> That sounds promising. We cannot easily fix `T,R,O` though right at the class level. it will be different in each setting, right?\r\n> \r\n> We can use generic methods though I think and what I had in mind was something little different\r\n> if the method signature can be the following\r\n> \r\n> ```\r\n> public abstract <I,O> List<O> parallelDo(List<I> data, Function<I, O> func, int parallelism); \r\n> ```\r\n> \r\n> and for the Spark the implementation\r\n> \r\n> ```\r\n> public <I,O> List<O> parallelDo(List<I> data, Function<I, O> func, int parallelism) {\r\n>    return jsc.parallelize(data, parallelism).map(func).collectAsList();\r\n> }\r\n> ```\r\n> \r\n> and the invocation\r\n> \r\n> ```\r\n> engineContext.parallelDo(Arrays.asList(1,2,3), (a) -> {\r\n>       return 0;\r\n>     }, 3);\r\n> ```\r\n> \r\n> Can you a small Flink implementation as well and confirm this approach will work for Flink as well. Thats another key thing to answer, before we finalize the approach.\r\n> \r\n> Note that\r\n> \r\n> a) some of the code that uses `rdd.mapToPair()` etc before collecting, have to be rewritten using Java Streams. i.e\r\n> \r\n> `engineContext.parallelDo().map()` instead of `jsc.parallelize().map().mapToPair()` currently.\r\n> \r\n> b) Also we should limit the use of parallelDo() to only cases where there are no grouping/aggregations involved. for e.g if we are doing `jsc.parallelize(files, ..).map(record).reduceByKey(..)`, collecting without reducing will lead to OOMs. We can preserve what you are doing currently, for such cases.\r\n> \r\n> But the parallelDo should help up reduce the amount of code broken up (and thus reduce the effort for the flink engine).\r\n> \r\n> @wangxianghu I am happy to shepherd this PR through from this point as well. lmk\r\n> \r\n> cc @yanghua @leesf\r\n\r\nYes, I noticed that problem lately. we should use generic methods. I also agree with the a) and b) you mentioned.\r\nIn the `parallelDo` method flink engine can operate the list directly using Java Stream, I have verified that. but there is a problem:\r\nThe function used in spark map operator is `org.apache.spark.api.java.function.Function` while what flink can use is `java.util.function.Function` we should align it. \r\n\r\nmaybe we can use `java.util.function.Function` only,  If this way, there is no need to distinguish spark and flink, there both use java stream to implement those kind operations. the generic method could be like this:\r\n```\r\npublic class HoodieEngineContext {\r\n  public <I, O> List<O> parallelDo(List<I> data, Function<I, O> func) {\r\n    return data.stream().map(func).collect(Collectors.toList());\r\n  }\r\n}\r\n```\r\nor implement the logic using java stream directly and don't introduce the `parallelDo` method.\r\nWDYT?\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692604118/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692741066","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-692741066","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":692741066,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mjc0MTA2Ng==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T14:10:56Z","updated_at":"2020-09-15T14:10:56Z","author_association":"MEMBER","body":">maybe we can use java.util.function.Function only, If this way, there is no need to distinguish spark and flink, \r\n\r\nI'd prefer this if possible. Makes life much simpler. We can add more overloads down the line with BiFunction etc.. as we go. \r\nwdyt? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692741066/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692747232","html_url":"https://github.com/apache/hudi/issues/2086#issuecomment-692747232","issue_url":"https://api.github.com/repos/apache/hudi/issues/2086","id":692747232,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mjc0NzIzMg==","user":{"login":"rajgowtham24","id":68343070,"node_id":"MDQ6VXNlcjY4MzQzMDcw","avatar_url":"https://avatars.githubusercontent.com/u/68343070?v=4","gravatar_id":"","url":"https://api.github.com/users/rajgowtham24","html_url":"https://github.com/rajgowtham24","followers_url":"https://api.github.com/users/rajgowtham24/followers","following_url":"https://api.github.com/users/rajgowtham24/following{/other_user}","gists_url":"https://api.github.com/users/rajgowtham24/gists{/gist_id}","starred_url":"https://api.github.com/users/rajgowtham24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rajgowtham24/subscriptions","organizations_url":"https://api.github.com/users/rajgowtham24/orgs","repos_url":"https://api.github.com/users/rajgowtham24/repos","events_url":"https://api.github.com/users/rajgowtham24/events{/privacy}","received_events_url":"https://api.github.com/users/rajgowtham24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T14:20:22Z","updated_at":"2020-09-15T14:20:49Z","author_association":"NONE","body":"Thanks @bvaradar. AWS Support have suggested to add the below HIVE_METASTORE in run_sync_tool.sh file and now i'm facing a different exception. Working with AWS Support team. Will keep this thread posted with the update.\r\n\r\nadded for AWS Glue Catalog hive metastore libraries.\r\nHIVE_METASTORE=/usr/share/aws/aws-java-sdk/aws-java-sdk-glue-1.11.682.jar:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-hive2-client-1.11.0.jar:$HIVE_METASTORE\r\n\r\nOn top of your head, do you think of any alternate approach for writing hudi tables into multiple buckets? TIA!","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692747232/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692766014","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-692766014","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":692766014,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mjc2NjAxNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T14:48:57Z","updated_at":"2020-09-15T14:48:57Z","author_association":"MEMBER","body":"@wangxianghu on the checkstyle change to bump up the line count to 500, I think we should revert to 200 as it is now. \r\nI checked out a few of the issues. they can be brought within limit, by folding like below. \r\nif not, we can turn off checkstyle selectively in that block?  \r\n\r\n\r\n```\r\n\r\npublic class HoodieSparkMergeHandle<T extends HoodieRecordPayload> extends\r\n    HoodieWriteHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692766014/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692911383","html_url":"https://github.com/apache/hudi/issues/1694#issuecomment-692911383","issue_url":"https://api.github.com/repos/apache/hudi/issues/1694","id":692911383,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MjkxMTM4Mw==","user":{"login":"rafaelhbarros","id":3138550,"node_id":"MDQ6VXNlcjMxMzg1NTA=","avatar_url":"https://avatars.githubusercontent.com/u/3138550?v=4","gravatar_id":"","url":"https://api.github.com/users/rafaelhbarros","html_url":"https://github.com/rafaelhbarros","followers_url":"https://api.github.com/users/rafaelhbarros/followers","following_url":"https://api.github.com/users/rafaelhbarros/following{/other_user}","gists_url":"https://api.github.com/users/rafaelhbarros/gists{/gist_id}","starred_url":"https://api.github.com/users/rafaelhbarros/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rafaelhbarros/subscriptions","organizations_url":"https://api.github.com/users/rafaelhbarros/orgs","repos_url":"https://api.github.com/users/rafaelhbarros/repos","events_url":"https://api.github.com/users/rafaelhbarros/events{/privacy}","received_events_url":"https://api.github.com/users/rafaelhbarros/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T18:59:33Z","updated_at":"2020-09-15T18:59:33Z","author_association":"NONE","body":"@Raghvendradubey did the `GLOBAL_SIMPLE` index solve your issue?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692911383/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692974719","html_url":"https://github.com/apache/hudi/issues/2083#issuecomment-692974719","issue_url":"https://api.github.com/repos/apache/hudi/issues/2083","id":692974719,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mjk3NDcxOQ==","user":{"login":"rafaelhbarros","id":3138550,"node_id":"MDQ6VXNlcjMxMzg1NTA=","avatar_url":"https://avatars.githubusercontent.com/u/3138550?v=4","gravatar_id":"","url":"https://api.github.com/users/rafaelhbarros","html_url":"https://github.com/rafaelhbarros","followers_url":"https://api.github.com/users/rafaelhbarros/followers","following_url":"https://api.github.com/users/rafaelhbarros/following{/other_user}","gists_url":"https://api.github.com/users/rafaelhbarros/gists{/gist_id}","starred_url":"https://api.github.com/users/rafaelhbarros/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rafaelhbarros/subscriptions","organizations_url":"https://api.github.com/users/rafaelhbarros/orgs","repos_url":"https://api.github.com/users/rafaelhbarros/repos","events_url":"https://api.github.com/users/rafaelhbarros/events{/privacy}","received_events_url":"https://api.github.com/users/rafaelhbarros/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-15T20:56:49Z","updated_at":"2020-09-15T20:56:49Z","author_association":"NONE","body":"Still no success, our issue seems to be very closely aligned with [this](https://github.com/apache/hudi/issues/1694) and [this](https://github.com/apache/hudi/issues/1830)","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/692974719/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/693112779","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-693112779","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":693112779,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzExMjc3OQ==","user":{"login":"wangxianghu","id":49835526,"node_id":"MDQ6VXNlcjQ5ODM1NTI2","avatar_url":"https://avatars.githubusercontent.com/u/49835526?v=4","gravatar_id":"","url":"https://api.github.com/users/wangxianghu","html_url":"https://github.com/wangxianghu","followers_url":"https://api.github.com/users/wangxianghu/followers","following_url":"https://api.github.com/users/wangxianghu/following{/other_user}","gists_url":"https://api.github.com/users/wangxianghu/gists{/gist_id}","starred_url":"https://api.github.com/users/wangxianghu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangxianghu/subscriptions","organizations_url":"https://api.github.com/users/wangxianghu/orgs","repos_url":"https://api.github.com/users/wangxianghu/repos","events_url":"https://api.github.com/users/wangxianghu/events{/privacy}","received_events_url":"https://api.github.com/users/wangxianghu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T01:17:49Z","updated_at":"2020-09-16T01:17:49Z","author_association":"CONTRIBUTOR","body":"> > maybe we can use java.util.function.Function only, If this way, there is no need to distinguish spark and flink,\r\n> \r\n> I'd prefer this if possible. Makes life much simpler. We can add more overloads down the line with BiFunction etc.. as we go.\r\n> wdyt?\r\nIt's good to me. I'll refactor the code with the BiFunction.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/693112779/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/693113074","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-693113074","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":693113074,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzExMzA3NA==","user":{"login":"wangxianghu","id":49835526,"node_id":"MDQ6VXNlcjQ5ODM1NTI2","avatar_url":"https://avatars.githubusercontent.com/u/49835526?v=4","gravatar_id":"","url":"https://api.github.com/users/wangxianghu","html_url":"https://github.com/wangxianghu","followers_url":"https://api.github.com/users/wangxianghu/followers","following_url":"https://api.github.com/users/wangxianghu/following{/other_user}","gists_url":"https://api.github.com/users/wangxianghu/gists{/gist_id}","starred_url":"https://api.github.com/users/wangxianghu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangxianghu/subscriptions","organizations_url":"https://api.github.com/users/wangxianghu/orgs","repos_url":"https://api.github.com/users/wangxianghu/repos","events_url":"https://api.github.com/users/wangxianghu/events{/privacy}","received_events_url":"https://api.github.com/users/wangxianghu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T01:19:01Z","updated_at":"2020-09-16T01:19:01Z","author_association":"CONTRIBUTOR","body":"> @wangxianghu on the checkstyle change to bump up the line count to 500, I think we should revert to 200 as it is now.\r\n> I checked out a few of the issues. they can be brought within limit, by folding like below.\r\n> if not, we can turn off checkstyle selectively in that block?\r\n> \r\n> ```\r\n> \r\n> public class HoodieSparkMergeHandle<T extends HoodieRecordPayload> extends\r\n>     HoodieWriteHandle<T, JavaRDD<HoodieRecord<T>>, JavaRDD<HoodieKey>, JavaRDD<WriteStatus>, JavaPairRDD<HoodieKey, Option<Pair<String, String>>>> {\r\n> ```\r\n\r\nGood idea. I'll give a try.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/693113074/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/693135106","html_url":"https://github.com/apache/hudi/issues/2089#issuecomment-693135106","issue_url":"https://api.github.com/repos/apache/hudi/issues/2089","id":693135106,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzEzNTEwNg==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T02:40:34Z","updated_at":"2020-09-16T02:40:34Z","author_association":"CONTRIBUTOR","body":"@harishchanderramesh Like mentioned by @bvaradar this appears to be an S3 server side issue, or might be a bug . Is this happening sporadically or you are consistently observing this ? If its happening consistently you should open a ticket with AWS Support.\r\n\r\nAlso are you running this on EMR ? If yes, you might want to try using EmrFS instead of s3a.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/693135106/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]