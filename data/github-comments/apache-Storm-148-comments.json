[{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612573761","html_url":"https://github.com/apache/storm/issues/4739#issuecomment-2612573761","issue_url":"https://api.github.com/repos/apache/storm/issues/4739","id":2612573761,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI1NzM3NjE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T02:15:22Z","updated_at":"2025-01-24T13:48:44Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/822#issuecomment-163813319\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/822#issuecomment-163813319</a></p>\n\n<p>    @revans2 @zhuoliu This solution looks good to me too.<br/>\n    My initial concern was sort of like @erikdw 's use case. And usually files in logs directory do a rolling update/delete(maybe by a cron job), while we may not want a rolling delete for metadata(or at least different delete stragety), so it's good to add an extra config in storm.yaml to specify this directory.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612573761/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163817030","html_url":"https://github.com/apache/storm/pull/942#issuecomment-163817030","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":163817030,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzgxNzAzMA==","user":{"login":"arunmahadevan","id":6792890,"node_id":"MDQ6VXNlcjY3OTI4OTA=","avatar_url":"https://avatars.githubusercontent.com/u/6792890?v=4","gravatar_id":"","url":"https://api.github.com/users/arunmahadevan","html_url":"https://github.com/arunmahadevan","followers_url":"https://api.github.com/users/arunmahadevan/followers","following_url":"https://api.github.com/users/arunmahadevan/following{/other_user}","gists_url":"https://api.github.com/users/arunmahadevan/gists{/gist_id}","starred_url":"https://api.github.com/users/arunmahadevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arunmahadevan/subscriptions","organizations_url":"https://api.github.com/users/arunmahadevan/orgs","repos_url":"https://api.github.com/users/arunmahadevan/repos","events_url":"https://api.github.com/users/arunmahadevan/events{/privacy}","received_events_url":"https://api.github.com/users/arunmahadevan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T02:28:44Z","updated_at":"2015-12-11T02:28:44Z","author_association":"CONTRIBUTOR","body":"@zhuoliu \nI see a reference to `absolute-storm-local-dir` from `worker-artifacts-root`\nhttps://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/config.clj#L255\nShould this be changed ?\n\nNot in the scope of this PR but are we going to address creating symlinks to the current worker log files under logs/ ?\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163817030/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688191","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688191","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688191,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgxOTE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T02:28:47Z","updated_at":"2025-01-24T14:39:08Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-163817030\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-163817030</a></p>\n\n<p>    @zhuoliu <br/>\n    I see a reference to `absolute-storm-local-dir` from `worker-artifacts-root`<br/>\n    <a href=\"https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/config.clj#L255\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/blob/master/storm-core/src/clj/backtype/storm/config.clj#L255</a><br/>\n    Should this be changed ?</p>\n\n<p>    Not in the scope of this PR but are we going to address creating symlinks to the current worker log files under logs/ ?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688191/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163832607","html_url":"https://github.com/apache/storm/pull/942#issuecomment-163832607","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":163832607,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzgzMjYwNw==","user":{"login":"zhuoliu","id":11683054,"node_id":"MDQ6VXNlcjExNjgzMDU0","avatar_url":"https://avatars.githubusercontent.com/u/11683054?v=4","gravatar_id":"","url":"https://api.github.com/users/zhuoliu","html_url":"https://github.com/zhuoliu","followers_url":"https://api.github.com/users/zhuoliu/followers","following_url":"https://api.github.com/users/zhuoliu/following{/other_user}","gists_url":"https://api.github.com/users/zhuoliu/gists{/gist_id}","starred_url":"https://api.github.com/users/zhuoliu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhuoliu/subscriptions","organizations_url":"https://api.github.com/users/zhuoliu/orgs","repos_url":"https://api.github.com/users/zhuoliu/repos","events_url":"https://api.github.com/users/zhuoliu/events{/privacy}","received_events_url":"https://api.github.com/users/zhuoliu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T03:55:08Z","updated_at":"2015-12-11T03:55:08Z","author_association":"NONE","body":"Hi Arun,, in the pull request, worker-artifacts-root will no longer reference to absolute-storm-local-dir (already changed).\n\nIn STORM-901, harshach commented for the symlink stuff: \"As Arun said above if the current worker-port.log is in root dir it will be helpful to tail the logs. This preferable but not necessary.\"\nIf we think the symlink stuff is really in good need, we can file another JIRA to do that.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163832607/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688199","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688199","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688199,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgxOTk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T03:55:09Z","updated_at":"2025-01-24T14:39:08Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-163832607\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-163832607</a></p>\n\n<p>    Hi Arun,, in the pull request, worker-artifacts-root will no longer reference to absolute-storm-local-dir (already changed).</p>\n\n<p>    In <a href=\"https://issues.apache.org/jira/browse/STORM-901\" title=\"Worker Artifacts Directory\" class=\"issue-link\" data-issue-key=\"STORM-901\"><del>STORM-901</del></a>, harshach commented for the symlink stuff: \"As Arun said above if the current worker-port.log is in root dir it will be helpful to tail the logs. This preferable but not necessary.\"<br/>\n    If we think the symlink stuff is really in good need, we can file another JIRA to do that.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688199/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163835814","html_url":"https://github.com/apache/storm/pull/942#issuecomment-163835814","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":163835814,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzgzNTgxNA==","user":{"login":"unsleepy22","id":631361,"node_id":"MDQ6VXNlcjYzMTM2MQ==","avatar_url":"https://avatars.githubusercontent.com/u/631361?v=4","gravatar_id":"","url":"https://api.github.com/users/unsleepy22","html_url":"https://github.com/unsleepy22","followers_url":"https://api.github.com/users/unsleepy22/followers","following_url":"https://api.github.com/users/unsleepy22/following{/other_user}","gists_url":"https://api.github.com/users/unsleepy22/gists{/gist_id}","starred_url":"https://api.github.com/users/unsleepy22/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/unsleepy22/subscriptions","organizations_url":"https://api.github.com/users/unsleepy22/orgs","repos_url":"https://api.github.com/users/unsleepy22/repos","events_url":"https://api.github.com/users/unsleepy22/events{/privacy}","received_events_url":"https://api.github.com/users/unsleepy22/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:25:04Z","updated_at":"2015-12-11T04:25:04Z","author_association":"NONE","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163835814/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688206","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688206","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688206,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyMDY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:25:08Z","updated_at":"2025-01-24T14:39:08Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-163835814\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-163835814</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688206/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687946","html_url":"https://github.com/apache/storm/issues/5220#issuecomment-2612687946","issue_url":"https://api.github.com/repos/apache/storm/issues/5220","id":2612687946,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODc5NDY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:30:02Z","updated_at":"2025-01-24T14:39:01Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user asfgit closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/937\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/937</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687946/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163838439","html_url":"https://github.com/apache/storm/pull/908#issuecomment-163838439","issue_url":"https://api.github.com/repos/apache/storm/issues/908","id":163838439,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzgzODQzOQ==","user":{"login":"knusbaum","id":1819836,"node_id":"MDQ6VXNlcjE4MTk4MzY=","avatar_url":"https://avatars.githubusercontent.com/u/1819836?v=4","gravatar_id":"","url":"https://api.github.com/users/knusbaum","html_url":"https://github.com/knusbaum","followers_url":"https://api.github.com/users/knusbaum/followers","following_url":"https://api.github.com/users/knusbaum/following{/other_user}","gists_url":"https://api.github.com/users/knusbaum/gists{/gist_id}","starred_url":"https://api.github.com/users/knusbaum/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/knusbaum/subscriptions","organizations_url":"https://api.github.com/users/knusbaum/orgs","repos_url":"https://api.github.com/users/knusbaum/repos","events_url":"https://api.github.com/users/knusbaum/events{/privacy}","received_events_url":"https://api.github.com/users/knusbaum/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:41:03Z","updated_at":"2015-12-11T04:41:03Z","author_association":"CONTRIBUTOR","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163838439/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612682138","html_url":"https://github.com/apache/storm/issues/5194#issuecomment-2612682138","issue_url":"https://api.github.com/repos/apache/storm/issues/5194","id":2612682138,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODIxMzg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:41:03Z","updated_at":"2025-01-24T14:37:03Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user knusbaum commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/908#issuecomment-163838439\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/908#issuecomment-163838439</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612682138/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163838627","html_url":"https://github.com/apache/storm/pull/909#issuecomment-163838627","issue_url":"https://api.github.com/repos/apache/storm/issues/909","id":163838627,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzgzODYyNw==","user":{"login":"knusbaum","id":1819836,"node_id":"MDQ6VXNlcjE4MTk4MzY=","avatar_url":"https://avatars.githubusercontent.com/u/1819836?v=4","gravatar_id":"","url":"https://api.github.com/users/knusbaum","html_url":"https://github.com/knusbaum","followers_url":"https://api.github.com/users/knusbaum/followers","following_url":"https://api.github.com/users/knusbaum/following{/other_user}","gists_url":"https://api.github.com/users/knusbaum/gists{/gist_id}","starred_url":"https://api.github.com/users/knusbaum/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/knusbaum/subscriptions","organizations_url":"https://api.github.com/users/knusbaum/orgs","repos_url":"https://api.github.com/users/knusbaum/repos","events_url":"https://api.github.com/users/knusbaum/events{/privacy}","received_events_url":"https://api.github.com/users/knusbaum/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:42:59Z","updated_at":"2015-12-11T04:42:59Z","author_association":"CONTRIBUTOR","body":"@rfarivar @darionyaphet #908 is a pull to a different branch.\n\n+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163838627/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612682149","html_url":"https://github.com/apache/storm/issues/5194#issuecomment-2612682149","issue_url":"https://api.github.com/repos/apache/storm/issues/5194","id":2612682149,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODIxNDk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:43:00Z","updated_at":"2025-01-24T14:37:03Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user knusbaum commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/909#issuecomment-163838627\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/909#issuecomment-163838627</a></p>\n\n<p>    @rfarivar @darionyaphet #908 is a pull to a different branch.</p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612682149/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612682156","html_url":"https://github.com/apache/storm/issues/5194#issuecomment-2612682156","issue_url":"https://api.github.com/repos/apache/storm/issues/5194","id":2612682156,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODIxNTY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:51:56Z","updated_at":"2025-01-24T14:37:03Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user asfgit closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/908\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/908</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612682156/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163839685","html_url":"https://github.com/apache/storm/pull/942#issuecomment-163839685","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":163839685,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzgzOTY4NQ==","user":{"login":"arunmahadevan","id":6792890,"node_id":"MDQ6VXNlcjY3OTI4OTA=","avatar_url":"https://avatars.githubusercontent.com/u/6792890?v=4","gravatar_id":"","url":"https://api.github.com/users/arunmahadevan","html_url":"https://github.com/arunmahadevan","followers_url":"https://api.github.com/users/arunmahadevan/followers","following_url":"https://api.github.com/users/arunmahadevan/following{/other_user}","gists_url":"https://api.github.com/users/arunmahadevan/gists{/gist_id}","starred_url":"https://api.github.com/users/arunmahadevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arunmahadevan/subscriptions","organizations_url":"https://api.github.com/users/arunmahadevan/orgs","repos_url":"https://api.github.com/users/arunmahadevan/repos","events_url":"https://api.github.com/users/arunmahadevan/events{/privacy}","received_events_url":"https://api.github.com/users/arunmahadevan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:56:08Z","updated_at":"2015-12-11T04:56:08Z","author_association":"CONTRIBUTOR","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163839685/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688212","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688212","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688212,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyMTI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:56:11Z","updated_at":"2025-01-24T14:39:08Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-163839685\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-163839685</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688212/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612682167","html_url":"https://github.com/apache/storm/issues/5194#issuecomment-2612682167","issue_url":"https://api.github.com/repos/apache/storm/issues/5194","id":2612682167,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODIxNjc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T04:57:53Z","updated_at":"2025-01-24T14:37:03Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user asfgit closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/909\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/909</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612682167/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688318","html_url":"https://github.com/apache/storm/issues/5223#issuecomment-2612688318","issue_url":"https://api.github.com/repos/apache/storm/issues/5223","id":2612688318,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgzMTg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:31:14Z","updated_at":"2025-01-24T14:39:11Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>GitHub user vesense opened a pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/943\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/943</a></p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1388\" title=\"Fix url and email links in README file\" class=\"issue-link\" data-issue-key=\"STORM-1388\"><del>STORM-1388</del></a> Fix url and email links in README file</p>\n\n\n\n<p>You can merge this pull request into a Git repository by running:</p>\n\n<p>    $ git pull <a href=\"https://github.com/vesense/storm\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/vesense/storm</a> patch-11</p>\n\n<p>Alternatively you can review and apply these changes as the patch at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/943.patch\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/943.patch</a></p>\n\n<p>To close this pull request, make a commit to your master/trunk branch<br/>\nwith (at least) the following in the commit message:</p>\n\n<p>    This closes #943</p>\n\n<hr />\n<p>commit 9d2361577e1c44043436cb163b02c57fbd3c8d49<br/>\nAuthor: Xin Wang <best.wangxin@163.com><br/>\nDate:   2015-12-11T06:28:20Z</p>\n\n<p>    fix url and email links in README file</p>\n\n<hr />","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688318/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612633081","html_url":"https://github.com/apache/storm/issues/5049#issuecomment-2612633081","issue_url":"https://api.github.com/repos/apache/storm/issues/5049","id":2612633081,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzMwODE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:41:38Z","updated_at":"2025-01-24T14:15:00Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user asfgit closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/915\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/915</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612633081/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163856921","html_url":"https://github.com/apache/storm/pull/915#issuecomment-163856921","issue_url":"https://api.github.com/repos/apache/storm/issues/915","id":163856921,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzg1NjkyMQ==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:42:20Z","updated_at":"2015-12-11T06:42:20Z","author_association":"CONTRIBUTOR","body":"Thanks @satishd merged into master.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163856921/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612633084","html_url":"https://github.com/apache/storm/issues/5049#issuecomment-2612633084","issue_url":"https://api.github.com/repos/apache/storm/issues/5049","id":2612633084,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzMwODQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:42:23Z","updated_at":"2025-01-24T14:15:00Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/915#issuecomment-163856921\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/915#issuecomment-163856921</a></p>\n\n<p>    Thanks @satishd merged into master.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612633084/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686307","html_url":"https://github.com/apache/storm/issues/5211#issuecomment-2612686307","issue_url":"https://api.github.com/repos/apache/storm/issues/5211","id":2612686307,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODYzMDc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:57:04Z","updated_at":"2025-01-24T14:38:18Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sanket991\">sanket991</a>:</i>\n<p><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=danielschonfeld\" class=\"user-hover\" rel=\"danielschonfeld\">Daniel Schonfeld</a> Can you let me know if you are abserving high cpu usage for log writer processes for your topologies</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686307/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163859359","html_url":"https://github.com/apache/storm/pull/943#issuecomment-163859359","issue_url":"https://api.github.com/repos/apache/storm/issues/943","id":163859359,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzg1OTM1OQ==","user":{"login":"2new","id":11454270,"node_id":"MDQ6VXNlcjExNDU0Mjcw","avatar_url":"https://avatars.githubusercontent.com/u/11454270?v=4","gravatar_id":"","url":"https://api.github.com/users/2new","html_url":"https://github.com/2new","followers_url":"https://api.github.com/users/2new/followers","following_url":"https://api.github.com/users/2new/following{/other_user}","gists_url":"https://api.github.com/users/2new/gists{/gist_id}","starred_url":"https://api.github.com/users/2new/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2new/subscriptions","organizations_url":"https://api.github.com/users/2new/orgs","repos_url":"https://api.github.com/users/2new/repos","events_url":"https://api.github.com/users/2new/events{/privacy}","received_events_url":"https://api.github.com/users/2new/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:57:38Z","updated_at":"2015-12-11T06:57:38Z","author_association":"NONE","body":"+1 LGTM\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163859359/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688323","html_url":"https://github.com/apache/storm/issues/5223#issuecomment-2612688323","issue_url":"https://api.github.com/repos/apache/storm/issues/5223","id":2612688323,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgzMjM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:57:38Z","updated_at":"2025-01-24T14:39:11Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user 2new commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/943#issuecomment-163859359\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/943#issuecomment-163859359</a></p>\n\n<p>    +1 LGTM</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688323/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163859647","html_url":"https://github.com/apache/storm/pull/935#issuecomment-163859647","issue_url":"https://api.github.com/repos/apache/storm/issues/935","id":163859647,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzg1OTY0Nw==","user":{"login":"2new","id":11454270,"node_id":"MDQ6VXNlcjExNDU0Mjcw","avatar_url":"https://avatars.githubusercontent.com/u/11454270?v=4","gravatar_id":"","url":"https://api.github.com/users/2new","html_url":"https://github.com/2new","followers_url":"https://api.github.com/users/2new/followers","following_url":"https://api.github.com/users/2new/following{/other_user}","gists_url":"https://api.github.com/users/2new/gists{/gist_id}","starred_url":"https://api.github.com/users/2new/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/2new/subscriptions","organizations_url":"https://api.github.com/users/2new/orgs","repos_url":"https://api.github.com/users/2new/repos","events_url":"https://api.github.com/users/2new/events{/privacy}","received_events_url":"https://api.github.com/users/2new/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:59:23Z","updated_at":"2015-12-11T06:59:23Z","author_association":"NONE","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163859647/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687212","html_url":"https://github.com/apache/storm/issues/5216#issuecomment-2612687212","issue_url":"https://api.github.com/repos/apache/storm/issues/5216","id":2612687212,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODcyMTI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T06:59:24Z","updated_at":"2025-01-24T14:38:43Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user 2new commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/935#issuecomment-163859647\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/935#issuecomment-163859647</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687212/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163860313","html_url":"https://github.com/apache/storm/pull/942#issuecomment-163860313","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":163860313,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzg2MDMxMw==","user":{"login":"vesense","id":6711230,"node_id":"MDQ6VXNlcjY3MTEyMzA=","avatar_url":"https://avatars.githubusercontent.com/u/6711230?v=4","gravatar_id":"","url":"https://api.github.com/users/vesense","html_url":"https://github.com/vesense","followers_url":"https://api.github.com/users/vesense/followers","following_url":"https://api.github.com/users/vesense/following{/other_user}","gists_url":"https://api.github.com/users/vesense/gists{/gist_id}","starred_url":"https://api.github.com/users/vesense/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vesense/subscriptions","organizations_url":"https://api.github.com/users/vesense/orgs","repos_url":"https://api.github.com/users/vesense/repos","events_url":"https://api.github.com/users/vesense/events{/privacy}","received_events_url":"https://api.github.com/users/vesense/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T07:06:04Z","updated_at":"2015-12-11T07:06:04Z","author_association":"MEMBER","body":"+1 LGTM\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163860313/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688217","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688217","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688217,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyMTc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T07:06:05Z","updated_at":"2025-01-24T14:39:08Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user vesense commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-163860313\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-163860313</a></p>\n\n<p>    +1 LGTM</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688217/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163868358","html_url":"https://github.com/apache/storm/pull/944#issuecomment-163868358","issue_url":"https://api.github.com/repos/apache/storm/issues/944","id":163868358,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzg2ODM1OA==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T07:55:30Z","updated_at":"2015-12-11T07:55:30Z","author_association":"CONTRIBUTOR","body":"@jerrypeng No need to make a PR. It's the way for you to check that you can push to asf git repo directly.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163868358/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163868648","html_url":"https://github.com/apache/storm/pull/944#issuecomment-163868648","issue_url":"https://api.github.com/repos/apache/storm/issues/944","id":163868648,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzg2ODY0OA==","user":{"login":"jerrypeng","id":3613359,"node_id":"MDQ6VXNlcjM2MTMzNTk=","avatar_url":"https://avatars.githubusercontent.com/u/3613359?v=4","gravatar_id":"","url":"https://api.github.com/users/jerrypeng","html_url":"https://github.com/jerrypeng","followers_url":"https://api.github.com/users/jerrypeng/followers","following_url":"https://api.github.com/users/jerrypeng/following{/other_user}","gists_url":"https://api.github.com/users/jerrypeng/gists{/gist_id}","starred_url":"https://api.github.com/users/jerrypeng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jerrypeng/subscriptions","organizations_url":"https://api.github.com/users/jerrypeng/orgs","repos_url":"https://api.github.com/users/jerrypeng/repos","events_url":"https://api.github.com/users/jerrypeng/events{/privacy}","received_events_url":"https://api.github.com/users/jerrypeng/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T07:57:06Z","updated_at":"2015-12-11T07:57:06Z","author_association":"CONTRIBUTOR","body":"Ya I thought I could but I can't so I closed this PR for now\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163868648/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163920211","html_url":"https://github.com/apache/storm/pull/942#issuecomment-163920211","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":163920211,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzkyMDIxMQ==","user":{"login":"satishd","id":2577761,"node_id":"MDQ6VXNlcjI1Nzc3NjE=","avatar_url":"https://avatars.githubusercontent.com/u/2577761?v=4","gravatar_id":"","url":"https://api.github.com/users/satishd","html_url":"https://github.com/satishd","followers_url":"https://api.github.com/users/satishd/followers","following_url":"https://api.github.com/users/satishd/following{/other_user}","gists_url":"https://api.github.com/users/satishd/gists{/gist_id}","starred_url":"https://api.github.com/users/satishd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishd/subscriptions","organizations_url":"https://api.github.com/users/satishd/orgs","repos_url":"https://api.github.com/users/satishd/repos","events_url":"https://api.github.com/users/satishd/events{/privacy}","received_events_url":"https://api.github.com/users/satishd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T11:50:25Z","updated_at":"2015-12-11T11:50:25Z","author_association":"MEMBER","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163920211/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688231","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688231","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688231,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyMzE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T11:50:26Z","updated_at":"2025-01-24T14:39:09Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-163920211\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-163920211</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688231/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163920493","html_url":"https://github.com/apache/storm/pull/937#issuecomment-163920493","issue_url":"https://api.github.com/repos/apache/storm/issues/937","id":163920493,"node_id":"MDEyOklzc3VlQ29tbWVudDE2MzkyMDQ5Mw==","user":{"login":"hustfxj","id":7270212,"node_id":"MDQ6VXNlcjcyNzAyMTI=","avatar_url":"https://avatars.githubusercontent.com/u/7270212?v=4","gravatar_id":"","url":"https://api.github.com/users/hustfxj","html_url":"https://github.com/hustfxj","followers_url":"https://api.github.com/users/hustfxj/followers","following_url":"https://api.github.com/users/hustfxj/following{/other_user}","gists_url":"https://api.github.com/users/hustfxj/gists{/gist_id}","starred_url":"https://api.github.com/users/hustfxj/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hustfxj/subscriptions","organizations_url":"https://api.github.com/users/hustfxj/orgs","repos_url":"https://api.github.com/users/hustfxj/repos","events_url":"https://api.github.com/users/hustfxj/events{/privacy}","received_events_url":"https://api.github.com/users/hustfxj/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T11:52:19Z","updated_at":"2015-12-11T11:52:19Z","author_association":"CONTRIBUTOR","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163920493/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687953","html_url":"https://github.com/apache/storm/issues/5220#issuecomment-2612687953","issue_url":"https://api.github.com/repos/apache/storm/issues/5220","id":2612687953,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODc5NTM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T11:52:20Z","updated_at":"2025-01-24T14:39:01Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user hustfxj commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/937#issuecomment-163920493\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/937#issuecomment-163920493</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687953/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163960627","html_url":"https://github.com/apache/storm/pull/941#issuecomment-163960627","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":163960627,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzk2MDYyNw==","user":{"login":"d2r","id":905298,"node_id":"MDQ6VXNlcjkwNTI5OA==","avatar_url":"https://avatars.githubusercontent.com/u/905298?v=4","gravatar_id":"","url":"https://api.github.com/users/d2r","html_url":"https://github.com/d2r","followers_url":"https://api.github.com/users/d2r/followers","following_url":"https://api.github.com/users/d2r/following{/other_user}","gists_url":"https://api.github.com/users/d2r/gists{/gist_id}","starred_url":"https://api.github.com/users/d2r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/d2r/subscriptions","organizations_url":"https://api.github.com/users/d2r/orgs","repos_url":"https://api.github.com/users/d2r/repos","events_url":"https://api.github.com/users/d2r/events{/privacy}","received_events_url":"https://api.github.com/users/d2r/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T15:15:05Z","updated_at":"2015-12-11T15:15:05Z","author_association":"NONE","body":"jdk7 build failed for backtype.storm.integration-test.  Closing and reopening.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163960627/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686583","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686583","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686583,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY1ODM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T15:15:06Z","updated_at":"2025-01-24T14:38:25Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-163960627\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-163960627</a></p>\n\n<p>    jdk7 build failed for backtype.storm.integration-test.  Closing and reopening.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686583/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686588","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686588","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686588,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY1ODg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T15:15:06Z","updated_at":"2025-01-24T14:38:25Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686588/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686597","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686597","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686597,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY1OTc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T15:15:12Z","updated_at":"2025-01-24T14:38:26Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>GitHub user d2r reopened a pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941</a></p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1377\" title=\"nimbus_auth_test: very short timeouts causing spurious failures\" class=\"issue-link\" data-issue-key=\"STORM-1377\"><del>STORM-1377</del></a> Correct timeouts for milliseconds</p>\n\n<p>    Thrift client timeouts had been given in seconds, when java.net.Socket expects them in milliseconds.</p>\n\n<p>You can merge this pull request into a Git repository by running:</p>\n\n<p>    $ git pull <a href=\"https://github.com/d2r/storm\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/d2r/storm</a> storm-1377-auth-tests-correct-timeouts</p>\n\n<p>Alternatively you can review and apply these changes as the patch at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941.patch\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941.patch</a></p>\n\n<p>To close this pull request, make a commit to your master/trunk branch<br/>\nwith (at least) the following in the commit message:</p>\n\n<p>    This closes #941</p>\n\n<hr />\n<p>commit 402b8d215e2cebb7d7f15931aabaee695520f92d<br/>\nAuthor: Derek Dagit <derekd@yahoo-inc.com><br/>\nDate:   2015-12-10T23:26:12Z</p>\n\n<p>    Correct timeouts to match units (milliseconds)</p>\n\n<hr />","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686597/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163962303","html_url":"https://github.com/apache/storm/pull/921#issuecomment-163962303","issue_url":"https://api.github.com/repos/apache/storm/issues/921","id":163962303,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzk2MjMwMw==","user":{"login":"d2r","id":905298,"node_id":"MDQ6VXNlcjkwNTI5OA==","avatar_url":"https://avatars.githubusercontent.com/u/905298?v=4","gravatar_id":"","url":"https://api.github.com/users/d2r","html_url":"https://github.com/d2r","followers_url":"https://api.github.com/users/d2r/followers","following_url":"https://api.github.com/users/d2r/following{/other_user}","gists_url":"https://api.github.com/users/d2r/gists{/gist_id}","starred_url":"https://api.github.com/users/d2r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/d2r/subscriptions","organizations_url":"https://api.github.com/users/d2r/orgs","repos_url":"https://api.github.com/users/d2r/repos","events_url":"https://api.github.com/users/d2r/events{/privacy}","received_events_url":"https://api.github.com/users/d2r/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T15:22:29Z","updated_at":"2015-12-11T15:22:29Z","author_association":"NONE","body":"> Unrelated Travis failure for core java 8 build\n\n#941 should fix one of the most common\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163962303/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163988918","html_url":"https://github.com/apache/storm/pull/933#issuecomment-163988918","issue_url":"https://api.github.com/repos/apache/storm/issues/933","id":163988918,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzk4ODkxOA==","user":{"login":"redsanket","id":8295799,"node_id":"MDQ6VXNlcjgyOTU3OTk=","avatar_url":"https://avatars.githubusercontent.com/u/8295799?v=4","gravatar_id":"","url":"https://api.github.com/users/redsanket","html_url":"https://github.com/redsanket","followers_url":"https://api.github.com/users/redsanket/followers","following_url":"https://api.github.com/users/redsanket/following{/other_user}","gists_url":"https://api.github.com/users/redsanket/gists{/gist_id}","starred_url":"https://api.github.com/users/redsanket/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/redsanket/subscriptions","organizations_url":"https://api.github.com/users/redsanket/orgs","repos_url":"https://api.github.com/users/redsanket/repos","events_url":"https://api.github.com/users/redsanket/events{/privacy}","received_events_url":"https://api.github.com/users/redsanket/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T16:53:26Z","updated_at":"2015-12-11T16:55:29Z","author_association":"NONE","body":"@revans2 STORM-1215 has done some changes to Async Logging, I removed those commits and tested it again with word count topology and now 0.10.x and 0.11 current version are doing similar. Not particularly sure why it would happen though\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163988918/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686311","html_url":"https://github.com/apache/storm/issues/5211#issuecomment-2612686311","issue_url":"https://api.github.com/repos/apache/storm/issues/5211","id":2612686311,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODYzMTE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T16:53:28Z","updated_at":"2025-01-24T14:38:18Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/933#issuecomment-163988918\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/933#issuecomment-163988918</a></p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1215\" title=\"Use Async Loggers to avoid locking  and logging overhead\" class=\"issue-link\" data-issue-key=\"STORM-1215\"><del>STORM-1215</del></a> has done some changes to Async Logging, I removed those commits and tested it again and now 0.10.x and 0.11 current version are doing similar</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686311/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/163995677","html_url":"https://github.com/apache/storm/pull/822#issuecomment-163995677","issue_url":"https://api.github.com/repos/apache/storm/issues/822","id":163995677,"node_id":"MDEyOklzc3VlQ29tbWVudDE2Mzk5NTY3Nw==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-11T17:20:17Z","updated_at":"2015-12-11T17:20:17Z","author_association":"CONTRIBUTOR","body":"Thanks @zhuoliu @revans2 \n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/163995677/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685675","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685675","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685675,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU2NzU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-12T16:54:17Z","updated_at":"2025-01-24T14:38:04Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47433889\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47433889</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,248 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.LocalCluster;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.IOException;<br/>\n    +import java.io.InputStreamReader;<br/>\n    +import java.util.Arrays;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static NimbusBlobStore store = new NimbusBlobStore(); // Client API to invoke blob store API functionality<br/>\n    +    private static String key = \"key1\";<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreAPIWordCountTopology.class);<br/>\n    +    private static final int READ = 0x01;<br/>\n    +    private static final int WRITE = 0x02;<br/>\n    +    private static final int ADMIN = 0x04;<br/>\n    +    private static final List<AccessControl> WORLD_EVERYTHING =<br/>\n    +    Arrays.asList(new AccessControl(AccessControlType.OTHER, READ | WRITE | ADMIN));<br/>\n    +<br/>\n    +    // Spout implementation<br/>\n    +    public static class RandomSentenceSpout extends BaseRichSpout {<br/>\n    +SpoutOutputCollector _collector;<br/>\n    +BlobStoreAPIWordCountTopology wc;<br/>\n    +String key;<br/>\n    +NimbusBlobStore store;<br/>\n    +<br/>\n    +<br/>\n    +@Override<br/>\n    +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) </p>\n{\n    +    _collector = collector;\n    +    wc = new BlobStoreAPIWordCountTopology();\n    +    key = \"key1\";\n    +    store = new NimbusBlobStore();\n    +    store.prepare(Utils.readStormConfig());\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void nextTuple() {<br/>\n    +    Utils.sleep(100);<br/>\n    +    try </p>\n{\n    + _collector.emit(new Values(wc.getBlobContent(key, store)));\n    +    }\n<p> catch (AuthorizationException | KeyNotFoundException | IOException exp) </p>\n{\n    +throw new RuntimeException(exp);\n    +    }\n<p>    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void ack(Object id) </p>\n{\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void fail(Object id) {    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) {<br/>\n    +    declarer.declare(new Fields(\"word\"));<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    I guess you meant to say the spout part to be @Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"sentence\"));\n    +}\n<p> rather than bolt which splits the sentence. Made the changes thanks for the catch</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685675/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685117","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685117","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685117,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxMTc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-12T19:15:44Z","updated_at":"2025-01-24T14:37:53Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>GitHub user redsanket opened a pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945</a></p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1372\" title=\"Update BlobStore Documentation - Follow up STORM-876\" class=\"issue-link\" data-issue-key=\"STORM-1372\"><del>STORM-1372</del></a> Merging design and usage documents for distcache</p>\n\n\n\n<p>You can merge this pull request into a Git repository by running:</p>\n\n<p>    $ git pull <a href=\"https://github.com/redsanket/storm\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/redsanket/storm</a> distcache-docs</p>\n\n<p>Alternatively you can review and apply these changes as the patch at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945.patch\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945.patch</a></p>\n\n<p>To close this pull request, make a commit to your master/trunk branch<br/>\nwith (at least) the following in the commit message:</p>\n\n<p>    This closes #945</p>\n\n<hr />\n<p>commit ebd60c9a6d1aa0b8ddae9ed44d94df8d3987ac95<br/>\nAuthor: Sanket <schintap@untilservice-lm><br/>\nDate:   2015-12-07T15:43:19Z</p>\n\n<p>    merging distcache a.k.a blobstore design and API usage docs</p>\n\n<p>commit 835caa77f926adbc2c74f6e6ff38a41f7dd1c675<br/>\nAuthor: Sanket <schintap@untilservice-lm><br/>\nDate:   2015-12-12T18:52:59Z</p>\n\n<p>    formatting changes</p>\n\n<hr />","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685117/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685125","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685125","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685125,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxMjU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-13T21:08:54Z","updated_at":"2025-01-24T14:37:53Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47450187\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47450187</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,734 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometime a few<br/>\n    +a day with updated jars, because the large cached files will remain available<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    \"sometime a few a day\"?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685125/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685135","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685135","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685135,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxMzU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-13T21:22:36Z","updated_at":"2025-01-24T14:37:53Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47450346\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47450346</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,734 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometime a few<br/>\n    +a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    barring?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685135/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685144","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685144","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685144,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxNDQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-13T21:25:41Z","updated_at":"2025-01-24T14:37:53Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47450390\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47450390</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,734 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometime a few<br/>\n    +a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.,<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    May remove the last \",\"</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685144/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685158","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685158","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685158,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxNTg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-13T21:26:49Z","updated_at":"2025-01-24T14:37:53Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47450414\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47450414</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,734 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometime a few<br/>\n    +a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.,<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and there by making it a very useful feature.<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    \"there by\" => thereby</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685158/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685168","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685168","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685168,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxNjg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-13T21:30:16Z","updated_at":"2025-01-24T14:37:53Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47450464\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47450464</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,734 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometime a few<br/>\n    +a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.,<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and there by making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs and thereby avoid it becoming <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    may change to \"... store the blobs, which avoids it becoming ...\"</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685168/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685175","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685175","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685175,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxNzU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-13T21:31:38Z","updated_at":"2025-01-24T14:37:54Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47450492\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47450492</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,734 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometime a few<br/>\n    +a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.,<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and there by making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs and thereby avoid it becoming <br/>\n    +a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and there by reduces the load and dependency on nimbus.<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    thereby</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685175/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164316721","html_url":"https://github.com/apache/storm/pull/930#issuecomment-164316721","issue_url":"https://api.github.com/repos/apache/storm/issues/930","id":164316721,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDMxNjcyMQ==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T01:01:43Z","updated_at":"2015-12-14T01:01:43Z","author_association":"CONTRIBUTOR","body":"@revans2 can you take a look at as well.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164316721/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627441","html_url":"https://github.com/apache/storm/issues/5017#issuecomment-2612627441","issue_url":"https://api.github.com/repos/apache/storm/issues/5017","id":2612627441,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjc0NDE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T01:01:45Z","updated_at":"2025-01-24T14:12:32Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/930#issuecomment-164316721\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/930#issuecomment-164316721</a></p>\n\n<p>    @revans2 can you take a look at as well.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627441/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164321832","html_url":"https://github.com/apache/storm/pull/942#issuecomment-164321832","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":164321832,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDMyMTgzMg==","user":{"login":"wuchong","id":5378924,"node_id":"MDQ6VXNlcjUzNzg5MjQ=","avatar_url":"https://avatars.githubusercontent.com/u/5378924?v=4","gravatar_id":"","url":"https://api.github.com/users/wuchong","html_url":"https://github.com/wuchong","followers_url":"https://api.github.com/users/wuchong/followers","following_url":"https://api.github.com/users/wuchong/following{/other_user}","gists_url":"https://api.github.com/users/wuchong/gists{/gist_id}","starred_url":"https://api.github.com/users/wuchong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wuchong/subscriptions","organizations_url":"https://api.github.com/users/wuchong/orgs","repos_url":"https://api.github.com/users/wuchong/repos","events_url":"https://api.github.com/users/wuchong/events{/privacy}","received_events_url":"https://api.github.com/users/wuchong/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T01:58:19Z","updated_at":"2015-12-14T01:58:19Z","author_association":"MEMBER","body":"+1 LGTM\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164321832/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688237","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688237","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688237,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyMzc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T01:58:22Z","updated_at":"2025-01-24T14:39:09Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user wuchong commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-164321832\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-164321832</a></p>\n\n<p>    +1 LGTM</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688237/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164322012","html_url":"https://github.com/apache/storm/pull/942#issuecomment-164322012","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":164322012,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDMyMjAxMg==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T02:00:23Z","updated_at":"2015-12-14T02:00:23Z","author_association":"CONTRIBUTOR","body":"@zhuoliu I see 2 unit tests failures can you take a look. \n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164322012/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688246","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688246","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688246,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyNDY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T02:00:24Z","updated_at":"2025-01-24T14:39:09Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-164322012\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-164322012</a></p>\n\n<p>    @zhuoliu I see 2 unit tests failures can you take a look. </p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688246/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164333988","html_url":"https://github.com/apache/storm/pull/942#issuecomment-164333988","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":164333988,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDMzMzk4OA==","user":{"login":"zhuoliu","id":11683054,"node_id":"MDQ6VXNlcjExNjgzMDU0","avatar_url":"https://avatars.githubusercontent.com/u/11683054?v=4","gravatar_id":"","url":"https://api.github.com/users/zhuoliu","html_url":"https://github.com/zhuoliu","followers_url":"https://api.github.com/users/zhuoliu/followers","following_url":"https://api.github.com/users/zhuoliu/following{/other_user}","gists_url":"https://api.github.com/users/zhuoliu/gists{/gist_id}","starred_url":"https://api.github.com/users/zhuoliu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhuoliu/subscriptions","organizations_url":"https://api.github.com/users/zhuoliu/orgs","repos_url":"https://api.github.com/users/zhuoliu/repos","events_url":"https://api.github.com/users/zhuoliu/events{/privacy}","received_events_url":"https://api.github.com/users/zhuoliu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T03:41:07Z","updated_at":"2015-12-14T03:41:07Z","author_association":"NONE","body":"@harshach sure, thanks. Fixed.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164333988/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688253","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688253","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688253,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyNTM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T03:41:08Z","updated_at":"2025-01-24T14:39:09Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-164333988\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-164333988</a></p>\n\n<p>    @harshach sure, thanks. Fixed.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688253/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164339252","html_url":"https://github.com/apache/storm/pull/929#issuecomment-164339252","issue_url":"https://api.github.com/repos/apache/storm/issues/929","id":164339252,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDMzOTI1Mg==","user":{"login":"satishd","id":2577761,"node_id":"MDQ6VXNlcjI1Nzc3NjE=","avatar_url":"https://avatars.githubusercontent.com/u/2577761?v=4","gravatar_id":"","url":"https://api.github.com/users/satishd","html_url":"https://github.com/satishd","followers_url":"https://api.github.com/users/satishd/followers","following_url":"https://api.github.com/users/satishd/following{/other_user}","gists_url":"https://api.github.com/users/satishd/gists{/gist_id}","starred_url":"https://api.github.com/users/satishd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishd/subscriptions","organizations_url":"https://api.github.com/users/satishd/orgs","repos_url":"https://api.github.com/users/satishd/repos","events_url":"https://api.github.com/users/satishd/events{/privacy}","received_events_url":"https://api.github.com/users/satishd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T04:15:40Z","updated_at":"2015-12-14T04:15:40Z","author_association":"MEMBER","body":"@fhussonnois Can you merge with latest master and address remaining review comments?\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164339252/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612680444","html_url":"https://github.com/apache/storm/issues/5183#issuecomment-2612680444","issue_url":"https://api.github.com/repos/apache/storm/issues/5183","id":2612680444,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODA0NDQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T04:15:41Z","updated_at":"2025-01-24T14:36:22Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/929#issuecomment-164339252\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/929#issuecomment-164339252</a></p>\n\n<p>    @fhussonnois Can you merge with latest master and address remaining review comments?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612680444/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164343943","html_url":"https://github.com/apache/storm/pull/893#issuecomment-164343943","issue_url":"https://api.github.com/repos/apache/storm/issues/893","id":164343943,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDM0Mzk0Mw==","user":{"login":"arunmahadevan","id":6792890,"node_id":"MDQ6VXNlcjY3OTI4OTA=","avatar_url":"https://avatars.githubusercontent.com/u/6792890?v=4","gravatar_id":"","url":"https://api.github.com/users/arunmahadevan","html_url":"https://github.com/arunmahadevan","followers_url":"https://api.github.com/users/arunmahadevan/followers","following_url":"https://api.github.com/users/arunmahadevan/following{/other_user}","gists_url":"https://api.github.com/users/arunmahadevan/gists{/gist_id}","starred_url":"https://api.github.com/users/arunmahadevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arunmahadevan/subscriptions","organizations_url":"https://api.github.com/users/arunmahadevan/orgs","repos_url":"https://api.github.com/users/arunmahadevan/repos","events_url":"https://api.github.com/users/arunmahadevan/events{/privacy}","received_events_url":"https://api.github.com/users/arunmahadevan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T05:12:19Z","updated_at":"2015-12-14T05:12:19Z","author_association":"CONTRIBUTOR","body":"@harshach @dossett any other comments? Can we merge ?\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164343943/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634437","html_url":"https://github.com/apache/storm/issues/5057#issuecomment-2612634437","issue_url":"https://api.github.com/repos/apache/storm/issues/5057","id":2612634437,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzQ0Mzc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T05:12:20Z","updated_at":"2025-01-24T14:15:35Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/893#issuecomment-164343943\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/893#issuecomment-164343943</a></p>\n\n<p>    @harshach @dossett any other comments? Can we merge ?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634437/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612532127","html_url":"https://github.com/apache/storm/issues/4558#issuecomment-2612532127","issue_url":"https://api.github.com/repos/apache/storm/issues/4558","id":2612532127,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI1MzIxMjc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T06:05:44Z","updated_at":"2025-01-24T13:28:14Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kingzone\">kingzone</a>:</i>\n<p>Very nice job, and I'd like to contribute too.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612532127/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688434","html_url":"https://github.com/apache/storm/issues/5224#issuecomment-2612688434","issue_url":"https://api.github.com/repos/apache/storm/issues/5224","id":2612688434,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg0MzQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T06:21:14Z","updated_at":"2025-01-24T14:39:14Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>GitHub user satishd opened a pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/946\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/946</a></p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1389\" title=\"unnecessary generatiion of projection tuples again in StateQueryProcessor#finishBatch\" class=\"issue-link\" data-issue-key=\"STORM-1389\"><del>STORM-1389</del></a> Removed creation of projection tuples as they are already available</p>\n\n\n\n<p>You can merge this pull request into a Git repository by running:</p>\n\n<p>    $ git pull <a href=\"https://github.com/satishd/storm\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/satishd/storm</a> <a href=\"https://issues.apache.org/jira/browse/STORM-1389\" title=\"unnecessary generatiion of projection tuples again in StateQueryProcessor#finishBatch\" class=\"issue-link\" data-issue-key=\"STORM-1389\"><del>STORM-1389</del></a></p>\n\n<p>Alternatively you can review and apply these changes as the patch at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/946.patch\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/946.patch</a></p>\n\n<p>To close this pull request, make a commit to your master/trunk branch<br/>\nwith (at least) the following in the commit message:</p>\n\n<p>    This closes #946</p>\n\n<hr />\n<p>commit f3d1c0888dbebb2b91f144b34443ecc0cd1a6069<br/>\nAuthor: Satish Duggana <sduggana@hortonworks.com><br/>\nDate:   2015-12-14T06:19:42Z</p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1389\" title=\"unnecessary generatiion of projection tuples again in StateQueryProcessor#finishBatch\" class=\"issue-link\" data-issue-key=\"STORM-1389\"><del>STORM-1389</del></a> Removed creation of projection tuples as they are already available.</p>\n\n<hr />","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688434/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685183","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685183","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685183,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxODM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T06:44:42Z","updated_at":"2025-01-24T14:37:54Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47465275\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47465275</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,734 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometime a few<br/>\n    +a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    except synonym</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685183/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164410061","html_url":"https://github.com/apache/storm/pull/929#issuecomment-164410061","issue_url":"https://api.github.com/repos/apache/storm/issues/929","id":164410061,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDQxMDA2MQ==","user":{"login":"fhussonnois","id":1786576,"node_id":"MDQ6VXNlcjE3ODY1NzY=","avatar_url":"https://avatars.githubusercontent.com/u/1786576?v=4","gravatar_id":"","url":"https://api.github.com/users/fhussonnois","html_url":"https://github.com/fhussonnois","followers_url":"https://api.github.com/users/fhussonnois/followers","following_url":"https://api.github.com/users/fhussonnois/following{/other_user}","gists_url":"https://api.github.com/users/fhussonnois/gists{/gist_id}","starred_url":"https://api.github.com/users/fhussonnois/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fhussonnois/subscriptions","organizations_url":"https://api.github.com/users/fhussonnois/orgs","repos_url":"https://api.github.com/users/fhussonnois/repos","events_url":"https://api.github.com/users/fhussonnois/events{/privacy}","received_events_url":"https://api.github.com/users/fhussonnois/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T11:03:42Z","updated_at":"2015-12-14T11:03:42Z","author_association":"CONTRIBUTOR","body":"@satishd done ;)\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164410061/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612680452","html_url":"https://github.com/apache/storm/issues/5183#issuecomment-2612680452","issue_url":"https://api.github.com/repos/apache/storm/issues/5183","id":2612680452,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODA0NTI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T11:03:43Z","updated_at":"2025-01-24T14:36:22Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user fhussonnois commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/929#issuecomment-164410061\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/929#issuecomment-164410061</a></p>\n\n<p>    @satishd done <img class=\"emoticon\" src=\"https://issues.apache.org/jira/images/icons/emoticons/wink.png\" height=\"16\" width=\"16\" align=\"absmiddle\" alt=\"\" border=\"0\"/></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612680452/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164415840","html_url":"https://github.com/apache/storm/pull/929#issuecomment-164415840","issue_url":"https://api.github.com/repos/apache/storm/issues/929","id":164415840,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDQxNTg0MA==","user":{"login":"satishd","id":2577761,"node_id":"MDQ6VXNlcjI1Nzc3NjE=","avatar_url":"https://avatars.githubusercontent.com/u/2577761?v=4","gravatar_id":"","url":"https://api.github.com/users/satishd","html_url":"https://github.com/satishd","followers_url":"https://api.github.com/users/satishd/followers","following_url":"https://api.github.com/users/satishd/following{/other_user}","gists_url":"https://api.github.com/users/satishd/gists{/gist_id}","starred_url":"https://api.github.com/users/satishd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishd/subscriptions","organizations_url":"https://api.github.com/users/satishd/orgs","repos_url":"https://api.github.com/users/satishd/repos","events_url":"https://api.github.com/users/satishd/events{/privacy}","received_events_url":"https://api.github.com/users/satishd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T11:35:36Z","updated_at":"2015-12-14T11:35:36Z","author_association":"MEMBER","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164415840/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612680454","html_url":"https://github.com/apache/storm/issues/5183#issuecomment-2612680454","issue_url":"https://api.github.com/repos/apache/storm/issues/5183","id":2612680454,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODA0NTQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T11:35:38Z","updated_at":"2025-01-24T14:36:22Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/929#issuecomment-164415840\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/929#issuecomment-164415840</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612680454/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164461493","html_url":"https://github.com/apache/storm/pull/947#issuecomment-164461493","issue_url":"https://api.github.com/repos/apache/storm/issues/947","id":164461493,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDQ2MTQ5Mw==","user":{"login":"unsleepy22","id":631361,"node_id":"MDQ6VXNlcjYzMTM2MQ==","avatar_url":"https://avatars.githubusercontent.com/u/631361?v=4","gravatar_id":"","url":"https://api.github.com/users/unsleepy22","html_url":"https://github.com/unsleepy22","followers_url":"https://api.github.com/users/unsleepy22/followers","following_url":"https://api.github.com/users/unsleepy22/following{/other_user}","gists_url":"https://api.github.com/users/unsleepy22/gists{/gist_id}","starred_url":"https://api.github.com/users/unsleepy22/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/unsleepy22/subscriptions","organizations_url":"https://api.github.com/users/unsleepy22/orgs","repos_url":"https://api.github.com/users/unsleepy22/repos","events_url":"https://api.github.com/users/unsleepy22/events{/privacy}","received_events_url":"https://api.github.com/users/unsleepy22/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:09:33Z","updated_at":"2015-12-14T15:09:33Z","author_association":"NONE","body":"@lispking regarding the original code, I think it means to give a warning when it meets multiple eligible constructors (sort of like multiple bindings for slf4j), so I don't think we should break on first match.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164461493/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688589","html_url":"https://github.com/apache/storm/issues/5225#issuecomment-2612688589","issue_url":"https://api.github.com/repos/apache/storm/issues/5225","id":2612688589,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg1ODk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:09:35Z","updated_at":"2025-01-24T14:39:19Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/947#issuecomment-164461493\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/947#issuecomment-164461493</a></p>\n\n<p>    @lispking regarding the original code, I think it means to give a warning when it meets multiple eligible constructors (sort of like multiple bindings for slf4j), so I don't think we should break on first match.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688589/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164462099","html_url":"https://github.com/apache/storm/pull/947#issuecomment-164462099","issue_url":"https://api.github.com/repos/apache/storm/issues/947","id":164462099,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDQ2MjA5OQ==","user":{"login":"lispking","id":4446580,"node_id":"MDQ6VXNlcjQ0NDY1ODA=","avatar_url":"https://avatars.githubusercontent.com/u/4446580?v=4","gravatar_id":"","url":"https://api.github.com/users/lispking","html_url":"https://github.com/lispking","followers_url":"https://api.github.com/users/lispking/followers","following_url":"https://api.github.com/users/lispking/following{/other_user}","gists_url":"https://api.github.com/users/lispking/gists{/gist_id}","starred_url":"https://api.github.com/users/lispking/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lispking/subscriptions","organizations_url":"https://api.github.com/users/lispking/orgs","repos_url":"https://api.github.com/users/lispking/repos","events_url":"https://api.github.com/users/lispking/events{/privacy}","received_events_url":"https://api.github.com/users/lispking/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:12:10Z","updated_at":"2015-12-14T15:28:06Z","author_association":"CONTRIBUTOR","body":"@unsleepy22 i see, but this may make people confused.\nIn addition, there may be such a problem, not the same type as the number of arguments constructor, may be a problem\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164462099/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688596","html_url":"https://github.com/apache/storm/issues/5225#issuecomment-2612688596","issue_url":"https://api.github.com/repos/apache/storm/issues/5225","id":2612688596,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg1OTY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:12:11Z","updated_at":"2025-01-24T14:39:19Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user lispking commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/947#issuecomment-164462099\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/947#issuecomment-164462099</a></p>\n\n<p>    @unsleepy22 i see, but this may make people confused.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688596/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685190","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685190","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685190,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxOTA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:19:25Z","updated_at":"2025-01-24T14:37:54Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47508932\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47508932</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    nimbode?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685190/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164467455","html_url":"https://github.com/apache/storm/pull/942#issuecomment-164467455","issue_url":"https://api.github.com/repos/apache/storm/issues/942","id":164467455,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDQ2NzQ1NQ==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:29:52Z","updated_at":"2015-12-14T15:29:52Z","author_association":"CONTRIBUTOR","body":"Thanks @zhuoliu merged into master.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164467455/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688256","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688256","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688256,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyNTY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:29:54Z","updated_at":"2025-01-24T14:39:09Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942#issuecomment-164467455\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942#issuecomment-164467455</a></p>\n\n<p>    Thanks @zhuoliu merged into master.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688256/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688264","html_url":"https://github.com/apache/storm/issues/5222#issuecomment-2612688264","issue_url":"https://api.github.com/repos/apache/storm/issues/5222","id":2612688264,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgyNjQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:29:54Z","updated_at":"2025-01-24T14:39:09Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user asfgit closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/942\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/942</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688264/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685195","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685195","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685195,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUxOTU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:31:03Z","updated_at":"2025-01-24T14:37:54Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47510625\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47510625</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other  <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    missing something?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685195/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685201","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685201","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685201,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyMDE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:32:55Z","updated_at":"2025-01-24T14:37:54Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47510918\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47510918</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other  <br/>\n    +<br/>\n    +The first implementation will be Zookeeper based. If the zookeeper connection is lost/resetted resulting in loss of lock<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    lost/resetted -> lost/reset</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685201/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685210","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685210","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685210,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyMTA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:36:29Z","updated_at":"2025-01-24T14:37:54Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47511497\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47511497</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other  <br/>\n    +<br/>\n    +The first implementation will be Zookeeper based. If the zookeeper connection is lost/resetted resulting in loss of lock<br/>\n    +or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the <br/>\n    +current status.The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure<br/>\n    +the lock was held by nimbus for the entire duration of the action (Not sure if we want to just state this expectation <br/>\n    +and ensure that zk configurations are set high enough which will result in higher failover time or we actually want to <br/>\n    +create some sort of rollback mechanism for all actions, the second option needs a lot of code). If a nimbus that is not <br/>\n    +leader receives a request that only a leader can perform it will throw a RunTimeException.<br/>\n    +<br/>\n    +### Nimbus state store:<br/>\n    +<br/>\n    +To achieve fail over from primary to backup servers nimbus state/data needs to be replicated across all nimbus hosts or <br/>\n    +needs to be stored in a distributed storage. Replicating the data correctly involves state management, consistency checks<br/>\n    +and it is hard to test for correctness. However many storm users do not want to take extra dependency on another replicated<br/>\n    +storage system like HDFS and still need high availability. The blob store implementation along with the state storage helps<br/>\n    +to overcome the failover scenarios is case a leader nimbus goes down.<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    is case -> in case?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685210/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685219","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685219","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685219,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyMTk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:41:26Z","updated_at":"2025-01-24T14:37:54Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47512213\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47512213</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other  <br/>\n    +<br/>\n    +The first implementation will be Zookeeper based. If the zookeeper connection is lost/resetted resulting in loss of lock<br/>\n    +or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the <br/>\n    +current status.The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure<br/>\n    +the lock was held by nimbus for the entire duration of the action (Not sure if we want to just state this expectation <br/>\n    +and ensure that zk configurations are set high enough which will result in higher failover time or we actually want to <br/>\n    +create some sort of rollback mechanism for all actions, the second option needs a lot of code). If a nimbus that is not <br/>\n    +leader receives a request that only a leader can perform it will throw a RunTimeException.<br/>\n    +<br/>\n    +### Nimbus state store:<br/>\n    +<br/>\n    +To achieve fail over from primary to backup servers nimbus state/data needs to be replicated across all nimbus hosts or <br/>\n    +needs to be stored in a distributed storage. Replicating the data correctly involves state management, consistency checks<br/>\n    +and it is hard to test for correctness. However many storm users do not want to take extra dependency on another replicated<br/>\n    +storage system like HDFS and still need high availability. The blob store implementation along with the state storage helps<br/>\n    +to overcome the failover scenarios is case a leader nimbus goes down.<br/>\n    +<br/>\n    +To support replication we will allow the user to define a code replication factor which would reflect number of nimbus <br/>\n    +hosts to which the code must be replicated before starting the topology. With replication comes the issue of consistency. <br/>\n    +The topology is launched once the code, jar and conf blob files are replicated based on the \"topology.min.replication\" config.<br/>\n    +Maintaining state for failover scenarios is important for local file system. The current implementation makes sure one of the<br/>\n    +available nimbus is elected as a leader in the case of a failure. If the topology specific blobs are missing, the leader nimbus<br/>\n    +tries to download them as and when they are needed. With this current architecture, we do not have to download all the blobs <br/>\n    +required for a topology for a nimbus to accept leadership. This helps us in case the blobs are very large and avoid causing any <br/>\n    +inadvertant delays in electing a leader.<br/>\n    +<br/>\n    +The state for every blob is relevant for the local blob store implementation. For HDFS blob store the replication<br/>\n    +is taken care by the HDFS. For handling the fail over scenarios for a local blob store we need to store the state of the leader and<br/>\n    +non leader nimbodes within the zookeeper.<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    non-leader</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685219/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685224","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685224","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685224,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyMjQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:44:50Z","updated_at":"2025-01-24T14:37:55Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47512705\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47512705</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other  <br/>\n    +<br/>\n    +The first implementation will be Zookeeper based. If the zookeeper connection is lost/resetted resulting in loss of lock<br/>\n    +or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the <br/>\n    +current status.The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure<br/>\n    +the lock was held by nimbus for the entire duration of the action (Not sure if we want to just state this expectation <br/>\n    +and ensure that zk configurations are set high enough which will result in higher failover time or we actually want to <br/>\n    +create some sort of rollback mechanism for all actions, the second option needs a lot of code). If a nimbus that is not <br/>\n    +leader receives a request that only a leader can perform it will throw a RunTimeException.<br/>\n    +<br/>\n    +### Nimbus state store:<br/>\n    +<br/>\n    +To achieve fail over from primary to backup servers nimbus state/data needs to be replicated across all nimbus hosts or <br/>\n    +needs to be stored in a distributed storage. Replicating the data correctly involves state management, consistency checks<br/>\n    +and it is hard to test for correctness. However many storm users do not want to take extra dependency on another replicated<br/>\n    +storage system like HDFS and still need high availability. The blob store implementation along with the state storage helps<br/>\n    +to overcome the failover scenarios is case a leader nimbus goes down.<br/>\n    +<br/>\n    +To support replication we will allow the user to define a code replication factor which would reflect number of nimbus <br/>\n    +hosts to which the code must be replicated before starting the topology. With replication comes the issue of consistency. <br/>\n    +The topology is launched once the code, jar and conf blob files are replicated based on the \"topology.min.replication\" config.<br/>\n    +Maintaining state for failover scenarios is important for local file system. The current implementation makes sure one of the<br/>\n    +available nimbus is elected as a leader in the case of a failure. If the topology specific blobs are missing, the leader nimbus<br/>\n    +tries to download them as and when they are needed. With this current architecture, we do not have to download all the blobs <br/>\n    +required for a topology for a nimbus to accept leadership. This helps us in case the blobs are very large and avoid causing any <br/>\n    +inadvertant delays in electing a leader.<br/>\n    +<br/>\n    +The state for every blob is relevant for the local blob store implementation. For HDFS blob store the replication<br/>\n    +is taken care by the HDFS. For handling the fail over scenarios for a local blob store we need to store the state of the leader and<br/>\n    +non leader nimbodes within the zookeeper.<br/>\n    +<br/>\n    +The state is stored under /storm/blobstore/key/nimbusHostPort:SequenceNumber for the blob store to work to make nimbus highly available. <br/>\n    +This state is used in the local file system blobstore to support replication. The HDFS blobstore does not have to store the state inside the <br/>\n    +zookeeper.<br/>\n    +<br/>\n    +* NimbusHostPort: This piece of information generally contains the parsed string holding the hostname and port of the nimbus. <br/>\n    +  It uses the same class NimbusHostPortInfo used earlier by the code-distributor interface to store the state and parse the data.<br/>\n    +<br/>\n    +* SequenceNumber: This is the blob sequence number information. The SequenceNumber information is implemented by a KeySequenceNumber class. <br/>\n    +The sequence numbers are generated for every key. For every update, the sequence numbers are assigned based ona global sequence number <br/>\n    +stored under /storm/blobstoremaxsequencenumber/key. For more details about how the numbers are generated you can look at the java docs for <br/>\n    +KeySequenceNumber.<br/>\n    +<br/>\n    +!<span class=\"error\">&#91;Nimbus High Availability - BlobStore&#93;</span>(images/nimbus_ha_blobstore.png)<br/>\n    +<br/>\n    +The sequence diagram proposes how the blob store works and the state storage inside the zookeeper makes the nimbus highly available.<br/>\n    +Currently, the thread to sync the blobs on a non-leader is within the nimbus. In the future, it will be nice to move the thread around<br/>\n    +to the blob store to make the blobstore coordinate the state change and blob download as per the sequence diagram.<br/>\n    +<br/>\n    +## Thrift and Rest API <br/>\n    +In order to avoid workers/supervisors/ui talking to zookeeper for getting master nimbus address we are going to modify the <br/>\n    +`getClusterInfo` API so it can also return nimbus information. getClusterInfo currently returns `ClusterSummary` instance<br/>\n    +which has a list of `supervisorSummary` and a list of 'topologySummary` instances. We will add a list of `NimbusSummary` <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    'topologySummary` -> `topologySummary`</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685224/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164474600","html_url":"https://github.com/apache/storm/pull/921#issuecomment-164474600","issue_url":"https://api.github.com/repos/apache/storm/issues/921","id":164474600,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDQ3NDYwMA==","user":{"login":"jerrypeng","id":3613359,"node_id":"MDQ6VXNlcjM2MTMzNTk=","avatar_url":"https://avatars.githubusercontent.com/u/3613359?v=4","gravatar_id":"","url":"https://api.github.com/users/jerrypeng","html_url":"https://github.com/jerrypeng","followers_url":"https://api.github.com/users/jerrypeng/followers","following_url":"https://api.github.com/users/jerrypeng/following{/other_user}","gists_url":"https://api.github.com/users/jerrypeng/gists{/gist_id}","starred_url":"https://api.github.com/users/jerrypeng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jerrypeng/subscriptions","organizations_url":"https://api.github.com/users/jerrypeng/orgs","repos_url":"https://api.github.com/users/jerrypeng/repos","events_url":"https://api.github.com/users/jerrypeng/events{/privacy}","received_events_url":"https://api.github.com/users/jerrypeng/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:56:26Z","updated_at":"2015-12-14T15:56:26Z","author_association":"CONTRIBUTOR","body":"@d2r awesome!   Do you have any other concerns for my PR?\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164474600/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685234","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685234","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685234,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyMzQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:56:41Z","updated_at":"2025-01-24T14:37:55Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47514566\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47514566</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other  <br/>\n    +<br/>\n    +The first implementation will be Zookeeper based. If the zookeeper connection is lost/resetted resulting in loss of lock<br/>\n    +or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the <br/>\n    +current status.The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure<br/>\n    +the lock was held by nimbus for the entire duration of the action (Not sure if we want to just state this expectation <br/>\n    +and ensure that zk configurations are set high enough which will result in higher failover time or we actually want to <br/>\n    +create some sort of rollback mechanism for all actions, the second option needs a lot of code). If a nimbus that is not <br/>\n    +leader receives a request that only a leader can perform it will throw a RunTimeException.<br/>\n    +<br/>\n    +### Nimbus state store:<br/>\n    +<br/>\n    +To achieve fail over from primary to backup servers nimbus state/data needs to be replicated across all nimbus hosts or <br/>\n    +needs to be stored in a distributed storage. Replicating the data correctly involves state management, consistency checks<br/>\n    +and it is hard to test for correctness. However many storm users do not want to take extra dependency on another replicated<br/>\n    +storage system like HDFS and still need high availability. The blob store implementation along with the state storage helps<br/>\n    +to overcome the failover scenarios is case a leader nimbus goes down.<br/>\n    +<br/>\n    +To support replication we will allow the user to define a code replication factor which would reflect number of nimbus <br/>\n    +hosts to which the code must be replicated before starting the topology. With replication comes the issue of consistency. <br/>\n    +The topology is launched once the code, jar and conf blob files are replicated based on the \"topology.min.replication\" config.<br/>\n    +Maintaining state for failover scenarios is important for local file system. The current implementation makes sure one of the<br/>\n    +available nimbus is elected as a leader in the case of a failure. If the topology specific blobs are missing, the leader nimbus<br/>\n    +tries to download them as and when they are needed. With this current architecture, we do not have to download all the blobs <br/>\n    +required for a topology for a nimbus to accept leadership. This helps us in case the blobs are very large and avoid causing any <br/>\n    +inadvertant delays in electing a leader.<br/>\n    +<br/>\n    +The state for every blob is relevant for the local blob store implementation. For HDFS blob store the replication<br/>\n    +is taken care by the HDFS. For handling the fail over scenarios for a local blob store we need to store the state of the leader and<br/>\n    +non leader nimbodes within the zookeeper.<br/>\n    +<br/>\n    +The state is stored under /storm/blobstore/key/nimbusHostPort:SequenceNumber for the blob store to work to make nimbus highly available. <br/>\n    +This state is used in the local file system blobstore to support replication. The HDFS blobstore does not have to store the state inside the <br/>\n    +zookeeper.<br/>\n    +<br/>\n    +* NimbusHostPort: This piece of information generally contains the parsed string holding the hostname and port of the nimbus. <br/>\n    +  It uses the same class NimbusHostPortInfo used earlier by the code-distributor interface to store the state and parse the data.<br/>\n    +<br/>\n    +* SequenceNumber: This is the blob sequence number information. The SequenceNumber information is implemented by a KeySequenceNumber class. <br/>\n    +The sequence numbers are generated for every key. For every update, the sequence numbers are assigned based ona global sequence number <br/>\n    +stored under /storm/blobstoremaxsequencenumber/key. For more details about how the numbers are generated you can look at the java docs for <br/>\n    +KeySequenceNumber.<br/>\n    +<br/>\n    +!<span class=\"error\">&#91;Nimbus High Availability - BlobStore&#93;</span>(images/nimbus_ha_blobstore.png)<br/>\n    +<br/>\n    +The sequence diagram proposes how the blob store works and the state storage inside the zookeeper makes the nimbus highly available.<br/>\n    +Currently, the thread to sync the blobs on a non-leader is within the nimbus. In the future, it will be nice to move the thread around<br/>\n    +to the blob store to make the blobstore coordinate the state change and blob download as per the sequence diagram.<br/>\n    +<br/>\n    +## Thrift and Rest API <br/>\n    +In order to avoid workers/supervisors/ui talking to zookeeper for getting master nimbus address we are going to modify the <br/>\n    +`getClusterInfo` API so it can also return nimbus information. getClusterInfo currently returns `ClusterSummary` instance<br/>\n    +which has a list of `supervisorSummary` and a list of 'topologySummary` instances. We will add a list of `NimbusSummary` <br/>\n    +to the `ClusterSummary`. See the structures below:<br/>\n    +<br/>\n    +```thrift<br/>\n    +struct ClusterSummary </p>\n{\n    +  1: required list<SupervisorSummary> supervisors;\n    +  3: required list<TopologySummary> topologies;\n    +  4: required list<NimbusSummary> nimbuses;\n    +}\n<p>    +<br/>\n    +struct NimbusSummary </p>\n{\n    +  1: required string host;\n    +  2: required i32 port;\n    +  3: required i32 uptime_secs;\n    +  4: required bool isLeader;\n    +  5: required string version;\n    +}\n<p>    +```<br/>\n    +<br/>\n    +This will be used by StormSubmitter, Nimbus clients, supervisors and ui to discover the current leaders and participating <br/>\n    +nimbus hosts. Any nimbus host will be able to respond to these requests. The nimbus hosts can read this information once <br/>\n    +from zookeeper and cache it and keep updating the cache when the watchers are fired to indicate any changes,which should <br/>\n    +be rare in general case.<br/>\n    +<br/>\n    +Note: All nimbus hosts have watchers on zookeeper to be notified immediately as soon as a new blobs is available for download, the callback may or may not download<br/>\n    +the code. Therefore, a background thread is triggered to download the respective blobs to run the topologies. The replication is achieved when the blobs are downloaded<br/>\n    +onto non-leader nimbodes. So you should expect your topology submission time to be somewhere between 0 to (2 * nimbus.code.sync.freq.secs) for any <br/>\n    +nimbus.min.replication.count > 1.<br/>\n    +<br/>\n    +## Configuration<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +blobstore.dir: The directory where all blobs are stored. For local file system it represents the directory on the nimbus <br/>\n    +node and for HDFS file system it represents the hdfs file system path.<br/>\n    +<br/>\n    +supervisor.blobstore.class: This configuration is meant to set the client for  the supervisor  in order to talk to the blob store. <br/>\n    +For a local file system blob store it is set to backtype.storm.blobstore.NimbusBlobStore and for the HDFS blob store it is set <br/>\n    +to backtype.storm.blobstore.HdfsClientBlobStore.<br/>\n    +<br/>\n    +supervisor.blobstore.download.thread.count: This configuration spawns multiple threads for from the supervisor in order download <br/>\n    +blobs concurrently. The default is set to 5<br/>\n    +<br/>\n    +supervisor.blobstore.download.max_retries: This configuration is set to allow the supervisor to retry for the blob download. <br/>\n    +By default it is set to 3.<br/>\n    +<br/>\n    +supervisor.localizer.cache.target.size.mb: The jvm opts provided to workers launched by this supervisor. All \"%ID%\" substrings <br/>\n    +are replaced with an identifier for this worker. Also, \"%WORKER-ID%\", \"%STORM-ID%\" and \"%WORKER-PORT%\" are replaced with <br/>\n    +appropriate runtime values for this worker. The distributed cache target size in MB. This is a soft limit to the size <br/>\n    +of the distributed cache contents. It is set to 10240 MB.<br/>\n    +<br/>\n    +supervisor.localizer.cleanup.interval.ms: The distributed cache cleanup interval. Controls how often it scans to attempt to <br/>\n    +cleanup anything over the cache target size. By default it is set to 600000 milliseconds.<br/>\n    +<br/>\n    +nimbus.blobstore.class:  Sets the blobstore implementation nimbus uses. It is set to \"backtype.storm.blobstore.LocalFsBlobStore\"<br/>\n    +<br/>\n    +nimbus.blobstore.expiration.secs: During operations with the blob store, via master, how long a connection is idle before nimbus <br/>\n    +considers it dead and drops the session and any associated connections. The default is set to 600.<br/>\n    +<br/>\n    +storm.blobstore.inputstream.buffer.size.bytes: The buffer size it uses for blob store upload. It is set to 65536 bytes.<br/>\n    +<br/>\n    +client.blobstore.class: The blob store implementation the storm client uses. The current implementation uses the default <br/>\n    +config \"backtype.storm.blobstore.NimbusBlobStore\".<br/>\n    +<br/>\n    +blobstore.replication.factor: It sets the replication for each blob within the blob store. The topology.min.replication.count <br/>\n    +ensures the minimum replication the topology specific blobs are set before launching the topology. You might want to set the <br/>\n    +topology.min.replication.count <= blobstore.replication. The default is set to 3.<br/>\n    +<br/>\n    +topology.min.replication.count : Minimum number of nimbus hosts where the code must be replicated before leader nimbus<br/>\n    +can mark the topology as active and create assignments. Default is 1.<br/>\n    +<br/>\n    +topology.max.replication.wait.time.sec: Maximum wait time for the nimbus host replication to achieve the nimbus.min.replication.count.<br/>\n    +Once this time is elapsed nimbus will go ahead and perform topology activation tasks even if required nimbus.min.replication.count is not achieved. <br/>\n    +The default is 60 seconds, a value of -1 indicates to wait for ever.<br/>\n    +* nimbus.code.sync.freq.secs: Frequency at which the background thread on nimbus which syncs code for locally missing blobs. Default is 2 minutes.<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +<br/>\n    +## Using the Distributed Cache API, Command Line Interface (CLI)<br/>\n    +<br/>\n    +### Creating blobs <br/>\n    +<br/>\n    +To use the distributed cache feature, the user first has to \"introduce\" files<br/>\n    +that need to be cached and bind them to key strings. To achieve this, the user<br/>\n    +uses the \"blobstore create\" command of the storm executable, as follows:<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore create <span class=\"error\">&#91;-f|--file FILE&#93;</span> <span class=\"error\">&#91;-a|--acl ACL1,ACL2,...&#93;</span> <span class=\"error\">&#91;--repl-fctr NUMBER&#93;</span> <span class=\"error\">&#91;keyname&#93;</span><br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +The contents come from a FILE, if provided by -f or --file option, otherwise<br/>\n    +from STDIN.  <br/>\n    +The ACLs, which can also be a comma separated list of many ACLs, is of the<br/>\n    +following format:<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +> <span class=\"error\">&#91;u|o&#93;</span>:<span class=\"error\">&#91;username&#93;</span>:<span class=\"error\">&#91;r-|w-|a-|_&#93;</span><br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +where:  <br/>\n    +<br/>\n    +* u = user  <br/>\n    +* o = other  <br/>\n    +* username = user for this particular ACL  <br/>\n    +* r = read access  <br/>\n    +* w = write access  <br/>\n    +* a = admin access  <br/>\n    +* _ = ignored  <br/>\n    +<br/>\n    +The replication factor can be set to a value greater than 1 using --repl-fctr.<br/>\n    +<br/>\n    +Note: The replication right now is configurable for a hdfs blobstore but for a<br/>\n    +local blobstore the replication always stays at 1. For a hdfs blobstore<br/>\n    +the default replication is set to 3.<br/>\n    +<br/>\n    +###### Example:  <br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +In the above example, the <b>README.txt</b> file is added to the distributed cache.<br/>\n    +It can be accessed using the key string \"<b>key1</b>\" for any topology that needs<br/>\n    +it. The file is set to have read/write/admin access for others, a.k.a world<br/>\n    +everything and the replication is set to 4.<br/>\n    +<br/>\n    +###### Example:  <br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore create mytopo:data.tgz -f data.tgz -a u:alice:rwa,u:bob:rw,o::r  <br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +The above example createss a mytopo:data.tgz key using the data stored in<br/>\n    +data.tgz.  User alice would have full access, bob would have read/write access<br/>\n    +and everyone else would have read access.<br/>\n    +<br/>\n    +### Making dist. cache files accessible to topologies<br/>\n    +<br/>\n    +Once a blob is created, we can use it for topologies. This is generally achieved<br/>\n    +by including the key string among the configurations of a topology, with the<br/>\n    +following format. A shortcut is to add the configuration item on the command<br/>\n    +line when starting a topology by using the *<b>-c</b>* command:<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +-c topology.blobstore.map='{\"<span class=\"error\">&#91;KEY&#93;</span>\":{\"localname\":\"<span class=\"error\">&#91;VALUE&#93;</span>\", \"uncompress\":\"<span class=\"error\">&#91;true|false&#93;</span>\"}}'<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +Note: Please take care of the quotes.<br/>\n    +<br/>\n    +The cache file would then be accessible to the topology as a local file with the<br/>\n    +name <span class=\"error\">&#91;VALUE&#93;</span>.  <br/>\n    +The localname parameter is optional, if omitted the local cached file will have<br/>\n    +the same name as <span class=\"error\">&#91;KEY&#93;</span>.  <br/>\n    +The uncompress parameter is optional, if omitted the local cached file will not<br/>\n    +be uncompressed.  Note that the key string needs to have the appropriate<br/>\n    +file-name-like format and extension, so it can be uncompressed correctly.<br/>\n    +<br/>\n    +###### Example:  <br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +Note: Please take care of the quotes.<br/>\n    +<br/>\n    +In the above example, we start the <b>word_count</b> topology (stored in the<br/>\n    +<b>storm-starter-jar-with-dependencies.jar</b> file), and ask it to have access<br/>\n    +to the cached file stored with key string = <b>key1</b>. This file would then be<br/>\n    +accessible to the topology as a local file called <b>blob_file</b>, and the<br/>\n    +supervisor will not try to uncompress the file. Note that in our example, the<br/>\n    +file's content originally came from <b>README.txt</b>. We also ask for the file<br/>\n    +stored with the key string = <b>key2</b> to be accessible to the topology. Since<br/>\n    +both the optional parameters are omitted, this file will get the local name =<br/>\n    +<b>key2</b>, and will not be uncompressed.<br/>\n    +<br/>\n    +### Updating a cached file<br/>\n    +<br/>\n    +It is possible for the cached files to be updated while topologies are running.<br/>\n    +The update happens in an eventual consistency model, where the supervisors poll<br/>\n    +Nimbus every 30 seconds, and update their local copies. In the current version,<br/>\n    +it is the user's responsibility to check whether a new file is available.<br/>\n    +<br/>\n    +To update a cached file, use the following command. Contents come from a FILE or<br/>\n    +STDIN. Write access is required to be able to update a cached file.<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore update <span class=\"error\">&#91;-f|--file NEW_FILE&#93;</span> <span class=\"error\">&#91;KEYSTRING&#93;</span><br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +###### Example:  <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    why ###### ...</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685234/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685243","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685243","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685243,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyNDM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T15:58:09Z","updated_at":"2025-01-24T14:37:55Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47514765\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47514765</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other  <br/>\n    +<br/>\n    +The first implementation will be Zookeeper based. If the zookeeper connection is lost/resetted resulting in loss of lock<br/>\n    +or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the <br/>\n    +current status.The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure<br/>\n    +the lock was held by nimbus for the entire duration of the action (Not sure if we want to just state this expectation <br/>\n    +and ensure that zk configurations are set high enough which will result in higher failover time or we actually want to <br/>\n    +create some sort of rollback mechanism for all actions, the second option needs a lot of code). If a nimbus that is not <br/>\n    +leader receives a request that only a leader can perform it will throw a RunTimeException.<br/>\n    +<br/>\n    +### Nimbus state store:<br/>\n    +<br/>\n    +To achieve fail over from primary to backup servers nimbus state/data needs to be replicated across all nimbus hosts or <br/>\n    +needs to be stored in a distributed storage. Replicating the data correctly involves state management, consistency checks<br/>\n    +and it is hard to test for correctness. However many storm users do not want to take extra dependency on another replicated<br/>\n    +storage system like HDFS and still need high availability. The blob store implementation along with the state storage helps<br/>\n    +to overcome the failover scenarios is case a leader nimbus goes down.<br/>\n    +<br/>\n    +To support replication we will allow the user to define a code replication factor which would reflect number of nimbus <br/>\n    +hosts to which the code must be replicated before starting the topology. With replication comes the issue of consistency. <br/>\n    +The topology is launched once the code, jar and conf blob files are replicated based on the \"topology.min.replication\" config.<br/>\n    +Maintaining state for failover scenarios is important for local file system. The current implementation makes sure one of the<br/>\n    +available nimbus is elected as a leader in the case of a failure. If the topology specific blobs are missing, the leader nimbus<br/>\n    +tries to download them as and when they are needed. With this current architecture, we do not have to download all the blobs <br/>\n    +required for a topology for a nimbus to accept leadership. This helps us in case the blobs are very large and avoid causing any <br/>\n    +inadvertant delays in electing a leader.<br/>\n    +<br/>\n    +The state for every blob is relevant for the local blob store implementation. For HDFS blob store the replication<br/>\n    +is taken care by the HDFS. For handling the fail over scenarios for a local blob store we need to store the state of the leader and<br/>\n    +non leader nimbodes within the zookeeper.<br/>\n    +<br/>\n    +The state is stored under /storm/blobstore/key/nimbusHostPort:SequenceNumber for the blob store to work to make nimbus highly available. <br/>\n    +This state is used in the local file system blobstore to support replication. The HDFS blobstore does not have to store the state inside the <br/>\n    +zookeeper.<br/>\n    +<br/>\n    +* NimbusHostPort: This piece of information generally contains the parsed string holding the hostname and port of the nimbus. <br/>\n    +  It uses the same class NimbusHostPortInfo used earlier by the code-distributor interface to store the state and parse the data.<br/>\n    +<br/>\n    +* SequenceNumber: This is the blob sequence number information. The SequenceNumber information is implemented by a KeySequenceNumber class. <br/>\n    +The sequence numbers are generated for every key. For every update, the sequence numbers are assigned based ona global sequence number <br/>\n    +stored under /storm/blobstoremaxsequencenumber/key. For more details about how the numbers are generated you can look at the java docs for <br/>\n    +KeySequenceNumber.<br/>\n    +<br/>\n    +!<span class=\"error\">&#91;Nimbus High Availability - BlobStore&#93;</span>(images/nimbus_ha_blobstore.png)<br/>\n    +<br/>\n    +The sequence diagram proposes how the blob store works and the state storage inside the zookeeper makes the nimbus highly available.<br/>\n    +Currently, the thread to sync the blobs on a non-leader is within the nimbus. In the future, it will be nice to move the thread around<br/>\n    +to the blob store to make the blobstore coordinate the state change and blob download as per the sequence diagram.<br/>\n    +<br/>\n    +## Thrift and Rest API <br/>\n    +In order to avoid workers/supervisors/ui talking to zookeeper for getting master nimbus address we are going to modify the <br/>\n    +`getClusterInfo` API so it can also return nimbus information. getClusterInfo currently returns `ClusterSummary` instance<br/>\n    +which has a list of `supervisorSummary` and a list of 'topologySummary` instances. We will add a list of `NimbusSummary` <br/>\n    +to the `ClusterSummary`. See the structures below:<br/>\n    +<br/>\n    +```thrift<br/>\n    +struct ClusterSummary </p>\n{\n    +  1: required list<SupervisorSummary> supervisors;\n    +  3: required list<TopologySummary> topologies;\n    +  4: required list<NimbusSummary> nimbuses;\n    +}\n<p>    +<br/>\n    +struct NimbusSummary </p>\n{\n    +  1: required string host;\n    +  2: required i32 port;\n    +  3: required i32 uptime_secs;\n    +  4: required bool isLeader;\n    +  5: required string version;\n    +}\n<p>    +```<br/>\n    +<br/>\n    +This will be used by StormSubmitter, Nimbus clients, supervisors and ui to discover the current leaders and participating <br/>\n    +nimbus hosts. Any nimbus host will be able to respond to these requests. The nimbus hosts can read this information once <br/>\n    +from zookeeper and cache it and keep updating the cache when the watchers are fired to indicate any changes,which should <br/>\n    +be rare in general case.<br/>\n    +<br/>\n    +Note: All nimbus hosts have watchers on zookeeper to be notified immediately as soon as a new blobs is available for download, the callback may or may not download<br/>\n    +the code. Therefore, a background thread is triggered to download the respective blobs to run the topologies. The replication is achieved when the blobs are downloaded<br/>\n    +onto non-leader nimbodes. So you should expect your topology submission time to be somewhere between 0 to (2 * nimbus.code.sync.freq.secs) for any <br/>\n    +nimbus.min.replication.count > 1.<br/>\n    +<br/>\n    +## Configuration<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +blobstore.dir: The directory where all blobs are stored. For local file system it represents the directory on the nimbus <br/>\n    +node and for HDFS file system it represents the hdfs file system path.<br/>\n    +<br/>\n    +supervisor.blobstore.class: This configuration is meant to set the client for  the supervisor  in order to talk to the blob store. <br/>\n    +For a local file system blob store it is set to backtype.storm.blobstore.NimbusBlobStore and for the HDFS blob store it is set <br/>\n    +to backtype.storm.blobstore.HdfsClientBlobStore.<br/>\n    +<br/>\n    +supervisor.blobstore.download.thread.count: This configuration spawns multiple threads for from the supervisor in order download <br/>\n    +blobs concurrently. The default is set to 5<br/>\n    +<br/>\n    +supervisor.blobstore.download.max_retries: This configuration is set to allow the supervisor to retry for the blob download. <br/>\n    +By default it is set to 3.<br/>\n    +<br/>\n    +supervisor.localizer.cache.target.size.mb: The jvm opts provided to workers launched by this supervisor. All \"%ID%\" substrings <br/>\n    +are replaced with an identifier for this worker. Also, \"%WORKER-ID%\", \"%STORM-ID%\" and \"%WORKER-PORT%\" are replaced with <br/>\n    +appropriate runtime values for this worker. The distributed cache target size in MB. This is a soft limit to the size <br/>\n    +of the distributed cache contents. It is set to 10240 MB.<br/>\n    +<br/>\n    +supervisor.localizer.cleanup.interval.ms: The distributed cache cleanup interval. Controls how often it scans to attempt to <br/>\n    +cleanup anything over the cache target size. By default it is set to 600000 milliseconds.<br/>\n    +<br/>\n    +nimbus.blobstore.class:  Sets the blobstore implementation nimbus uses. It is set to \"backtype.storm.blobstore.LocalFsBlobStore\"<br/>\n    +<br/>\n    +nimbus.blobstore.expiration.secs: During operations with the blob store, via master, how long a connection is idle before nimbus <br/>\n    +considers it dead and drops the session and any associated connections. The default is set to 600.<br/>\n    +<br/>\n    +storm.blobstore.inputstream.buffer.size.bytes: The buffer size it uses for blob store upload. It is set to 65536 bytes.<br/>\n    +<br/>\n    +client.blobstore.class: The blob store implementation the storm client uses. The current implementation uses the default <br/>\n    +config \"backtype.storm.blobstore.NimbusBlobStore\".<br/>\n    +<br/>\n    +blobstore.replication.factor: It sets the replication for each blob within the blob store. The topology.min.replication.count <br/>\n    +ensures the minimum replication the topology specific blobs are set before launching the topology. You might want to set the <br/>\n    +topology.min.replication.count <= blobstore.replication. The default is set to 3.<br/>\n    +<br/>\n    +topology.min.replication.count : Minimum number of nimbus hosts where the code must be replicated before leader nimbus<br/>\n    +can mark the topology as active and create assignments. Default is 1.<br/>\n    +<br/>\n    +topology.max.replication.wait.time.sec: Maximum wait time for the nimbus host replication to achieve the nimbus.min.replication.count.<br/>\n    +Once this time is elapsed nimbus will go ahead and perform topology activation tasks even if required nimbus.min.replication.count is not achieved. <br/>\n    +The default is 60 seconds, a value of -1 indicates to wait for ever.<br/>\n    +* nimbus.code.sync.freq.secs: Frequency at which the background thread on nimbus which syncs code for locally missing blobs. Default is 2 minutes.<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +<br/>\n    +## Using the Distributed Cache API, Command Line Interface (CLI)<br/>\n    +<br/>\n    +### Creating blobs <br/>\n    +<br/>\n    +To use the distributed cache feature, the user first has to \"introduce\" files<br/>\n    +that need to be cached and bind them to key strings. To achieve this, the user<br/>\n    +uses the \"blobstore create\" command of the storm executable, as follows:<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore create <span class=\"error\">&#91;-f|--file FILE&#93;</span> <span class=\"error\">&#91;-a|--acl ACL1,ACL2,...&#93;</span> <span class=\"error\">&#91;--repl-fctr NUMBER&#93;</span> <span class=\"error\">&#91;keyname&#93;</span><br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +The contents come from a FILE, if provided by -f or --file option, otherwise<br/>\n    +from STDIN.  <br/>\n    +The ACLs, which can also be a comma separated list of many ACLs, is of the<br/>\n    +following format:<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +> <span class=\"error\">&#91;u|o&#93;</span>:<span class=\"error\">&#91;username&#93;</span>:<span class=\"error\">&#91;r-|w-|a-|_&#93;</span><br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +where:  <br/>\n    +<br/>\n    +* u = user  <br/>\n    +* o = other  <br/>\n    +* username = user for this particular ACL  <br/>\n    +* r = read access  <br/>\n    +* w = write access  <br/>\n    +* a = admin access  <br/>\n    +* _ = ignored  <br/>\n    +<br/>\n    +The replication factor can be set to a value greater than 1 using --repl-fctr.<br/>\n    +<br/>\n    +Note: The replication right now is configurable for a hdfs blobstore but for a<br/>\n    +local blobstore the replication always stays at 1. For a hdfs blobstore<br/>\n    +the default replication is set to 3.<br/>\n    +<br/>\n    +###### Example:  <br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +In the above example, the <b>README.txt</b> file is added to the distributed cache.<br/>\n    +It can be accessed using the key string \"<b>key1</b>\" for any topology that needs<br/>\n    +it. The file is set to have read/write/admin access for others, a.k.a world<br/>\n    +everything and the replication is set to 4.<br/>\n    +<br/>\n    +###### Example:  <br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore create mytopo:data.tgz -f data.tgz -a u:alice:rwa,u:bob:rw,o::r  <br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +The above example createss a mytopo:data.tgz key using the data stored in<br/>\n    +data.tgz.  User alice would have full access, bob would have read/write access<br/>\n    +and everyone else would have read access.<br/>\n    +<br/>\n    +### Making dist. cache files accessible to topologies<br/>\n    +<br/>\n    +Once a blob is created, we can use it for topologies. This is generally achieved<br/>\n    +by including the key string among the configurations of a topology, with the<br/>\n    +following format. A shortcut is to add the configuration item on the command<br/>\n    +line when starting a topology by using the *<b>-c</b>* command:<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +-c topology.blobstore.map='{\"<span class=\"error\">&#91;KEY&#93;</span>\":{\"localname\":\"<span class=\"error\">&#91;VALUE&#93;</span>\", \"uncompress\":\"<span class=\"error\">&#91;true|false&#93;</span>\"}}'<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +Note: Please take care of the quotes.<br/>\n    +<br/>\n    +The cache file would then be accessible to the topology as a local file with the<br/>\n    +name <span class=\"error\">&#91;VALUE&#93;</span>.  <br/>\n    +The localname parameter is optional, if omitted the local cached file will have<br/>\n    +the same name as <span class=\"error\">&#91;KEY&#93;</span>.  <br/>\n    +The uncompress parameter is optional, if omitted the local cached file will not<br/>\n    +be uncompressed.  Note that the key string needs to have the appropriate<br/>\n    +file-name-like format and extension, so it can be uncompressed correctly.<br/>\n    +<br/>\n    +###### Example:  <br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +Note: Please take care of the quotes.<br/>\n    +<br/>\n    +In the above example, we start the <b>word_count</b> topology (stored in the<br/>\n    +<b>storm-starter-jar-with-dependencies.jar</b> file), and ask it to have access<br/>\n    +to the cached file stored with key string = <b>key1</b>. This file would then be<br/>\n    +accessible to the topology as a local file called <b>blob_file</b>, and the<br/>\n    +supervisor will not try to uncompress the file. Note that in our example, the<br/>\n    +file's content originally came from <b>README.txt</b>. We also ask for the file<br/>\n    +stored with the key string = <b>key2</b> to be accessible to the topology. Since<br/>\n    +both the optional parameters are omitted, this file will get the local name =<br/>\n    +<b>key2</b>, and will not be uncompressed.<br/>\n    +<br/>\n    +### Updating a cached file<br/>\n    +<br/>\n    +It is possible for the cached files to be updated while topologies are running.<br/>\n    +The update happens in an eventual consistency model, where the supervisors poll<br/>\n    +Nimbus every 30 seconds, and update their local copies. In the current version,<br/>\n    +it is the user's responsibility to check whether a new file is available.<br/>\n    +<br/>\n    +To update a cached file, use the following command. Contents come from a FILE or<br/>\n    +STDIN. Write access is required to be able to update a cached file.<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore update <span class=\"error\">&#91;-f|--file NEW_FILE&#93;</span> <span class=\"error\">&#91;KEYSTRING&#93;</span><br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +###### Example:  <br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore update -f updates.txt key1<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +In the above example, the topologies will be presented with the contents of the<br/>\n    +file <b>updates.txt</b> instead of <b>README.txt</b> (from the previous example), even<br/>\n    +though their access by the topology is still through a file called<br/>\n    +<b>blob_file</b>.<br/>\n    +<br/>\n    +### Removing a cached file<br/>\n    +<br/>\n    +To remove a file from the distributed cache, use the following command. Removing<br/>\n    +a file requires write access.<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore delete <span class=\"error\">&#91;KEYSTRING&#93;</span><br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +### Listing Blobs currently in the distributed cache blob store<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore list <span class=\"error\">&#91;KEY...&#93;</span><br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +lists blobs currently in the blob store<br/>\n    +<br/>\n    +### Reading the contents of a blob<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore cat <span class=\"error\">&#91;-f|--file FILE&#93;</span> KEY<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +read a blob and then either write it to a file, or STDOUT. Reading a blob<br/>\n    +requires read access.<br/>\n    +<br/>\n    +### Setting the access control for a blob<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +set-acl <span class=\"error\">&#91;-s ACL&#93;</span> KEY<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +ACL is in the form <span class=\"error\">&#91;uo&#93;</span>:<span class=\"error\">&#91;username&#93;</span>:<span class=\"error\">&#91;r-&#93;</span><span class=\"error\">&#91;w-&#93;</span><span class=\"error\">&#91;a-&#93;</span> can be comma  separated list<br/>\n    +(requires admin access).<br/>\n    +<br/>\n    +### Update the replication factor for a blob<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore replication --update --repl-fctr 5 key1<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +### Read the replication factor of a blob<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm blobstore replication --read key1<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +### Command line help<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +storm help blobstore<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +<br/>\n    +## Using the Distributed Cache API from Java<br/>\n    +<br/>\n    +We start by getting a ClientBlobStore object by calling this function:<br/>\n    +<br/>\n    +~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~<br/>\n    +Config theconf = new Config();<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    could you use ```  ``` to quote java code? same for all below.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685243/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685252","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685252","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685252,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyNTI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T16:51:31Z","updated_at":"2025-01-24T14:37:55Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47522518\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47522518</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    just used a fancy term for plural for nimbus instead of nimbuses</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685252/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685258","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685258","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685258,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyNTg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T16:56:44Z","updated_at":"2025-01-24T14:37:55Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47523298\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47523298</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other  <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    thanks for catching it, I do not how the sentence got deleted</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685258/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164517610","html_url":"https://github.com/apache/storm/pull/893#issuecomment-164517610","issue_url":"https://api.github.com/repos/apache/storm/issues/893","id":164517610,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDUxNzYxMA==","user":{"login":"dossett","id":7207582,"node_id":"MDQ6VXNlcjcyMDc1ODI=","avatar_url":"https://avatars.githubusercontent.com/u/7207582?v=4","gravatar_id":"","url":"https://api.github.com/users/dossett","html_url":"https://github.com/dossett","followers_url":"https://api.github.com/users/dossett/followers","following_url":"https://api.github.com/users/dossett/following{/other_user}","gists_url":"https://api.github.com/users/dossett/gists{/gist_id}","starred_url":"https://api.github.com/users/dossett/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dossett/subscriptions","organizations_url":"https://api.github.com/users/dossett/orgs","repos_url":"https://api.github.com/users/dossett/repos","events_url":"https://api.github.com/users/dossett/events{/privacy}","received_events_url":"https://api.github.com/users/dossett/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T18:25:50Z","updated_at":"2015-12-14T18:25:50Z","author_association":"CONTRIBUTOR","body":"It's still not clear to me why the original approach doesn't work, but that's probably a problem with my understanding of storm internals.\n\n+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164517610/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634442","html_url":"https://github.com/apache/storm/issues/5057#issuecomment-2612634442","issue_url":"https://api.github.com/repos/apache/storm/issues/5057","id":2612634442,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzQ0NDI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T18:25:51Z","updated_at":"2025-01-24T14:15:36Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user dossett commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/893#issuecomment-164517610\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/893#issuecomment-164517610</a></p>\n\n<p>    It's still not clear to me why the original approach doesn't work, but that's probably a problem with my understanding of storm internals.</p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634442/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685272","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685272","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685272,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyNzI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:34:17Z","updated_at":"2025-01-24T14:37:55Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user rfarivar commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47545780\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47545780</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,733 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    I thought nimbii is probably clearer...</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685272/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686319","html_url":"https://github.com/apache/storm/issues/5211#issuecomment-2612686319","issue_url":"https://api.github.com/repos/apache/storm/issues/5211","id":2612686319,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODYzMTk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:36:22Z","updated_at":"2025-01-24T14:38:19Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user revans2 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/933#discussion_r47546102\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/933#discussion_r47546102</a></p>\n\n<p>    &#8212; Diff: conf/defaults.yaml &#8212;<br/>\n    @@ -179,7 +179,7 @@ task.refresh.poll.secs: 10<br/>\n     task.credentials.poll.secs: 30</p>\n\n<ol>\n\t<li>now should be null by default<br/>\n    -topology.backpressure.enable: true<br/>\n    +topology.backpressure.enable: false\n\t<ul class=\"alternate\" type=\"square\">\n\t\t<li>\n\t\t<ul class=\"alternate\" type=\"square\">\n\t\t\t<li>End diff &#8211;</li>\n\t\t</ul>\n\t\t</li>\n\t</ul>\n\t</li>\n</ol>\n\n\n<p>    Do we want this off by default?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686319/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685275","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685275","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685275,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyNzU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:36:42Z","updated_at":"2025-01-24T14:37:55Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user rfarivar commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47546157\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47546157</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,732 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    All across this document you either use \"blob store\" or \"blobstore\". Please choose one version and change everything else. </p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685275/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685283","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685283","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685283,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyODM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:39:12Z","updated_at":"2025-01-24T14:37:55Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user rfarivar commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47546495\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47546495</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,732 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    This sentence sounds strange.... nimbus restarted by supervisor?!</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685283/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686327","html_url":"https://github.com/apache/storm/issues/5211#issuecomment-2612686327","issue_url":"https://api.github.com/repos/apache/storm/issues/5211","id":2612686327,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODYzMjc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:39:46Z","updated_at":"2025-01-24T14:38:19Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user ppoulosk commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/933#discussion_r47546588\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/933#discussion_r47546588</a></p>\n\n<p>    &#8212; Diff: storm-core/test/clj/backtype/storm/nimbus_test.clj &#8212;<br/>\n    @@ -1238,10 +1238,11 @@<br/>\n       (testing \"nimbus-data uses correct ACLs\"<br/>\n (let [scheme \"digest\"<br/>\n       digest \"storm:thisisapoorpassword\"</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>auth-conf \n<div class=\"error\"><span class=\"error\">Unknown macro: {STORM-ZOOKEEPER-AUTH-SCHEME scheme    +  auth-conf (merge (read-storm-config)    +    {STORM-ZOOKEEPER-AUTH-SCHEME scheme\n  STORM-ZOOKEEPER-AUTH-PAYLOAD digest\n  STORM-PRINCIPAL-TO-LOCAL-PLUGIN \"backtype.storm.security.auth.DefaultPrincipalToLocal\"\n    -     NIMBUS-THRIFT-PORT 6666}    +     NIMBUS-THRIFT-PORT 6666}</span> </div>\n<p>)</p>\n\t<ul class=\"alternate\" type=\"square\">\n\t\t<li>\n\t\t<ul class=\"alternate\" type=\"square\">\n\t\t\t<li>End diff &#8211;</li>\n\t\t</ul>\n\t\t</li>\n\t</ul>\n\t</li>\n</ul>\n\n\n<p>    I know that the code was already there, but is it possible to use an ephemeral port here?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686327/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164538102","html_url":"https://github.com/apache/storm/pull/933#issuecomment-164538102","issue_url":"https://api.github.com/repos/apache/storm/issues/933","id":164538102,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDUzODEwMg==","user":{"login":"revans2","id":3441321,"node_id":"MDQ6VXNlcjM0NDEzMjE=","avatar_url":"https://avatars.githubusercontent.com/u/3441321?v=4","gravatar_id":"","url":"https://api.github.com/users/revans2","html_url":"https://github.com/revans2","followers_url":"https://api.github.com/users/revans2/followers","following_url":"https://api.github.com/users/revans2/following{/other_user}","gists_url":"https://api.github.com/users/revans2/gists{/gist_id}","starred_url":"https://api.github.com/users/revans2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/revans2/subscriptions","organizations_url":"https://api.github.com/users/revans2/orgs","repos_url":"https://api.github.com/users/revans2/repos","events_url":"https://api.github.com/users/revans2/events{/privacy}","received_events_url":"https://api.github.com/users/revans2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:41:36Z","updated_at":"2015-12-14T19:41:36Z","author_association":"CONTRIBUTOR","body":"Overall the patch looks good, just curious why we turned off backpressure by default.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164538102/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686332","html_url":"https://github.com/apache/storm/issues/5211#issuecomment-2612686332","issue_url":"https://api.github.com/repos/apache/storm/issues/5211","id":2612686332,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODYzMzI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:41:37Z","updated_at":"2025-01-24T14:38:19Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user revans2 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/933#issuecomment-164538102\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/933#issuecomment-164538102</a></p>\n\n<p>    Overall the patch looks good, just curious why we turned off backpressure by default.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686332/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685680","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685680","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685680,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU2ODA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:46:09Z","updated_at":"2025-01-24T14:38:04Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user ppoulosk commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47547449\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47547449</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,246 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.LocalCluster;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.blobstore.BlobStoreAclHandler;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.IOException;<br/>\n    +import java.io.InputStreamReader;<br/>\n    +import java.util.Arrays;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static NimbusBlobStore store = new NimbusBlobStore(); // Client API to invoke blob store API functionality<br/>\n    +    private static String key = \"key1\";<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreAPIWordCountTopology.class);<br/>\n    +    private static final List<AccessControl> WORLD_EVERYTHING = Arrays.asList(new AccessControl(AccessControlType.OTHER,<br/>\n    +    BlobStoreAclHandler.READ | BlobStoreAclHandler.WRITE | BlobStoreAclHandler.ADMIN));<br/>\n    +<br/>\n    +    // Spout implementation<br/>\n    +    public static class BlobStoreSpout extends BaseRichSpout {<br/>\n    +SpoutOutputCollector _collector;<br/>\n    +BlobStoreAPIWordCountTopology wc;<br/>\n    +String key;<br/>\n    +NimbusBlobStore store;<br/>\n    +<br/>\n    +<br/>\n    +@Override<br/>\n    +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) </p>\n{\n    +    _collector = collector;\n    +    wc = new BlobStoreAPIWordCountTopology();\n    +    key = \"key1\";\n    +    store = new NimbusBlobStore();\n    +    store.prepare(Utils.readStormConfig());\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void nextTuple() {<br/>\n    +    Utils.sleep(100);<br/>\n    +    try </p>\n{\n    + _collector.emit(new Values(wc.getBlobContent(key, store)));\n    +    }\n<p> catch (AuthorizationException | KeyNotFoundException | IOException exp) </p>\n{\n    +throw new RuntimeException(exp);\n    +    }\n<p>    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void ack(Object id) </p>\n{\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void fail(Object id) {    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"sentence\"));\n    +}\n<p>    +<br/>\n    +    }<br/>\n    +<br/>\n    +    // Bolt implementation<br/>\n    +    public static class SplitSentence extends ShellBolt implements IRichBolt {<br/>\n    +<br/>\n    +public SplitSentence() </p>\n{\n    +    super(\"python\", \"splitsentence.py\");\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"word\"));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public Map<String, Object> getComponentConfiguration() </p>\n{\n    +    return null;\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    public static class WordCount extends BaseBasicBolt {<br/>\n    +Map<String, Integer> counts = new HashMap<String, Integer>();<br/>\n    +<br/>\n    +@Override<br/>\n    +public void execute(Tuple tuple, BasicOutputCollector collector) </p>\n{\n    +    String word = tuple.getString(0);\n    +    Integer count = counts.get(word);\n    +    if (count == null)\n    +count = 0;\n    +    count++;\n    +    counts.put(word, count);\n    +    collector.emit(new Values(word, count));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"word\", \"count\"));\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    public void buildAndLaunchWordCountTopology(String[] args) {<br/>\n    +<br/>\n    +TopologyBuilder builder = new TopologyBuilder();<br/>\n    +<br/>\n    +builder.setSpout(\"spout\", new BlobStoreSpout(), 5);<br/>\n    +<br/>\n    +builder.setBolt(\"split\", new SplitSentence(), 8).shuffleGrouping(\"spout\");<br/>\n    +builder.setBolt(\"count\", new WordCount(), 12).fieldsGrouping(\"split\", new Fields(\"word\"));<br/>\n    +<br/>\n    +Config conf = new Config();<br/>\n    +conf.setDebug(true);<br/>\n    +<br/>\n    +try {<br/>\n    +    if (args != null && args.length > 0) </p>\n{\n    +conf.setNumWorkers(3);\n    +StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());\n    +    }\n<p> else </p>\n{\n    +conf.setMaxTaskParallelism(3);\n    +\n    +LocalCluster cluster = new LocalCluster();\n    +cluster.submitTopology(\"word-count\", conf, builder.createTopology());\n    +\n    +Thread.sleep(10000);\n    +\n    +cluster.shutdown();\n    +    }\n<p>    +} catch (InvalidTopologyException | AuthorizationException | AlreadyAliveException | InterruptedException exp) </p>\n{\n    +    throw new RuntimeException(exp);\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    private static void createBlobWithContent(String blobKey, ClientBlobStore clientBlobStore, SettableBlobMeta settableBlobMeta)<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Should these helper routines be in some Util class?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685680/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164539376","html_url":"https://github.com/apache/storm/pull/934#issuecomment-164539376","issue_url":"https://api.github.com/repos/apache/storm/issues/934","id":164539376,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDUzOTM3Ng==","user":{"login":"ppoulosk","id":4550393,"node_id":"MDQ6VXNlcjQ1NTAzOTM=","avatar_url":"https://avatars.githubusercontent.com/u/4550393?v=4","gravatar_id":"","url":"https://api.github.com/users/ppoulosk","html_url":"https://github.com/ppoulosk","followers_url":"https://api.github.com/users/ppoulosk/followers","following_url":"https://api.github.com/users/ppoulosk/following{/other_user}","gists_url":"https://api.github.com/users/ppoulosk/gists{/gist_id}","starred_url":"https://api.github.com/users/ppoulosk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ppoulosk/subscriptions","organizations_url":"https://api.github.com/users/ppoulosk/orgs","repos_url":"https://api.github.com/users/ppoulosk/repos","events_url":"https://api.github.com/users/ppoulosk/events{/privacy}","received_events_url":"https://api.github.com/users/ppoulosk/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:46:41Z","updated_at":"2015-12-14T19:46:41Z","author_association":"CONTRIBUTOR","body":"+1 NB\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164539376/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685684","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685684","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685684,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU2ODQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:46:42Z","updated_at":"2025-01-24T14:38:04Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user ppoulosk commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#issuecomment-164539376\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#issuecomment-164539376</a></p>\n\n<p>    +1 NB</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685684/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685290","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685290","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685290,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUyOTA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:46:45Z","updated_at":"2025-01-24T14:37:56Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user rfarivar commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47547551\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47547551</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,732 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the supervisor. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other nimbus which is up and running.<br/>\n    +<br/>\n    +The first implementation will be Zookeeper based. If the zookeeper connection is lost/reset resulting in loss of lock<br/>\n    +or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the <br/>\n    +current status.The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    status. The</p>\n\n<p>    Missing space</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685290/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164540466","html_url":"https://github.com/apache/storm/pull/945#issuecomment-164540466","issue_url":"https://api.github.com/repos/apache/storm/issues/945","id":164540466,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDU0MDQ2Ng==","user":{"login":"rfarivar","id":8742608,"node_id":"MDQ6VXNlcjg3NDI2MDg=","avatar_url":"https://avatars.githubusercontent.com/u/8742608?v=4","gravatar_id":"","url":"https://api.github.com/users/rfarivar","html_url":"https://github.com/rfarivar","followers_url":"https://api.github.com/users/rfarivar/followers","following_url":"https://api.github.com/users/rfarivar/following{/other_user}","gists_url":"https://api.github.com/users/rfarivar/gists{/gist_id}","starred_url":"https://api.github.com/users/rfarivar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rfarivar/subscriptions","organizations_url":"https://api.github.com/users/rfarivar/orgs","repos_url":"https://api.github.com/users/rfarivar/repos","events_url":"https://api.github.com/users/rfarivar/events{/privacy}","received_events_url":"https://api.github.com/users/rfarivar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-14T19:50:48Z","updated_at":"2015-12-14T19:50:48Z","author_association":"CONTRIBUTOR","body":"Minor nit picks, +1 after the changes.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164540466/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]