[{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733405329","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733405329","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733405329,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzQwNTMyOQ==","user":{"login":"liukun4515","id":7450163,"node_id":"MDQ6VXNlcjc0NTAxNjM=","avatar_url":"https://avatars.githubusercontent.com/u/7450163?v=4","gravatar_id":"","url":"https://api.github.com/users/liukun4515","html_url":"https://github.com/liukun4515","followers_url":"https://api.github.com/users/liukun4515/followers","following_url":"https://api.github.com/users/liukun4515/following{/other_user}","gists_url":"https://api.github.com/users/liukun4515/gists{/gist_id}","starred_url":"https://api.github.com/users/liukun4515/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/liukun4515/subscriptions","organizations_url":"https://api.github.com/users/liukun4515/orgs","repos_url":"https://api.github.com/users/liukun4515/repos","events_url":"https://api.github.com/users/liukun4515/events{/privacy}","received_events_url":"https://api.github.com/users/liukun4515/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T01:31:19Z","updated_at":"2020-11-25T01:32:29Z","author_association":"NONE","body":"> Also, per design doc, `older_than` is optional and should be defaulted to `System.currentTimeMillis()` if not set.\r\n> \r\n> Then we can keep only a given number of snapshots:\r\n> \r\n> ```\r\n> CALL catalog.schema.expire_snapshots(\r\n>   namespace => 'namespace_name',\r\n>   table => 'table_name',\r\n>   retain_last => 100\r\n> )\r\n> ```\r\n\r\nThe the default `older_than`(`System.currentTime`) is a very dangerous.\r\n\r\nIf don't give the `older_than` value, all the snapshots will be dropped.\r\n\r\n@chenjunjiedada ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733405329/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733415844","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733415844","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733415844,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzQxNTg0NA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T02:08:07Z","updated_at":"2020-11-25T02:08:07Z","author_association":"CONTRIBUTOR","body":"> If don't give the older_than value, all the snapshots will be dropped.\r\n\r\nThe current snapshot is never dropped, but you're right that it is easy to drop lots of data doing this.\r\n\r\nWe've thought about introducing table properties to default these. Seems like a good time to add them. I'm thinking some time interval for keeping snapshots (so expireOlderThan would default to `System.currentTimeMillis() - interval`) and some default number of snapshots to keep.\r\n\r\n@aokolnychyi, do you agree?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733415844/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733425291","html_url":"https://github.com/apache/iceberg/pull/1790#issuecomment-733425291","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1790","id":733425291,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzQyNTI5MQ==","user":{"login":"yyanyy","id":71906210,"node_id":"MDQ6VXNlcjcxOTA2MjEw","avatar_url":"https://avatars.githubusercontent.com/u/71906210?v=4","gravatar_id":"","url":"https://api.github.com/users/yyanyy","html_url":"https://github.com/yyanyy","followers_url":"https://api.github.com/users/yyanyy/followers","following_url":"https://api.github.com/users/yyanyy/following{/other_user}","gists_url":"https://api.github.com/users/yyanyy/gists{/gist_id}","starred_url":"https://api.github.com/users/yyanyy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yyanyy/subscriptions","organizations_url":"https://api.github.com/users/yyanyy/orgs","repos_url":"https://api.github.com/users/yyanyy/repos","events_url":"https://api.github.com/users/yyanyy/events{/privacy}","received_events_url":"https://api.github.com/users/yyanyy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T02:37:53Z","updated_at":"2020-11-25T02:37:53Z","author_association":"CONTRIBUTOR","body":"> This touches a lot of files. Can we separate the refactoring out into a separate PR and focus on just ORC implementations here?\r\n\r\nSure, I'll send out a PR that only contains refactoring, and once that's merged I'll update this to depend on that one, unless you want it the other way around? ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733425291/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733429122","html_url":"https://github.com/apache/iceberg/pull/1815#issuecomment-733429122","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1815","id":733429122,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzQyOTEyMg==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T02:51:25Z","updated_at":"2020-11-25T02:51:25Z","author_association":"CONTRIBUTOR","body":"hi, @rdblue  thanks very much for your suggestion, I have modified all which you mention ï¼Œ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733429122/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733432037","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733432037","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733432037,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzQzMjAzNw==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T03:01:24Z","updated_at":"2020-11-25T03:01:24Z","author_association":"CONTRIBUTOR","body":"The design doc assumed that while both `retain_last` and `older_than` are optional, at least one of them is required to prevent the situation described above. That being said, we can probably support `NOW()` literals later to just keep a given number of snapshots. So maybe we don't have to make `older_than` optional.\r\n\r\n@rdblue, I am +1 for adding table props to control the default behavior. I was saving this until we have an idea how a generic VACUUM command is going to look like but we can probably introduce properties for retention right now.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733432037/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733432153","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733432153","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733432153,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzQzMjE1Mw==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T03:01:52Z","updated_at":"2020-11-25T03:01:52Z","author_association":"CONTRIBUTOR","body":"I think we can add table props in a follow-up PR.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733432153/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733438097","html_url":"https://github.com/apache/iceberg/issues/1610#issuecomment-733438097","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1610","id":733438097,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzQzODA5Nw==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T03:21:58Z","updated_at":"2020-11-25T03:21:58Z","author_association":"CONTRIBUTOR","body":"done ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733438097/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733547523","html_url":"https://github.com/apache/iceberg/pull/1757#issuecomment-733547523","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1757","id":733547523,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzU0NzUyMw==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T08:25:00Z","updated_at":"2020-11-25T08:25:00Z","author_association":"CONTRIBUTOR","body":"> @pvary, what's the status here? I took a quick look at the changes and I see some timestamp changes mixed in?\r\n\r\nThis patch is changing the big test class used by most of the Hive/Tez/Mr tests (HiveIcebergStorageHandlerBaseTest), so this messes up with most of the Hive patches. I based this patch on the current version of `CREATE TABLE` PR  (#1612) at that time, because I did not want to mess-up with the review there. Of course this patch is outdated since then, and I will have to rebase once #1612 gets in.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733547523/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733569256","html_url":"https://github.com/apache/iceberg/pull/1774#issuecomment-733569256","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1774","id":733569256,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzU2OTI1Ng==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T09:06:16Z","updated_at":"2020-11-25T12:58:41Z","author_association":"MEMBER","body":"I removed the `PartitionedFanoutWriter`  in #1818 because: \r\n1.  I found it's easy and more simpler to understand after unifying the unpartitioned & partitioned fanout writer in a single [RowDataTaskWriter](https://github.com/apache/iceberg/pull/1818/files#diff-137cbe4278e90eab7d4d545be87f5daf929e48a012f1c791ca1e7fc7d7fe5eddR41). \r\n2.  The flink need to parse the `RowKind` to decide whether the row should be dispatched to `write` method or `delete` method,  the previous abstraction is not suitable for the requirement, So I created an unified task writer for flink. \r\n\r\nFor spark fanout task writer,  I think it's reasonable for the spark streaming scenarios because in that case we don't necessary to shuffle the records based on partition keys.   Moving the `PartitionedFanoutWriter` from `flink` module to the `core`  module looks good to me. \r\n\r\n@XuQianJin-Stars  Mind to update this PR to address the CI issue ? \r\n\r\nThanks.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733569256/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733623563","html_url":"https://github.com/apache/iceberg/pull/1815#issuecomment-733623563","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1815","id":733623563,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzYyMzU2Mw==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T10:38:15Z","updated_at":"2020-11-25T10:38:15Z","author_association":"CONTRIBUTOR","body":"@openinx ,thanks very much for your suggestion ,I updated all","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733623563/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733739690","html_url":"https://github.com/apache/iceberg/issues/1780#issuecomment-733739690","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1780","id":733739690,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzczOTY5MA==","user":{"login":"kukuzidian","id":3753735,"node_id":"MDQ6VXNlcjM3NTM3MzU=","avatar_url":"https://avatars.githubusercontent.com/u/3753735?v=4","gravatar_id":"","url":"https://api.github.com/users/kukuzidian","html_url":"https://github.com/kukuzidian","followers_url":"https://api.github.com/users/kukuzidian/followers","following_url":"https://api.github.com/users/kukuzidian/following{/other_user}","gists_url":"https://api.github.com/users/kukuzidian/gists{/gist_id}","starred_url":"https://api.github.com/users/kukuzidian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kukuzidian/subscriptions","organizations_url":"https://api.github.com/users/kukuzidian/orgs","repos_url":"https://api.github.com/users/kukuzidian/repos","events_url":"https://api.github.com/users/kukuzidian/events{/privacy}","received_events_url":"https://api.github.com/users/kukuzidian/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T14:27:54Z","updated_at":"2020-11-25T14:37:38Z","author_association":"NONE","body":"1.  in iceberg 0.10.0 branch  IcebergTimestampObjectInspector.java  line 75 :\r\n public Object copyObject(Object o) {\r\n    if (o == null) {\r\n      return null;\r\n    }\r\n\r\n    Timestamp ts = (Timestamp) o;\r\n    Timestamp copy = new Timestamp(ts.getTime());\r\n    copy.setNanos(ts.getNanos());\r\n    return copy;\r\n  }\r\n\r\n2. but in iceberg master  branch  IcebergTimestampObjectInspector.java  line 75 :\r\npublic Object copyObject(Object o) {\r\n    if (o == null) {\r\n      return null;\r\n    }\r\n\r\n    if (o instanceof Timestamp) {\r\n      Timestamp ts = (Timestamp) o;\r\n      Timestamp copy = new Timestamp(ts.getTime());\r\n      copy.setNanos(ts.getNanos());\r\n      return copy;\r\n    } else if (o instanceof OffsetDateTime) {\r\n      return OffsetDateTime.of(((OffsetDateTime) o).toLocalDateTime(), ((OffsetDateTime) o).getOffset());\r\n    } else if (o instanceof LocalDateTime) {\r\n      return LocalDateTime.of(((LocalDateTime) o).toLocalDate(), ((LocalDateTime) o).toLocalTime());\r\n    } else {\r\n      return o;\r\n    }\r\n  }\r\n\r\n\r\nso when i get this exception , I find code in master brach  is different from 0.10.0 branch. Then i use new code in master branch can read parquest file ok . so I guess this case already finshed in master . \r\n\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733739690/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733744768","html_url":"https://github.com/apache/iceberg/issues/1780#issuecomment-733744768","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1780","id":733744768,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzc0NDc2OA==","user":{"login":"kukuzidian","id":3753735,"node_id":"MDQ6VXNlcjM3NTM3MzU=","avatar_url":"https://avatars.githubusercontent.com/u/3753735?v=4","gravatar_id":"","url":"https://api.github.com/users/kukuzidian","html_url":"https://github.com/kukuzidian","followers_url":"https://api.github.com/users/kukuzidian/followers","following_url":"https://api.github.com/users/kukuzidian/following{/other_user}","gists_url":"https://api.github.com/users/kukuzidian/gists{/gist_id}","starred_url":"https://api.github.com/users/kukuzidian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kukuzidian/subscriptions","organizations_url":"https://api.github.com/users/kukuzidian/orgs","repos_url":"https://api.github.com/users/kukuzidian/repos","events_url":"https://api.github.com/users/kukuzidian/events{/privacy}","received_events_url":"https://api.github.com/users/kukuzidian/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T14:36:27Z","updated_at":"2020-11-25T14:36:27Z","author_association":"NONE","body":"because in my company the network can not connect to the internet . so I can't  copy the full exception message.  and  this issues can't  upload pictures .  I find it difficult to new a issues.  @zhangdove ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733744768/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733751984","html_url":"https://github.com/apache/iceberg/issues/1780#issuecomment-733751984","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1780","id":733751984,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzc1MTk4NA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T14:48:51Z","updated_at":"2020-11-25T14:48:51Z","author_association":"CONTRIBUTOR","body":"@kukuzidian: I think #1740 created by @lcspinter fixes the issue. It did not make it into 0.10.0 ðŸ˜¢, so you have to build iceberg yourself until we have another release. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733751984/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733818295","html_url":"https://github.com/apache/iceberg/pull/1824#issuecomment-733818295","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1824","id":733818295,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzgxODI5NQ==","user":{"login":"shardulm94","id":6961317,"node_id":"MDQ6VXNlcjY5NjEzMTc=","avatar_url":"https://avatars.githubusercontent.com/u/6961317?v=4","gravatar_id":"","url":"https://api.github.com/users/shardulm94","html_url":"https://github.com/shardulm94","followers_url":"https://api.github.com/users/shardulm94/followers","following_url":"https://api.github.com/users/shardulm94/following{/other_user}","gists_url":"https://api.github.com/users/shardulm94/gists{/gist_id}","starred_url":"https://api.github.com/users/shardulm94/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shardulm94/subscriptions","organizations_url":"https://api.github.com/users/shardulm94/orgs","repos_url":"https://api.github.com/users/shardulm94/repos","events_url":"https://api.github.com/users/shardulm94/events{/privacy}","received_events_url":"https://api.github.com/users/shardulm94/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T16:38:43Z","updated_at":"2020-11-25T16:38:43Z","author_association":"CONTRIBUTOR","body":"@pvary There was a failure in https://github.com/apache/iceberg/pull/1827/checks?check_run_id=1454358891, not sure if it's the same one you are trying to repro.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733818295/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733853956","html_url":"https://github.com/apache/iceberg/pull/1821#issuecomment-733853956","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1821","id":733853956,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzg1Mzk1Ng==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T17:42:59Z","updated_at":"2020-11-25T17:42:59Z","author_association":"CONTRIBUTOR","body":"Thanks @shardulm94! Looks great.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733853956/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733859164","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733859164","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733859164,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzg1OTE2NA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T17:53:13Z","updated_at":"2020-11-25T17:53:13Z","author_association":"CONTRIBUTOR","body":"If we want to add table config in a follow-up, then I think we should go ahead and use a reasonable default here. If `older_than` is not set, then let's just default it to `System.currentTimeMillis() - (3 days in millis)`. I think that's a good default and we can fix the default interval with a table property after this is done.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733859164/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733874190","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733874190","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733874190,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzg3NDE5MA==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T18:20:14Z","updated_at":"2020-11-25T18:20:14Z","author_association":"MEMBER","body":"Something that this brings up for me, is how does a user discover the\ninformation about these defaults? Particularly from a full SQL context, I\nwonder if it makes sense to create a \"Help\" command which prints out this\ninfo or returns it as a dataframe. Something like\n\n```help expireSnapshots```\n\nOn Wed, Nov 25, 2020 at 11:53 AM Ryan Blue <notifications@github.com> wrote:\n\n> If we want to add table config in a follow-up, then I think we should go\n> ahead and use a reasonable default here. If older_than is not set, then\n> let's just default it to System.currentTimeMillis() - (3 days in millis).\n> I think that's a good default and we can fix the default interval with a\n> table property after this is done.\n>\n> â€”\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/apache/iceberg/pull/1819#issuecomment-733859164>, or\n> unsubscribe\n> <https://github.com/notifications/unsubscribe-auth/AADE2YPIB7YP2VRRYOAEP2LSRVABRANCNFSM4UBAJ5BQ>\n> .\n>\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733874190/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733891480","html_url":"https://github.com/apache/iceberg/pull/1815#issuecomment-733891480","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1815","id":733891480,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzg5MTQ4MA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T18:53:29Z","updated_at":"2020-11-25T18:53:29Z","author_association":"CONTRIBUTOR","body":"Looks okay to me overall.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733891480/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733892432","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733892432","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733892432,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzg5MjQzMg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T18:55:30Z","updated_at":"2020-11-25T18:55:30Z","author_association":"CONTRIBUTOR","body":"> Something that this brings up for me, is how does a user discover the information about these defaults?\r\n\r\nI think that we should probably add `docs()` to the stored procedure interface and add a command to show them, like `HELP system.expire_snapshots`. Probably not `DESCRIBE` since that is used for tables and views.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733892432/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733892986","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733892986","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733892986,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzg5Mjk4Ng==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T18:56:36Z","updated_at":"2020-11-25T18:56:36Z","author_association":"CONTRIBUTOR","body":"If we are going to add defaults, I'd say let's go ahead and define table properties now.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733892986/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733895223","html_url":"https://github.com/apache/iceberg/pull/1824#issuecomment-733895223","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1824","id":733895223,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzg5NTIyMw==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T19:01:17Z","updated_at":"2020-11-25T19:01:17Z","author_association":"CONTRIBUTOR","body":"> @pvary There was a failure in https://github.com/apache/iceberg/pull/1827/checks?check_run_id=1454358891, not sure if it's the same one you are trying to repro.\r\n\r\nYes, this is the one","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733895223/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733905249","html_url":"https://github.com/apache/iceberg/pull/1823#issuecomment-733905249","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1823","id":733905249,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzkwNTI0OQ==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T19:24:06Z","updated_at":"2020-11-25T19:24:06Z","author_association":"CONTRIBUTOR","body":"Thanks a lot @jackye1995 for the awesome work! Just left a few suggestions to help w/ integration work.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733905249/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733906396","html_url":"https://github.com/apache/iceberg/pull/1783#issuecomment-733906396","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1783","id":733906396,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzkwNjM5Ng==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T19:26:54Z","updated_at":"2020-11-25T19:26:54Z","author_association":"CONTRIBUTOR","body":"Thanks all for the input. \r\n\r\nSo I think there is enough info that I can clean up this PR to use `SupportsCatalogOptions` and the process described above to extract a table/catalog from the source.\r\n\r\n* If there is a catalog in the identifier -> use that catalog\r\n* if no catalog in the identifier -> use session/default catalog (favouring Spark3 convention over IcebergSource in 2.4 convention)\r\n* if catalog in the identifier isn't an iceberg catalog -> throw\r\n* if path then handle with `iceberg.path` option as per above. Seems like should be done as a separate patch?\r\n\r\nDoes that make sense? Am I missing anything?\r\n\r\nPS Any thoughts on custom catalogs in Spark2.4? I don't know what the migration plan looks like but I feel like Spark2.4 will be a thing for some time.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733906396/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733917263","html_url":"https://github.com/apache/iceberg/issues/1617#issuecomment-733917263","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1617","id":733917263,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzkxNzI2Mw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T19:53:51Z","updated_at":"2020-11-25T19:53:51Z","author_association":"CONTRIBUTOR","body":"I'll add this as a discussion topic for the next Iceberg sync.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733917263/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733932509","html_url":"https://github.com/apache/iceberg/pull/1783#issuecomment-733932509","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1783","id":733932509,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzkzMjUwOQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T20:30:52Z","updated_at":"2020-11-25T20:31:41Z","author_association":"CONTRIBUTOR","body":"> if path then handle with iceberg.path option as per above. Seems like should be done as a separate patch?\r\n\r\nI think that the problem earlier was that once you implement `SupportsCatalogOptions`, you can't also return a table. You have to return a catalog and identifier for everything.\r\n\r\nI think that will require that all of our catalogs support an identifier that contains a path and use that path to load the table.\r\n\r\n> Any thoughts on custom catalogs in Spark2.4?\r\n\r\nWe could do this in the `IcebergSource` for 2.4. I think that could be a nice feature.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733932509/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733936427","html_url":"https://github.com/apache/iceberg/pull/1783#issuecomment-733936427","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1783","id":733936427,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzkzNjQyNw==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T20:41:33Z","updated_at":"2020-11-25T20:41:33Z","author_association":"CONTRIBUTOR","body":" \n> I think that will require that all of our catalogs support an identifier that contains a path and use that path to load the table.\n\nAgreed. I was going to open a PR for that to merge before this one. Unless you want it all in 1 PR?\n\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733936427/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733942197","html_url":"https://github.com/apache/iceberg/pull/1783#issuecomment-733942197","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1783","id":733942197,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzk0MjE5Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T20:57:32Z","updated_at":"2020-11-25T20:57:32Z","author_association":"CONTRIBUTOR","body":"Separate PRs works for me. I think you'll need to add that first, or else you won't be able to get tests passing.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733942197/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733974712","html_url":"https://github.com/apache/iceberg/pull/1819#issuecomment-733974712","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1819","id":733974712,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzk3NDcxMg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T22:32:18Z","updated_at":"2020-11-25T22:32:18Z","author_association":"CONTRIBUTOR","body":"@aokolnychyi, what about `history.expire.snapshot-age-ms` and `history.expire.min-snapshots`? Those would control the age after which a snapshot can expire (the interval for `expireOlderThan(now - interval)`) and the retain-last as minimum number of snapshots to keep.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733974712/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733981188","html_url":"https://github.com/apache/iceberg/issues/1826#issuecomment-733981188","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1826","id":733981188,"node_id":"MDEyOklzc3VlQ29tbWVudDczMzk4MTE4OA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-25T22:55:28Z","updated_at":"2020-11-25T22:55:28Z","author_association":"CONTRIBUTOR","body":"> Decide when we need to run the integration tests\r\n\r\nI think that we would want to create a new GitHub action that runs Hive integration tests separately and for just one JVM profile. That way, we have an action for JDK8 unit tests, an action for JDK11 unit tests, and a Hive integration test action that runs in parallel with those. That would save overall time by running in parallel, and would save resources by running the expensive tests just once.\r\n\r\nI would still want to run the tests on each PR because we don't want changes to core breaking the Hive integration tests. Also, we could possibly run the integration tests with Hive 3 if anything in iceberg-mr or iceberg-hive-metastore changes.\r\n\r\n> Run the actual tests on the CI\r\n\r\nWhen we moved to GitHub actions, I updated the python tests to only run when a file in `python/` changes. We could use the same strategy for these tests. But the main thing is moving expensive tests into an `integrationTest` task and calling it from a GitHub action with `./gradlew iceberg-mr:integrationTest -x test`.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/733981188/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734007781","html_url":"https://github.com/apache/iceberg/pull/1612#issuecomment-734007781","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1612","id":734007781,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDAwNzc4MQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T00:34:50Z","updated_at":"2020-11-26T00:34:50Z","author_association":"CONTRIBUTOR","body":"Thanks for all your work on this, @pvary! I'll merge it.\r\n\r\nThanks for reviewing, @shardulm94!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734007781/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734021776","html_url":"https://github.com/apache/iceberg/pull/1815#issuecomment-734021776","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1815","id":734021776,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDAyMTc3Ng==","user":{"login":"zhangjun0x01","id":25563794,"node_id":"MDQ6VXNlcjI1NTYzNzk0","avatar_url":"https://avatars.githubusercontent.com/u/25563794?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjun0x01","html_url":"https://github.com/zhangjun0x01","followers_url":"https://api.github.com/users/zhangjun0x01/followers","following_url":"https://api.github.com/users/zhangjun0x01/following{/other_user}","gists_url":"https://api.github.com/users/zhangjun0x01/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjun0x01/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjun0x01/subscriptions","organizations_url":"https://api.github.com/users/zhangjun0x01/orgs","repos_url":"https://api.github.com/users/zhangjun0x01/repos","events_url":"https://api.github.com/users/zhangjun0x01/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjun0x01/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T01:30:17Z","updated_at":"2020-11-26T01:30:17Z","author_association":"CONTRIBUTOR","body":"> Looks okay to me overall.\r\n\r\nI updated all.\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734021776/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734022232","html_url":"https://github.com/apache/iceberg/pull/1815#issuecomment-734022232","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1815","id":734022232,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDAyMjIzMg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T01:32:13Z","updated_at":"2020-11-26T01:32:13Z","author_association":"CONTRIBUTOR","body":"+1 assuming tests pass. I'll merge this when I'm back on Monday if @openinx doesn't have a chance for another look at it in the mean time. I think all of his concerns have been addressed.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734022232/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734027292","html_url":"https://github.com/apache/iceberg/issues/1764#issuecomment-734027292","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1764","id":734027292,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDAyNzI5Mg==","user":{"login":"wypb","id":5170878,"node_id":"MDQ6VXNlcjUxNzA4Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/5170878?v=4","gravatar_id":"","url":"https://api.github.com/users/wypb","html_url":"https://github.com/wypb","followers_url":"https://api.github.com/users/wypb/followers","following_url":"https://api.github.com/users/wypb/following{/other_user}","gists_url":"https://api.github.com/users/wypb/gists{/gist_id}","starred_url":"https://api.github.com/users/wypb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wypb/subscriptions","organizations_url":"https://api.github.com/users/wypb/orgs","repos_url":"https://api.github.com/users/wypb/repos","events_url":"https://api.github.com/users/wypb/events{/privacy}","received_events_url":"https://api.github.com/users/wypb/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T01:52:18Z","updated_at":"2020-11-26T01:52:18Z","author_association":"NONE","body":"Hi @zhangdove, `Spark.sql (\" drop table hadoop_prod.db.tb \")` can delete table directories, but `catalog.dropTable(name)`  cannot completely delete table directories.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734027292/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734048396","html_url":"https://github.com/apache/iceberg/pull/1747#issuecomment-734048396","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1747","id":734048396,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDA0ODM5Ng==","user":{"login":"yyanyy","id":71906210,"node_id":"MDQ6VXNlcjcxOTA2MjEw","avatar_url":"https://avatars.githubusercontent.com/u/71906210?v=4","gravatar_id":"","url":"https://api.github.com/users/yyanyy","html_url":"https://github.com/yyanyy","followers_url":"https://api.github.com/users/yyanyy/followers","following_url":"https://api.github.com/users/yyanyy/following{/other_user}","gists_url":"https://api.github.com/users/yyanyy/gists{/gist_id}","starred_url":"https://api.github.com/users/yyanyy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yyanyy/subscriptions","organizations_url":"https://api.github.com/users/yyanyy/orgs","repos_url":"https://api.github.com/users/yyanyy/repos","events_url":"https://api.github.com/users/yyanyy/events{/privacy}","received_events_url":"https://api.github.com/users/yyanyy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T03:14:34Z","updated_at":"2020-11-26T03:14:34Z","author_association":"CONTRIBUTOR","body":"> > Do you have comment on the case of \"this may result in v2 returning more files than v1\" when literal is not NaN but the data to be compared have NaN? We might need to accept that to keep behavior of comparing with NaN consistent across different files?\r\n> \r\n> I don't think this is a v2 problem, it is a bug in how we currently handle NaN right?\r\n\r\nThanks for pointing this out! After thinking about this I realized that my original concern probably shouldn't be a problem. My concern was that to make sure v2 could return exactly the same result as v1 when doing NaN comparison would require extra efforts, since the behavior of metrics evaluators now change. However, doing comparison with NaN is actually an invalid operation, and regardless of how each individual engine treats this (e.g. I think Spark consider NaN as Max, as for a column `col` containing NaNs, `where col > 0` always return NaN records) that should be something to be fixed on the engine side. \r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734048396/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734112444","html_url":"https://github.com/apache/iceberg/pull/1815#issuecomment-734112444","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1815","id":734112444,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDExMjQ0NA==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T06:56:43Z","updated_at":"2020-11-26T06:56:43Z","author_association":"MEMBER","body":"+1  if the travis says all unit tests are passed.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734112444/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734125149","html_url":"https://github.com/apache/iceberg/issues/1764#issuecomment-734125149","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1764","id":734125149,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDEyNTE0OQ==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T07:27:31Z","updated_at":"2020-11-26T07:27:31Z","author_association":"CONTRIBUTOR","body":"Yes, here's what I tested:\r\n1.drop iceberg table by hive catalog, left some table directories.\r\n2.drop iceberg table by hadoop catalog, it's normal(the directory will be deleted).","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734125149/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734128018","html_url":"https://github.com/apache/iceberg/issues/1764#issuecomment-734128018","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1764","id":734128018,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDEyODAxOA==","user":{"login":"wypb","id":5170878,"node_id":"MDQ6VXNlcjUxNzA4Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/5170878?v=4","gravatar_id":"","url":"https://api.github.com/users/wypb","html_url":"https://github.com/wypb","followers_url":"https://api.github.com/users/wypb/followers","following_url":"https://api.github.com/users/wypb/following{/other_user}","gists_url":"https://api.github.com/users/wypb/gists{/gist_id}","starred_url":"https://api.github.com/users/wypb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wypb/subscriptions","organizations_url":"https://api.github.com/users/wypb/orgs","repos_url":"https://api.github.com/users/wypb/repos","events_url":"https://api.github.com/users/wypb/events{/privacy}","received_events_url":"https://api.github.com/users/wypb/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T07:34:34Z","updated_at":"2020-11-26T07:34:34Z","author_association":"NONE","body":"Thanks for your reply, I will submit an RP to make these behaviors consistent.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734128018/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734134984","html_url":"https://github.com/apache/iceberg/pull/1612#issuecomment-734134984","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1612","id":734134984,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDEzNDk4NA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T07:51:41Z","updated_at":"2020-11-26T07:51:41Z","author_association":"CONTRIBUTOR","body":"Big thanks @rdblue and @shardulm94 for following this through!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734134984/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734159034","html_url":"https://github.com/apache/iceberg/issues/1833#issuecomment-734159034","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1833","id":734159034,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDE1OTAzNA==","user":{"login":"vvellanki","id":5296341,"node_id":"MDQ6VXNlcjUyOTYzNDE=","avatar_url":"https://avatars.githubusercontent.com/u/5296341?v=4","gravatar_id":"","url":"https://api.github.com/users/vvellanki","html_url":"https://github.com/vvellanki","followers_url":"https://api.github.com/users/vvellanki/followers","following_url":"https://api.github.com/users/vvellanki/following{/other_user}","gists_url":"https://api.github.com/users/vvellanki/gists{/gist_id}","starred_url":"https://api.github.com/users/vvellanki/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vvellanki/subscriptions","organizations_url":"https://api.github.com/users/vvellanki/orgs","repos_url":"https://api.github.com/users/vvellanki/repos","events_url":"https://api.github.com/users/vvellanki/events{/privacy}","received_events_url":"https://api.github.com/users/vvellanki/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T08:44:36Z","updated_at":"2020-11-26T08:44:36Z","author_association":"NONE","body":"This change should also include changes to read the contents of a Partition Index","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734159034/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734169841","html_url":"https://github.com/apache/iceberg/issues/1764#issuecomment-734169841","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1764","id":734169841,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDE2OTg0MQ==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T09:05:22Z","updated_at":"2020-11-26T09:05:22Z","author_association":"CONTRIBUTOR","body":"@rdblue I was wondering if the community, in its initial design #350 , kept the directory of data and metadata as an external data source, not allowing Iceberg or Hive's API to directly delete the directory files, in the same way as Hive's external attributes?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734169841/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734255454","html_url":"https://github.com/apache/iceberg/issues/1780#issuecomment-734255454","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1780","id":734255454,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDI1NTQ1NA==","user":{"login":"kukuzidian","id":3753735,"node_id":"MDQ6VXNlcjM3NTM3MzU=","avatar_url":"https://avatars.githubusercontent.com/u/3753735?v=4","gravatar_id":"","url":"https://api.github.com/users/kukuzidian","html_url":"https://github.com/kukuzidian","followers_url":"https://api.github.com/users/kukuzidian/followers","following_url":"https://api.github.com/users/kukuzidian/following{/other_user}","gists_url":"https://api.github.com/users/kukuzidian/gists{/gist_id}","starred_url":"https://api.github.com/users/kukuzidian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kukuzidian/subscriptions","organizations_url":"https://api.github.com/users/kukuzidian/orgs","repos_url":"https://api.github.com/users/kukuzidian/repos","events_url":"https://api.github.com/users/kukuzidian/events{/privacy}","received_events_url":"https://api.github.com/users/kukuzidian/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T11:54:27Z","updated_at":"2020-11-26T11:54:27Z","author_association":"NONE","body":"@pvary  @zhangdove OK , Thanks , now I close this issues . ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734255454/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734266112","html_url":"https://github.com/apache/iceberg/pull/1815#issuecomment-734266112","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1815","id":734266112,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDI2NjExMg==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T12:18:17Z","updated_at":"2020-11-26T12:18:17Z","author_association":"MEMBER","body":"There is still a failure: \r\n\r\n```\r\n2020-11-26T11:45:11.9709379Z org.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithHadoopCatalog > testCreatePartitionedTable[fileFormat=AVRO, engine=tez] FAILED\r\n2020-11-26T11:45:11.9713827Z     java.lang.IllegalArgumentException: Failed to execute Hive query 'SELECT * FROM default.customers ORDER BY customer_id DESC': Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n2020-11-26T11:45:11.9715796Z \r\n2020-11-26T11:45:11.9716299Z         Caused by:\r\n2020-11-26T11:45:11.9717865Z         org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734266112/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734269749","html_url":"https://github.com/apache/iceberg/pull/1815#issuecomment-734269749","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1815","id":734269749,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDI2OTc0OQ==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T12:26:42Z","updated_at":"2020-11-26T12:26:42Z","author_association":"MEMBER","body":"I run the broken unit tests with this patch ,  all of them are OK. Checked this PR again, it should not be related to this broken unit tests.  I think it has to do with the environment that Travis is running in,  so I plan to merge this patch. \r\n\r\n![image](https://user-images.githubusercontent.com/5028729/100350746-4ab17880-3025-11eb-8861-538e004fee1c.png)\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734269749/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734384294","html_url":"https://github.com/apache/iceberg/pull/1417#issuecomment-734384294","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1417","id":734384294,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDM4NDI5NA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T16:12:15Z","updated_at":"2020-11-26T16:12:15Z","author_association":"CONTRIBUTOR","body":"Thanks for updating this! I merged it.\r\n\r\nNice work, everyone!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734384294/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734419726","html_url":"https://github.com/apache/iceberg/issues/1306#issuecomment-734419726","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1306","id":734419726,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDQxOTcyNg==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T17:36:37Z","updated_at":"2020-11-26T17:36:37Z","author_association":"CONTRIBUTOR","body":"Just a heads up, I have picked this up and have a partial impl which I hope to publish a draft PR of soon.\r\n\r\nAs per https://github.com/apache/iceberg/issues/1306#issuecomment-671627947 it is not clear to me how this will work for filesystems that don't offer atomic primitives. Should we check the uri schema and reject if it isn't supported?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734419726/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734448070","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734448070","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734448070,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDQ0ODA3MA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-26T19:12:23Z","updated_at":"2020-11-26T19:12:23Z","author_association":"CONTRIBUTOR","body":"@zhangdove: Could you please share the HiveServer2 logs for the `SELECT * FROM db.tb` query? Maybe even DEBUG level logs, if possible. Also the results of the `DESCRIBE FORMATTED db.tb` command from BeeLine might help?\r\nThanks, Peter ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734448070/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734532263","html_url":"https://github.com/apache/iceberg/issues/1834#issuecomment-734532263","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1834","id":734532263,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDUzMjI2Mw==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T01:54:55Z","updated_at":"2020-11-27T01:54:55Z","author_association":"MEMBER","body":"@jxeditor What is your unmet requirement ?   Could you provide more details ? Thanks. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734532263/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734629607","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734629607","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734629607,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDYyOTYwNw==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T03:42:03Z","updated_at":"2020-11-27T03:42:03Z","author_association":"CONTRIBUTOR","body":"@pvary The DEBUG log information of HiveServer2 is quite large, so I haven't found any useful logs yet. I will make a further investigation later.\r\n\r\nAnd there is `DESCRIBE FORMATTED db.tb`\r\n```\r\n0: jdbc:hive2://localhost:10000> DESCRIBE FORMATTED db.tb;\r\nOK\r\n+-------------------------------+----------------------------------------------------+----------------------------------------------------+\r\n|           col_name            |                     data_type                      |                      comment                       |\r\n+-------------------------------+----------------------------------------------------+----------------------------------------------------+\r\n| # col_name                    | data_type                                          | comment                                            |\r\n|                               | NULL                                               | NULL                                               |\r\n| id                            | int                                                |                                                    |\r\n| ts                            | timestamp                                          |                                                    |\r\n|                               | NULL                                               | NULL                                               |\r\n| # Detailed Table Information  | NULL                                               | NULL                                               |\r\n| Database:                     | db                                                 | NULL                                               |\r\n| Owner:                        | dovezhang                                          | NULL                                               |\r\n| CreateTime:                   | Thu Nov 26 17:56:31 CST 2020                       | NULL                                               |\r\n| LastAccessTime:               | Fri Jan 02 02:33:42 CST 1970                       | NULL                                               |\r\n| Retention:                    | 2147483647                                         | NULL                                               |\r\n| Location:                     | hdfs://localhost:8020/usr/hive/warehouse/db.db/tb  | NULL                                               |\r\n| Table Type:                   | EXTERNAL_TABLE                                     | NULL                                               |\r\n| Table Parameters:             | NULL                                               | NULL                                               |\r\n|                               | EXTERNAL                                           | TRUE                                               |\r\n|                               | metadata_location                                  | hdfs://localhost:8020/usr/hive/warehouse/db.db/tb/metadata/00001-ffce82b8-fba2-4681-b095-67f472653333.metadata.json |\r\n|                               | numFiles                                           | 3                                                  |\r\n|                               | previous_metadata_location                         | hdfs://localhost:8020/usr/hive/warehouse/db.db/tb/metadata/00000-3c8cce22-19c5-4926-9da5-9bf6cd9cc0f8.metadata.json |\r\n|                               | table_type                                         | ICEBERG                                            |\r\n|                               | totalSize                                          | 3195                                               |\r\n|                               | transient_lastDdlTime                              | 1606384591                                         |\r\n|                               | NULL                                               | NULL                                               |\r\n| # Storage Information         | NULL                                               | NULL                                               |\r\n| SerDe Library:                | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | NULL                                               |\r\n| InputFormat:                  | org.apache.hadoop.mapred.FileInputFormat           | NULL                                               |\r\n| OutputFormat:                 | org.apache.hadoop.mapred.FileOutputFormat          | NULL                                               |\r\n| Compressed:                   | No                                                 | NULL                                               |\r\n| Num Buckets:                  | 0                                                  | NULL                                               |\r\n| Bucket Columns:               | []                                                 | NULL                                               |\r\n| Sort Columns:                 | []                                                 | NULL                                               |\r\n+-------------------------------+----------------------------------------------------+----------------------------------------------------+\r\n30 rows selected (0.14 seconds)\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734629607/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734670458","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734670458","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734670458,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY3MDQ1OA==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T06:34:25Z","updated_at":"2020-11-27T09:32:40Z","author_association":"CONTRIBUTOR","body":"```\r\n2020-11-27T14:10:51,474 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: SERVER: reading data length: 137\r\n2020-11-27T14:10:51,476  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Updating thread name to 0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,477  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] operation.OperationManager: Adding operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]\r\n2020-11-27T14:10:51,478 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Driver: Acquired the compile lock.\r\n2020-11-27T14:10:51,478 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,478 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,478 DEBUG [HiveServer2-Handler-Pool: Thread-32] conf.VariableSubstitution: Substitution is on: select * from db.tb\r\n2020-11-27T14:10:51,478  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Driver: Compiling command(queryId=dovezhang_20201127141051_08c245d8-c816-41fb-8d0a-4f06d6e2b50d): select * from db.tb\r\n2020-11-27T14:10:51,479 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,479 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.ParseDriver: Parsing command: select * from db.tb\r\n2020-11-27T14:10:51,479 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.ParseDriver: Parse Completed\r\n2020-11-27T14:10:51,479 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=parse start=1606457451479 end=1606457451479 duration=0 from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,479 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,481  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Starting Semantic Analysis\r\n2020-11-27T14:10:51,481  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Completed phase 1 of Semantic Analysis\r\n2020-11-27T14:10:51,481  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Get metadata for source tables\r\n2020-11-27T14:10:51,528  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Get metadata for subqueries\r\n2020-11-27T14:10:51,528  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Get metadata for destination tables\r\n2020-11-27T14:10:51,529 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.Client: The ping interval is 60000 ms.\r\n2020-11-27T14:10:51,529 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.Client: Connecting to localhost/127.0.0.1:8020\r\n2020-11-27T14:10:51,532 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive: starting, having connections 1\r\n2020-11-27T14:10:51,532 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #193\r\n2020-11-27T14:10:51,548 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #193\r\n2020-11-27T14:10:51,549 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getEZForPath took 21ms\r\n2020-11-27T14:10:51,549 DEBUG [HiveServer2-Handler-Pool: Thread-32] hdfs.DFSClient: /usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1: masked=rwx------\r\n2020-11-27T14:10:51,549 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #194\r\n2020-11-27T14:10:51,551 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #194\r\n2020-11-27T14:10:51,551 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: mkdirs took 2ms\r\n2020-11-27T14:10:51,551 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #195\r\n2020-11-27T14:10:51,552 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #195\r\n2020-11-27T14:10:51,552 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms\r\n2020-11-27T14:10:51,552  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Context: New scratch dir is hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1\r\n2020-11-27T14:10:51,553  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Completed getting MetaData in Semantic Analysis\r\n2020-11-27T14:10:51,554 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,554 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] hive.log: DDL: struct tb { i32 id, timestamp ts}\r\n2020-11-27T14:10:51,554 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] lazy.LazySerDeParameters: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[id, ts] columnTypes=[int, timestamp] separator=[[B@10ff9ec1] nullstring=\\N lastColumnTakesRest=false timestampFormats=null\r\n2020-11-27T14:10:51,555 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Created Plan for Query Block null\r\n2020-11-27T14:10:51,555 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451554 end=1606457451555 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Plan generation>\r\n2020-11-27T14:10:51,556 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] calcite.sql2rel: Plan after trimming unused fields\r\nHiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,564 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Plan before removing subquery:\r\nHiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,564 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#296:HiveProject.HIVE.[](input=HepRelVertex#295,id=$0,ts=$1)\r\n2020-11-27T14:10:51,564 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#291:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,565 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Plan just after removing subquery:\r\nHiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,565 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Plan after decorrelation:\r\nHiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,565 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,565 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#299:HiveProject.HIVE.[](input=HepRelVertex#298,id=$0,ts=$1)\r\n2020-11-27T14:10:51,565 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#291:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,565 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451565 end=1606457451565 duration=0 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: HiveProjectOverIntersectRemoveRule and HiveIntersectMerge rules>\r\n2020-11-27T14:10:51,566 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,566 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#302:HiveProject.HIVE.[](input=HepRelVertex#301,id=$0,ts=$1)\r\n2020-11-27T14:10:51,566 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#291:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,566 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451565 end=1606457451566 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: HiveIntersectRewrite rule>\r\n2020-11-27T14:10:51,566 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,566 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#305:HiveProject.HIVE.[](input=HepRelVertex#304,id=$0,ts=$1)\r\n2020-11-27T14:10:51,566 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#291:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,566 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451566 end=1606457451566 duration=0 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: HiveExceptRewrite rule>\r\n2020-11-27T14:10:51,567 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,567 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#308:HiveProject.HIVE.[](input=HepRelVertex#307,id=$0,ts=$1)\r\n2020-11-27T14:10:51,567 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#291:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,567 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451567 end=1606457451567 duration=0 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Prejoin ordering transformation, factor out common filter elements and separating deterministic vs non-deterministic UDF>\r\n2020-11-27T14:10:51,568 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,568 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: call#11: Apply rule [ReduceExpressionsRule(Project)] to [rel#311:HiveProject.HIVE.[](input=HepRelVertex#310,id=$0,ts=$1)]\r\n2020-11-27T14:10:51,579 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#311:HiveProject.HIVE.[](input=HepRelVertex#310,id=$0,ts=$1)\r\n2020-11-27T14:10:51,579 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#291:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,580 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451567 end=1606457451580 duration=13 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Prejoin ordering transformation, PPD, not null predicates, transitive inference, constant folding>\r\n2020-11-27T14:10:51,580 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,581 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#314:HiveProject.HIVE.[](input=HepRelVertex#313,id=$0,ts=$1)\r\n2020-11-27T14:10:51,581 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#291:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,581 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451580 end=1606457451581 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Prejoin ordering transformation, Push Down Semi Joins>\r\n2020-11-27T14:10:51,581 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,581 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#317:HiveProject.HIVE.[](input=HepRelVertex#316,id=$0,ts=$1)\r\n2020-11-27T14:10:51,582 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#291:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,582 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451581 end=1606457451582 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Prejoin ordering transformation, Partition Pruning>\r\n2020-11-27T14:10:51,582 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,592 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] calcite.sql2rel: Plan after trimming unused fields\r\nHiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,592 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451582 end=1606457451592 duration=10 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Prejoin ordering transformation, Projection Pruning>\r\n2020-11-27T14:10:51,592 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,593 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#322:HiveProject.HIVE.[](input=HepRelVertex#321,id=$0,ts=$1)\r\n2020-11-27T14:10:51,593 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#319:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,593 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451592 end=1606457451593 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Prejoin ordering transformation, Merge Project-Project>\r\n2020-11-27T14:10:51,593 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,594 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#325:HiveProject.HIVE.[](input=HepRelVertex#324,id=$0,ts=$1)\r\n2020-11-27T14:10:51,594 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#319:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,594 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451593 end=1606457451594 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Prejoin ordering transformation, Rerun PPD>\r\n2020-11-27T14:10:51,596 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,596 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#328:HiveProject.HIVE.[](input=HepRelVertex#327,id=$0,ts=$1)\r\n2020-11-27T14:10:51,597 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#319:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,597 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451596 end=1606457451597 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Optimizations without stats>\r\n2020-11-27T14:10:51,597 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,597 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#331:HiveProject.HIVE.[](input=HepRelVertex#330,id=$0,ts=$1)\r\n2020-11-27T14:10:51,598 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#319:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,598 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451597 end=1606457451598 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Semijoin conversion>\r\n2020-11-27T14:10:51,598 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction>\r\n2020-11-27T14:10:51,598 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#334:HiveProject.HIVE.[](input=HepRelVertex#333,id=$0,ts=$1)\r\n2020-11-27T14:10:51,599 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] plan.RelOptPlanner: For final plan, using rel#319:HiveTableScan.HIVE.[](table=[db.tb],table:alias=tb)[false]\r\n2020-11-27T14:10:51,599 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451598 end=1606457451599 duration=1 from=org.apache.hadoop.hive.ql.parse.CalcitePlanner$CalcitePlannerAction Calcite: Druid transformation rules>\r\n2020-11-27T14:10:51,599 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: CBO Planning details:\r\n\r\n2020-11-27T14:10:51,600 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Original Plan:\r\nHiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,600 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Plan After PPD, PartPruning, ColumnPruning:\r\nHiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,610 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=partition-retrieving from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>\r\n2020-11-27T14:10:51,610 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=partition-retrieving start=1606457451610 end=1606457451610 duration=0 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>\r\n2020-11-27T14:10:51,628 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Plan After Join Reordering:\r\nHiveProject(id=[$0], ts=[$1]): rowcount = 1.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 334\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb]): rowcount = 1.0, cumulative cost = {0}, id = 319\r\n\r\n2020-11-27T14:10:51,628 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] translator.PlanModifierForASTConv: Original plan for PlanModifier\r\n HiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,629 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] translator.PlanModifierForASTConv: Plan after nested convertOpTree\r\n HiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,629 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] translator.PlanModifierForASTConv: Plan after propagating order\r\n HiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,629 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] translator.PlanModifierForASTConv: Plan after fixTopOBSchema\r\n HiveProject(id=[$0], ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,629 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] translator.PlanModifierForASTConv: Final plan after modifier\r\n HiveProject(tb.id=[$0], tb.ts=[$1])\r\n  HiveTableScan(table=[[db.tb]], table:alias=[tb])\r\n\r\n2020-11-27T14:10:51,630  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Get metadata for source tables\r\n2020-11-27T14:10:51,647  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Get metadata for subqueries\r\n2020-11-27T14:10:51,647  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Get metadata for destination tables\r\n2020-11-27T14:10:51,647 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #196\r\n2020-11-27T14:10:51,648 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #196\r\n2020-11-27T14:10:51,648 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getEZForPath took 1ms\r\n2020-11-27T14:10:51,648  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Context: New scratch dir is hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1\r\n2020-11-27T14:10:51,649 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] hive.log: DDL: struct tb { i32 id, timestamp ts}\r\n2020-11-27T14:10:51,649 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] lazy.LazySerDeParameters: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[id, ts] columnTypes=[int, timestamp] separator=[[B@2b2f626f] nullstring=\\N lastColumnTakesRest=false timestampFormats=null\r\n2020-11-27T14:10:51,649 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Created Table Plan for tb TS[0]\r\n2020-11-27T14:10:51,649 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: RR before GB tb{(id,id: int)(ts,ts: timestamp)(block__offset__inside__file,BLOCK__OFFSET__INSIDE__FILE: bigint)(input__file__name,INPUT__FILE__NAME: string)(row__id,ROW__ID: struct<transactionid:bigint,bucketid:int,rowid:bigint>)}  after GB tb{(id,id: int)(ts,ts: timestamp)(block__offset__inside__file,BLOCK__OFFSET__INSIDE__FILE: bigint)(input__file__name,INPUT__FILE__NAME: string)(row__id,ROW__ID: struct<transactionid:bigint,bucketid:int,rowid:bigint>)}\r\n2020-11-27T14:10:51,649 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: tree: (tok_select (tok_selexpr (. (tok_table_or_col tb) id) tb.id) (tok_selexpr (. (tok_table_or_col tb) ts) tb.ts))\r\n2020-11-27T14:10:51,649 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: genSelectPlan: input = tb{(id,id: int)(ts,ts: timestamp)(block__offset__inside__file,BLOCK__OFFSET__INSIDE__FILE: bigint)(input__file__name,INPUT__FILE__NAME: string)(row__id,ROW__ID: struct<transactionid:bigint,bucketid:int,rowid:bigint>)}  starRr = null\r\n2020-11-27T14:10:51,650 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Created Select Plan row schema: null{(tb.id,_col0: int)(tb.ts,_col1: timestamp)}\r\n2020-11-27T14:10:51,650 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Created Select Plan for clause: insclause-0\r\n2020-11-27T14:10:51,651 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Context: Created staging dir = hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001/.hive-staging_hive_2020-11-27_14-10-51_478_1086002515739816328-1 for path = hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001\r\n2020-11-27T14:10:51,651 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #197\r\n2020-11-27T14:10:51,651 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #197\r\n2020-11-27T14:10:51,652 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms\r\n2020-11-27T14:10:51,653 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #198\r\n2020-11-27T14:10:51,653 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #198\r\n2020-11-27T14:10:51,653 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms\r\n2020-11-27T14:10:51,654 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #199\r\n2020-11-27T14:10:51,654 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #199\r\n2020-11-27T14:10:51,654 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms\r\n2020-11-27T14:10:51,655 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #200\r\n2020-11-27T14:10:51,655 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #200\r\n2020-11-27T14:10:51,655 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms\r\n2020-11-27T14:10:51,655 DEBUG [HiveServer2-Handler-Pool: Thread-32] hdfs.DFSClient: /usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001/.hive-staging_hive_2020-11-27_14-10-51_478_1086002515739816328-1: masked=rwxr-xr-x\r\n2020-11-27T14:10:51,656 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #201\r\n2020-11-27T14:10:51,657 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #201\r\n2020-11-27T14:10:51,657 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: mkdirs took 1ms\r\n2020-11-27T14:10:51,657 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #202\r\n2020-11-27T14:10:51,658 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #202\r\n2020-11-27T14:10:51,658 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms\r\n2020-11-27T14:10:51,658 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] shims.HdfsUtils: {-chgrp,-R,supergroup,hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001}\r\n2020-11-27T14:10:51,659 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #203\r\n2020-11-27T14:10:51,659 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #203\r\n2020-11-27T14:10:51,660 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 0ms\r\n2020-11-27T14:10:51,660 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #204\r\n2020-11-27T14:10:51,661 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #204\r\n2020-11-27T14:10:51,661 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getListing took 1ms\r\n2020-11-27T14:10:51,661 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #205\r\n2020-11-27T14:10:51,662 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #205\r\n2020-11-27T14:10:51,662 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getListing took 1ms\r\n2020-11-27T14:10:51,662 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] shims.HdfsUtils: Return value is :0\r\n2020-11-27T14:10:51,662 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] shims.HdfsUtils: {-chmod,-R,700,hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001}\r\n2020-11-27T14:10:51,663 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #206\r\n2020-11-27T14:10:51,663 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #206\r\n2020-11-27T14:10:51,663 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms\r\n2020-11-27T14:10:51,664 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #207\r\n2020-11-27T14:10:51,665 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #207\r\n2020-11-27T14:10:51,666 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: setPermission took 2ms\r\n2020-11-27T14:10:51,666 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #208\r\n2020-11-27T14:10:51,667 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #208\r\n2020-11-27T14:10:51,667 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getListing took 1ms\r\n2020-11-27T14:10:51,667 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #209\r\n2020-11-27T14:10:51,668 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #209\r\n2020-11-27T14:10:51,668 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: setPermission took 1ms\r\n2020-11-27T14:10:51,668 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #210\r\n2020-11-27T14:10:51,669 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #210\r\n2020-11-27T14:10:51,669 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getListing took 1ms\r\n2020-11-27T14:10:51,669 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] shims.HdfsUtils: Return value is :0\r\n2020-11-27T14:10:51,670 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #211\r\n2020-11-27T14:10:51,671 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #211\r\n2020-11-27T14:10:51,671 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 2ms\r\n2020-11-27T14:10:51,671 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] lazy.LazySerDeParameters: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[_col0, _col1] columnTypes=[int, timestamp] separator=[[B@40ac0d3b] nullstring=\\N lastColumnTakesRest=false timestampFormats=null\r\n2020-11-27T14:10:51,672 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Set stats collection dir : hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001/.hive-staging_hive_2020-11-27_14-10-51_478_1086002515739816328-1/-ext-10003\r\n2020-11-27T14:10:51,672 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Created FileSink Plan for clause: insclause-0dest_path: hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001 row schema: null{(tb.id,_col0: int)(tb.ts,_col1: timestamp)}\r\n2020-11-27T14:10:51,672 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Created Body Plan for Query Block null\r\n2020-11-27T14:10:51,672 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Created Plan for Query Block null\r\n2020-11-27T14:10:51,672  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: CBO Succeeded; optimized logical plan.\r\n2020-11-27T14:10:51,672 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Before logical optimization\r\nTS[0]-SEL[1]-FS[2]\r\n2020-11-27T14:10:51,672 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverterPostProc>\r\n2020-11-27T14:10:51,673 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451672 end=1606457451672 duration=0 from=org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverterPostProc org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverterPostProc@760b4478>\r\n2020-11-27T14:10:51,673 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.PartitionColumnsSeparator>\r\n2020-11-27T14:10:51,673 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451673 end=1606457451673 duration=0 from=org.apache.hadoop.hive.ql.optimizer.PartitionColumnsSeparator org.apache.hadoop.hive.ql.optimizer.PartitionColumnsSeparator@5d48eed>\r\n2020-11-27T14:10:51,673 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate>\r\n2020-11-27T14:10:51,673 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451673 end=1606457451673 duration=0 from=org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate@6a477d49>\r\n2020-11-27T14:10:51,673 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.ppd.SimplePredicatePushDown>\r\n2020-11-27T14:10:51,673  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ppd.OpProcFactory: Processing for FS(2)\r\n2020-11-27T14:10:51,673  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ppd.OpProcFactory: Processing for SEL(1)\r\n2020-11-27T14:10:51,673  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ppd.OpProcFactory: Processing for TS(0)\r\n2020-11-27T14:10:51,673 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ppd.SimplePredicatePushDown: After PPD:\r\nTS[0]-SEL[1]-FS[2]\r\n2020-11-27T14:10:51,673 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451673 end=1606457451673 duration=0 from=org.apache.hadoop.hive.ql.ppd.SimplePredicatePushDown org.apache.hadoop.hive.ql.ppd.SimplePredicatePushDown@72158335>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.RedundantDynamicPruningConditionsRemoval>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451673 end=1606457451674 duration=1 from=org.apache.hadoop.hive.ql.optimizer.RedundantDynamicPruningConditionsRemoval org.apache.hadoop.hive.ql.optimizer.RedundantDynamicPruningConditionsRemoval@7d746fb2>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451674 end=1606457451674 duration=0 from=org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer@1726619d>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451674 end=1606457451674 duration=0 from=org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner@489d27e2>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.pcr.PartitionConditionRemover>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451674 end=1606457451674 duration=0 from=org.apache.hadoop.hive.ql.optimizer.pcr.PartitionConditionRemover org.apache.hadoop.hive.ql.optimizer.pcr.PartitionConditionRemover@7af2b6c>\r\n2020-11-27T14:10:51,674 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer>\r\n2020-11-27T14:10:51,675 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451674 end=1606457451675 duration=1 from=org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer@2cddcf68>\r\n2020-11-27T14:10:51,675 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.ColumnPruner>\r\n2020-11-27T14:10:51,675 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451675 end=1606457451675 duration=0 from=org.apache.hadoop.hive.ql.optimizer.ColumnPruner org.apache.hadoop.hive.ql.optimizer.ColumnPruner@7cec34e2>\r\n2020-11-27T14:10:51,675 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.SamplePruner>\r\n2020-11-27T14:10:51,675 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451675 end=1606457451675 duration=0 from=org.apache.hadoop.hive.ql.optimizer.SamplePruner org.apache.hadoop.hive.ql.optimizer.SamplePruner@65557583>\r\n2020-11-27T14:10:51,675 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor>\r\n2020-11-27T14:10:51,676 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451675 end=1606457451675 duration=0 from=org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor@589f86ea>\r\n2020-11-27T14:10:51,676 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer>\r\n2020-11-27T14:10:51,676 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451676 end=1606457451676 duration=0 from=org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer@449fb42c>\r\n2020-11-27T14:10:51,676 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcessor>\r\n2020-11-27T14:10:51,676 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451676 end=1606457451676 duration=0 from=org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcessor org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcessor@31115d>\r\n2020-11-27T14:10:51,676 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.JoinReorder>\r\n2020-11-27T14:10:51,676 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451676 end=1606457451676 duration=0 from=org.apache.hadoop.hive.ql.optimizer.JoinReorder org.apache.hadoop.hive.ql.optimizer.JoinReorder@5c034950>\r\n2020-11-27T14:10:51,676 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication>\r\n2020-11-27T14:10:51,677 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451676 end=1606457451677 duration=1 from=org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication@416212aa>\r\n2020-11-27T14:10:51,677 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc>\r\n2020-11-27T14:10:51,677 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451677 end=1606457451677 duration=0 from=org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc org.apache.hadoop.hive.ql.optimizer.NonBlockingOpDeDupProc@68aaa904>\r\n2020-11-27T14:10:51,677 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.IdentityProjectRemover>\r\n2020-11-27T14:10:51,677 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451677 end=1606457451677 duration=0 from=org.apache.hadoop.hive.ql.optimizer.IdentityProjectRemover org.apache.hadoop.hive.ql.optimizer.IdentityProjectRemover@525ac9a5>\r\n2020-11-27T14:10:51,678 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer>\r\n2020-11-27T14:10:51,678 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451677 end=1606457451678 duration=1 from=org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer@176618ba>\r\n2020-11-27T14:10:51,678 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.StatsOptimizer>\r\n2020-11-27T14:10:51,678 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451678 end=1606457451678 duration=0 from=org.apache.hadoop.hive.ql.optimizer.StatsOptimizer org.apache.hadoop.hive.ql.optimizer.StatsOptimizer@55c3fc51>\r\n2020-11-27T14:10:51,678 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: <PERFLOG method=optimizer from=org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer>\r\n2020-11-27T14:10:51,678 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] hive.log: DDL: struct tb { i32 id, timestamp ts}\r\n2020-11-27T14:10:51,679 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=optimizer start=1606457451678 end=1606457451678 duration=0 from=org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer@72478d9b>\r\n2020-11-27T14:10:51,679 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: After logical optimization\r\nTS[0]-SEL[1]-LIST_SINK[3]\r\n2020-11-27T14:10:51,679  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: Completed plan generation\r\n2020-11-27T14:10:51,679  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Driver: Semantic Analysis Completed\r\n2020-11-27T14:10:51,679 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: validation start\r\n2020-11-27T14:10:51,679 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] parse.CalcitePlanner: not validating writeEntity, because entity is neither table nor partition\r\n2020-11-27T14:10:51,679 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=semanticAnalyze start=1606457451479 end=1606457451679 duration=200 from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,679  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Driver: Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:tb.id, type:int, comment:null), FieldSchema(name:tb.ts, type:timestamp, comment:null)], properties:null)\r\n2020-11-27T14:10:51,680 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] lazy.LazySerDeParameters: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe initialized with: columnNames=[id, ts] columnTypes=[int, timestamp] separator=[[B@5a4708b8] nullstring=\\N lastColumnTakesRest=false timestampFormats=null\r\n2020-11-27T14:10:51,680  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: Initializing operator TS[0]\r\n2020-11-27T14:10:51,680 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: Initialization Done 0 TS\r\n2020-11-27T14:10:51,680 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: Operator 0 TS initialized\r\n2020-11-27T14:10:51,680 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: Initializing children of 0 TS\r\n2020-11-27T14:10:51,680 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: Initializing child 1 SEL\r\n2020-11-27T14:10:51,680  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: Initializing operator SEL[1]\r\n2020-11-27T14:10:51,680  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: SELECT struct<id:int,ts:timestamp>\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: Initialization Done 1 SEL\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: Operator 1 SEL initialized\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: Initializing children of 1 SEL\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: Initializing child 3 LIST_SINK\r\n2020-11-27T14:10:51,681  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: Initializing operator LIST_SINK[3]\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: Initialization Done 3 LIST_SINK\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: Operator 3 LIST_SINK initialized\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: Initialization Done 3 LIST_SINK done is reset.\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: Initialization Done 1 SEL done is reset.\r\n2020-11-27T14:10:51,681 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: Initialization Done 0 TS done is reset.\r\n2020-11-27T14:10:51,682 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] log.PerfLogger: </PERFLOG method=compile start=1606457451478 end=1606457451681 duration=203 from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,682  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] metadata.Hive: Dumping metastore api call timing information for : compilation phase\r\n2020-11-27T14:10:51,682 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] metadata.Hive: Total time spent in each metastore function (ms): {isCompatibleWith_(HiveConf, )=0, getTable_(String, String, )=64, flushCache_()=0}\r\n2020-11-27T14:10:51,682  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Driver: Completed compiling command(queryId=dovezhang_20201127141051_08c245d8-c816-41fb-8d0a-4f06d6e2b50d); Time taken: 0.203 seconds\r\n2020-11-27T14:10:51,683  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Resetting thread name to  HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,683 DEBUG [HiveServer2-Handler-Pool: Thread-32] cli.CLIService: SessionHandle [0dbe373c-c442-49f2-ad5e-e65735d41988]: executeStatementAsync()\r\n2020-11-27T14:10:51,683 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: writing data length: 109\r\n2020-11-27T14:10:51,684 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: SERVER: reading data length: 104\r\n2020-11-27T14:10:51,684  INFO [HiveServer2-Background-Pool: Thread-73] ql.Driver: Concurrency mode is disabled, not creating a lock manager\r\n2020-11-27T14:10:51,684 DEBUG [HiveServer2-Background-Pool: Thread-73] log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,684  INFO [HiveServer2-Background-Pool: Thread-73] ql.Driver: Executing command(queryId=dovezhang_20201127141051_08c245d8-c816-41fb-8d0a-4f06d6e2b50d): select * from db.tb\r\n2020-11-27T14:10:51,866 DEBUG [HiveServer2-Background-Pool: Thread-73] log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,866 DEBUG [HiveServer2-Background-Pool: Thread-73] log.PerfLogger: </PERFLOG method=runTasks start=1606457451866 end=1606457451866 duration=0 from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,866 DEBUG [HiveServer2-Background-Pool: Thread-73] log.PerfLogger: </PERFLOG method=Driver.execute start=1606457451684 end=1606457451866 duration=182 from=org.apache.hadoop.hive.ql.Driver>\r\nOK\r\n2020-11-27T14:10:51,866  INFO [HiveServer2-Background-Pool: Thread-73] metadata.Hive: Dumping metastore api call timing information for : execution phase\r\n2020-11-27T14:10:51,867 DEBUG [HiveServer2-Background-Pool: Thread-73] metadata.Hive: Total time spent in each metastore function (ms): {}\r\n2020-11-27T14:10:51,867  INFO [HiveServer2-Background-Pool: Thread-73] ql.Driver: Completed executing command(queryId=dovezhang_20201127141051_08c245d8-c816-41fb-8d0a-4f06d6e2b50d); Time taken: 0.182 seconds\r\n2020-11-27T14:10:51,867  INFO [HiveServer2-Background-Pool: Thread-73] ql.Driver: OK\r\n2020-11-27T14:10:51,867 DEBUG [HiveServer2-Background-Pool: Thread-73] log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,867 DEBUG [HiveServer2-Background-Pool: Thread-73] log.PerfLogger: </PERFLOG method=releaseLocks start=1606457451867 end=1606457451867 duration=0 from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,867 DEBUG [HiveServer2-Background-Pool: Thread-73] log.PerfLogger: </PERFLOG method=Driver.run start=1606457451478 end=1606457451867 duration=389 from=org.apache.hadoop.hive.ql.Driver>\r\n2020-11-27T14:10:51,867 DEBUG [HiveServer2-Background-Pool: Thread-73] ql.Driver: Shutting down query select * from db.tb\r\n2020-11-27T14:10:51,870 DEBUG [HiveServer2-Handler-Pool: Thread-32] cli.CLIService: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]: getOperationStatus()\r\n2020-11-27T14:10:51,870 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: writing data length: 144\r\n2020-11-27T14:10:51,871 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: SERVER: reading data length: 102\r\n2020-11-27T14:10:51,872  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Updating thread name to 0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,872  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Resetting thread name to  HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,872 DEBUG [HiveServer2-Handler-Pool: Thread-32] cli.CLIService: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]: getResultSetMetadata()\r\n2020-11-27T14:10:51,872 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: writing data length: 148\r\n2020-11-27T14:10:51,874 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: SERVER: reading data length: 117\r\n2020-11-27T14:10:51,874  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Updating thread name to 0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,875  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Resetting thread name to  HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,875 DEBUG [HiveServer2-Handler-Pool: Thread-32] cli.CLIService: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]: fetchResults()\r\n2020-11-27T14:10:51,875 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: writing data length: 96\r\n2020-11-27T14:10:51,876 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: SERVER: reading data length: 112\r\n2020-11-27T14:10:51,876  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Updating thread name to 0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,877 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #212\r\n2020-11-27T14:10:51,877 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #212\r\n2020-11-27T14:10:51,877 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms\r\n2020-11-27T14:10:51,878 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #213\r\n2020-11-27T14:10:51,878 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #213\r\n2020-11-27T14:10:51,878 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getListing took 1ms\r\n2020-11-27T14:10:51,879 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #214\r\n2020-11-27T14:10:51,879 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #214\r\n2020-11-27T14:10:51,879 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: getListing took 1ms\r\n2020-11-27T14:10:51,879 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: close called for operator TS[0]\r\n2020-11-27T14:10:51,880  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: Closing operator TS[0]\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: Closing child = SEL[1]\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: close called for operator SEL[1]\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: allInitializedParentsAreClosed? parent.state = CLOSE\r\n2020-11-27T14:10:51,880  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: Closing operator SEL[1]\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: Closing child = LIST_SINK[3]\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: close called for operator LIST_SINK[3]\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: allInitializedParentsAreClosed? parent.state = CLOSE\r\n2020-11-27T14:10:51,880  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: Closing operator LIST_SINK[3]\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.ListSinkOperator: 3 Close done\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.SelectOperator: 1 Close done\r\n2020-11-27T14:10:51,880 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] exec.TableScanOperator: 0 Close done\r\n2020-11-27T14:10:51,881  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Resetting thread name to  HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,881 DEBUG [HiveServer2-Handler-Pool: Thread-32] cli.CLIService: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]: fetchResults()\r\n2020-11-27T14:10:51,881 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: writing data length: 117\r\n2020-11-27T14:10:51,882 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: SERVER: reading data length: 112\r\n2020-11-27T14:10:51,882  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Updating thread name to 0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,882  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Resetting thread name to  HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,882 DEBUG [HiveServer2-Handler-Pool: Thread-32] cli.CLIService: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]: fetchResults()\r\n2020-11-27T14:10:51,882 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: writing data length: 117\r\n2020-11-27T14:10:51,883 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: SERVER: reading data length: 117\r\n2020-11-27T14:10:51,883  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Updating thread name to 0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,884  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Resetting thread name to  HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,884 DEBUG [HiveServer2-Handler-Pool: Thread-32] cli.CLIService: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]: fetchResults()\r\n2020-11-27T14:10:51,884 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: writing data length: 96\r\n2020-11-27T14:10:51,884 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: SERVER: reading data length: 96\r\n2020-11-27T14:10:51,884  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Updating thread name to 0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,885  INFO [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] operation.OperationManager: Closing operation: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]\r\n2020-11-27T14:10:51,885 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Context: Deleting result dir: hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001\r\n2020-11-27T14:10:51,885 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #215\r\n2020-11-27T14:10:51,888 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #215\r\n2020-11-27T14:10:51,888 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: delete took 3ms\r\n2020-11-27T14:10:51,888 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Context: Deleting scratch dir: hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1/-mr-10001/.hive-staging_hive_2020-11-27_14-10-51_478_1086002515739816328-1\r\n2020-11-27T14:10:51,888 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #216\r\n2020-11-27T14:10:51,889 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #216\r\n2020-11-27T14:10:51,889 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: delete took 1ms\r\n2020-11-27T14:10:51,889 DEBUG [0dbe373c-c442-49f2-ad5e-e65735d41988 HiveServer2-Handler-Pool: Thread-32] ql.Context: Deleting scratch dir: hdfs://localhost:8020/usr/hive/scratchdir/hive/0dbe373c-c442-49f2-ad5e-e65735d41988/hive_2020-11-27_14-10-51_478_1086002515739816328-1\r\n2020-11-27T14:10:51,889 DEBUG [IPC Parameter Sending Thread #5] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive sending #217\r\n2020-11-27T14:10:51,890 DEBUG [IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive] ipc.Client: IPC Client (188791662) connection to localhost/127.0.0.1:8020 from hive got value #217\r\n2020-11-27T14:10:51,890 DEBUG [HiveServer2-Handler-Pool: Thread-32] ipc.ProtobufRpcEngine: Call: delete took 1ms\r\n2020-11-27T14:10:51,891  INFO [HiveServer2-Handler-Pool: Thread-32] session.SessionState: Resetting thread name to  HiveServer2-Handler-Pool: Thread-32\r\n2020-11-27T14:10:51,891 DEBUG [HiveServer2-Handler-Pool: Thread-32] cli.CLIService: OperationHandle [opType=EXECUTE_STATEMENT, getHandleIdentifier()=84ff9912-e1db-4722-8a18-5a0543f6b1bd]: closeOperation\r\n2020-11-27T14:10:51,891 DEBUG [HiveServer2-Handler-Pool: Thread-32] transport.TSaslTransport: writing data length: 42\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734670458/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734670613","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734670613","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734670613,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY3MDYxMw==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T06:34:51Z","updated_at":"2020-11-27T06:34:51Z","author_association":"CONTRIBUTOR","body":"@pvary Start HiveServer2 by `bin/hiveserver2 --hiveconf hive.root.logger=DEBUG,console`, and the above information is the log information when 'select * from db.tb' is made.\r\n\r\nI'm not sure if I configured the environment the wrong way, I didn't find the information I wanted from this pile of logs.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734670613/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734680709","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734680709","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734680709,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDY4MDcwOQ==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T07:04:29Z","updated_at":"2020-11-27T07:04:29Z","author_association":"CONTRIBUTOR","body":"@pvary And the strange thing is that In this [issue-comment](https://github.com/apache/iceberg/issues/1780#issuecomment-732741938), I used the HadoopCatalog to create a table, write data, and then read it in the way of Hive external table, and the result could be returned normally.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734680709/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734716744","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734716744","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734716744,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDcxNjc0NA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T08:42:27Z","updated_at":"2020-11-27T08:42:27Z","author_association":"CONTRIBUTOR","body":"@zhangdove: The main problem is this:\r\n```\r\n| SerDe Library:                | org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe | NULL                                               |\r\n| InputFormat:                  | org.apache.hadoop.mapred.FileInputFormat           | NULL                                               |\r\n| OutputFormat:                 | org.apache.hadoop.mapred.FileOutputFormat          | NULL                                               |\r\n```\r\nSomehow the SerDe is not set.\r\nWe should find out why the following line is not working:\r\n```\r\n    hadoopConfiguration.set(org.apache.iceberg.hadoop.ConfigProperties.ENGINE_HIVE_ENABLED, \"true\"); //iceberg.engine.hive.enabled=true\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734716744/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734727775","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734727775","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734727775,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDcyNzc3NQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T09:07:25Z","updated_at":"2020-11-27T09:07:25Z","author_association":"CONTRIBUTOR","body":"There might be some issue with the table properties:\r\n```\r\n  private static boolean hiveEngineEnabled(TableMetadata metadata, Configuration conf) {\r\n    if (metadata.properties().get(TableProperties.ENGINE_HIVE_ENABLED) != null) {\r\n      // We know that the property is set, so default value will not be used,\r\n      return metadata.propertyAsBoolean(TableProperties.ENGINE_HIVE_ENABLED, false);\r\n    }\r\n\r\n    return conf.getBoolean(ConfigProperties.ENGINE_HIVE_ENABLED, TableProperties.ENGINE_HIVE_ENABLED_DEFAULT);\r\n  }\r\n```\r\nCould we check the metadata value for `TableProperties.ENGINE_HIVE_ENABLED`?\r\n\r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734727775/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734746179","html_url":"https://github.com/apache/iceberg/issues/1838#issuecomment-734746179","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1838","id":734746179,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDc0NjE3OQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T09:47:33Z","updated_at":"2020-11-27T09:47:33Z","author_association":"CONTRIBUTOR","body":"@liudi1184: Thanks for reporting this!\r\n@lcspinter found the issue and fixed it. See: Hive: Fix casting bugs in Tez joins (#1740)\r\n\r\nYou might want to build your own Iceberg until we can release a version which contains the fix ðŸ˜¢ ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734746179/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734756041","html_url":"https://github.com/apache/iceberg/pull/1757#issuecomment-734756041","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1757","id":734756041,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDc1NjA0MQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T10:09:12Z","updated_at":"2020-11-27T10:09:12Z","author_association":"CONTRIBUTOR","body":"Created #1840 (Hive: Refactor HiveIcebergStorageHandler tests to use catalogs as parameters) as a first part of this change","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734756041/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734767996","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734767996","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734767996,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDc2Nzk5Ng==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T10:37:06Z","updated_at":"2020-11-27T10:37:06Z","author_association":"CONTRIBUTOR","body":"@pvary Thank you for the information you provided. Based on the information you provided, I made a survey and found the following.\r\n\r\nIceberg table is created using the Hive Catalog. The properties of the table are normal:\r\n```\r\n| SerDe Library:                | org.apache.iceberg.mr.hive.HiveIcebergSerDe        | NULL                                               |\r\n| InputFormat:                  | null                                               | NULL                                               |\r\n| OutputFormat:                 | null                                               | NULL                                               |\r\n```\r\n\r\nHowever, when I write data to Iceberg, as follows, the SerDe properties of the table have been modified.\r\n```scala\r\n    df.writeTo(s\"hive_prod.db.tb\").overwrite(functions.lit(true))\r\n    df.writeTo(s\"hive_prod.db.tb\").overwritePartitions()\r\n    df.writeTo(s\"hive_prod.db.tb\").append()\r\n```\r\n\r\nI am still learning this module of Hive, so I am not familiar with it. I have two ideas that are less than exact:\r\na) Save the value of `TableProperties.ENGINE_HIVE_ENABLED`. I am not suer whether the property will be reloaded when iceberg is written to use Spark.\r\nb) Set the `TableProperties.ENGINE_HIVE_ENABLED` property to true when building the Hive Catalog and add it to conf.\r\nhttps://github.com/apache/iceberg/blob/master/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java#L114\r\n\r\nI will do some tests next, but I hope to get feedback from the everyone if possible.\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734767996/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734771005","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-734771005","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":734771005,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDc3MTAwNQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T10:44:11Z","updated_at":"2020-11-27T10:44:11Z","author_association":"CONTRIBUTOR","body":"Could it be that the `iceberg.engine.hive.enabled` is only changed in the config at creation time? It it is not set when we overwrite the data then the new snapshot might remove these values.\r\nIf we set it on the table properties then it might help","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734771005/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734802219","html_url":"https://github.com/apache/iceberg/pull/1842#issuecomment-734802219","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1842","id":734802219,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDgwMjIxOQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T12:02:23Z","updated_at":"2020-11-27T12:02:23Z","author_association":"CONTRIBUTOR","body":"CC: @boroknagyz ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734802219/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734828999","html_url":"https://github.com/apache/iceberg/pull/1843#issuecomment-734828999","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1843","id":734828999,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDgyODk5OQ==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T13:13:00Z","updated_at":"2020-11-27T13:13:00Z","author_association":"CONTRIBUTOR","body":"This is still a bit rough, doesn't pass 1 test and I haven't tested `ALTER` commands but I wanted to get it out for review early. \r\n\r\nOne key issue:\r\nIt appears that deleting tables is only a `V1` Datasource operation from the Session Catalog currently in Spark (see [here](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveSessionCatalog.scala#L366)). So when trying to drop a table via file path eg: ```DROP TABLE `file://path/to/iceberg/table\\` ``` when using a Session catalog you get an error. This is because the V1 Session Catalog looks in Hive and doesn't find the table. \r\n\r\nThe `DROP` commands work when using a `HiveCatalog` and the Session Catalog because the table was created via a `HiveCatalog` and our `SparkSessionCatalog` and deleted via a V1 Session Catalog but both are talking to Hive so we don't notice.\r\n\r\nIn my mind this means the Spark3 tests that run against the session catalog are (partially) broken as the `DROP` command won't work for anything other than a Hive based table.\r\n\r\nSo:\r\n* should we document this limitation and ignore?\r\n* should we fix it on the Spark side?\r\n* Should we fix it on our side (I honestly don't know how to though)?\r\n\r\n@rdblue @aokolnychyi any thoughts?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734828999/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734852303","html_url":"https://github.com/apache/iceberg/pull/1843#issuecomment-734852303","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1843","id":734852303,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDg1MjMwMw==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T14:08:38Z","updated_at":"2020-11-27T14:25:09Z","author_association":"MEMBER","body":"I was thinking about this a bit differently, rather than providing a full catalog I was thinking that we just have the various spark catalogs treat file types as namespaces. Then we just have them switch to returning Hadoop tables in the load table method.\r\n\r\nHere is an example @aokolnychyi was typing up on the create ticket\r\n\r\n```\r\n// Case 2.1: migrate a location using HadoopTables (or path-based catalog) for target\r\n\r\nMIGRATE TABLE parquet.`path/to/table`\r\nUSING iceberg\r\nTBLPROPERTIES (\r\n  'key' 'value'\r\n)\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734852303/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734857984","html_url":"https://github.com/apache/iceberg/pull/1841#issuecomment-734857984","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1841","id":734857984,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDg1Nzk4NA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T14:21:46Z","updated_at":"2020-11-27T14:21:46Z","author_association":"CONTRIBUTOR","body":"@rdblue or @shardulm94: Could you please check this out after Thanksgiving?\r\nThanks,\r\nPeter ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734857984/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734858679","html_url":"https://github.com/apache/iceberg/pull/1842#issuecomment-734858679","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1842","id":734858679,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDg1ODY3OQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T14:23:26Z","updated_at":"2020-11-27T14:23:26Z","author_association":"CONTRIBUTOR","body":"Since we move toward the 1-on-1 Hive/Iceberg schema mapping, I think we should centralize the conversions in a single place, so it is easier to maintain and share with other projects using HiveCatalog.\r\n\r\n@rdblue or @shardulm94: Could you please check this out after Thanksgiving?\r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734858679/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734867957","html_url":"https://github.com/apache/iceberg/pull/1843#issuecomment-734867957","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1843","id":734867957,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDg2Nzk1Nw==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T14:43:59Z","updated_at":"2020-11-27T14:43:59Z","author_association":"CONTRIBUTOR","body":"> I was thinking about this a bit differently, rather than providing a full catalog I was thinking that we just have the various spark catalogs treat file types as namespaces. Then we just have them switch to returning Hadoop tables in the load table method.\r\n\r\nI was thinking this way originally as well. My first attempt at it made for weird interactions between `BaseMetastoreCatalog` and the individual catalogs. And a bunch of dupe code in the various `Catalog` impls. I am still not satisfied w/ this solution as the hadoop delegate catalog is a bit strange but at least it centralises the logic a bit.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734867957/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734869844","html_url":"https://github.com/apache/iceberg/pull/1843#issuecomment-734869844","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1843","id":734869844,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDg2OTg0NA==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T14:48:12Z","updated_at":"2020-11-27T14:48:12Z","author_association":"CONTRIBUTOR","body":"\r\n> Here is an example @aokolnychyi was typing up on the create ticket\r\n> \r\n> ```\r\n> // Case 2.1: migrate a location using HadoopTables (or path-based catalog) for target\r\n> \r\n> MIGRATE TABLE parquet.`path/to/table`\r\n> USING iceberg\r\n> TBLPROPERTIES (\r\n>   'key' 'value'\r\n> )\r\n> ```\r\n\r\nI like this example too. A very useful feature. This is slightly different/more complicated though right? That would add a sql extension and a separate impl to do write the metadata of the new table.  \r\n\r\nOr do you mean the path based table is already an iceberg table and the migrate is effectively reduced to a rename from `path` to `tablename` ?\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734869844/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734892295","html_url":"https://github.com/apache/iceberg/pull/1837#issuecomment-734892295","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1837","id":734892295,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDg5MjI5NQ==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T15:46:12Z","updated_at":"2020-11-27T15:46:12Z","author_association":"CONTRIBUTOR","body":"This is now ready for review, I'd appreciate a look from anyone who has time. @pvary @shardulm94 @rdblue @rdsr @marton-bod ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734892295/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734932589","html_url":"https://github.com/apache/iceberg/pull/1843#issuecomment-734932589","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1843","id":734932589,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDkzMjU4OQ==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T17:34:35Z","updated_at":"2020-11-27T17:34:35Z","author_association":"CONTRIBUTOR","body":"> > I was thinking about this a bit differently, rather than providing a full catalog I was thinking that we just have the various spark catalogs treat file types as namespaces. Then we just have them switch to returning Hadoop tables in the load table method.\r\n> \r\n> I was thinking this way originally as well. My first attempt at it made for weird interactions between `BaseMetastoreCatalog` and the individual catalogs. And a bunch of dupe code in the various `Catalog` impls. I am still not satisfied w/ this solution as the hadoop delegate catalog is a bit strange but at least it centralises the logic a bit.\r\n\r\nI re-read what you said earlier @RussellSpitzer and I realized I mis-read it. Your suggestion to just modify the spark catalogs may be easier but it still has a lot of duplication and would require a mechanism to handle `StagedTable`s with `HadoopTables`. I believe the change will end up very similar to my current patch. I can still do the change if you want to see the difference?\r\n\r\n*Note* regardless of how we implement the change `DROP` still causes a problem in session catalogs, drop never passes through our session catalog.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734932589/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734935681","html_url":"https://github.com/apache/iceberg/pull/1841#issuecomment-734935681","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1841","id":734935681,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDkzNTY4MQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T17:46:17Z","updated_at":"2020-11-27T17:46:17Z","author_association":"CONTRIBUTOR","body":"Thanks for remembering to do this! Always good to remove deprecated methods when we can.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734935681/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734956739","html_url":"https://github.com/apache/iceberg/pull/1841#issuecomment-734956739","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1841","id":734956739,"node_id":"MDEyOklzc3VlQ29tbWVudDczNDk1NjczOQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-27T19:09:56Z","updated_at":"2020-11-27T19:09:56Z","author_association":"CONTRIBUTOR","body":"> Thanks for remembering to do this! Always good to remove deprecated methods when we can.\r\n\r\nTo be honest, I forgot this one too, but stumbled upon the code accidentally ðŸ˜„ \r\nThanks for the merge!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/734956739/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735047043","html_url":"https://github.com/apache/iceberg/pull/1845#issuecomment-735047043","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1845","id":735047043,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTA0NzA0Mw==","user":{"login":"johnclara","id":9331049,"node_id":"MDQ6VXNlcjkzMzEwNDk=","avatar_url":"https://avatars.githubusercontent.com/u/9331049?v=4","gravatar_id":"","url":"https://api.github.com/users/johnclara","html_url":"https://github.com/johnclara","followers_url":"https://api.github.com/users/johnclara/followers","following_url":"https://api.github.com/users/johnclara/following{/other_user}","gists_url":"https://api.github.com/users/johnclara/gists{/gist_id}","starred_url":"https://api.github.com/users/johnclara/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/johnclara/subscriptions","organizations_url":"https://api.github.com/users/johnclara/orgs","repos_url":"https://api.github.com/users/johnclara/repos","events_url":"https://api.github.com/users/johnclara/events{/privacy}","received_events_url":"https://api.github.com/users/johnclara/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-28T06:37:28Z","updated_at":"2020-11-28T06:37:28Z","author_association":"NONE","body":"```\r\n--------\r\n- Test log for: Test testJoinTables[fileFormat=AVRO, engine=tez](org.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithHadoopCatalog)\r\n--------\r\nStdOut 2020-11-28T06:18:03,093 WARN  [pool-9513-thread-5] metastore.ObjectStore (ObjectStore.java:correctAutoStartMechanism(638)) - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\r\nStdErr OKStdErr \r\nStdErr OKStdErr \r\nStdErr Query ID = runner_20201128061803_6dae7077-6e85-4d25-9d87-4249df3e96c8StdErr \r\nStdErr Total jobs = 1StdErr \r\nStdErr Launching Job 1 out of 1StdErr \r\nStdOut 2020-11-28T06:18:03,390 INFO  [b8e5b932-98e6-466b-ab7d-059a462af581 Test worker] exec.Task (TezTask.java:execute(190)) - Subscribed to counters: [] for queryId: runner_20201128061803_6dae7077-6e85-4d25-9d87-4249df3e96c8\r\nStdOut 2020-11-28T06:18:03,391 INFO  [b8e5b932-98e6-466b-ab7d-059a462af581 Test worker] exec.Task (TezTask.java:ensureSessionHasResources(367)) - Tez session hasn't been created yet. Opening session\r\nStdOut 2020-11-28T06:18:03,694 ERROR [ServiceThread:DAGClientRPCServer] client.DAGClientServer (DAGClientServer.java:serviceStart(99)) - Failed to start DAGClientServer: \r\nStdOut org.apache.hadoop.metrics2.MetricsException: Metrics source RpcActivityForPort34189 already exists!\r\nStdOut \tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.metrics.RpcMetrics.create(RpcMetrics.java:87) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.Server.<init>(Server.java:2810) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:960) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:421) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:342) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:802) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.tez.dag.api.client.DAGClientServer.createServer(DAGClientServer.java:134) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.dag.api.client.DAGClientServer.serviceStart(DAGClientServer.java:82) [tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194) [hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.tez.dag.app.DAGAppMaster$ServiceWithDependency.start(DAGAppMaster.java:1865) [tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.dag.app.DAGAppMaster$ServiceThread.run(DAGAppMaster.java:1886) [tez-dag-0.9.1.jar:0.9.1]\r\nStdOut 2020-11-28T06:18:03,721 ERROR [DAGAppMaster Thread] client.LocalClient (LocalClient.java:run(331)) - Error starting DAGAppMaster\r\nStdOut org.apache.tez.dag.api.TezUncheckedException: org.apache.hadoop.metrics2.MetricsException: Metrics source RpcActivityForPort34189 already exists!\r\nStdOut \tat org.apache.tez.dag.api.client.DAGClientServer.serviceStart(DAGClientServer.java:100) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.tez.dag.app.DAGAppMaster$ServiceWithDependency.start(DAGAppMaster.java:1865) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.dag.app.DAGAppMaster$ServiceThread.run(DAGAppMaster.java:1886) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut Caused by: org.apache.hadoop.metrics2.MetricsException: Metrics source RpcActivityForPort34189 already exists!\r\nStdOut \tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.metrics.RpcMetrics.create(RpcMetrics.java:87) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.Server.<init>(Server.java:2810) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:960) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:421) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:342) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:802) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.tez.dag.api.client.DAGClientServer.createServer(DAGClientServer.java:134) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.dag.api.client.DAGClientServer.serviceStart(DAGClientServer.java:82) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \t... 3 more\r\nStdOut 2020-11-28T06:18:03,801 ERROR [b8e5b932-98e6-466b-ab7d-059a462af581 Test worker] exec.Task (TezTask.java:execute(284)) - Failed to execute tez graph.\r\nStdOut java.io.IOException: org.apache.tez.dag.api.TezUncheckedException: org.apache.hadoop.metrics2.MetricsException: Metrics source RpcActivityForPort34189 already exists!\r\nStdOut \tat org.apache.tez.client.LocalClient.startDAGAppMaster(LocalClient.java:276) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.client.LocalClient.submitApplication(LocalClient.java:136) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.client.TezClient.start(TezClient.java:440) ~[tez-api-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.hadoop.hive.ql.exec.tez.TezSessionState.startSessionAndContainers(TezSessionState.java:433) ~[hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:368) ~[hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.openInternal(TezSessionPoolSession.java:124) ~[hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:245) ~[hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.exec.tez.TezTask.ensureSessionHasResources(TezTask.java:368) ~[hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:195) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:205) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2664) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2335) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2011) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1709) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1703) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) [hive-exec-3.1.2-core.jar:3.1.2]\r\nStdOut \tat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:224) [hive-service-3.1.2.jar:3.1.2]\r\nStdOut \tat org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:263) [hive-service-3.1.2.jar:3.1.2]\r\nStdOut \tat org.apache.hive.service.cli.operation.Operation.run(Operation.java:247) [hive-service-3.1.2.jar:3.1.2]\r\nStdOut \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:541) [hive-service-3.1.2.jar:3.1.2]\r\nStdOut \tat org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:510) [hive-service-3.1.2.jar:3.1.2]\r\nStdOut \tat org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:267) [hive-service-3.1.2.jar:3.1.2]\r\nStdOut \tat org.apache.iceberg.mr.hive.TestHiveShell.executeStatement(TestHiveShell.java:129) [test/:?]\r\nStdOut \tat org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest.testJoinTables(HiveIcebergStorageHandlerBaseTest.java:258) [test/:?]\r\nStdOut \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_275]\r\nStdOut \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_275]\r\nStdOut \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_275]\r\nStdOut \tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_275]\r\nStdOut \tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.rules.RunRules.evaluate(RunRules.java:20) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner.run(ParentRunner.java:363) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.Suite.runChild(Suite.java:128) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.Suite.runChild(Suite.java:27) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) [junit-4.12.jar:4.12]\r\nStdOut \tat org.junit.runners.ParentRunner.run(ParentRunner.java:363) [junit-4.12.jar:4.12]\r\nStdOut \tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110) [gradle-testing-jvm-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58) [gradle-testing-jvm-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38) [gradle-testing-jvm-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62) [gradle-testing-jvm-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51) [gradle-testing-base-5.4.1.jar:5.4.1]\r\nStdOut \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_275]\r\nStdOut \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_275]\r\nStdOut \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_275]\r\nStdOut \tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_275]\r\nStdOut \tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat com.sun.proxy.$Proxy2.processTestClass(Unknown Source) [?:?]\r\nStdOut \tat org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118) [gradle-testing-base-5.4.1.jar:5.4.1]\r\nStdOut \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_275]\r\nStdOut \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_275]\r\nStdOut \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_275]\r\nStdOut \tat java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_275]\r\nStdOut \tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404) [gradle-messaging-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63) [gradle-base-services-5.4.1.jar:5.4.1]\r\nStdOut \tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46) [gradle-base-services-5.4.1.jar:5.4.1]\r\nStdOut \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_275]\r\nStdOut \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_275]\r\nStdOut \tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55) [gradle-base-services-5.4.1.jar:5.4.1]\r\nStdOut \tat java.lang.Thread.run(Thread.java:748) [?:1.8.0_275]\r\nStdOut Caused by: org.apache.tez.dag.api.TezUncheckedException: org.apache.hadoop.metrics2.MetricsException: Metrics source RpcActivityForPort34189 already exists!\r\nStdOut \tat org.apache.tez.dag.api.client.DAGClientServer.serviceStart(DAGClientServer.java:100) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.tez.dag.app.DAGAppMaster$ServiceWithDependency.start(DAGAppMaster.java:1865) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.dag.app.DAGAppMaster$ServiceThread.run(DAGAppMaster.java:1886) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut Caused by: org.apache.hadoop.metrics2.MetricsException: Metrics source RpcActivityForPort34189 already exists!\r\nStdOut \tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.metrics.RpcMetrics.create(RpcMetrics.java:87) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.Server.<init>(Server.java:2810) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:960) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:421) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:342) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:802) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.tez.dag.api.client.DAGClientServer.createServer(DAGClientServer.java:134) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.dag.api.client.DAGClientServer.serviceStart(DAGClientServer.java:82) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.hadoop.service.AbstractService.start(AbstractService.java:194) ~[hadoop-common-3.1.0.jar:?]\r\nStdOut \tat org.apache.tez.dag.app.DAGAppMaster$ServiceWithDependency.start(DAGAppMaster.java:1865) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdOut \tat org.apache.tez.dag.app.DAGAppMaster$ServiceThread.run(DAGAppMaster.java:1886) ~[tez-dag-0.9.1.jar:0.9.1]\r\nStdErr FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTaskStdErr \r\nStdOut 2020-11-28T06:18:03,803 ERROR [b8e5b932-98e6-466b-ab7d-059a462af581 Test worker] ql.Driver (SessionState.java:printError(1250)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n```\r\nNot sure if there's some port collision during the CI?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735047043/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735276676","html_url":"https://github.com/apache/iceberg/pull/1845#issuecomment-735276676","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1845","id":735276676,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTI3NjY3Ng==","user":{"login":"johnclara","id":9331049,"node_id":"MDQ6VXNlcjkzMzEwNDk=","avatar_url":"https://avatars.githubusercontent.com/u/9331049?v=4","gravatar_id":"","url":"https://api.github.com/users/johnclara","html_url":"https://github.com/johnclara","followers_url":"https://api.github.com/users/johnclara/followers","following_url":"https://api.github.com/users/johnclara/following{/other_user}","gists_url":"https://api.github.com/users/johnclara/gists{/gist_id}","starred_url":"https://api.github.com/users/johnclara/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/johnclara/subscriptions","organizations_url":"https://api.github.com/users/johnclara/orgs","repos_url":"https://api.github.com/users/johnclara/repos","events_url":"https://api.github.com/users/johnclara/events{/privacy}","received_events_url":"https://api.github.com/users/johnclara/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-28T18:57:33Z","updated_at":"2020-11-28T18:57:33Z","author_association":"NONE","body":"needs tests","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735276676/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735422251","html_url":"https://github.com/apache/iceberg/pull/1847#issuecomment-735422251","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1847","id":735422251,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTQyMjI1MQ==","user":{"login":"jun-he","id":12246263,"node_id":"MDQ6VXNlcjEyMjQ2MjYz","avatar_url":"https://avatars.githubusercontent.com/u/12246263?v=4","gravatar_id":"","url":"https://api.github.com/users/jun-he","html_url":"https://github.com/jun-he","followers_url":"https://api.github.com/users/jun-he/followers","following_url":"https://api.github.com/users/jun-he/following{/other_user}","gists_url":"https://api.github.com/users/jun-he/gists{/gist_id}","starred_url":"https://api.github.com/users/jun-he/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jun-he/subscriptions","organizations_url":"https://api.github.com/users/jun-he/orgs","repos_url":"https://api.github.com/users/jun-he/repos","events_url":"https://api.github.com/users/jun-he/events{/privacy}","received_events_url":"https://api.github.com/users/jun-he/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-29T16:44:27Z","updated_at":"2020-11-29T16:44:27Z","author_association":"COLLABORATOR","body":"This build failure is similar to https://github.com/apache/iceberg/issues/1817 (cc @pvary) with the error:\r\n```\r\n2020-11-29T08:18:49.5344738Z org.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithCustomCatalog > testCreatePartitionedTable[fileFormat=ORC, engine=tez] FAILED\r\n2020-11-29T08:18:49.5349046Z     java.lang.IllegalArgumentException: Failed to execute Hive query 'SELECT * FROM default.customers ORDER BY customer_id DESC': Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n2020-11-29T08:18:49.5350923Z \r\n2020-11-29T08:18:49.5351370Z         Caused by:\r\n2020-11-29T08:18:49.5353221Z         org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n```\r\nand \r\n```\r\n2020-11-29T08:28:35.2356807Z org.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithHiveCatalog > testScanTable[fileFormat=ORC, engine=tez] FAILED\r\n2020-11-29T08:28:35.2361330Z     java.lang.IllegalArgumentException: Failed to execute Hive query 'SELECT first_name, customer_id FROM default.customers ORDER BY customer_id DESC': Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n2020-11-29T08:28:35.2363464Z \r\n2020-11-29T08:28:35.2363962Z         Caused by:\r\n2020-11-29T08:28:35.2365843Z         org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735422251/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735503454","html_url":"https://github.com/apache/iceberg/pull/1849#issuecomment-735503454","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1849","id":735503454,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTUwMzQ1NA==","user":{"login":"jacques-n","id":183350,"node_id":"MDQ6VXNlcjE4MzM1MA==","avatar_url":"https://avatars.githubusercontent.com/u/183350?v=4","gravatar_id":"","url":"https://api.github.com/users/jacques-n","html_url":"https://github.com/jacques-n","followers_url":"https://api.github.com/users/jacques-n/followers","following_url":"https://api.github.com/users/jacques-n/following{/other_user}","gists_url":"https://api.github.com/users/jacques-n/gists{/gist_id}","starred_url":"https://api.github.com/users/jacques-n/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jacques-n/subscriptions","organizations_url":"https://api.github.com/users/jacques-n/orgs","repos_url":"https://api.github.com/users/jacques-n/repos","events_url":"https://api.github.com/users/jacques-n/events{/privacy}","received_events_url":"https://api.github.com/users/jacques-n/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T02:01:45Z","updated_at":"2020-11-30T02:01:45Z","author_association":"NONE","body":"@rdblue @aokolnychyi @RussellSpitzer would love your thoughts.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735503454/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735503548","html_url":"https://github.com/apache/iceberg/pull/1849#issuecomment-735503548","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1849","id":735503548,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTUwMzU0OA==","user":{"login":"jacques-n","id":183350,"node_id":"MDQ6VXNlcjE4MzM1MA==","avatar_url":"https://avatars.githubusercontent.com/u/183350?v=4","gravatar_id":"","url":"https://api.github.com/users/jacques-n","html_url":"https://github.com/jacques-n","followers_url":"https://api.github.com/users/jacques-n/followers","following_url":"https://api.github.com/users/jacques-n/following{/other_user}","gists_url":"https://api.github.com/users/jacques-n/gists{/gist_id}","starred_url":"https://api.github.com/users/jacques-n/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jacques-n/subscriptions","organizations_url":"https://api.github.com/users/jacques-n/orgs","repos_url":"https://api.github.com/users/jacques-n/repos","events_url":"https://api.github.com/users/jacques-n/events{/privacy}","received_events_url":"https://api.github.com/users/jacques-n/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T02:02:06Z","updated_at":"2020-11-30T02:02:06Z","author_association":"NONE","body":"CC @rymurr","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735503548/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735757555","html_url":"https://github.com/apache/iceberg/pull/1852#issuecomment-735757555","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1852","id":735757555,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTc1NzU1NQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T12:29:43Z","updated_at":"2020-11-30T12:29:43Z","author_association":"CONTRIBUTOR","body":"@rdblue, this is the Spark part for supporting DELETE statements.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735757555/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735768634","html_url":"https://github.com/apache/iceberg/pull/1850#issuecomment-735768634","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1850","id":735768634,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTc2ODYzNA==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T12:54:41Z","updated_at":"2020-11-30T12:54:41Z","author_association":"CONTRIBUTOR","body":"Have anyone know the reason for this unusual information?\r\n```\r\norg.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithHiveCatalog > testSelectDistinctFromTable[fileFormat=AVRO, engine=tez] FAILED\r\n    java.lang.IllegalArgumentException: Failed to execute Hive query 'select count(distinct(boolean_column)) from default.boolean_table_0': Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n\r\n        Caused by:\r\n        org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735768634/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735771376","html_url":"https://github.com/apache/iceberg/pull/1850#issuecomment-735771376","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1850","id":735771376,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTc3MTM3Ng==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T13:00:34Z","updated_at":"2020-11-30T13:00:34Z","author_association":"CONTRIBUTOR","body":"> Have anyone know the reason for this unusual information?\r\n> \r\n> ```\r\n> org.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithHiveCatalog > testSelectDistinctFromTable[fileFormat=AVRO, engine=tez] FAILED\r\n>     java.lang.IllegalArgumentException: Failed to execute Hive query 'select count(distinct(boolean_column)) from default.boolean_table_0': Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n> \r\n>         Caused by:\r\n>         org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask\r\n> ```\r\n\r\nYeah :(. This is fixed by #1824","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735771376/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735772488","html_url":"https://github.com/apache/iceberg/pull/1824#issuecomment-735772488","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1824","id":735772488,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTc3MjQ4OA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T13:02:55Z","updated_at":"2020-11-30T13:02:55Z","author_association":"CONTRIBUTOR","body":"@rdblue or @shardulm94: Could you please review? Without the patch we had 2/4 failures, with the patch we have 4 straight green runs. I think this fixes the flakiness, and it is a test only failure where multiple Metrics server is instantiated in a single JVM. HBase used the same fix in some of their tests, so I think it is safe to do here as well.\r\n\r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735772488/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735816916","html_url":"https://github.com/apache/iceberg/pull/1840#issuecomment-735816916","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1840","id":735816916,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTgxNjkxNg==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T14:25:09Z","updated_at":"2020-11-30T14:25:09Z","author_association":"CONTRIBUTOR","body":"@marton-bod, @lcspinter: Would you mind reviewing the changes?\r\n\r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735816916/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735825093","html_url":"https://github.com/apache/iceberg/pull/1847#issuecomment-735825093","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1847","id":735825093,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTgyNTA5Mw==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T14:38:59Z","updated_at":"2020-11-30T14:40:34Z","author_association":"CONTRIBUTOR","body":"@jun-he: How sure are we that this is the Heap size? Do we have a log for it (See: #1789 how to get the logs `<ICEBERG_ROOT>/build/testlogs/`)? The #1817 is about creating the Metrics RPC, which is fixed there.\r\n\r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735825093/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735877737","html_url":"https://github.com/apache/iceberg/pull/1525#issuecomment-735877737","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1525","id":735877737,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTg3NzczNw==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T16:02:26Z","updated_at":"2020-11-30T16:02:26Z","author_association":"MEMBER","body":"@aokolnychyi Ready for another look.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735877737/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735951276","html_url":"https://github.com/apache/iceberg/pull/1783#issuecomment-735951276","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1783","id":735951276,"node_id":"MDEyOklzc3VlQ29tbWVudDczNTk1MTI3Ng==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T18:09:36Z","updated_at":"2020-11-30T18:09:36Z","author_association":"CONTRIBUTOR","body":"FYI this has been updated based on our discussions last week. Build is still failing but once #1843 is closed it should be good to go.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/735951276/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736005115","html_url":"https://github.com/apache/iceberg/pull/1824#issuecomment-736005115","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1824","id":736005115,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjAwNTExNQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T19:53:15Z","updated_at":"2020-11-30T19:53:15Z","author_association":"CONTRIBUTOR","body":"Thanks @pvary! Looks good to me. Nice work tracking this down.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736005115/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736029875","html_url":"https://github.com/apache/iceberg/pull/1847#issuecomment-736029875","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1847","id":736029875,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjAyOTg3NQ==","user":{"login":"jun-he","id":12246263,"node_id":"MDQ6VXNlcjEyMjQ2MjYz","avatar_url":"https://avatars.githubusercontent.com/u/12246263?v=4","gravatar_id":"","url":"https://api.github.com/users/jun-he","html_url":"https://github.com/jun-he","followers_url":"https://api.github.com/users/jun-he/followers","following_url":"https://api.github.com/users/jun-he/following{/other_user}","gists_url":"https://api.github.com/users/jun-he/gists{/gist_id}","starred_url":"https://api.github.com/users/jun-he/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jun-he/subscriptions","organizations_url":"https://api.github.com/users/jun-he/orgs","repos_url":"https://api.github.com/users/jun-he/repos","events_url":"https://api.github.com/users/jun-he/events{/privacy}","received_events_url":"https://api.github.com/users/jun-he/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T20:40:13Z","updated_at":"2020-11-30T20:46:52Z","author_association":"COLLABORATOR","body":"@pvary Thanks for the info to get the test log.\r\nThe error seems related to `Metrics source already exists`. I will rebase the PR. thanks.\r\n\r\n```\r\n- Test log for: Test testCreatePartitionedTable[fileFormat=ORC, engine=tez](org.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithCustomCatalog)\r\n--------\r\nStdOut 2020-11-29T08:18:38,090 WARN  [pool-3263-thread-5] metastore.ObjectStore (ObjectStore.java:correctAutoStartMechanism(638)) - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\r\nStdErr OKStdErr \r\nStdErr Query ID = runner_20201129081838_936fe330-7888-41a9-9219-c7fa5ef3a5b8StdErr \r\nStdErr Total jobs = 1StdErr \r\nStdErr Launching Job 1 out of 1StdErr \r\nStdOut 2020-11-29T08:18:38,604 INFO  [337376db-6c07-463a-a7cf-213c9d8ec809 Test worker] exec.Task (TezTask.java:execute(190)) - Subscribed to counters: [] for queryId: runner_20201129081838_936fe330-7888-41a9-9219-c7fa5ef3a5b8\r\nStdOut 2020-11-29T08:18:38,605 INFO  [337376db-6c07-463a-a7cf-213c9d8ec809 Test worker] exec.Task (TezTask.java:ensureSessionHasResources(367)) - Tez session hasn't been created yet. Opening session\r\nStdOut 2020-11-29T08:18:38,777 ERROR [ServiceThread:DAGClientRPCServer] client.DAGClientServer (DAGClientServer.java:serviceStart(99)) - Failed to start DAGClientServer: \r\nStdOut org.apache.hadoop.metrics2.MetricsException: Metrics source RpcActivityForPort41173 already exists!\r\n```\r\n```\r\n- Test log for: Test testScanTable[fileFormat=ORC, engine=tez](org.apache.iceberg.mr.hive.TestHiveIcebergStorageHandlerWithHiveCatalog)\r\n--------\r\nStdOut 2020-11-29T08:28:18,310 WARN  [pool-6496-thread-4] metastore.ObjectStore (ObjectStore.java:correctAutoStartMechanism(638)) - datanucleus.autoStartMechanismMode is set to unsupported value null . Setting it to value: ignored\r\nStdErr OKStdErr \r\nStdErr Query ID = runner_20201129082818_d6cdb374-f476-4a20-8593-48bbd4c859d8StdErr \r\nStdErr Total jobs = 1StdErr \r\nStdErr Launching Job 1 out of 1StdErr \r\nStdOut 2020-11-29T08:28:18,476 INFO  [4be58fef-bdce-46ea-9f60-91894aae50a7 Test worker] exec.Task (TezTask.java:execute(190)) - Subscribed to counters: [] for queryId: runner_20201129082818_d6cdb374-f476-4a20-8593-48bbd4c859d8\r\nStdOut 2020-11-29T08:28:18,477 INFO  [4be58fef-bdce-46ea-9f60-91894aae50a7 Test worker] exec.Task (TezTask.java:ensureSessionHasResources(367)) - Tez session hasn't been created yet. Opening session\r\nStdOut 2020-11-29T08:28:18,691 ERROR [ServiceThread:DAGClientRPCServer] client.DAGClientServer (DAGClientServer.java:serviceStart(99)) - Failed to start DAGClientServer: \r\nStdOut org.apache.hadoop.metrics2.MetricsException: Metrics source RpcActivityForPort37535 already exists!\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736029875/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736039647","html_url":"https://github.com/apache/iceberg/pull/1847#issuecomment-736039647","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1847","id":736039647,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjAzOTY0Nw==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T20:57:28Z","updated_at":"2020-11-30T20:57:28Z","author_association":"CONTRIBUTOR","body":"@jun-he: the fix for the Metrics error is merged. You might want to rebase and run the tests again.\r\nThanks, Peter ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736039647/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736056836","html_url":"https://github.com/apache/iceberg/pull/1848#issuecomment-736056836","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1848","id":736056836,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjA1NjgzNg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T21:07:44Z","updated_at":"2020-11-30T21:07:44Z","author_association":"CONTRIBUTOR","body":"Thanks, @grantatspothero! Would you like to follow up with support for the other Avro timestamp?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736056836/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736072454","html_url":"https://github.com/apache/iceberg/pull/1852#issuecomment-736072454","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1852","id":736072454,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjA3MjQ1NA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T21:39:22Z","updated_at":"2020-11-30T21:39:22Z","author_association":"CONTRIBUTOR","body":"@aokolnychyi, why not add the tests in this PR as well?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736072454/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736076743","html_url":"https://github.com/apache/iceberg/pull/1837#issuecomment-736076743","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1837","id":736076743,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjA3Njc0Mw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T21:48:54Z","updated_at":"2020-11-30T21:48:54Z","author_association":"CONTRIBUTOR","body":"Thanks, @massdosage! This looks great. I'll merge and redeploy the docs.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736076743/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736106939","html_url":"https://github.com/apache/iceberg/pull/1848#issuecomment-736106939","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1848","id":736106939,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjEwNjkzOQ==","user":{"login":"grantatspothero","id":43971820,"node_id":"MDQ6VXNlcjQzOTcxODIw","avatar_url":"https://avatars.githubusercontent.com/u/43971820?v=4","gravatar_id":"","url":"https://api.github.com/users/grantatspothero","html_url":"https://github.com/grantatspothero","followers_url":"https://api.github.com/users/grantatspothero/followers","following_url":"https://api.github.com/users/grantatspothero/following{/other_user}","gists_url":"https://api.github.com/users/grantatspothero/gists{/gist_id}","starred_url":"https://api.github.com/users/grantatspothero/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/grantatspothero/subscriptions","organizations_url":"https://api.github.com/users/grantatspothero/orgs","repos_url":"https://api.github.com/users/grantatspothero/repos","events_url":"https://api.github.com/users/grantatspothero/events{/privacy}","received_events_url":"https://api.github.com/users/grantatspothero/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-11-30T23:00:57Z","updated_at":"2020-11-30T23:00:57Z","author_association":"CONTRIBUTOR","body":"@rdblue yup that is the plan, just wanted to get the bugfix out first. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736106939/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736153262","html_url":"https://github.com/apache/iceberg/issues/1831#issuecomment-736153262","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1831","id":736153262,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjE1MzI2Mg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T01:17:08Z","updated_at":"2020-12-01T01:17:08Z","author_association":"CONTRIBUTOR","body":"I think @pvary is right: the property may not be set when the write happens and that causes the serde to get reset to the generic one. I think the right thing to do is to configure either hive-site.xml (or make sure the config is consistent in Spark) or to set the table property, which will ensure Hive support is always enabled.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736153262/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736153641","html_url":"https://github.com/apache/iceberg/issues/1817#issuecomment-736153641","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1817","id":736153641,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjE1MzY0MQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T01:18:15Z","updated_at":"2020-12-01T01:18:15Z","author_association":"CONTRIBUTOR","body":"Fixed by #1824","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736153641/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736188272","html_url":"https://github.com/apache/iceberg/pull/1847#issuecomment-736188272","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1847","id":736188272,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjE4ODI3Mg==","user":{"login":"jun-he","id":12246263,"node_id":"MDQ6VXNlcjEyMjQ2MjYz","avatar_url":"https://avatars.githubusercontent.com/u/12246263?v=4","gravatar_id":"","url":"https://api.github.com/users/jun-he","html_url":"https://github.com/jun-he","followers_url":"https://api.github.com/users/jun-he/followers","following_url":"https://api.github.com/users/jun-he/following{/other_user}","gists_url":"https://api.github.com/users/jun-he/gists{/gist_id}","starred_url":"https://api.github.com/users/jun-he/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jun-he/subscriptions","organizations_url":"https://api.github.com/users/jun-he/orgs","repos_url":"https://api.github.com/users/jun-he/repos","events_url":"https://api.github.com/users/jun-he/events{/privacy}","received_events_url":"https://api.github.com/users/jun-he/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T03:09:58Z","updated_at":"2020-12-01T03:09:58Z","author_association":"COLLABORATOR","body":"@rdblue @aokolnychyi Can you please review it for the issue #1846 (i.e. `./gradlew clean build` fails due to `Expiring Daemon because JVM heap space is exhausted`)? Thanks.\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736188272/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736373699","html_url":"https://github.com/apache/iceberg/issues/1838#issuecomment-736373699","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1838","id":736373699,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjM3MzY5OQ==","user":{"login":"liudi1184","id":30306636,"node_id":"MDQ6VXNlcjMwMzA2NjM2","avatar_url":"https://avatars.githubusercontent.com/u/30306636?v=4","gravatar_id":"","url":"https://api.github.com/users/liudi1184","html_url":"https://github.com/liudi1184","followers_url":"https://api.github.com/users/liudi1184/followers","following_url":"https://api.github.com/users/liudi1184/following{/other_user}","gists_url":"https://api.github.com/users/liudi1184/gists{/gist_id}","starred_url":"https://api.github.com/users/liudi1184/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/liudi1184/subscriptions","organizations_url":"https://api.github.com/users/liudi1184/orgs","repos_url":"https://api.github.com/users/liudi1184/repos","events_url":"https://api.github.com/users/liudi1184/events{/privacy}","received_events_url":"https://api.github.com/users/liudi1184/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T10:08:25Z","updated_at":"2020-12-01T10:08:25Z","author_association":"NONE","body":"> @liudi1184: Thanks for reporting this!\r\n> @lcspinter found the issue and fixed it. See: Hive: Fix casting bugs in Tez joins (#1740)\r\n> \r\n> You might want to build your own Iceberg until we can release a version which contains the fix ðŸ˜¢\r\n\r\nThank you. It's done","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736373699/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736523661","html_url":"https://github.com/apache/iceberg/pull/1825#issuecomment-736523661","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1825","id":736523661,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjUyMzY2MQ==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T12:33:16Z","updated_at":"2020-12-01T12:33:16Z","author_association":"CONTRIBUTOR","body":"Thanks for the code review @rdblue , made some updates based on your comments.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736523661/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736552366","html_url":"https://github.com/apache/iceberg/pull/1852#issuecomment-736552366","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1852","id":736552366,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjU1MjM2Ng==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T13:30:32Z","updated_at":"2020-12-01T13:30:32Z","author_association":"CONTRIBUTOR","body":"@rdblue, trying to reduce the review scope. Want me to include the Iceberg part too?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736552366/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736575340","html_url":"https://github.com/apache/iceberg/pull/1852#issuecomment-736575340","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1852","id":736575340,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjU3NTM0MA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T14:10:28Z","updated_at":"2020-12-01T14:10:41Z","author_association":"CONTRIBUTOR","body":"Updated this PR. @RussellSpitzer @rdblue, let me know if you want me to include the other parts including tests.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736575340/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736576495","html_url":"https://github.com/apache/iceberg/pull/1825#issuecomment-736576495","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1825","id":736576495,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjU3NjQ5NQ==","user":{"login":"jacques-n","id":183350,"node_id":"MDQ6VXNlcjE4MzM1MA==","avatar_url":"https://avatars.githubusercontent.com/u/183350?v=4","gravatar_id":"","url":"https://api.github.com/users/jacques-n","html_url":"https://github.com/jacques-n","followers_url":"https://api.github.com/users/jacques-n/followers","following_url":"https://api.github.com/users/jacques-n/following{/other_user}","gists_url":"https://api.github.com/users/jacques-n/gists{/gist_id}","starred_url":"https://api.github.com/users/jacques-n/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jacques-n/subscriptions","organizations_url":"https://api.github.com/users/jacques-n/orgs","repos_url":"https://api.github.com/users/jacques-n/repos","events_url":"https://api.github.com/users/jacques-n/events{/privacy}","received_events_url":"https://api.github.com/users/jacques-n/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T14:12:35Z","updated_at":"2020-12-01T14:12:35Z","author_association":"NONE","body":"I agree with @rdblue wrt end of day/period.\n\nWriting '#2019' should mean what is the last value for that period. This should actually be consistent across the board. Give me the most recent commit that matches this declaration. Even if you give seconds, if there two commits within that second, you should get the most recent one. The key here is that the person is giving a year or day reference, not a time reference. Whatever internal resolution we use should not influence what they mean when they express a particular period.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736576495/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736579194","html_url":"https://github.com/apache/iceberg/pull/1825#issuecomment-736579194","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1825","id":736579194,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjU3OTE5NA==","user":{"login":"jacques-n","id":183350,"node_id":"MDQ6VXNlcjE4MzM1MA==","avatar_url":"https://avatars.githubusercontent.com/u/183350?v=4","gravatar_id":"","url":"https://api.github.com/users/jacques-n","html_url":"https://github.com/jacques-n","followers_url":"https://api.github.com/users/jacques-n/followers","following_url":"https://api.github.com/users/jacques-n/following{/other_user}","gists_url":"https://api.github.com/users/jacques-n/gists{/gist_id}","starred_url":"https://api.github.com/users/jacques-n/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jacques-n/subscriptions","organizations_url":"https://api.github.com/users/jacques-n/orgs","repos_url":"https://api.github.com/users/jacques-n/repos","events_url":"https://api.github.com/users/jacques-n/events{/privacy}","received_events_url":"https://api.github.com/users/jacques-n/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T14:17:07Z","updated_at":"2020-12-01T14:17:07Z","author_association":"NONE","body":"To put another way. The table identifier is expressing a pattern. Pick the most recent commit matching a pattern using a \"startswith\" behavior.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736579194/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736584986","html_url":"https://github.com/apache/iceberg/pull/1843#issuecomment-736584986","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1843","id":736584986,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjU4NDk4Ng==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T14:26:09Z","updated_at":"2020-12-01T14:26:09Z","author_association":"CONTRIBUTOR","body":"I have added `ALTER` tests and have skipped `DROP` when using SessionCatalog. I have also used @RussellSpitzer's suggestion for implementation. Its a lot cleaner and smaller but means we cant perform certain operations on filesystem tables: some transactions, renames and list by namespace.\r\n\r\nI am not a huge fan of identifying a path based table by simply checking if there is a `/` in the name. Any thoughts on how to make this less fragile?\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736584986/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736597087","html_url":"https://github.com/apache/iceberg/pull/1825#issuecomment-736597087","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1825","id":736597087,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjU5NzA4Nw==","user":{"login":"rymurr","id":2022305,"node_id":"MDQ6VXNlcjIwMjIzMDU=","avatar_url":"https://avatars.githubusercontent.com/u/2022305?v=4","gravatar_id":"","url":"https://api.github.com/users/rymurr","html_url":"https://github.com/rymurr","followers_url":"https://api.github.com/users/rymurr/followers","following_url":"https://api.github.com/users/rymurr/following{/other_user}","gists_url":"https://api.github.com/users/rymurr/gists{/gist_id}","starred_url":"https://api.github.com/users/rymurr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rymurr/subscriptions","organizations_url":"https://api.github.com/users/rymurr/orgs","repos_url":"https://api.github.com/users/rymurr/repos","events_url":"https://api.github.com/users/rymurr/events{/privacy}","received_events_url":"https://api.github.com/users/rymurr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T14:46:14Z","updated_at":"2020-12-01T14:46:14Z","author_association":"CONTRIBUTOR","body":"end of period wins by a vote of 2-1. ;-) fixed and added some tests","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736597087/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736603260","html_url":"https://github.com/apache/iceberg/pull/1843#issuecomment-736603260","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1843","id":736603260,"node_id":"MDEyOklzc3VlQ29tbWVudDczNjYwMzI2MA==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-12-01T14:53:20Z","updated_at":"2020-12-01T14:53:20Z","author_association":"MEMBER","body":"I'm not sure we want to allow those other operations, since I'm not sure they actually have a lot of meaning. Like a rename on a path based table in a filesystem without renames means a full copy of the entire dataset. So it may be best to treat file based path's as static or at least immovable.\r\n\r\nI think we probably could identify path based tables based on the namespace too, like \"parquet or file\" or something. I'll take a look today","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/736603260/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]