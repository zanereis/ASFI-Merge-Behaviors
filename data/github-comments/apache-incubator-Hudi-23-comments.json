[{"url":"https://api.github.com/repos/apache/hudi/issues/comments/504802414","html_url":"https://github.com/apache/hudi/issues/757#issuecomment-504802414","issue_url":"https://api.github.com/repos/apache/hudi/issues/757","id":504802414,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDgwMjQxNA==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T00:31:22Z","updated_at":"2019-06-24T00:31:22Z","author_association":"CONTRIBUTOR","body":"@bvaradar Yeah, I have found this problem, I didn't use NonpartitionedKeyGenerator to insert data to non-partition table. So that it has a \"default\" path, Hive can not read non-partition table with a \"default\" path.\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/504802414/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/504807854","html_url":"https://github.com/apache/hudi/issues/543#issuecomment-504807854","issue_url":"https://api.github.com/repos/apache/hudi/issues/543","id":504807854,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDgwNzg1NA==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T01:25:22Z","updated_at":"2019-06-24T01:25:22Z","author_association":"CONTRIBUTOR","body":"add a comment in https://issues.apache.org/jira/browse/HUDI-91","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/504807854/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/504868305","html_url":"https://github.com/apache/hudi/issues/736#issuecomment-504868305","issue_url":"https://api.github.com/repos/apache/hudi/issues/736","id":504868305,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDg2ODMwNQ==","user":{"login":"eisig","id":1745057,"node_id":"MDQ6VXNlcjE3NDUwNTc=","avatar_url":"https://avatars.githubusercontent.com/u/1745057?v=4","gravatar_id":"","url":"https://api.github.com/users/eisig","html_url":"https://github.com/eisig","followers_url":"https://api.github.com/users/eisig/followers","following_url":"https://api.github.com/users/eisig/following{/other_user}","gists_url":"https://api.github.com/users/eisig/gists{/gist_id}","starred_url":"https://api.github.com/users/eisig/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eisig/subscriptions","organizations_url":"https://api.github.com/users/eisig/orgs","repos_url":"https://api.github.com/users/eisig/repos","events_url":"https://api.github.com/users/eisig/events{/privacy}","received_events_url":"https://api.github.com/users/eisig/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T05:55:14Z","updated_at":"2019-06-24T05:55:14Z","author_association":"CONTRIBUTOR","body":"@cdmikechen \r\nDo you have set hive metastore uris in the hive config file?\r\nor export an env\r\n`export HOODIE_ENV_hive_DOT_metastore_DOT_uris=\"thrift://xx.xx.xx.xx:9083\"`\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/504868305/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/504884083","html_url":"https://github.com/apache/hudi/issues/736#issuecomment-504884083","issue_url":"https://api.github.com/repos/apache/hudi/issues/736","id":504884083,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNDg4NDA4Mw==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T06:59:10Z","updated_at":"2019-06-24T06:59:10Z","author_association":"CONTRIBUTOR","body":"@eisig \r\nhave set. I run shell with all hive jars like that:\r\n```bash\r\n#java -cp $HOODIE_HIVE_UBER_JAR:${HADOOP_HIVE_JARS}:${HADOOP_CONF_DIR} com.uber.hoodie.hive.HiveSyncTool \"$@\"\r\njava -cp $HOODIE_HIVE_UBER_JAR:${HADOOP_HIVE_JARS}:${HADOOP_CONF_DIR}:${HIVE_HOME}/lib/* com.uber.hoodie.hive.HiveSyncTool \"$@\"\r\n```\r\nand it can run.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/504884083/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505012458","html_url":"https://github.com/apache/hudi/pull/753#issuecomment-505012458","issue_url":"https://api.github.com/repos/apache/hudi/issues/753","id":505012458,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTAxMjQ1OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T13:35:31Z","updated_at":"2019-06-24T13:35:31Z","author_association":"MEMBER","body":"sg. In parallel, let me try to fully understand these gaps. overall lg otherwise","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505012458/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505075130","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-505075130","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":505075130,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTA3NTEzMA==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T16:06:23Z","updated_at":"2019-06-24T16:06:23Z","author_association":"NONE","body":"Hey, can I be added to the slack channel?  - amarnath.venkataswamy@gmail.com","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505075130/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505172535","html_url":"https://github.com/apache/hudi/issues/143#issuecomment-505172535","issue_url":"https://api.github.com/repos/apache/hudi/issues/143","id":505172535,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTE3MjUzNQ==","user":{"login":"shinray","id":6334845,"node_id":"MDQ6VXNlcjYzMzQ4NDU=","avatar_url":"https://avatars.githubusercontent.com/u/6334845?v=4","gravatar_id":"","url":"https://api.github.com/users/shinray","html_url":"https://github.com/shinray","followers_url":"https://api.github.com/users/shinray/followers","following_url":"https://api.github.com/users/shinray/following{/other_user}","gists_url":"https://api.github.com/users/shinray/gists{/gist_id}","starred_url":"https://api.github.com/users/shinray/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shinray/subscriptions","organizations_url":"https://api.github.com/users/shinray/orgs","repos_url":"https://api.github.com/users/shinray/repos","events_url":"https://api.github.com/users/shinray/events{/privacy}","received_events_url":"https://api.github.com/users/shinray/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T20:41:26Z","updated_at":"2019-06-24T20:41:26Z","author_association":"NONE","body":"Please add shinray_kuo@intuit.com to slack! Already joined the mailing list.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505172535/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505175375","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505175375","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505175375,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTE3NTM3NQ==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T20:49:17Z","updated_at":"2019-06-24T20:49:17Z","author_association":"NONE","body":"Hive sync is failing when i try to create hoodie data set using deltastreamer without any partition column.\r\n\r\ncreating the output files with default folder after base path.\r\n\r\n/stock_ticks_cow_no_part1/default/5f41f4c6-a334-4e73-a8b9-03303335c0df-0_0-20-21_20190624093814.parquet\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505175375/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505197469","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505197469","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505197469,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTE5NzQ2OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T21:57:59Z","updated_at":"2019-06-24T21:57:59Z","author_association":"CONTRIBUTOR","body":"@amaranathv \r\n\r\nApart from changing the partition extractor class, you would need to change the keyGeneratorClass for non-partitioned table.\r\n\r\nUse this param \"--key-generator-class com.uber.hoodie.NonpartitionedKeyGenerator\" as part of DeltaStreamer command-line execution.\r\n\r\nAlso, ensure we have the following configs defined in the properties file used by delta-streamer:\r\n\r\nhoodie.datasource.write.keygenerator.class=com.uber.hoodie.NonpartitionedKeyGenerator\r\nhoodie.datasource.hive_sync.partition_extractor_class=com.uber.hoodie.hive.NonPartitionedExtractor\r\nWe will eventually remove the DeltaStreamer CLI and rely on the properties config for uniform handling.\r\n\r\nThanks,\r\nBalaji.V","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505197469/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505198243","html_url":"https://github.com/apache/hudi/issues/757#issuecomment-505198243","issue_url":"https://api.github.com/repos/apache/hudi/issues/757","id":505198243,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTE5ODI0Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T22:00:42Z","updated_at":"2019-06-24T22:00:42Z","author_association":"CONTRIBUTOR","body":"Thanks @cdmikechen \r\n\r\nAlso add the below config in the properties file. \r\n\r\nhoodie.datasource.write.keygenerator.class=com.uber.hoodie.NonpartitionedKeyGenerator\r\n\r\nWe will eventually remove the DeltaStreamer CLI and rely on the properties config for uniform handling.\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505198243/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505198534","html_url":"https://github.com/apache/hudi/issues/757#issuecomment-505198534","issue_url":"https://api.github.com/repos/apache/hudi/issues/757","id":505198534,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTE5ODUzNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T22:01:46Z","updated_at":"2019-06-24T22:01:46Z","author_association":"CONTRIBUTOR","body":"@amaranathv :  FYI ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505198534/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505215189","html_url":"https://github.com/apache/hudi/pull/760#issuecomment-505215189","issue_url":"https://api.github.com/repos/apache/hudi/issues/760","id":505215189,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTIxNTE4OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-24T23:12:23Z","updated_at":"2019-06-24T23:12:23Z","author_association":"CONTRIBUTOR","body":"@vinothchandar @n3nash : Please review and merge. Accompanying doc change : https://github.com/apache/incubator-hudi/pull/761","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505215189/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505524394","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505524394","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505524394,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTUyNDM5NA==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-25T16:38:06Z","updated_at":"2019-06-26T12:44:23Z","author_association":"NONE","body":"\r\n\r\n\r\nIs hive sync needed for non partition table?\r\n\r\nWithout hive sync i can able to query the table but if i try to re-run same command with second json file as in docker example I am unable to run the command.\r\n\r\nGetting below error :\r\n```\r\n[Stage 1:=============================>                             (1 + 1) / 2]19/06/25 11:31:46 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2, dbsld0070.uhc.com, executor 2): java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.deleteCleanedFiles(HoodieCopyOnWriteTable.java:343)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.lambda$rollback$dd200898$1(HoodieCopyOnWriteTable.java:373)\r\n        at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)\r\n        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n19/06/25 11:31:46 ERROR TaskSetManager: Task 1 in stage 1.0 failed 4 times; aborting job\r\nException in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 5, dbsld0070.uhc.com, executor 1): java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.deleteCleanedFiles(HoodieCopyOnWriteTable.java:343)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.lambda$rollback$dd200898$1(HoodieCopyOnWriteTable.java:373)\r\n        at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)\r\n        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n        at scala.Option.foreach(Option.scala:257)\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n        at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n        at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n        at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361)\r\n        at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.rollback(HoodieCopyOnWriteTable.java:376)\r\n        at com.uber.hoodie.HoodieWriteClient.doRollbackAndGetStats(HoodieWriteClient.java:880)\r\n        at com.uber.hoodie.HoodieWriteClient.rollbackInternal(HoodieWriteClient.java:958)\r\n        at com.uber.hoodie.HoodieWriteClient.rollback(HoodieWriteClient.java:769)\r\n        at com.uber.hoodie.HoodieWriteClient.rollbackInflightCommits(HoodieWriteClient.java:1180)\r\n        at com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:1046)\r\n        at com.uber.hoodie.HoodieWriteClient.startCommit(HoodieWriteClient.java:1039)\r\n        at com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer.sync(HoodieDeltaStreamer.java:265)\r\n        at com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer.main(HoodieDeltaStreamer.java:469)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:780)\r\n        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)\r\n        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)\r\n        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)\r\n        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.deleteCleanedFiles(HoodieCopyOnWriteTable.java:343)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.lambda$rollback$dd200898$1(HoodieCopyOnWriteTable.java:373)\r\n        at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)\r\n        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505524394/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505697259","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505697259","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505697259,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTY5NzI1OQ==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T02:56:24Z","updated_at":"2019-06-26T12:45:29Z","author_association":"NONE","body":"Can you please  anyone respond to this?\r\n\r\nThis might be some simple configuration or rebuild required.\r\n\r\nI am getting below error while doing the hive synch.\r\n```\r\n2019-06-25 21:54:31,374 INFO  [main] hive.HoodieHiveClient (HoodieHiveClient.java:getPartitionsWrittenToSince(556)) - Last commit time synced is not known, listing all partitions in /********/stock_ticks_cow_no_part4,FS :com.mapr.fs.MapRFileSystem@23c30a20\r\n2019-06-25 21:54:31,377 INFO  [main] hive.HiveSyncTool (HiveSyncTool.java:syncHoodieTable(107)) - Storage partitions scan complete. Found 1\r\nException in thread \"main\" com.uber.hoodie.hive.HoodieHiveSyncException: Failed to sync partitions for table stock_ticks_cow_no_part4\r\n        at com.uber.hoodie.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:169)\r\n        at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:109)\r\n        at com.uber.hoodie.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:68)\r\n        at com.uber.hoodie.hive.HiveSyncTool.main(HiveSyncTool.java:189)\r\nCaused by: java.lang.IllegalArgumentException: Partition path  is not in the form yyyy/mm/dd\r\n        at com.uber.hoodie.hive.SlashEncodedDayPartitionValueExtractor.extractPartitionValuesInPath(SlashEncodedDayPartitionValueExtractor.java:54)\r\n        at com.uber.hoodie.hive.HoodieHiveClient.getPartitionEvents(HoodieHiveClient.java:219)\r\n        at com.uber.hoodie.hive.HiveSyncTool.syncPartitions(HiveSyncTool.java:160)\r\n        ... 3 more\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505697259/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505714207","html_url":"https://github.com/apache/hudi/issues/754#issuecomment-505714207","issue_url":"https://api.github.com/repos/apache/hudi/issues/754","id":505714207,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTcxNDIwNw==","user":{"login":"xuFabius","id":6744900,"node_id":"MDQ6VXNlcjY3NDQ5MDA=","avatar_url":"https://avatars.githubusercontent.com/u/6744900?v=4","gravatar_id":"","url":"https://api.github.com/users/xuFabius","html_url":"https://github.com/xuFabius","followers_url":"https://api.github.com/users/xuFabius/followers","following_url":"https://api.github.com/users/xuFabius/following{/other_user}","gists_url":"https://api.github.com/users/xuFabius/gists{/gist_id}","starred_url":"https://api.github.com/users/xuFabius/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xuFabius/subscriptions","organizations_url":"https://api.github.com/users/xuFabius/orgs","repos_url":"https://api.github.com/users/xuFabius/repos","events_url":"https://api.github.com/users/xuFabius/events{/privacy}","received_events_url":"https://api.github.com/users/xuFabius/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T04:34:22Z","updated_at":"2019-06-26T04:34:22Z","author_association":"NONE","body":"Yes. My hive version is 2.3.5 .  it seems HoodieJavaApp can't work on hive 2 ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505714207/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505775902","html_url":"https://github.com/apache/hudi/issues/754#issuecomment-505775902","issue_url":"https://api.github.com/repos/apache/hudi/issues/754","id":505775902,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTc3NTkwMg==","user":{"login":"xuFabius","id":6744900,"node_id":"MDQ6VXNlcjY3NDQ5MDA=","avatar_url":"https://avatars.githubusercontent.com/u/6744900?v=4","gravatar_id":"","url":"https://api.github.com/users/xuFabius","html_url":"https://github.com/xuFabius","followers_url":"https://api.github.com/users/xuFabius/followers","following_url":"https://api.github.com/users/xuFabius/following{/other_user}","gists_url":"https://api.github.com/users/xuFabius/gists{/gist_id}","starred_url":"https://api.github.com/users/xuFabius/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xuFabius/subscriptions","organizations_url":"https://api.github.com/users/xuFabius/orgs","repos_url":"https://api.github.com/users/xuFabius/repos","events_url":"https://api.github.com/users/xuFabius/events{/privacy}","received_events_url":"https://api.github.com/users/xuFabius/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T08:29:34Z","updated_at":"2019-06-26T08:29:34Z","author_association":"NONE","body":"HI cdmikechen, this problem had been resolved. hive-2.3.* is built on parquet-1.8.1.  I had rebuild hive-2.3.5 on parquet-1.8.3 and HoodieJavaApp runs well.\r\n\r\nthanks ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505775902/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505859438","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505859438","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505859438,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTg1OTQzOA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T12:46:36Z","updated_at":"2019-06-26T12:46:36Z","author_association":"MEMBER","body":"@amaranathv is the following taking effect? I still see the SlashEncodedDayPartitionValueExtractor being in use? `hoodie.datasource.hive_sync.partition_extractor_class=com.uber.hoodie.hive.NonPartitionedExtractor`\r\n\r\nAlso did you get past the issue pasted above the hive sync one?  ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505859438/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505917279","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505917279","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505917279,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTkxNzI3OQ==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T15:07:31Z","updated_at":"2019-06-26T15:07:31Z","author_association":"NONE","body":"I didn't get your answer on this one.\r\nhoodie.datasource.hive_sync.partition_extractor_class=com.uber.hoodie.hive.NonPartitionedExtractor\r\n\r\nI am setting above property in property file.\r\nNo.I am unable to complete the hive sync successfully after first ingestion.\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505917279/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505921884","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505921884","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505921884,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTkyMTg4NA==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T15:18:43Z","updated_at":"2019-06-26T15:18:43Z","author_association":"NONE","body":"got it..I added this class and it works.\r\n-partition-value-extractor  com.uber.hoodie.hive.NonPartitionedExtractor","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505921884/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505922865","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505922865","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505922865,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTkyMjg2NQ==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T15:21:02Z","updated_at":"2019-06-26T15:21:02Z","author_association":"NONE","body":"hive sync is complete but my second ingestion to same table fails with same error.\r\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.deleteCleanedFiles(HoodieCopyOnWriteTable.java:343)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.lambda$rollback$dd200898$1(HoodieCopyOnWriteTable.java:373)\r\n        at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)\r\n        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505922865/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505924285","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505924285","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505924285,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTkyNDI4NQ==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T15:24:32Z","updated_at":"2019-06-26T15:24:32Z","author_association":"NONE","body":"If i change the target base path and table name it is working but not for the existing table.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505924285/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505926016","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505926016","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505926016,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTkyNjAxNg==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T15:28:46Z","updated_at":"2019-06-26T15:28:46Z","author_association":"NONE","body":"![image](https://user-images.githubusercontent.com/51413224/60193344-1ec11080-97fd-11e9-9067-903a87868cd7.png)\r\n![image](https://user-images.githubusercontent.com/51413224/60193367-2c769600-97fd-11e9-9612-c57748222495.png)\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505926016/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505945917","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505945917","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505945917,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTk0NTkxNw==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T16:19:43Z","updated_at":"2019-06-26T17:24:14Z","author_association":"NONE","body":"```\r\nJob aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 10, dbsls0406.uhc.com, executor 1): java.lang.IllegalArgumentException: Can not create a Path from an empty string \tat org.apache.hadoop.fs.Path.checkPathArg(Path.java:130) \tat org.apache.hadoop.fs.Path.<init>(Path.java:138) \tat org.apache.hadoop.fs.Path.<init>(Path.java:92) \tat com.uber.hoodie.common.table.view.AbstractTableFileSystemView.lambda$ensurePartitionLoadedCorrectly$5(AbstractTableFileSystemView.java:219) \tat java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1660) \tat com.uber.hoodie.common.table.view.AbstractTableFileSystemView.ensurePartitionLoadedCorrectly(AbstractTableFileSystemView.java:211) \tat com.uber.hoodie.common.table.view.AbstractTableFileSystemView.getLatestDataFilesBeforeOrOn(AbstractTableFileSystemView.java:350) \tat com.uber.hoodie.common.table.view.PriorityBasedFileSystemView.execute(PriorityBasedFileSystemView.java:99) \tat com.uber.hoodie.common.table.view.PriorityBasedFileSystemView.getLatestDataFilesBeforeOrOn(PriorityBasedFileSystemView.java:132) \tat com.uber.hoodie.index.bloom.HoodieBloomIndex.lambda$loadInvolvedFiles$350ff700$1(HoodieBloomIndex.java:241) \tat org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125) \tat org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125) \tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) \tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) \tat scala.collection.Iterator$class.foreach(Iterator.scala:893) \tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336) \tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59) \tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104) \tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48) \tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310) \tat scala.collection.AbstractIterator.to(Iterator.scala:1336) \tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302) \tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336) \tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289) \tat scala.collection.AbstractIterator.toArray(Iterator.scala:1336) \tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936) \tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936) \tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069) \tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069) \tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) \tat org.apache.spark.scheduler.Task.run(Task.scala:108) \tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) \tat java.lang.Thread.run(Thread.java:748)  Driver stacktrace:\r\n--\r\n```\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505945917/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505946200","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505946200","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505946200,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTk0NjIwMA==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T16:20:28Z","updated_at":"2019-06-26T16:20:28Z","author_association":"NONE","body":"This is latest error logs.Hopefully it will helpful to trace the issue.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505946200/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505969534","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-505969534","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":505969534,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNTk2OTUzNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T17:25:10Z","updated_at":"2019-06-26T17:25:10Z","author_association":"MEMBER","body":"@amaranathv thanks.. will take a closer look. Must be some config issue again.. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/505969534/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506015011","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506015011","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506015011,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjAxNTAxMQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T19:34:48Z","updated_at":"2019-06-26T19:34:48Z","author_association":"CONTRIBUTOR","body":"@amaranathv : We have fixed this issue in a PR https://github.com/apache/incubator-hudi/pull/760 This will be merged sometime today after some more safety tests.  For your POC, can you use this patch and ensure everything works end-to-end for you for non-partitioned table.\r\n\r\nThanks\r\nBalaji.V","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506015011/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506035729","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506035729","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506035729,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjAzNTcyOQ==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T20:38:44Z","updated_at":"2019-06-26T20:38:44Z","author_association":"NONE","body":"Is there any environment vaaibles needed to apply the patch?\r\nI am getting below error.\r\n\r\n\r\n\r\n$ curl -L https://github.com/apache/incubator-hudi/pull/760.patch > /tmp/760.patch\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100   147    0   147    0     0    272      0 --:--:-- --:--:-- --:--:--   272\r\n  0     0    0  7145    0     0   5527      0 --:--:--  0:00:01 --:--:-- 40367\r\navenka23@dbsld0031:/mapr/user/avenka23/hoodie/incubator-hudi\r\n$ git apply /tmp/760.patch\r\nerror: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/deltastreamer/DeltaSync.java: No such file or directory\r\nerror: patch failed: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/deltastreamer/HoodieDeltaStreamer.java:24\r\nerror: hoodie-utilities/src/main/java/com/uber/hoodie/utilities/deltastreamer/HoodieDeltaStreamer.java: patch does not apply\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506035729/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506038513","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506038513","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506038513,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjAzODUxMw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T20:47:30Z","updated_at":"2019-06-26T20:47:30Z","author_association":"MEMBER","body":"Easiest would be just try out master branch, by building it yourself.. if not, you can just `git cherry-pick` that commit alone to your branch.. Thats way more reliable","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506038513/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506046407","html_url":"https://github.com/apache/hudi/pull/739#issuecomment-506046407","issue_url":"https://api.github.com/repos/apache/hudi/issues/739","id":506046407,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjA0NjQwNw==","user":{"login":"tokaaron","id":49795237,"node_id":"MDQ6VXNlcjQ5Nzk1MjM3","avatar_url":"https://avatars.githubusercontent.com/u/49795237?v=4","gravatar_id":"","url":"https://api.github.com/users/tokaaron","html_url":"https://github.com/tokaaron","followers_url":"https://api.github.com/users/tokaaron/followers","following_url":"https://api.github.com/users/tokaaron/following{/other_user}","gists_url":"https://api.github.com/users/tokaaron/gists{/gist_id}","starred_url":"https://api.github.com/users/tokaaron/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tokaaron/subscriptions","organizations_url":"https://api.github.com/users/tokaaron/orgs","repos_url":"https://api.github.com/users/tokaaron/repos","events_url":"https://api.github.com/users/tokaaron/events{/privacy}","received_events_url":"https://api.github.com/users/tokaaron/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-26T21:10:29Z","updated_at":"2019-06-26T21:10:29Z","author_association":"NONE","body":"Generated a PR against that branch:\r\n\r\nhttps://github.com/apache/incubator-hudi/pull/762","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506046407/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506145450","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506145450","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506145450,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjE0NTQ1MA==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T03:48:48Z","updated_at":"2019-06-27T03:48:48Z","author_association":"NONE","body":"I am getting same issue again.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506145450/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506150296","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506150296","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506150296,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjE1MDI5Ng==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T04:18:21Z","updated_at":"2019-06-27T04:18:21Z","author_association":"NONE","body":"I am getting new error after rerun with different output path.\r\n\r\n19/06/26 22:59:58 ERROR FileSystemViewHandler: Got runtime exception servicing request partition=&basepath=%2Fdatalake%2Foptum%2Foptuminsight%2Fudw%2Fhive%2Fwarehouse%2Fstock_ticks_cow_no_part10&fileid=1349e964-8d2a-4928-a127-341cc33f37b8-0&lastinstantts=20190626224954&timelinehash=f3ed651effeb6c16de70cd22ee8d49758b4bd189b97d219a43a2d2a867713734\r\nio.javalin.BadRequestResponse: Query parameter 'partition' with value '' cannot be null or empty\r\n        at io.javalin.validation.TypedValidator.getOrThrow(Validator.kt:25)\r\n        at com.uber.hoodie.timeline.service.FileSystemViewHandler.lambda$registerDataFilesAPI$3(FileSystemViewHandler.java:174)\r\n        at com.uber.hoodie.timeline.service.FileSystemViewHandler$ViewHandler.handle(FileSystemViewHandler.java:326)\r\n        at io.javalin.security.SecurityUtil.noopAccessManager(SecurityUtil.kt:22)\r\n        at io.javalin.Javalin.lambda$addHandler$0(Javalin.java:606)\r\n        at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:46)\r\n        at io.javalin.core.JavalinServlet$service$2$1.invoke(JavalinServlet.kt:17)\r\n        at io.javalin.core.JavalinServlet$service$1.invoke(JavalinServlet.kt:143)\r\n        at io.javalin.core.JavalinServlet$service$2.invoke(JavalinServlet.kt:41)\r\n        at io.javalin.core.JavalinServlet.service(JavalinServlet.kt:107)\r\n        at io.javalin.core.util.JettyServerUtil$initialize$httpHandler$1.doHandle(JettyServerUtil.kt:72)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203)\r\n        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480)\r\n        at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1668)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201)\r\n        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247)\r\n        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144)\r\n        at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:61)\r\n        at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174)\r\n        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\r\n        at org.eclipse.jetty.server.Server.handle(Server.java:502)\r\n        at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370)\r\n        at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267)\r\n        at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305)\r\n        at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\r\n        at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\r\n        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:333)\r\n        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:310)\r\n        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:168)\r\n        at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:126)\r\n        at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:366)\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:765)\r\n        at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:683)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n19/06/26 22:59:58 WARN FileSystemViewHandler: Syncing view as client passed last known instant 20190626225950 as last known instant but server has the folling timeline :[[20190626224954__clean__COMPLETED], [20190626224954__commit__COMPLETED]]\r\n19/06/26 22:59:58 WARN IncrementalTimelineSyncFileSystemView: Incremental Sync of timeline is turned off or deemed unsafe. Will revert to full syncing\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506150296/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506176347","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506176347","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506176347,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjE3NjM0Nw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T04:46:48Z","updated_at":"2019-06-27T04:46:48Z","author_association":"CONTRIBUTOR","body":"@amaranathv : Can you disable embedded timeline-server to unblock:\r\n\r\nset hoodie.embed.timeline.server=false\r\nand also remove the config hoodie.filesystem.view.type\r\n\r\nI will look into why this is happening\r\n\r\nBalaji.V","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506176347/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506196520","html_url":"https://github.com/apache/hudi/pull/762#issuecomment-506196520","issue_url":"https://api.github.com/repos/apache/hudi/issues/762","id":506196520,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjE5NjUyMA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T05:31:21Z","updated_at":"2019-06-27T05:31:21Z","author_association":"MEMBER","body":"Thanks @tokaaron . Will ping the mailing list once I put up the PR for the overall changes.. Would love your take on it as well","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506196520/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506196624","html_url":"https://github.com/apache/hudi/pull/739#issuecomment-506196624","issue_url":"https://api.github.com/repos/apache/hudi/issues/739","id":506196624,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjE5NjYyNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T05:31:50Z","updated_at":"2019-06-27T05:31:50Z","author_association":"MEMBER","body":"Closing in favor of #762  ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506196624/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506364162","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506364162","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506364162,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjM2NDE2Mg==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T14:14:26Z","updated_at":"2019-06-27T14:14:26Z","author_association":"NONE","body":"I didn't have hoodie.filesystem.view.type in property  file","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506364162/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506368641","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506368641","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506368641,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjM2ODY0MQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T14:25:20Z","updated_at":"2019-06-27T14:25:20Z","author_association":"CONTRIBUTOR","body":"> I didn't have hoodie.filesystem.view.type in property file\r\n\r\nDid you try setting  hoodie.embed.timeline.server=false ?\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506368641/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506374174","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506374174","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506374174,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjM3NDE3NA==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T14:38:55Z","updated_at":"2019-06-27T14:38:55Z","author_association":"NONE","body":"I am not getting any error but i didn't see any output for the second ingestion.Empty stages in Spark History UI.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506374174/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506376636","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506376636","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506376636,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjM3NjYzNg==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T14:44:50Z","updated_at":"2019-06-27T14:44:50Z","author_association":"NONE","body":"I can setup a webex if you need to debug.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506376636/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506467973","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-506467973","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":506467973,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjQ2Nzk3Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T18:52:13Z","updated_at":"2019-06-27T18:52:13Z","author_association":"MEMBER","body":"Looks similar to https://issues.apache.org/jira/browse/HUDI-116 , although 0.4.7 should have the fix already.. Seems like a tagging issue again? are you able to trim the data down and give me a reproducible case?\r\n\r\nAnd for context, you upgraded to 0.4.7 and it started failing right away?  were things working fine in prior version (can you share that?) \r\n\r\n@bvaradar @n3nash as well..  ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506467973/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506487456","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506487456","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506487456,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjQ4NzQ1Ng==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T19:55:02Z","updated_at":"2019-06-27T19:55:02Z","author_association":"CONTRIBUTOR","body":"Yeah, it is hard to understand with this setup. Please setup a webex either tonight or tomorrow ( between 9 p.m - 11 p.m PST works for me) and email me.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506487456/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506499260","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-506499260","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":506499260,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjQ5OTI2MA==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-27T20:30:04Z","updated_at":"2019-06-27T20:30:04Z","author_association":"NONE","body":"sure..","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506499260/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506599672","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-506599672","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":506599672,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjU5OTY3Mg==","user":{"login":"jackwang2","id":26425937,"node_id":"MDQ6VXNlcjI2NDI1OTM3","avatar_url":"https://avatars.githubusercontent.com/u/26425937?v=4","gravatar_id":"","url":"https://api.github.com/users/jackwang2","html_url":"https://github.com/jackwang2","followers_url":"https://api.github.com/users/jackwang2/followers","following_url":"https://api.github.com/users/jackwang2/following{/other_user}","gists_url":"https://api.github.com/users/jackwang2/gists{/gist_id}","starred_url":"https://api.github.com/users/jackwang2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jackwang2/subscriptions","organizations_url":"https://api.github.com/users/jackwang2/orgs","repos_url":"https://api.github.com/users/jackwang2/repos","events_url":"https://api.github.com/users/jackwang2/events{/privacy}","received_events_url":"https://api.github.com/users/jackwang2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-28T04:54:24Z","updated_at":"2019-06-28T05:03:02Z","author_association":"NONE","body":"@vinothchandar Thanks for looking this. Yes, seems the version of 0.4.7 is not compatible with 0.4.6, and it throws exception like below, but I use the new version 0.4.7 and write data to a new location to skip the `incompatible issue`.\r\n\r\n```\r\n19/06/25 12:37:51 INFO HoodieCommitArchiveLog: Archiving instants [[20190618193930_cleanCOMPLETED], [20190618201752cleanCOMPLETED], [20190618204851cleanCOMPLETED], [20190618212741cleanCOMPLETED], [20190618215856cleanCOMPLETED], [20190618223733cleanCOMPLETED], [20190618231034cleanCOMPLETED], [20190618234543cleanCOMPLETED], [20190619002337cleanCOMPLETED], [20190619005833cleanCOMPLETED], [20190619013959clean_COMPLETED]][WARNING] Avro: Invalid default for field hoodieCommitMetadata: \"null\" not a [\"null\",{\"type\":\"record\",\"name\":\"HoodieCommitMetadata\",\"namespace\":\"com.uber.hoodie.avro.model\",\"fields\":[{\"name\":\"partitionToWriteStats\",\"type\":[\"null\",{\"type\":\"map\",\"values\":{\"type\":\"array\",\"items\":{\"type\":\"record\",\"name\":\"HoodieWriteStat\",\"fields\":[{\"name\":\"fileId\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"path\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"prevCommit\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"numWrites\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numDeletes\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numUpdateWrites\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalWriteBytes\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalWriteErrors\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"partitionPath\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"totalLogRecords\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalLogFiles\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalUpdatedRecordsCompacted\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numInserts\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalLogBlocks\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalCorruptLogBlock\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalRollbackBlocks\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"fileSizeInBytes\",\"type\":[\"null\",\"long\"],\"default\":null}]}},\"avro.java.string\":\"String\"}]},{\"name\":\"extraMetadata\",\"type\":[\"null\",{\"type\":\"map\",\"values\":{\"type\":\"string\",\"avro.java.string\":\"String\"},\"avro.java.string\":\"String\"}]}]}][WARNING] Avro: Invalid default for field hoodieCleanMetadata: \"null\" not a [\"null\",{\"type\":\"record\",\"name\":\"HoodieCleanMetadata\",\"namespace\":\"com.uber.hoodie.avro.model\",\"fields\":[{\"name\":\"startCleanTime\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"timeTakenInMillis\",\"type\":\"long\"},{\"name\":\"totalFilesDeleted\",\"type\":\"int\"},{\"name\":\"earliestCommitToRetain\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"partitionMetadata\",\"type\":{\"type\":\"map\",\"values\":{\"type\":\"record\",\"name\":\"HoodieCleanPartitionMetadata\",\"fields\":[{\"name\":\"partitionPath\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"policy\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"deletePathPatterns\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}},{\"name\":\"successDeleteFiles\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}},{\"name\":\"failedDeleteFiles\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}}]},\"avro.java.string\":\"String\"}}]}][WARNING] Avro: Invalid default for field hoodieCompactionMetadata: \"null\" not a [\"null\",{\"type\":\"record\",\"name\":\"HoodieCompactionMetadata\",\"namespace\":\"com.uber.hoodie.avro.model\",\"fields\":[{\"name\":\"partitionToCompactionWriteStats\",\"type\":[\"null\",{\"type\":\"map\",\"values\":{\"type\":\"array\",\"items\":{\"type\":\"record\",\"name\":\"HoodieCompactionWriteStat\",\"fields\":[{\"name\":\"partitionPath\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}]},{\"name\":\"totalLogRecords\",\"type\":[\"null\",\"long\"]},{\"name\":\"totalLogFiles\",\"type\":[\"null\",\"long\"]},{\"name\":\"totalUpdatedRecordsCompacted\",\"type\":[\"null\",\"long\"]},{\"name\":\"hoodieWriteStat\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"HoodieWriteStat\",\"fields\":[{\"name\":\"fileId\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"path\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"prevCommit\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"numWrites\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numDeletes\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numUpdateWrites\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalWriteBytes\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalWriteErrors\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"partitionPath\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"totalLogRecords\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalLogFiles\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalUpdatedRecordsCompacted\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numInserts\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalLogBlocks\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalCorruptLogBlock\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalRollbackBlocks\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"fileSizeInBytes\",\"type\":[\"null\",\"long\"],\"default\":null}]}]}]}},\"avro.java.string\":\"String\"}]}]}][WARNING] Avro: Invalid default for field hoodieRollbackMetadata: \"null\" not a [\"null\",{\"type\":\"record\",\"name\":\"HoodieRollbackMetadata\",\"namespace\":\"com.uber.hoodie.avro.model\",\"fields\":[{\"name\":\"startRollbackTime\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"timeTakenInMillis\",\"type\":\"long\"},{\"name\":\"totalFilesDeleted\",\"type\":\"int\"},{\"name\":\"commitsRollback\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}},{\"name\":\"partitionMetadata\",\"type\":{\"type\":\"map\",\"values\":{\"type\":\"record\",\"name\":\"HoodieRollbackPartitionMetadata\",\"fields\":[{\"name\":\"partitionPath\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"successDeleteFiles\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}},{\"name\":\"failedDeleteFiles\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}}]},\"avro.java.string\":\"String\"}}]}][WARNING] Avro: Invalid default for field hoodieSavePointMetadata: \"null\" not a [\"null\",{\"type\":\"record\",\"name\":\"HoodieSavepointMetadata\",\"namespace\":\"com.uber.hoodie.avro.model\",\"fields\":[{\"name\":\"savepointedBy\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"savepointedAt\",\"type\":\"long\"},{\"name\":\"comments\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"partitionMetadata\",\"type\":{\"type\":\"map\",\"values\":{\"type\":\"record\",\"name\":\"HoodieSavepointPartitionMetadata\",\"fields\":[{\"name\":\"partitionPath\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"savepointDataFile\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}}]},\"avro.java.string\":\"String\"}}]}]\r\n19/06/25 12:37:51 INFO HoodieCommitArchiveLog: Wrapper schema {\"type\":\"record\",\"name\":\"HoodieArchivedMetaEntry\",\"namespace\":\"com.uber.hoodie.avro.model\",\"fields\":[{\"name\":\"hoodieCommitMetadata\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"HoodieCommitMetadata\",\"fields\":[{\"name\":\"partitionToWriteStats\",\"type\":[\"null\",{\"type\":\"map\",\"values\":{\"type\":\"array\",\"items\":{\"type\":\"record\",\"name\":\"HoodieWriteStat\",\"fields\":[{\"name\":\"fileId\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"path\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"prevCommit\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"numWrites\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numDeletes\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numUpdateWrites\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalWriteBytes\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalWriteErrors\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"partitionPath\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}],\"default\":null},{\"name\":\"totalLogRecords\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalLogFiles\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalUpdatedRecordsCompacted\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"numInserts\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalLogBlocks\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalCorruptLogBlock\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"totalRollbackBlocks\",\"type\":[\"null\",\"long\"],\"default\":null},{\"name\":\"fileSizeInBytes\",\"type\":[\"null\",\"long\"],\"default\":null}]}},\"avro.java.string\":\"String\"}]},{\"name\":\"extraMetadata\",\"type\":[\"null\",{\"type\":\"map\",\"values\":{\"type\":\"string\",\"avro.java.string\":\"String\"},\"avro.java.string\":\"String\"}]}]}],\"default\":\"null\"},{\"name\":\"hoodieCleanMetadata\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"HoodieCleanMetadata\",\"fields\":[{\"name\":\"startCleanTime\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"timeTakenInMillis\",\"type\":\"long\"},{\"name\":\"totalFilesDeleted\",\"type\":\"int\"},{\"name\":\"earliestCommitToRetain\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"partitionMetadata\",\"type\":{\"type\":\"map\",\"values\":{\"type\":\"record\",\"name\":\"HoodieCleanPartitionMetadata\",\"fields\":[{\"name\":\"partitionPath\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"policy\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"deletePathPatterns\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}},{\"name\":\"successDeleteFiles\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}},{\"name\":\"failedDeleteFiles\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}}]},\"avro.java.string\":\"String\"}}]}],\"default\":\"null\"},{\"name\":\"hoodieCompactionMetadata\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"HoodieCompactionMetadata\",\"fields\":[{\"name\":\"partitionToCompactionWriteStats\",\"type\":[\"null\",{\"type\":\"map\",\"values\":{\"type\":\"array\",\"items\":{\"type\":\"record\",\"name\":\"HoodieCompactionWriteStat\",\"fields\":[{\"name\":\"partitionPath\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}]},{\"name\":\"totalLogRecords\",\"type\":[\"null\",\"long\"]},{\"name\":\"totalLogFiles\",\"type\":[\"null\",\"long\"]},{\"name\":\"totalUpdatedRecordsCompacted\",\"type\":[\"null\",\"long\"]},{\"name\":\"hoodieWriteStat\",\"type\":[\"null\",\"HoodieWriteStat\"]}]}},\"avro.java.string\":\"String\"}]}]}],\"default\":\"null\"},{\"name\":\"hoodieRollbackMetadata\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"HoodieRollbackMetadata\",\"fields\":[{\"name\":\"startRollbackTime\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"timeTakenInMillis\",\"type\":\"long\"},{\"name\":\"totalFilesDeleted\",\"type\":\"int\"},{\"name\":\"commitsRollback\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}},{\"name\":\"partitionMetadata\",\"type\":{\"type\":\"map\",\"values\":{\"type\":\"record\",\"name\":\"HoodieRollbackPartitionMetadata\",\"fields\":[{\"name\":\"partitionPath\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"successDeleteFiles\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}},{\"name\":\"failedDeleteFiles\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}}]},\"avro.java.string\":\"String\"}}]}],\"default\":\"null\"},{\"name\":\"hoodieSavePointMetadata\",\"type\":[\"null\",{\"type\":\"record\",\"name\":\"HoodieSavepointMetadata\",\"fields\":[{\"name\":\"savepointedBy\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"savepointedAt\",\"type\":\"long\"},{\"name\":\"comments\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"partitionMetadata\",\"type\":{\"type\":\"map\",\"values\":{\"type\":\"record\",\"name\":\"HoodieSavepointPartitionMetadata\",\"fields\":[{\"name\":\"partitionPath\",\"type\":{\"type\":\"string\",\"avro.java.string\":\"String\"}},{\"name\":\"savepointDataFile\",\"type\":{\"type\":\"array\",\"items\":{\"type\":\"string\",\"avro.java.string\":\"String\"}}}]},\"avro.java.string\":\"String\"}}]}],\"default\":\"null\"},{\"name\":\"commitTime\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}]},{\"name\":\"actionType\",\"type\":[\"null\",{\"type\":\"string\",\"avro.java.string\":\"String\"}]}]}\r\nException in thread \"main\" java.lang.reflect.InvocationTargetExceptionat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:65)\r\n\tat org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)\r\nCaused by: com.uber.hoodie.exception.HoodieCommitException: Failed to archive commitsat com.uber.hoodie.io.HoodieCommitArchiveLog.archive(HoodieCommitArchiveLog.java:254)\r\n\tat com.uber.hoodie.io.HoodieCommitArchiveLog.archiveIfRequired(HoodieCommitArchiveLog.java:117)\r\n\tat com.uber.hoodie.HoodieWriteClient.commit(HoodieWriteClient.java:531)\r\n\tat com.uber.hoodie.HoodieWriteClient.commit(HoodieWriteClient.java:491)\r\n\tat com.uber.hoodie.HoodieWriteClient.commit(HoodieWriteClient.java:482)\r\n\tat com.uber.hoodie.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:155)\r\n\tat com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:91)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\r\n\tat com.vungle.malygos.dedup.SparkMain$$anonfun$run$2.apply(SparkMain.scala:133)\r\n\tat com.vungle.malygos.dedup.SparkMain$$anonfun$run$2.apply(SparkMain.scala:99)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\r\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\r\n\tat com.vungle.malygos.dedup.SparkMain$.run(SparkMain.scala:99)\r\n\tat com.vungle.malygos.BoilerplateSparkMain$class.main(Boilerplate.scala:861)\r\n\tat com.vungle.malygos.dedup.SparkMain$.main(SparkMain.scala:22)\r\n\tat com.vungle.malygos.dedup.SparkMain.main(SparkMain.scala)... 6 more\r\nCaused by: java.io.IOException: Not an Avro data fileat org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:50)\r\n\tat com.uber.hoodie.common.util.AvroUtils.deserializeAvroMetadata(AvroUtils.java:215)\r\n\tat com.uber.hoodie.io.HoodieCommitArchiveLog.convertToAvroRecord(HoodieCommitArchiveLog.java:279)\r\n\tat com.uber.hoodie.io.HoodieCommitArchiveLog.archive(HoodieCommitArchiveLog.java:247)... 41 more\r\n19/06/25 12:37:52 INFO SparkContext: Invoking stop() from shutdown hook\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506599672/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506662764","html_url":"https://github.com/apache/hudi/issues/767#issuecomment-506662764","issue_url":"https://api.github.com/repos/apache/hudi/issues/767","id":506662764,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjY2Mjc2NA==","user":{"login":"hotienvu","id":1747423,"node_id":"MDQ6VXNlcjE3NDc0MjM=","avatar_url":"https://avatars.githubusercontent.com/u/1747423?v=4","gravatar_id":"","url":"https://api.github.com/users/hotienvu","html_url":"https://github.com/hotienvu","followers_url":"https://api.github.com/users/hotienvu/followers","following_url":"https://api.github.com/users/hotienvu/following{/other_user}","gists_url":"https://api.github.com/users/hotienvu/gists{/gist_id}","starred_url":"https://api.github.com/users/hotienvu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hotienvu/subscriptions","organizations_url":"https://api.github.com/users/hotienvu/orgs","repos_url":"https://api.github.com/users/hotienvu/repos","events_url":"https://api.github.com/users/hotienvu/events{/privacy}","received_events_url":"https://api.github.com/users/hotienvu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-28T09:07:17Z","updated_at":"2019-06-28T09:07:17Z","author_association":"CONTRIBUTOR","body":"Are you running the latest version? This issue has been fixed here? https://github.com/apache/incubator-hudi/pull/758","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506662764/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506676651","html_url":"https://github.com/apache/hudi/issues/767#issuecomment-506676651","issue_url":"https://api.github.com/repos/apache/hudi/issues/767","id":506676651,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjY3NjY1MQ==","user":{"login":"NadeemMehraj","id":52071328,"node_id":"MDQ6VXNlcjUyMDcxMzI4","avatar_url":"https://avatars.githubusercontent.com/u/52071328?v=4","gravatar_id":"","url":"https://api.github.com/users/NadeemMehraj","html_url":"https://github.com/NadeemMehraj","followers_url":"https://api.github.com/users/NadeemMehraj/followers","following_url":"https://api.github.com/users/NadeemMehraj/following{/other_user}","gists_url":"https://api.github.com/users/NadeemMehraj/gists{/gist_id}","starred_url":"https://api.github.com/users/NadeemMehraj/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/NadeemMehraj/subscriptions","organizations_url":"https://api.github.com/users/NadeemMehraj/orgs","repos_url":"https://api.github.com/users/NadeemMehraj/repos","events_url":"https://api.github.com/users/NadeemMehraj/events{/privacy}","received_events_url":"https://api.github.com/users/NadeemMehraj/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-28T09:53:22Z","updated_at":"2019-06-28T09:53:22Z","author_association":"NONE","body":"Thanks @hotienvu \r\nI cloned the repo again and the issue does not occur","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506676651/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506768425","html_url":"https://github.com/apache/hudi/issues/767#issuecomment-506768425","issue_url":"https://api.github.com/repos/apache/hudi/issues/767","id":506768425,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjc2ODQyNQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-28T15:07:39Z","updated_at":"2019-06-28T15:07:39Z","author_association":"MEMBER","body":"Thanks @hotienvu .. yes we are stabilizing things (landed 3 large changes recently) and will make a stable 0.4.8-SNAPSHOT or the first apache release in few weeks.. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506768425/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506824234","html_url":"https://github.com/apache/hudi/issues/766#issuecomment-506824234","issue_url":"https://api.github.com/repos/apache/hudi/issues/766","id":506824234,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjgyNDIzNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-28T17:56:46Z","updated_at":"2019-06-28T17:56:46Z","author_association":"MEMBER","body":"#760 should fix this? This was introduced by the long running/continuous mode in deltastreamer. There is a JIRA for making the demo run as a test. any takers? :) \r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506824234/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506825140","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-506825140","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":506825140,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjgyNTE0MA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-28T17:59:30Z","updated_at":"2019-06-28T17:59:30Z","author_association":"MEMBER","body":"@jackwang2 there should not be any compatibility issues between 0.4.6 and 0.4.7.  \r\nSo there are two issues you are facing? \r\n- Do you still run into the `No value present` issue? \r\n- I will look at this new stack trace later today.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506825140/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506909781","html_url":"https://github.com/apache/hudi/pull/753#issuecomment-506909781","issue_url":"https://api.github.com/repos/apache/hudi/issues/753","id":506909781,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjkwOTc4MQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-29T00:10:59Z","updated_at":"2019-06-29T00:10:59Z","author_association":"MEMBER","body":"@cdmikechen have not been able to spend time on this much.. any updates from your testing? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506909781/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506909921","html_url":"https://github.com/apache/hudi/pull/753#issuecomment-506909921","issue_url":"https://api.github.com/repos/apache/hudi/issues/753","id":506909921,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjkwOTkyMQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-29T00:12:24Z","updated_at":"2019-06-29T00:12:24Z","author_association":"MEMBER","body":"This timestamp handling has dogged us for a while. :( if you understand it fully, can you please put up a HIP with your suggestions.. we can then divvy up the actual impl.. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506909921/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506913191","html_url":"https://github.com/apache/hudi/pull/760#issuecomment-506913191","issue_url":"https://api.github.com/repos/apache/hudi/issues/760","id":506913191,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNjkxMzE5MQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-06-29T00:49:11Z","updated_at":"2019-06-29T00:49:11Z","author_association":"MEMBER","body":"tested demo and confirmed it exits properly.. Hive queries all work.. Merging","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/506913191/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507110137","html_url":"https://github.com/apache/hudi/pull/753#issuecomment-507110137","issue_url":"https://api.github.com/repos/apache/hudi/issues/753","id":507110137,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzExMDEzNw==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-01T04:19:45Z","updated_at":"2019-07-01T04:19:45Z","author_association":"CONTRIBUTOR","body":"@vinothchandar add a pr: https://github.com/apache/incubator-hudi/pull/770\r\nYou can see this pr, the problem of timestamp and decimal data types should be solved in spark. But timestamp can not be solved in Hive, because Hive don't support parquet logical_type.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507110137/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507123633","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-507123633","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":507123633,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzEyMzYzMw==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-01T05:43:58Z","updated_at":"2019-07-01T05:43:58Z","author_association":"CONTRIBUTOR","body":"@amaranathv \r\nyou can see this pr: https://github.com/apache/incubator-hudi/pull/771.\r\nI fixed `Can not create a Path from an empty string` error","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507123633/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507362869","html_url":"https://github.com/apache/hudi/issues/772#issuecomment-507362869","issue_url":"https://api.github.com/repos/apache/hudi/issues/772","id":507362869,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzM2Mjg2OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-01T17:52:49Z","updated_at":"2019-07-01T17:52:49Z","author_association":"CONTRIBUTOR","body":"@hotienvu : Agree timeline-service should not be a fat-jar. We had made a fat jar to run some standalone tests but did not get it reverted. \r\nWe are reworking our bundling strategy in this PR https://github.com/apache/incubator-hudi/pull/751 to give a proper definition of what should be and what should not be bundled. I have already addressed timeline-service in that PR.\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507362869/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507366779","html_url":"https://github.com/apache/hudi/issues/769#issuecomment-507366779","issue_url":"https://api.github.com/repos/apache/hudi/issues/769","id":507366779,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzM2Njc3OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-01T18:04:56Z","updated_at":"2019-07-01T18:04:56Z","author_association":"CONTRIBUTOR","body":"@chandresh-pancholi : Can you tell me where did you find this ?  I am not able to find this link from https://hudi.incubator.apache.org/","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507366779/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507485800","html_url":"https://github.com/apache/hudi/issues/772#issuecomment-507485800","issue_url":"https://api.github.com/repos/apache/hudi/issues/772","id":507485800,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzQ4NTgwMA==","user":{"login":"hotienvu","id":1747423,"node_id":"MDQ6VXNlcjE3NDc0MjM=","avatar_url":"https://avatars.githubusercontent.com/u/1747423?v=4","gravatar_id":"","url":"https://api.github.com/users/hotienvu","html_url":"https://github.com/hotienvu","followers_url":"https://api.github.com/users/hotienvu/followers","following_url":"https://api.github.com/users/hotienvu/following{/other_user}","gists_url":"https://api.github.com/users/hotienvu/gists{/gist_id}","starred_url":"https://api.github.com/users/hotienvu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hotienvu/subscriptions","organizations_url":"https://api.github.com/users/hotienvu/orgs","repos_url":"https://api.github.com/users/hotienvu/repos","events_url":"https://api.github.com/users/hotienvu/events{/privacy}","received_events_url":"https://api.github.com/users/hotienvu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-02T01:48:11Z","updated_at":"2019-07-02T01:48:11Z","author_association":"CONTRIBUTOR","body":"Good to know! Thanks @bvaradar.\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507485800/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507854896","html_url":"https://github.com/apache/hudi/issues/773#issuecomment-507854896","issue_url":"https://api.github.com/repos/apache/hudi/issues/773","id":507854896,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzg1NDg5Ng==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-02T21:40:00Z","updated_at":"2019-07-02T21:40:00Z","author_association":"CONTRIBUTOR","body":"@ovj will be submitting this PR shortly\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507854896/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507884124","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-507884124","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":507884124,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzg4NDEyNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-02T23:45:20Z","updated_at":"2019-07-02T23:45:20Z","author_association":"CONTRIBUTOR","body":"Thanks @cdmikechen for your contribution.  Can you let us know if it works end-to-end for non-partitioned table after this PR ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507884124/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507887116","html_url":"https://github.com/apache/hudi/pull/753#issuecomment-507887116","issue_url":"https://api.github.com/repos/apache/hudi/issues/753","id":507887116,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzg4NzExNg==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-03T00:01:19Z","updated_at":"2019-07-03T00:01:19Z","author_association":"CONTRIBUTOR","body":"@cdmikechen  -  @n3nash or myself will review this in couple of days and will get back to you","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507887116/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507893166","html_url":"https://github.com/apache/hudi/issues/774#issuecomment-507893166","issue_url":"https://api.github.com/repos/apache/hudi/issues/774","id":507893166,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzg5MzE2Ng==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-03T00:34:53Z","updated_at":"2019-07-03T00:34:53Z","author_association":"CONTRIBUTOR","body":"@cdmikechen :  The spark (2.[1-3].x and  hive-2.3.x compatibility we referred is w.r.t Hudi. If I remember correctly, the demo setup https://hudi.apache.org/docker_demo.html has Spark 2.3.1 shell and I can see the commands running. \r\n\r\nMaybe, I am missing some context about your question, but if you look at the demo, you can see the whole spark-hudi-hive integration working without seeing the exception you pointed to.  In that setup, I can see that spark has no problem listing and querying hive tables. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507893166/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507902009","html_url":"https://github.com/apache/hudi/issues/774#issuecomment-507902009","issue_url":"https://api.github.com/repos/apache/hudi/issues/774","id":507902009,"node_id":"MDEyOklzc3VlQ29tbWVudDUwNzkwMjAwOQ==","user":{"login":"cdmikechen","id":12069428,"node_id":"MDQ6VXNlcjEyMDY5NDI4","avatar_url":"https://avatars.githubusercontent.com/u/12069428?v=4","gravatar_id":"","url":"https://api.github.com/users/cdmikechen","html_url":"https://github.com/cdmikechen","followers_url":"https://api.github.com/users/cdmikechen/followers","following_url":"https://api.github.com/users/cdmikechen/following{/other_user}","gists_url":"https://api.github.com/users/cdmikechen/gists{/gist_id}","starred_url":"https://api.github.com/users/cdmikechen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cdmikechen/subscriptions","organizations_url":"https://api.github.com/users/cdmikechen/orgs","repos_url":"https://api.github.com/users/cdmikechen/repos","events_url":"https://api.github.com/users/cdmikechen/events{/privacy}","received_events_url":"https://api.github.com/users/cdmikechen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-03T01:26:30Z","updated_at":"2019-07-03T01:39:27Z","author_association":"CONTRIBUTOR","body":"@bvaradar \r\nI have some original hive tables and datas, I need to load these tables and datas and then save as hoodie tables.\r\nSo I added some configs in spark session refer to https://issues.apache.org/jira/browse/SPARK-18112\r\n```\r\nval sparkSession = SparkSession.builder()\r\n      ...\r\n      .config(\"spark.sql.hive.metastore.version\", \"2.3.3\")\r\n      .config(\"spark.sql.hive.metastore.jars\", \".../hive-jars/*\")\r\n      .enableHiveSupport()\r\n      .getOrCreate()\r\n```\r\nI think if we set `spark.sql.hive.metastore.version`, spark will not use basic hive-1.2.1 metastore in itself jars lib and check hive version and then use `org.apache.spark.sql.hive.client.IsolatedClientLoader`.\r\n\r\nI know this may not be hoodie problem, but if someone uses spark and hoodie both to process data in an application at the same time, it may be a problem.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/507902009/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509343426","html_url":"https://github.com/apache/hudi/issues/759#issuecomment-509343426","issue_url":"https://api.github.com/repos/apache/hudi/issues/759","id":509343426,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTM0MzQyNg==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-08T18:39:23Z","updated_at":"2019-07-08T18:39:23Z","author_association":"NONE","body":"It is working.Sorry for late reply due to long weekend.I will show the demo  to my team by this week..\r\nThanks! for your support on this.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509343426/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509353061","html_url":"https://github.com/apache/hudi/issues/498#issuecomment-509353061","issue_url":"https://api.github.com/repos/apache/hudi/issues/498","id":509353061,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTM1MzA2MQ==","user":{"login":"ghost","id":10137,"node_id":"MDQ6VXNlcjEwMTM3","avatar_url":"https://avatars.githubusercontent.com/u/10137?v=4","gravatar_id":"","url":"https://api.github.com/users/ghost","html_url":"https://github.com/ghost","followers_url":"https://api.github.com/users/ghost/followers","following_url":"https://api.github.com/users/ghost/following{/other_user}","gists_url":"https://api.github.com/users/ghost/gists{/gist_id}","starred_url":"https://api.github.com/users/ghost/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ghost/subscriptions","organizations_url":"https://api.github.com/users/ghost/orgs","repos_url":"https://api.github.com/users/ghost/repos","events_url":"https://api.github.com/users/ghost/events{/privacy}","received_events_url":"https://api.github.com/users/ghost/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-08T19:07:48Z","updated_at":"2019-07-08T19:07:48Z","author_association":"NONE","body":"Based on the current code, the correct way to hard delete is (tested in `COW` only):\r\n\r\n```\r\nwriter\r\n// specify record_key, partition_key and precombine_fieldkey\r\n.option(DataSourceWriteOptions.INSERT_DROP_DUPS_OPT_KEY, \"true\")\r\n.option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"com.uber.hoodie.EmptyHoodieRecordPayload\")\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509353061/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509353228","html_url":"https://github.com/apache/hudi/issues/531#issuecomment-509353228","issue_url":"https://api.github.com/repos/apache/hudi/issues/531","id":509353228,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTM1MzIyOA==","user":{"login":"ghost","id":10137,"node_id":"MDQ6VXNlcjEwMTM3","avatar_url":"https://avatars.githubusercontent.com/u/10137?v=4","gravatar_id":"","url":"https://api.github.com/users/ghost","html_url":"https://github.com/ghost","followers_url":"https://api.github.com/users/ghost/followers","following_url":"https://api.github.com/users/ghost/following{/other_user}","gists_url":"https://api.github.com/users/ghost/gists{/gist_id}","starred_url":"https://api.github.com/users/ghost/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ghost/subscriptions","organizations_url":"https://api.github.com/users/ghost/orgs","repos_url":"https://api.github.com/users/ghost/repos","events_url":"https://api.github.com/users/ghost/events{/privacy}","received_events_url":"https://api.github.com/users/ghost/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-08T19:08:20Z","updated_at":"2019-07-08T19:08:20Z","author_association":"NONE","body":"Based on the current code, the correct way to hard delete is (tested in `COW` only):\r\n\r\n```\r\nwriter\r\n// specify record_key, partition_key and precombine_fieldkey\r\n.option(DataSourceWriteOptions.INSERT_DROP_DUPS_OPT_KEY, \"true\")\r\n.option(DataSourceWriteOptions.PAYLOAD_CLASS_OPT_KEY, \"com.uber.hoodie.EmptyHoodieRecordPayload\")\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509353228/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509425231","html_url":"https://github.com/apache/hudi/pull/753#issuecomment-509425231","issue_url":"https://api.github.com/repos/apache/hudi/issues/753","id":509425231,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTQyNTIzMQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-08T23:13:17Z","updated_at":"2019-07-08T23:13:17Z","author_association":"MEMBER","body":"@cdmikechen finally understood :) .. Thanks. merging! ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509425231/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509426168","html_url":"https://github.com/apache/hudi/pull/770#issuecomment-509426168","issue_url":"https://api.github.com/repos/apache/hudi/issues/770","id":509426168,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTQyNjE2OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-08T23:17:24Z","updated_at":"2019-07-08T23:17:24Z","author_association":"MEMBER","body":"I am all for removing the dependency. but since this touches a bunch of things, any idea how to test this more thoroughly? @n3nash @bvaradar as well ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509426168/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509429113","html_url":"https://github.com/apache/hudi/issues/776#issuecomment-509429113","issue_url":"https://api.github.com/repos/apache/hudi/issues/776","id":509429113,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTQyOTExMw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-08T23:29:02Z","updated_at":"2019-07-08T23:29:02Z","author_association":"MEMBER","body":"Closing in favor of JIRA HUDI-164 ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509429113/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509430232","html_url":"https://github.com/apache/hudi/issues/773#issuecomment-509430232","issue_url":"https://api.github.com/repos/apache/hudi/issues/773","id":509430232,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTQzMDIzMg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-08T23:34:01Z","updated_at":"2019-07-08T23:34:01Z","author_association":"MEMBER","body":"closing in favor of https://issues.apache.org/jira/projects/HUDI/issues/HUDI-163 ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509430232/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509430726","html_url":"https://github.com/apache/hudi/issues/768#issuecomment-509430726","issue_url":"https://api.github.com/repos/apache/hudi/issues/768","id":509430726,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTQzMDcyNg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-08T23:36:21Z","updated_at":"2019-07-08T23:36:21Z","author_association":"MEMBER","body":"@garyli1019 please post on the mailing list for faster responses.. \r\n\r\nis this the same issue as bumping up ulimit that was discussed on slack?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509430726/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509799599","html_url":"https://github.com/apache/hudi/pull/600#issuecomment-509799599","issue_url":"https://api.github.com/repos/apache/hudi/issues/600","id":509799599,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTc5OTU5OQ==","user":{"login":"n3nash","id":2722167,"node_id":"MDQ6VXNlcjI3MjIxNjc=","avatar_url":"https://avatars.githubusercontent.com/u/2722167?v=4","gravatar_id":"","url":"https://api.github.com/users/n3nash","html_url":"https://github.com/n3nash","followers_url":"https://api.github.com/users/n3nash/followers","following_url":"https://api.github.com/users/n3nash/following{/other_user}","gists_url":"https://api.github.com/users/n3nash/gists{/gist_id}","starred_url":"https://api.github.com/users/n3nash/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/n3nash/subscriptions","organizations_url":"https://api.github.com/users/n3nash/orgs","repos_url":"https://api.github.com/users/n3nash/repos","events_url":"https://api.github.com/users/n3nash/events{/privacy}","received_events_url":"https://api.github.com/users/n3nash/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-09T20:42:29Z","updated_at":"2019-07-09T20:42:29Z","author_association":"CONTRIBUTOR","body":"Accepted","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509799599/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509889062","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-509889062","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":509889062,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTg4OTA2Mg==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-10T03:06:16Z","updated_at":"2019-07-10T17:45:56Z","author_association":"NONE","body":"I am facing similar issue while creating the MOR tables.Please take a look.\r\n\r\nERROR Log :\r\n```\r\n spark-submit --master yarn  --class com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer `ls /mapr/user/avenka23/hoodie/incubator-hudi/packaging/hoodie-utilities-bundle/target/hoodie-utilities-bundle*-SNAPSHOT.jar`   --props /user/avenka23/delta-streamer/config/dfs-source_no_partition.properties   --schemaprovider-class com.uber.hoodie.utilities.schema.FilebasedSchemaProvider   --source-class com.uber.hoodie.utilities.sources.JsonDFSSource   --source-ordering-field ts   --target-base-path /........../stock_ticks_cow_no_part_DEMO_MR --target-table stock_ticks_cow_no_part_DEMO_MR  --storage-type MERGE_ON_READ --key-generator-class com.uber.hoodie.NonpartitionedKeyGenerator\r\n19/07/09 22:01:15 WARN SchedulerConfGenerator: Job Scheduling Configs will not be in effect as spark.scheduler.mode is not set to FAIR at instatiation time. Continuing without scheduling configs\r\n19/07/09 22:01:20 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\r\nERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.\r\n19/07/09 22:01:35 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.\r\n19/07/09 22:01:38 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2, dsfsdf.sdfsd.com, executor 2): java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.lambda$rollback$5(HoodieMergeOnReadTable.java:510)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\r\n        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\r\n        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)\r\n        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)\r\n        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)\r\n        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\r\n        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.rollback(HoodieMergeOnReadTable.java:505)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.lambda$rollback$328a965c$1(HoodieMergeOnReadTable.java:307)\r\n        at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)\r\n        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\n[Stage 1:>                                                          (0 + 2) / 2]19/07/09 22:01:39 ERROR TaskSetManager: Task 1 in stage 1.0 failed 4 times; aborting job\r\nException in thread \"main\" org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1.0 (TID 5, dbslt1835.uhc.com, executor 2): java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.lambda$rollback$5(HoodieMergeOnReadTable.java:510)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\r\n        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\r\n        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)\r\n        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)\r\n        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)\r\n        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\r\n        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.rollback(HoodieMergeOnReadTable.java:505)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.lambda$rollback$328a965c$1(HoodieMergeOnReadTable.java:307)\r\n        at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)\r\n        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n\r\nDriver stacktrace:\r\n        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n        at scala.Option.foreach(Option.scala:257)\r\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n        at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n        at org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n        at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361)\r\n        at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.rollback(HoodieMergeOnReadTable.java:318)\r\n        at com.uber.hoodie.HoodieWriteClient.doRollbackAndGetStats(HoodieWriteClient.java:887)\r\n        at com.uber.hoodie.HoodieWriteClient.rollbackInternal(HoodieWriteClient.java:965)\r\n        at com.uber.hoodie.HoodieWriteClient.rollback(HoodieWriteClient.java:776)\r\n        at com.uber.hoodie.HoodieWriteClient.rollbackInflightCommits(HoodieWriteClient.java:1187)\r\n        at com.uber.hoodie.HoodieWriteClient.startCommitWithTime(HoodieWriteClient.java:1053)\r\n        at com.uber.hoodie.HoodieWriteClient.startCommit(HoodieWriteClient.java:1046)\r\n        at com.uber.hoodie.utilities.deltastreamer.DeltaSync.startCommit(DeltaSync.java:404)\r\n        at com.uber.hoodie.utilities.deltastreamer.DeltaSync.writeToSink(DeltaSync.java:330)\r\n        at com.uber.hoodie.utilities.deltastreamer.DeltaSync.syncOnce(DeltaSync.java:227)\r\n        at com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer.sync(HoodieDeltaStreamer.java:125)\r\n        at com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer.main(HoodieDeltaStreamer.java:289)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:780)\r\n        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)\r\n        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)\r\n        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)\r\n        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.lambda$rollback$5(HoodieMergeOnReadTable.java:510)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\r\n        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\r\n        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)\r\n        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)\r\n        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)\r\n        at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\r\n        at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\r\n        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\r\n        at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.rollback(HoodieMergeOnReadTable.java:505)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.lambda$rollback$328a965c$1(HoodieMergeOnReadTable.java:307)\r\n        at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)\r\n        at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\r\n        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\r\n        at scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n        at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\r\n        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n        at scala.collection.AbstractIterator.to(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)\r\n        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n        at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509889062/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509995889","html_url":"https://github.com/apache/hudi/pull/778#issuecomment-509995889","issue_url":"https://api.github.com/repos/apache/hudi/issues/778","id":509995889,"node_id":"MDEyOklzc3VlQ29tbWVudDUwOTk5NTg4OQ==","user":{"login":"hotienvu","id":1747423,"node_id":"MDQ6VXNlcjE3NDc0MjM=","avatar_url":"https://avatars.githubusercontent.com/u/1747423?v=4","gravatar_id":"","url":"https://api.github.com/users/hotienvu","html_url":"https://github.com/hotienvu","followers_url":"https://api.github.com/users/hotienvu/followers","following_url":"https://api.github.com/users/hotienvu/following{/other_user}","gists_url":"https://api.github.com/users/hotienvu/gists{/gist_id}","starred_url":"https://api.github.com/users/hotienvu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hotienvu/subscriptions","organizations_url":"https://api.github.com/users/hotienvu/orgs","repos_url":"https://api.github.com/users/hotienvu/repos","events_url":"https://api.github.com/users/hotienvu/events{/privacy}","received_events_url":"https://api.github.com/users/hotienvu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-10T09:56:19Z","updated_at":"2019-07-10T09:56:19Z","author_association":"CONTRIBUTOR","body":"> LGTM overall. but is it a requirement to keep the checkpoints under the basePath? In general, would not advise having peripheral data stored with the dataset itself. thoughts?\r\n\r\nI think it's a matter of convenience/preference. The checkpoint data is an integral part of Spark SS so we like to keep it close to the source/sink unless there is reason not to do so. Also there could be other scenarios where directory is created before hand e.g. setup script","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/509995889/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510128716","html_url":"https://github.com/apache/hudi/issues/779#issuecomment-510128716","issue_url":"https://api.github.com/repos/apache/hudi/issues/779","id":510128716,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDEyODcxNg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-10T16:13:40Z","updated_at":"2019-07-10T16:13:40Z","author_association":"MEMBER","body":"are you using the upsert() operation or insert() ? Also, given the commit_time and record_key and partition_path are all same, it indicates preCombining is not on? can you share your configs/","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510128716/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510134353","html_url":"https://github.com/apache/hudi/pull/778#issuecomment-510134353","issue_url":"https://api.github.com/repos/apache/hudi/issues/778","id":510134353,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDEzNDM1Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-10T16:29:14Z","updated_at":"2019-07-10T16:29:14Z","author_association":"MEMBER","body":"@hotienvu can you clean up the commits and squash them before I merge? for eg. https://github.com/apache/incubator-hudi/commit/e48e35385a91cf6d03a0a91d71aabfe78fe82928 is in the list and I see that its already merged in another PR ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510134353/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510168048","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-510168048","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":510168048,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDE2ODA0OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-10T18:05:30Z","updated_at":"2019-07-10T18:05:30Z","author_association":"MEMBER","body":"@jackwang2 @amaranathv there are 3 issues here with different stack traces. \r\n\r\n@jackwang2 first issue you reported . is that gone? or you waiting for 0.4.7 to be fixed with issue below to try that?\r\n```\r\nCaused by: java.util.NoSuchElementException: No value present\r\n\tat com.uber.hoodie.common.util.Option.get(Option.java:112)\r\n\tat com.uber.hoodie.io.HoodieMergeHandle.(HoodieMergeHandle.java:71)\r\n\tat com.uber.hoodie.table.HoodieCopyOnWriteTable.getUpdateHandle(HoodieCopyOnWriteTable.java:226)\r\n\tat com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpdate(HoodieCopyOnWriteTable.java:180)\r\n\tat com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:263)\r\n\t... 28 more\r\n```\r\n\r\n@jackwang2 second issue (probably same as what you mentioned on slack?) . are you able to reproduce it? https://github.com/apache/incubator-hudi/blob/hoodie-0.4.7/hoodie-client/src/main/java/com/uber/hoodie/io/HoodieCommitArchiveLog.java#L279 seems its related to archiving CLEAN action. The schema for this has not changed in years. it looks like some corruption. are you able to print out `new String(commitTimeline.getInstantDetails(hoodieInstant).get())`, so we can see the bytes.. \r\n\r\n```\r\nCaused by: java.io.IOException: Not an Avro data fileat org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:50)\r\n\tat com.uber.hoodie.common.util.AvroUtils.deserializeAvroMetadata(AvroUtils.java:215)\r\n\tat com.uber.hoodie.io.HoodieCommitArchiveLog.convertToAvroRecord(HoodieCommitArchiveLog.java:279)\r\n\tat com.uber.hoodie.io.HoodieCommitArchiveLog.archive(HoodieCommitArchiveLog.java:247)... 41 more\r\n19/06/25 12:37:52 INFO SparkContext: Invoking stop() from shutdown hook\r\n```\r\n\r\n@amaranathv your stack trace is different.. \r\n```\r\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.lambda$rollback$5(HoodieMergeOnReadTable.java:510)\r\n```\r\n\r\ncan you print out partitionPath below? It seems like its null .. Please check if your input has valid non-null partition paths.\r\n\r\n```\r\n          writer = HoodieLogFormat.newWriterBuilder().onParentPath(\r\n                new Path(this.getMetaClient().getBasePath(), partitionPath))\r\n                .withFileId(wStat.getFileId()).overBaseCommit(baseCommitTime)\r\n                .withFs(this.metaClient.getFs())\r\n                .withFileExtension(HoodieLogFile.DELTA_EXTENSION).build();\r\n```\r\n\r\n\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510168048/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510169394","html_url":"https://github.com/apache/hudi/issues/774#issuecomment-510169394","issue_url":"https://api.github.com/repos/apache/hudi/issues/774","id":510169394,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDE2OTM5NA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-10T18:09:16Z","updated_at":"2019-07-10T18:09:16Z","author_association":"MEMBER","body":"@cdmikechen the Hive integration is something we have had similar troubles with. On master, we no longer support Hive 1.x. Are you saying that unless someone sets these configs Hive sync does not work? \r\n\r\nAlso we are completely reworking the bundles atm. Any help testing and refining that would be appreciated.. Will also send an email to mailing list when ready ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510169394/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510169557","html_url":"https://github.com/apache/hudi/issues/769#issuecomment-510169557","issue_url":"https://api.github.com/repos/apache/hudi/issues/769","id":510169557,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDE2OTU1Nw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-10T18:09:44Z","updated_at":"2019-07-10T18:09:44Z","author_association":"MEMBER","body":"Please reopen as needed ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510169557/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510289770","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-510289770","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":510289770,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDI4OTc3MA==","user":{"login":"jackwang2","id":26425937,"node_id":"MDQ6VXNlcjI2NDI1OTM3","avatar_url":"https://avatars.githubusercontent.com/u/26425937?v=4","gravatar_id":"","url":"https://api.github.com/users/jackwang2","html_url":"https://github.com/jackwang2","followers_url":"https://api.github.com/users/jackwang2/followers","following_url":"https://api.github.com/users/jackwang2/following{/other_user}","gists_url":"https://api.github.com/users/jackwang2/gists{/gist_id}","starred_url":"https://api.github.com/users/jackwang2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jackwang2/subscriptions","organizations_url":"https://api.github.com/users/jackwang2/orgs","repos_url":"https://api.github.com/users/jackwang2/repos","events_url":"https://api.github.com/users/jackwang2/events{/privacy}","received_events_url":"https://api.github.com/users/jackwang2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T01:27:34Z","updated_at":"2019-07-11T01:27:34Z","author_association":"NONE","body":"@vinothchandar for the issue `java.util.NoSuchElementException: No value present`, It was not reproduced after I changing to another column for partition. Anyway, I would try to print the info like you suggested and update you.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510289770/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510504769","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-510504769","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":510504769,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDUwNDc2OQ==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T14:17:09Z","updated_at":"2019-07-11T14:17:09Z","author_association":"NONE","body":"I am creating table without partition column .\nI can try to rerun the process again and check.\nWill delta streamer support MOR without partition column?\nI have tested the copy on write without partition column.\nNow I am testing Merge on Read without partition column.\nThat is when I got this issue.\n\nSent from my iPhone\n\n> On Jul 10, 2019, at 8:29 PM, jackwang2 <notifications@github.com> wrote:\n> \n> @vinothchandar for the issue java.util.NoSuchElementException: No value present, It was not reproduced after I changing to another column for partition. Anyway, I would try to print the info like you suggested and update you.\n> \n> \n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub, or mute the thread.\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510504769/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510538727","html_url":"https://github.com/apache/hudi/pull/780#issuecomment-510538727","issue_url":"https://api.github.com/repos/apache/hudi/issues/780","id":510538727,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDUzODcyNw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T15:38:44Z","updated_at":"2019-07-11T15:38:44Z","author_association":"MEMBER","body":"@thesuperzapper Thanks!  This is great.. let me review these. I have also been working on #751 around the same thing.. Ideally, if we can merge these changes onto a branch and test, that would be ideal.. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510538727/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510547665","html_url":"https://github.com/apache/hudi/pull/780#issuecomment-510547665","issue_url":"https://api.github.com/repos/apache/hudi/issues/780","id":510547665,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDU0NzY2NQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T16:01:00Z","updated_at":"2019-07-11T16:01:00Z","author_association":"MEMBER","body":"@thesuperzapper as for testing, if you can run the demo steps once and confirm there are no NoClassDefFound errors and such, it would be a good start. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510547665/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510562983","html_url":"https://github.com/apache/hudi/pull/700#issuecomment-510562983","issue_url":"https://api.github.com/repos/apache/hudi/issues/700","id":510562983,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDU2Mjk4Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T16:41:04Z","updated_at":"2019-07-11T16:41:04Z","author_association":"MEMBER","body":"@bvaradar is this ready for merging? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510562983/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510569606","html_url":"https://github.com/apache/hudi/issues/714#issuecomment-510569606","issue_url":"https://api.github.com/repos/apache/hudi/issues/714","id":510569606,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDU2OTYwNg==","user":{"login":"NetsanetGeb","id":25975892,"node_id":"MDQ6VXNlcjI1OTc1ODky","avatar_url":"https://avatars.githubusercontent.com/u/25975892?v=4","gravatar_id":"","url":"https://api.github.com/users/NetsanetGeb","html_url":"https://github.com/NetsanetGeb","followers_url":"https://api.github.com/users/NetsanetGeb/followers","following_url":"https://api.github.com/users/NetsanetGeb/following{/other_user}","gists_url":"https://api.github.com/users/NetsanetGeb/gists{/gist_id}","starred_url":"https://api.github.com/users/NetsanetGeb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/NetsanetGeb/subscriptions","organizations_url":"https://api.github.com/users/NetsanetGeb/orgs","repos_url":"https://api.github.com/users/NetsanetGeb/repos","events_url":"https://api.github.com/users/NetsanetGeb/events{/privacy}","received_events_url":"https://api.github.com/users/NetsanetGeb/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T17:00:03Z","updated_at":"2019-07-15T10:28:54Z","author_association":"NONE","body":"**Benchmarking Hudi Upsert** \r\n\r\nI am trying to bench mark Hudi upsert operation and the latency of ingesting 6 GB of data is 38 minutes with the cluster i provided. How can i enhance this?\r\n\r\nFor my specific use case, i used a spliced JSON data source with the schema having 20 columns.  \r\nThe settings i used  for a cluster with (30 GB of RAM   and  100 GB available disk) are:\r\nspark.driver.memory = 4096m\r\nspark.executor.memory = 6144m\r\nspark.executor.instances =3\r\nspark.driver.cores =1\r\nspark.executor.cores =1\r\nhoodie.datasource.write.operation=\"upsert\"\r\nhoodie.upsert.shuffle.parallellism=\"1500\"\r\n\r\nYou can see the details from the UI of the spark job provided below:\r\n\r\n![hudiUpsert1](https://user-images.githubusercontent.com/25975892/61070032-f39a0c00-a40d-11e9-9f41-7909f0a045d4.png)\r\n\r\n![hudiUpsert2](https://user-images.githubusercontent.com/25975892/61070050-057baf00-a40e-11e9-9139-b97c421ac99b.png)\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510569606/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510575991","html_url":"https://github.com/apache/hudi/pull/638#issuecomment-510575991","issue_url":"https://api.github.com/repos/apache/hudi/issues/638","id":510575991,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDU3NTk5MQ==","user":{"login":"tooptoop4","id":33283496,"node_id":"MDQ6VXNlcjMzMjgzNDk2","avatar_url":"https://avatars.githubusercontent.com/u/33283496?v=4","gravatar_id":"","url":"https://api.github.com/users/tooptoop4","html_url":"https://github.com/tooptoop4","followers_url":"https://api.github.com/users/tooptoop4/followers","following_url":"https://api.github.com/users/tooptoop4/following{/other_user}","gists_url":"https://api.github.com/users/tooptoop4/gists{/gist_id}","starred_url":"https://api.github.com/users/tooptoop4/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tooptoop4/subscriptions","organizations_url":"https://api.github.com/users/tooptoop4/orgs","repos_url":"https://api.github.com/users/tooptoop4/repos","events_url":"https://api.github.com/users/tooptoop4/events{/privacy}","received_events_url":"https://api.github.com/users/tooptoop4/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T17:19:39Z","updated_at":"2019-07-11T17:19:39Z","author_association":"NONE","body":"@thesuperzapper @bvaradar gentle ping","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510575991/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510579271","html_url":"https://github.com/apache/hudi/issues/735#issuecomment-510579271","issue_url":"https://api.github.com/repos/apache/hudi/issues/735","id":510579271,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDU3OTI3MQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T17:28:54Z","updated_at":"2019-07-11T17:28:54Z","author_association":"MEMBER","body":"@Gowthamsb12  #751 is redoing the bundles. but for this issue, I think you should be able to find log4J if the classpath is set correctly? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510579271/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510581260","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-510581260","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":510581260,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDU4MTI2MA==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T17:34:34Z","updated_at":"2019-07-11T18:42:09Z","author_association":"NONE","body":"I am getting same error.\r\n```\r\nscala> .save(\"/datalake/888/888/888/hive/warehouse/test_hudi_spark_no_part_1_mor\")\r\n19/07/11 12:31:45 WARN TaskSetManager: Lost task 0.0 in stage 304.0 (TID 464, 88888.uhc.com, executor 2): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:274)\r\n        at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:451)\r\n        at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\r\n        at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\r\n        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)\r\n        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n        at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\r\n        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\r\n        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\r\n        at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\r\n        at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\r\n        at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\r\n        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.uber.hoodie.exception.HoodieUpsertException: Failed to initialize HoodieAppendHandle for FileId: 951d569b-188d-46e4-ad94-a32525fac797-0 on commit 20190711123144 on HDFS path /datalake/optum/optuminsight/udw/hive/warehouse/test_hudi_spark_no_part_1_mor\r\n        at com.uber.hoodie.io.HoodieAppendHandle.init(HoodieAppendHandle.java:141)\r\n        at com.uber.hoodie.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:193)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:118)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:266)\r\n        ... 28 more\r\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.io.HoodieAppendHandle.createLogWriter(HoodieAppendHandle.java:277)\r\n        at com.uber.hoodie.io.HoodieAppendHandle.init(HoodieAppendHandle.java:132)\r\n        ... 31 more\r\n\r\n19/07/11 12:31:45 ERROR TaskSetManager: Task 0 in stage 304.0 failed 4 times; aborting job\r\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 304.0 failed 4 times, most recent failure: Lost task 0.3 in stage 304.0 (TID 467, dbslt1829.uhc.com, executor 2): com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:274)\r\n        at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:451)\r\n        at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\r\n        at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\r\n        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)\r\n        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n        at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\r\n        at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\r\n        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\r\n        at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\r\n        at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\r\n        at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\r\n        at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\r\n        at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\r\n        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n        at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n        at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.uber.hoodie.exception.HoodieUpsertException: Failed to initialize HoodieAppendHandle for FileId: 951d569b-188d-46e4-ad94-a32525fac797-0 on commit 20190711123144 on HDFS path /datalake/888/99999/9999/hive/warehouse/test_hudi_spark_no_part_1_mor\r\n        at com.uber.hoodie.io.HoodieAppendHandle.init(HoodieAppendHandle.java:141)\r\n        at com.uber.hoodie.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:193)\r\n        at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:118)\r\n        at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:266)\r\n        ... 28 more\r\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n        at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n        at com.uber.hoodie.io.HoodieAppendHandle.createLogWriter(HoodieAppendHandle.java:277)\r\n        at com.uber.hoodie.io.HoodieAppendHandle.init(HoodieAppendHandle.java:132)\r\n        ... 31 more\r\n\r\nDriver stacktrace:\r\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504)\r\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n  at scala.Option.foreach(Option.scala:257)\r\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687)\r\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676)\r\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069)\r\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094)\r\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1158)\r\n  at com.uber.hoodie.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:149)\r\n  at com.uber.hoodie.DefaultSource.createRelation(DefaultSource.scala:90)\r\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:469)\r\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\r\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\r\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\r\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\r\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\r\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\r\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\r\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\r\n  ... 54 elided\r\nCaused by: com.uber.hoodie.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\r\n  at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:274)\r\n  at com.uber.hoodie.HoodieWriteClient.lambda$upsertRecordsInternal$7ef77fd$1(HoodieWriteClient.java:451)\r\n  at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\r\n  at org.apache.spark.api.java.JavaRDDLike$$anonfun$mapPartitionsWithIndex$1.apply(JavaRDDLike.scala:102)\r\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)\r\n  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:844)\r\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\r\n  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\r\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1055)\r\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1029)\r\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:969)\r\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1029)\r\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:760)\r\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\r\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\r\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\r\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n  at org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n  at java.lang.Thread.run(Thread.java:748)\r\nCaused by: com.uber.hoodie.exception.HoodieUpsertException: Failed to initialize HoodieAppendHandle for FileId: 951d569b-188d-46e4-ad94-a32525fac797-0 on commit 20190711123144 on HDFS path /datalake/9999/999/999/hive/warehouse/test_hudi_spark_no_part_1_mor\r\n  at com.uber.hoodie.io.HoodieAppendHandle.init(HoodieAppendHandle.java:141)\r\n  at com.uber.hoodie.io.HoodieAppendHandle.doAppend(HoodieAppendHandle.java:193)\r\n  at com.uber.hoodie.table.HoodieMergeOnReadTable.handleUpdate(HoodieMergeOnReadTable.java:118)\r\n  at com.uber.hoodie.table.HoodieCopyOnWriteTable.handleUpsertPartition(HoodieCopyOnWriteTable.java:266)\r\n  ... 28 more\r\nCaused by: java.lang.IllegalArgumentException: Can not create a Path from an empty string\r\n  at org.apache.hadoop.fs.Path.checkPathArg(Path.java:130)\r\n  at org.apache.hadoop.fs.Path.<init>(Path.java:138)\r\n  at org.apache.hadoop.fs.Path.<init>(Path.java:92)\r\n  at com.uber.hoodie.io.HoodieAppendHandle.createLogWriter(HoodieAppendHandle.java:277)\r\n  at com.uber.hoodie.io.HoodieAppendHandle.init(HoodieAppendHandle.java:132)\r\n  ... 31 more\r\n```","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510581260/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510581625","html_url":"https://github.com/apache/hudi/issues/736#issuecomment-510581625","issue_url":"https://api.github.com/repos/apache/hudi/issues/736","id":510581625,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDU4MTYyNQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T17:35:41Z","updated_at":"2019-07-11T17:35:41Z","author_association":"MEMBER","body":"@cdmikechen these jars are in the hive installation, thats why we don't bundle them. \r\n\r\n```\r\nls hive/lib/datanucleus-*\r\nhive/lib/datanucleus-api-jdo-4.2.4.jar\thive/lib/datanucleus-core-4.1.17.jar  hive/lib/datanucleus-rdbms-4.1.19.jar\r\nroot@adhoc-2:/opt#\r\n```\r\n\r\nis it possible the the script is not just picking them up?  are you able to repro this on top of #751 and see if this still is an issue?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510581625/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510590195","html_url":"https://github.com/apache/hudi/issues/550#issuecomment-510590195","issue_url":"https://api.github.com/repos/apache/hudi/issues/550","id":510590195,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDU5MDE5NQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T17:58:34Z","updated_at":"2019-07-11T17:58:34Z","author_association":"MEMBER","body":"In #751 , I actually found that the jetty jars are needed by the new embedded timeline server.. \r\n\r\n```\r\n2019-07-11 17:54:10 INFO  FileSystemViewManager:176 - Creating embedded rocks-db based Table View\r\nException in thread \"main\" java.lang.NoClassDefFoundError: org/eclipse/jetty/util/log/Log\r\n\tat io.javalin.core.util.JettyServerUtil.<clinit>(JettyServerUtil.kt:34)\r\n\tat io.javalin.Javalin.create(Javalin.java:106)\r\n\tat com.uber.hoodie.timeline.service.TimelineService.startService(TimelineService.java:101)\r\n\tat com.uber.hoodie.client.embedded.EmbeddedTimelineService.startServer(EmbeddedTimelineService.java:72)\r\n\tat com.uber.hoodie.AbstractHoodieClient.startEmbeddedServerView(AbstractHoodieClient.java:98)\r\n\tat com.uber.hoodie.AbstractHoodieClient.<init>(AbstractHoodieClient.java:66)\r\n\tat com.uber.hoodie.HoodieWriteClient.<init>(HoodieWriteClient.java:134)\r\n\tat com.uber.hoodie.HoodieWriteClient.<init>(HoodieWriteClient.java:129)\r\n\tat com.uber.hoodie.HoodieWriteClient.<init>(HoodieWriteClient.java:123)\r\n\tat com.uber.hoodie.utilities.deltastreamer.DeltaSync.setupWriteClient(DeltaSync.java:442)\r\n\tat com.uber.hoodie.utilities.deltastreamer.DeltaSync.<init>(DeltaSync.java:185)\r\n\tat com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer$DeltaSyncService.<init>(HoodieDeltaStreamer.java:362)\r\n\tat com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer.<init>(HoodieDeltaStreamer.java:101)\r\n\tat com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer.<init>(HoodieDeltaStreamer.java:95)\r\n\tat com.uber.hoodie.utilities.deltastreamer.HoodieDeltaStreamer.main(HoodieDeltaStreamer.java:289)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)\r\n\tat org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)\r\n\tat org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.lang.ClassNotFoundException: org.eclipse.jetty.util.log.Log\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\r\n```\r\n\r\nbut lets shade them. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510590195/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510605473","html_url":"https://github.com/apache/hudi/issues/774#issuecomment-510605473","issue_url":"https://api.github.com/repos/apache/hudi/issues/774","id":510605473,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDYwNTQ3Mw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T18:40:17Z","updated_at":"2019-07-11T18:40:17Z","author_association":"MEMBER","body":"#751 has the changes in mostly final form.. \r\n\r\n@cdmikechen we have been thinking about whether we should directly talk to the metastore to perform hive sync, instead of talking to HiveServer. wdyt? Would it simplify few things ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510605473/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510606829","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-510606829","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":510606829,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDYwNjgyOQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T18:44:23Z","updated_at":"2019-07-11T18:44:23Z","author_association":"MEMBER","body":"@amaranathv just to confirm you are using `NonpartitionedKeyGenerator` as the key generator? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510606829/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510609856","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-510609856","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":510609856,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDYwOTg1Ng==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T18:52:12Z","updated_at":"2019-07-11T18:52:12Z","author_association":"NONE","body":"yes","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510609856/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510609967","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-510609967","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":510609967,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDYwOTk2Nw==","user":{"login":"amarnathv9","id":51413224,"node_id":"MDQ6VXNlcjUxNDEzMjI0","avatar_url":"https://avatars.githubusercontent.com/u/51413224?v=4","gravatar_id":"","url":"https://api.github.com/users/amarnathv9","html_url":"https://github.com/amarnathv9","followers_url":"https://api.github.com/users/amarnathv9/followers","following_url":"https://api.github.com/users/amarnathv9/following{/other_user}","gists_url":"https://api.github.com/users/amarnathv9/gists{/gist_id}","starred_url":"https://api.github.com/users/amarnathv9/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amarnathv9/subscriptions","organizations_url":"https://api.github.com/users/amarnathv9/orgs","repos_url":"https://api.github.com/users/amarnathv9/repos","events_url":"https://api.github.com/users/amarnathv9/events{/privacy}","received_events_url":"https://api.github.com/users/amarnathv9/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T18:52:28Z","updated_at":"2019-07-11T18:52:28Z","author_association":"NONE","body":"DataSourceWriteOptions.KEYGENERATOR_CLASS_OPT_KEY-> \"com.uber.hoodie.NonpartitionedKeyGenerator\"\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510609967/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510636174","html_url":"https://github.com/apache/hudi/issues/714#issuecomment-510636174","issue_url":"https://api.github.com/repos/apache/hudi/issues/714","id":510636174,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDYzNjE3NA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T20:11:34Z","updated_at":"2019-07-11T20:11:34Z","author_association":"MEMBER","body":"One thing I notice is about 10 mins is spent in retrying the stages . This is usually indicatve that the job is running a degraded state. do you know what these failures are? also stage 3, simply parses the input and caches the hoodie records in memory.. Not sure why this takes 7 minutes - usually indicates input data parsing issues. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510636174/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510644864","html_url":"https://github.com/apache/hudi/issues/764#issuecomment-510644864","issue_url":"https://api.github.com/repos/apache/hudi/issues/764","id":510644864,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDY0NDg2NA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T20:38:24Z","updated_at":"2019-07-11T20:38:24Z","author_association":"MEMBER","body":"@amaranathv again the issue from this line `        .onParentPath(new Path(hoodieTable.getMetaClient().getBasePath(), partitionPath))\r\n` where I suspect partitionPath is null. Can you please ensure your dataset contains non-null partition paths? (I am going to add an explicit exception to flag this case in the KeyGenerator subclasses or somewhere else. ) ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510644864/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510649936","html_url":"https://github.com/apache/hudi/pull/638#issuecomment-510649936","issue_url":"https://api.github.com/repos/apache/hudi/issues/638","id":510649936,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDY0OTkzNg==","user":{"login":"thesuperzapper","id":5735406,"node_id":"MDQ6VXNlcjU3MzU0MDY=","avatar_url":"https://avatars.githubusercontent.com/u/5735406?v=4","gravatar_id":"","url":"https://api.github.com/users/thesuperzapper","html_url":"https://github.com/thesuperzapper","followers_url":"https://api.github.com/users/thesuperzapper/followers","following_url":"https://api.github.com/users/thesuperzapper/following{/other_user}","gists_url":"https://api.github.com/users/thesuperzapper/gists{/gist_id}","starred_url":"https://api.github.com/users/thesuperzapper/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/thesuperzapper/subscriptions","organizations_url":"https://api.github.com/users/thesuperzapper/orgs","repos_url":"https://api.github.com/users/thesuperzapper/repos","events_url":"https://api.github.com/users/thesuperzapper/events{/privacy}","received_events_url":"https://api.github.com/users/thesuperzapper/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-11T20:53:43Z","updated_at":"2019-07-11T20:53:43Z","author_association":"NONE","body":"We are working on #780 first","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510649936/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510818215","html_url":"https://github.com/apache/hudi/issues/714#issuecomment-510818215","issue_url":"https://api.github.com/repos/apache/hudi/issues/714","id":510818215,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDgxODIxNQ==","user":{"login":"NetsanetGeb","id":25975892,"node_id":"MDQ6VXNlcjI1OTc1ODky","avatar_url":"https://avatars.githubusercontent.com/u/25975892?v=4","gravatar_id":"","url":"https://api.github.com/users/NetsanetGeb","html_url":"https://github.com/NetsanetGeb","followers_url":"https://api.github.com/users/NetsanetGeb/followers","following_url":"https://api.github.com/users/NetsanetGeb/following{/other_user}","gists_url":"https://api.github.com/users/NetsanetGeb/gists{/gist_id}","starred_url":"https://api.github.com/users/NetsanetGeb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/NetsanetGeb/subscriptions","organizations_url":"https://api.github.com/users/NetsanetGeb/orgs","repos_url":"https://api.github.com/users/NetsanetGeb/repos","events_url":"https://api.github.com/users/NetsanetGeb/events{/privacy}","received_events_url":"https://api.github.com/users/NetsanetGeb/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-12T09:25:40Z","updated_at":"2019-07-12T09:27:10Z","author_association":"NONE","body":"The failures are: \r\n``` \r\norg.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3\r\n\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:882)\r\n\tat org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:878)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\r\n\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:878)\r\n\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:691)\r\n\tat org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49)\r\n\tat org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:148)\r\n\tat org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:137)\r\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\r\n\tat org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:137)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1182)\r\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1091)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1156)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:882)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\r\n```\r\n\r\nIn addition, stage 2 is showing that the input size is 1888.8 MB while stage 21 its showing  6.6 GB.  Is this showing that a total of 6.6 GB is written as a hoodie modeled table?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510818215/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510823736","html_url":"https://github.com/apache/hudi/issues/779#issuecomment-510823736","issue_url":"https://api.github.com/repos/apache/hudi/issues/779","id":510823736,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDgyMzczNg==","user":{"login":"eisig","id":1745057,"node_id":"MDQ6VXNlcjE3NDUwNTc=","avatar_url":"https://avatars.githubusercontent.com/u/1745057?v=4","gravatar_id":"","url":"https://api.github.com/users/eisig","html_url":"https://github.com/eisig","followers_url":"https://api.github.com/users/eisig/followers","following_url":"https://api.github.com/users/eisig/following{/other_user}","gists_url":"https://api.github.com/users/eisig/gists{/gist_id}","starred_url":"https://api.github.com/users/eisig/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eisig/subscriptions","organizations_url":"https://api.github.com/users/eisig/orgs","repos_url":"https://api.github.com/users/eisig/repos","events_url":"https://api.github.com/users/eisig/events{/privacy}","received_events_url":"https://api.github.com/users/eisig/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-12T09:43:00Z","updated_at":"2019-07-12T09:43:00Z","author_association":"CONTRIBUTOR","body":"@vinothchandar I\r\ndelete the folder and try again, It heppened again. \r\nHere is my configs:\r\n```\r\n - --storage-type\r\n    - MERGE_ON_READ\r\n    - --source-class\r\n    - com.uber.hoodie.utilities.sources.JsonKafkaSource \r\n    - --source-ordering-field\r\n    - create_date \r\n    - --target-table \r\n    - t_order_mor\r\n    - --target-base-path\r\n    - s3://xxx/t_order_mor03\r\n    - --props\r\n    - s3://xxx/kafka-source-mor.properties \r\n    - --schemaprovider-class \r\n    - com.uber.hoodie.utilities.schema.FilebasedSchemaProvider \r\n    - --key-generator-class \r\n    - com.uber.hoodie.utilities.keygen.TimestampBasedKeyGenerator \r\n    - --continuous\r\n    - --enable-hive-sync\r\n    - --source-limit\r\n    - '10000000'\r\n    # - --disable-compaction\r\n```\r\n```\r\nhoodie.consistency.check.enabled=true\r\nhoodie.embed.timeline.server=true\r\nhoodie.filesystem.view.type=EMBEDDED_KV_STORE\r\n\r\nhoodie.upsert.shuffle.parallelism=4\r\nhoodie.insert.shuffle.parallelism=4\r\nhoodie.bulkinsert.shuffle.parallelism=4\r\n\r\n# Key fields, for kafka example\r\nhoodie.datasource.write.recordkey.field=id\r\nhoodie.datasource.write.partitionpath.field=create_date\r\nhoodie.deltastreamer.keygen.timebased.timestamp.type=DATE_STRING\r\nhoodie.deltastreamer.keygen.timebased.input.dateformat=yyyy-MM-dd HH:mm:ss\r\nhoodie.deltastreamer.keygen.timebased.output.dateformat=yyyy\r\n# Schema provider props (change to absolute path based on your installation)\r\nhoodie.deltastreamer.schemaprovider.source.schema.file=s3://xxx/schema.avsc\r\nhoodie.deltastreamer.schemaprovider.target.schema.file=s3://xxxx/schema.avsc\r\n\r\n# Kafka Source\r\nhoodie.deltastreamer.source.kafka.topic=xx\r\n\r\n#Kafka props\r\nmetadata.broker.list=xxx:9092\r\nauto.offset.reset=smallest\r\n\r\nhoodie.datasource.hive_sync.database=test\r\nhoodie.datasource.hive_sync.table=t_order_mor03\r\nhoodie.datasource.hive_sync.username=hive\r\nhoodie.datasource.hive_sync.jdbcurl=jdbc:hive2://xxxx:10000\r\nhoodie.datasource.hive_sync.partition_fields=year\r\nhoodie.datasource.hive_sync.partition_extractor_class=com.xhqb.hoodie.utilities.hive.SimplePartitionValueExtractor\r\n\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510823736/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510828768","html_url":"https://github.com/apache/hudi/issues/779#issuecomment-510828768","issue_url":"https://api.github.com/repos/apache/hudi/issues/779","id":510828768,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDgyODc2OA==","user":{"login":"eisig","id":1745057,"node_id":"MDQ6VXNlcjE3NDUwNTc=","avatar_url":"https://avatars.githubusercontent.com/u/1745057?v=4","gravatar_id":"","url":"https://api.github.com/users/eisig","html_url":"https://github.com/eisig","followers_url":"https://api.github.com/users/eisig/followers","following_url":"https://api.github.com/users/eisig/following{/other_user}","gists_url":"https://api.github.com/users/eisig/gists{/gist_id}","starred_url":"https://api.github.com/users/eisig/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eisig/subscriptions","organizations_url":"https://api.github.com/users/eisig/orgs","repos_url":"https://api.github.com/users/eisig/repos","events_url":"https://api.github.com/users/eisig/events{/privacy}","received_events_url":"https://api.github.com/users/eisig/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-12T09:59:08Z","updated_at":"2019-07-12T10:12:41Z","author_association":"CONTRIBUTOR","body":" I have restart the job several times,\r\nand add  --disable-compaction\r\nOhter results seems wrong.\r\n\r\n```\r\nselect count(*) count1, count(distinct id) count2 from hive200.test.t_order_mor03_rt\r\nselect count(*) count3, count(distinct id) count4 from hive200.test.t_order_mor03 \r\n```\r\ncount1 == count3\r\ncount2 == count4\r\ncount1 != count2 and count3 != count4\r\n\r\n```\r\nselect (select max(_hoodie_commit_time) from hive200.test.t_order_mor03),\r\n(select max(_hoodie_commit_time) from hive200.test.t_order_mor03_rt)\r\n```\r\n_hoodie_commit_time are always the some.\r\n\r\n```\r\nselect count(*) count\r\nfrom  hive200.test.t_order_mor03_rt rt \r\n join hive200.test.t_order_mor03 ro\r\non ro.id = rt.id\r\nwhere rt.modify_date!=ro.modify_date\r\n```\r\ncount is going up","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510828768/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510830795","html_url":"https://github.com/apache/hudi/pull/778#issuecomment-510830795","issue_url":"https://api.github.com/repos/apache/hudi/issues/778","id":510830795,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDgzMDc5NQ==","user":{"login":"hotienvu","id":1747423,"node_id":"MDQ6VXNlcjE3NDc0MjM=","avatar_url":"https://avatars.githubusercontent.com/u/1747423?v=4","gravatar_id":"","url":"https://api.github.com/users/hotienvu","html_url":"https://github.com/hotienvu","followers_url":"https://api.github.com/users/hotienvu/followers","following_url":"https://api.github.com/users/hotienvu/following{/other_user}","gists_url":"https://api.github.com/users/hotienvu/gists{/gist_id}","starred_url":"https://api.github.com/users/hotienvu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hotienvu/subscriptions","organizations_url":"https://api.github.com/users/hotienvu/orgs","repos_url":"https://api.github.com/users/hotienvu/repos","events_url":"https://api.github.com/users/hotienvu/events{/privacy}","received_events_url":"https://api.github.com/users/hotienvu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-12T10:06:01Z","updated_at":"2019-07-12T10:06:01Z","author_association":"CONTRIBUTOR","body":"@vinothchandar commits squashed. thanks for looking into this.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510830795/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510959002","html_url":"https://github.com/apache/hudi/issues/714#issuecomment-510959002","issue_url":"https://api.github.com/repos/apache/hudi/issues/714","id":510959002,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDk1OTAwMg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-12T16:58:48Z","updated_at":"2019-07-12T16:58:56Z","author_association":"MEMBER","body":"This indicates general spark shuffle failures.. I'd suggest first running it in a larger say 20 executor cluster first, and then start shrinking. \r\n\r\n>>tage 2 is showing that the input size is 1888.8 MB while stage 21 its showing 6.6 GB\r\n\r\nThat expansion is just for the index checking operation. output table will not be 6.6GB ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510959002/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510960314","html_url":"https://github.com/apache/hudi/issues/784#issuecomment-510960314","issue_url":"https://api.github.com/repos/apache/hudi/issues/784","id":510960314,"node_id":"MDEyOklzc3VlQ29tbWVudDUxMDk2MDMxNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2019-07-12T17:02:56Z","updated_at":"2019-07-12T17:02:56Z","author_association":"MEMBER","body":"yes . you can use the `EmptyRecordPayload` as in #635 to perform hard deletes. upsert with null for soft deletes.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/510960314/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]