[{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612630996","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612630996","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612630996,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzA5OTY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T06:07:46Z","updated_at":"2025-01-24T14:14:04Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47599629\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47599629</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/FileLock.java &#8212;<br/>\n    @@ -0,0 +1,300 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +<br/>\n    +import org.apache.hadoop.fs.FSDataInputStream;<br/>\n    +import org.apache.hadoop.fs.FSDataOutputStream;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException;<br/>\n    +import org.apache.hadoop.ipc.RemoteException;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.IOException;<br/>\n    +import java.io.InputStreamReader;<br/>\n    +import java.util.Collection;<br/>\n    +<br/>\n    +/**<br/>\n    + * Facility to synchronize access to HDFS files. Thread gains exclusive access to a file by acquiring<br/>\n    + * a FileLock object. The lock itself is represented as file on HDFS. Relies on atomic file creation.<br/>\n    + * Owning thread must heartbeat periodically on the lock to prevent the lock from being deemed as<br/>\n    + * stale (i.e. lock whose owning thread have died).<br/>\n    + */<br/>\n    +public class FileLock {<br/>\n    +<br/>\n    +  private final FileSystem fs;<br/>\n    +  private final String componentID;<br/>\n    +  private final Path lockFile;<br/>\n    +  private final FSDataOutputStream lockFileStream;<br/>\n    +  private LogEntry lastEntry;<br/>\n    +<br/>\n    +  private static final Logger log = LoggerFactory.getLogger(DirLock.class);<br/>\n    +<br/>\n    +  private FileLock(FileSystem fs, Path lockFile, FSDataOutputStream lockFileStream, String spoutId)<br/>\n    +  throws IOException </p>\n{\n    +    this.fs = fs;\n    +    this.lockFile = lockFile;\n    +    this.lockFileStream = lockFileStream;\n    +    this.componentID = spoutId;\n    +    logProgress(\"0\", false);\n    +  }\n<p>    +<br/>\n    +  private FileLock(FileSystem fs, Path lockFile, String spoutId, LogEntry entry)<br/>\n    +  throws IOException {<br/>\n    +    this.fs = fs;<br/>\n    +    this.lockFile = lockFile;<br/>\n    +    this.lockFileStream =  fs.append(lockFile);<br/>\n    +    this.componentID = spoutId;<br/>\n    +    log.debug(\"Acquired abandoned lockFile {}\", lockFile);<br/>\n    +    logProgress(entry.fileOffset, true);<br/>\n    +  }<br/>\n    +<br/>\n    +  public void heartbeat(String fileOffset) throws IOException </p>\n{\n    +    logProgress(fileOffset, true);\n    +  }\n<p>    +<br/>\n    +  // new line is at beginning of each line (instead of end) for better recovery from<br/>\n    +  // partial writes of prior lines<br/>\n    +  private void logProgress(String fileOffset, boolean prefixNewLine)<br/>\n    +  throws IOException </p>\n{\n    +    long now = System.currentTimeMillis();\n    +    LogEntry entry = new LogEntry(now, componentID, fileOffset);\n    +    String line = entry.toString();\n    +    if(prefixNewLine)\n    +      lockFileStream.writeBytes(System.lineSeparator() + line);\n    +    else\n    +      lockFileStream.writeBytes(line);\n    +    lockFileStream.hflush();\n    +\n    +    lastEntry = entry; // update this only after writing to hdfs\n    +  }\n<p>    +<br/>\n    +  public void release() throws IOException {<br/>\n    +    lockFileStream.close();<br/>\n    +    fs.delete(lockFile, false);<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    fs.delete() returns false if the file is not deleted. You may want to throw an IOException for that. Caller of this method assumes file is not deleted only when IOException is thrown. We may even want to have log-warn when delete is not successful. <br/>\n    Can we have javadoc for this method to describe the contract?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612630996/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164663826","html_url":"https://github.com/apache/storm/pull/946#issuecomment-164663826","issue_url":"https://api.github.com/repos/apache/storm/issues/946","id":164663826,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDY2MzgyNg==","user":{"login":"arunmahadevan","id":6792890,"node_id":"MDQ6VXNlcjY3OTI4OTA=","avatar_url":"https://avatars.githubusercontent.com/u/6792890?v=4","gravatar_id":"","url":"https://api.github.com/users/arunmahadevan","html_url":"https://github.com/arunmahadevan","followers_url":"https://api.github.com/users/arunmahadevan/followers","following_url":"https://api.github.com/users/arunmahadevan/following{/other_user}","gists_url":"https://api.github.com/users/arunmahadevan/gists{/gist_id}","starred_url":"https://api.github.com/users/arunmahadevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arunmahadevan/subscriptions","organizations_url":"https://api.github.com/users/arunmahadevan/orgs","repos_url":"https://api.github.com/users/arunmahadevan/repos","events_url":"https://api.github.com/users/arunmahadevan/events{/privacy}","received_events_url":"https://api.github.com/users/arunmahadevan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T07:03:31Z","updated_at":"2015-12-15T07:03:31Z","author_association":"CONTRIBUTOR","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164663826/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688445","html_url":"https://github.com/apache/storm/issues/5224#issuecomment-2612688445","issue_url":"https://api.github.com/repos/apache/storm/issues/5224","id":2612688445,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg0NDU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T07:03:32Z","updated_at":"2025-01-24T14:39:15Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/946#issuecomment-164663826\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/946#issuecomment-164663826</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688445/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164674438","html_url":"https://github.com/apache/storm/pull/930#issuecomment-164674438","issue_url":"https://api.github.com/repos/apache/storm/issues/930","id":164674438,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDY3NDQzOA==","user":{"login":"unsleepy22","id":631361,"node_id":"MDQ6VXNlcjYzMTM2MQ==","avatar_url":"https://avatars.githubusercontent.com/u/631361?v=4","gravatar_id":"","url":"https://api.github.com/users/unsleepy22","html_url":"https://github.com/unsleepy22","followers_url":"https://api.github.com/users/unsleepy22/followers","following_url":"https://api.github.com/users/unsleepy22/following{/other_user}","gists_url":"https://api.github.com/users/unsleepy22/gists{/gist_id}","starred_url":"https://api.github.com/users/unsleepy22/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/unsleepy22/subscriptions","organizations_url":"https://api.github.com/users/unsleepy22/orgs","repos_url":"https://api.github.com/users/unsleepy22/repos","events_url":"https://api.github.com/users/unsleepy22/events{/privacy}","received_events_url":"https://api.github.com/users/unsleepy22/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T07:38:27Z","updated_at":"2015-12-15T07:38:27Z","author_association":"NONE","body":"+1\nI have a question, does this change affect travis ci since I don't see ci scripts changed.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164674438/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627454","html_url":"https://github.com/apache/storm/issues/5017#issuecomment-2612627454","issue_url":"https://api.github.com/repos/apache/storm/issues/5017","id":2612627454,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjc0NTQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T07:38:29Z","updated_at":"2025-01-24T14:12:32Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/930#issuecomment-164674438\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/930#issuecomment-164674438</a></p>\n\n<p>    +1<br/>\n    I have a question, does this change affect travis ci since I don't see ci scripts changed.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627454/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631000","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631000","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631000,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMDA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T08:26:05Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47608288\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47608288</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java &#8212;<br/>\n    @@ -0,0 +1,654 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +import java.io.IOException;<br/>\n    +import java.lang.reflect.Constructor;<br/>\n    +import java.util.Collection;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Timer;<br/>\n    +import java.util.TimerTask;<br/>\n    +import java.util.concurrent.LinkedBlockingQueue;<br/>\n    +import java.util.concurrent.atomic.AtomicBoolean;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import org.apache.hadoop.conf.Configuration;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.apache.storm.hdfs.common.security.HdfsSecurityUtil;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +<br/>\n    +public class HdfsSpout extends BaseRichSpout {<br/>\n    +<br/>\n    +  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);<br/>\n    +<br/>\n    +  private Path sourceDirPath;<br/>\n    +  private Path archiveDirPath;<br/>\n    +  private Path badFilesDirPath;<br/>\n    +  private Path lockDirPath;<br/>\n    +<br/>\n    +  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;<br/>\n    +  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;<br/>\n    +  private int maxDuplicates = Configs.DEFAULT_MAX_DUPLICATES;<br/>\n    +  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;<br/>\n    +  private boolean clocksInSync = true;<br/>\n    +<br/>\n    +  private ProgressTracker tracker = new ProgressTracker();<br/>\n    +<br/>\n    +  private FileSystem hdfs;<br/>\n    +  private FileReader reader;<br/>\n    +<br/>\n    +  private SpoutOutputCollector collector;<br/>\n    +  HashMap<MessageId, List<Object> > inflight = new HashMap<>();<br/>\n    +  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();<br/>\n    +<br/>\n    +  private String inprogress_suffix = \".inprogress\";<br/>\n    +  private String ignoreSuffix = \".ignore\";<br/>\n    +<br/>\n    +  private Configuration hdfsConfig;<br/>\n    +  private String readerType;<br/>\n    +<br/>\n    +  private Map conf = null;<br/>\n    +  private FileLock lock;<br/>\n    +  private String spoutId = null;<br/>\n    +<br/>\n    +  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;<br/>\n    +  private long lastExpiredLockTime = 0;<br/>\n    +<br/>\n    +  private long tupleCounter = 0;<br/>\n    +  private boolean ackEnabled = false;<br/>\n    +  private int acksSinceLastCommit = 0 ;<br/>\n    +  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);<br/>\n    +  private final Timer commitTimer = new Timer();<br/>\n    +  private boolean fileReadCompletely = false;<br/>\n    +<br/>\n    +  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs kerberos configs<br/>\n    +<br/>\n    +  public HdfsSpout() </p>\n{\n    +  }\n<p>    +<br/>\n    +  public Path getLockDirPath() </p>\n{\n    +    return lockDirPath;\n    +  }\n<p>    +<br/>\n    +  public SpoutOutputCollector getCollector() </p>\n{\n    +    return collector;\n    +  }\n<p>    +<br/>\n    +  public HdfsSpout withConfigKey(String configKey)</p>\n{\n    +    this.configKey = configKey;\n    +    return this;\n    +  }\n<p>    +<br/>\n    +  public void nextTuple() {<br/>\n    +    LOG.debug(\"Next Tuple\");<br/>\n    +    // 1) First re-emit any previously failed tuples (from retryList)<br/>\n    +    if (!retryList.isEmpty()) </p>\n{\n    +      LOG.debug(\"Sending from retry list\");\n    +      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();\n    +      emitData(pair.getValue(), pair.getKey());\n    +      return;\n    +    }\n<p>    +<br/>\n    +    if( ackEnabled  &&  tracker.size()>=maxDuplicates ) {<br/>\n    +      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +<br/>\n    +       \"Progress tracker size has reached limit {}\"<br/>\n    +      , maxDuplicates);<br/>\n    +      // Don't emit anything .. allow configured spout wait strategy to kick in<br/>\n    +      return;<br/>\n    +    }<br/>\n    +<br/>\n    +    // 2) If no failed tuples, then send tuples from hdfs<br/>\n    +    while (true) {<br/>\n    +      try {<br/>\n    +// 3) Select a new file if one is not open already<br/>\n    +if (reader == null) {<br/>\n    +  reader = pickNextFile();<br/>\n    +  if (reader == null) </p>\n{\n    +    LOG.info(\"Currently no new files to process under : \" + sourceDirPath);\n    +    return;\n    +  }\n<p>    +}<br/>\n    +<br/>\n    +// 4) Read record from file, emit to collector and record progress<br/>\n    +List<Object> tuple = reader.next();<br/>\n    +if (tuple != null) {<br/>\n    +  fileReadCompletely= false;<br/>\n    +  ++tupleCounter;<br/>\n    +  MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());<br/>\n    +  emitData(tuple, msgId);<br/>\n    +<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    ++acksSinceLastCommit; // assume message is immediately acked in non-ack mode\n    +    commitProgress(reader.getFileOffset());\n    +  }\n<p> else </p>\n{\n    +    commitProgress(tracker.getCommitPosition());\n    +  }\n<p>    +  return;<br/>\n    +} else {<br/>\n    +  fileReadCompletely = true;<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    markFileAsDone(reader.getFilePath());\n    +  }\n<p>    +}<br/>\n    +      } catch (IOException e) </p>\n{\n    +LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);\n    +// don't emit anything .. allow configured spout wait strategy to kick in\n    +return;\n    +      }\n<p> catch (ParseException e) </p>\n{\n    +LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader) +\n    +\". Skipping remainder of file.\", e);\n    +markFileAsBad(reader.getFilePath());\n    +// note: Unfortunately not emitting anything here due to parse error\n    +// will trigger the configured spout wait strategy which is unnecessary\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  // will commit progress into lock file if commit threshold is reached<br/>\n    +  private void commitProgress(FileOffset position) {<br/>\n    +    if ( lock!=null && canCommitNow() ) {<br/>\n    +      try </p>\n{\n    +lock.heartbeat(position.toString());\n    +acksSinceLastCommit = 0;\n    +commitTimeElapsed.set(false);\n    +setupCommitElapseTimer();\n    +      }\n<p> catch (IOException e) </p>\n{\n    +LOG.error(\"Unable to commit progress Will retry later.\", e);\n    +      }\n<p>    +    }<br/>\n    +  }<br/>\n    +<br/>\n    +  private void setupCommitElapseTimer() {<br/>\n    +    if(commitFrequencySec<=0)<br/>\n    +      return;<br/>\n    +    TimerTask timerTask = new TimerTask() {<br/>\n    +      @Override<br/>\n    +      public void run() </p>\n{\n    +commitTimeElapsed.set(false);\n    +      }\n<p>    +    };<br/>\n    +    commitTimer.schedule(timerTask, commitFrequencySec * 1000);<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +  private static String getFileProgress(FileReader reader) </p>\n{\n    +    return reader.getFilePath() + \" \" + reader.getFileOffset();\n    +  }\n<p>    +<br/>\n    +  private void markFileAsDone(Path filePath) {<br/>\n    +    fileReadCompletely = false;<br/>\n    +    try </p>\n{\n    +      renameCompletedFile(reader.getFilePath());\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to archive completed file\" + filePath, e);\n    +    }\n<p>    +    unlockAndCloseReader();<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  private void markFileAsBad(Path file) {<br/>\n    +    String fileName = file.toString();<br/>\n    +    String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));<br/>\n    +    String originalName = new Path(fileNameMinusSuffix).getName();<br/>\n    +    Path  newFile = new Path( badFilesDirPath + Path.SEPARATOR + originalName);<br/>\n    +<br/>\n    +    LOG.info(\"Moving bad file to \" + newFile);<br/>\n    +    try {<br/>\n    +      if (!hdfs.rename(file, newFile) ) </p>\n{ // seems this can fail by returning false or throwing exception\n    +throw new IOException(\"Move failed for bad file: \" + file); // convert false ret value to exception\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.warn(\"Error moving bad file: \" + file + \". to destination :  \" + newFile);\n    +    }\n<p>    +<br/>\n    +    unlockAndCloseReader();<br/>\n    +  }<br/>\n    +<br/>\n    +  private void unlockAndCloseReader() {<br/>\n    +    reader.close();<br/>\n    +    reader = null;<br/>\n    +    try </p>\n{\n    +      lock.release();\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to delete lock file : \" + this.lock.getLockFile(), e);\n    +    }\n<p>    +    lock = null;<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +<br/>\n    +  protected void emitData(List<Object> tuple, MessageId id) {<br/>\n    +    LOG.debug(\"Emitting - {}\", id);<br/>\n    +    this.collector.emit(tuple, id);<br/>\n    +    inflight.put(id, tuple);<br/>\n    +  }<br/>\n    +<br/>\n    +  public void open(Map conf, TopologyContext context,  SpoutOutputCollector collector) {<br/>\n    +    this.conf = conf;<br/>\n    +    final String FILE_SYSTEM = \"filesystem\";<br/>\n    +    LOG.info(\"Opening\");<br/>\n    +    this.collector = collector;<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    this.tupleCounter = 0;<br/>\n    +<br/>\n    +    for( Object k : conf.keySet() ) {<br/>\n    +      String key = k.toString();<br/>\n    +      if( ! FILE_SYSTEM.equalsIgnoreCase( key ) ) </p>\n{ // to support unit test only\n    +String val = conf.get(key).toString();\n    +LOG.info(\"Config setting : \" + key + \" = \" + val);\n    +this.hdfsConfig.set(key, val);\n    +      }\n<p>    +      else<br/>\n    +this.hdfs = (FileSystem) conf.get(key);<br/>\n    +<br/>\n    +      if(key.equalsIgnoreCase(Configs.READER_TYPE)) </p>\n{\n    +readerType = conf.get(key).toString();\n    +checkValidReader(readerType);\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    // - Hdfs configs<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    Map<String, Object> map = (Map<String, Object>)conf.get(this.configKey);<br/>\n    +    if(map != null){<br/>\n    +      for(String key : map.keySet())</p>\n{\n    +this.hdfsConfig.set(key, String.valueOf(map.get(key)));\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    try </p>\n{\n    +      HdfsSecurityUtil.login(conf, hdfsConfig);\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Failed to open \" + sourceDirPath);\n    +      throw new RuntimeException(e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; source dir config<br/>\n    +    if ( !conf.containsKey(Configs.SOURCE_DIR) ) </p>\n{\n    +      LOG.error(Configs.SOURCE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.SOURCE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.sourceDirPath = new Path( conf.get(Configs.SOURCE_DIR).toString() );<br/>\n    +<br/>\n    +    // &#8211; archive dir config<br/>\n    +    if ( !conf.containsKey(Configs.ARCHIVE_DIR) ) </p>\n{\n    +      LOG.error(Configs.ARCHIVE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.ARCHIVE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.archiveDirPath = new Path( conf.get(Configs.ARCHIVE_DIR).toString() );<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(archiveDirPath)) {<br/>\n    +if(! hdfs.isDirectory(archiveDirPath) ) </p>\n{\n    +  LOG.error(\"Archive directory is a file. \" + archiveDirPath);\n    +  throw new RuntimeException(\"Archive directory is a file. \" + archiveDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(archiveDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create archive directory. \" + archiveDirPath);\n    +throw new RuntimeException(\"Unable to create archive directory \" + archiveDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(\"Unable to create archive directory \", e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; bad files dir config<br/>\n    +    if ( !conf.containsKey(Configs.BAD_DIR) ) </p>\n{\n    +      LOG.error(Configs.BAD_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.BAD_DIR + \" setting is required\");\n    +    }\n<p>    &#8212; End diff &#8211;</p>\n\n<p>    Configs.BAD_DIR, Configs.ARCHIVE_DIR and Configs.SOURCE_DIR validation cheks are same. You may want to extract a method for these common validations.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631000/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612628679","html_url":"https://github.com/apache/storm/issues/5025#issuecomment-2612628679","issue_url":"https://api.github.com/repos/apache/storm/issues/5025","id":2612628679,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjg2Nzk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T08:51:20Z","updated_at":"2025-01-24T14:13:04Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/900#discussion_r47610293\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/900#discussion_r47610293</a></p>\n\n<p>    &#8212; Diff: storm-core/src/jvm/backtype/storm/windowing/WaterMarkEventGenerator.java &#8212;<br/>\n    @@ -0,0 +1,110 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package backtype.storm.windowing;<br/>\n    +<br/>\n    +import backtype.storm.generated.GlobalStreamId;<br/>\n    +import backtype.storm.topology.FailedException;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Set;<br/>\n    +import java.util.concurrent.ConcurrentHashMap;<br/>\n    +import java.util.concurrent.ExecutionException;<br/>\n    +import java.util.concurrent.Executors;<br/>\n    +import java.util.concurrent.ScheduledExecutorService;<br/>\n    +import java.util.concurrent.ScheduledFuture;<br/>\n    +import java.util.concurrent.TimeUnit;<br/>\n    +<br/>\n    +/**<br/>\n    + * Tracks tuples across input streams and periodically emits watermark events.<br/>\n    + * Watermark event timestamp is the minimum of the latest tuple timestamps<br/>\n    + * across all the input streams (minus the lag). Once a watermark event is emitted<br/>\n    + * any tuple coming with an earlier timestamp can be considered as late events.<br/>\n    + */<br/>\n    +public class WaterMarkEventGenerator<T> implements Runnable {<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(WaterMarkEventGenerator.class);<br/>\n    +    private final WindowManager<T> windowManager;<br/>\n    +    private final int eventTsLag;<br/>\n    +    private final Set<GlobalStreamId> inputStreams;<br/>\n    +    private final Map<GlobalStreamId, Long> streamToTs;<br/>\n    +    private final ScheduledExecutorService executorService;<br/>\n    +    private final ScheduledFuture<?> executorFuture;<br/>\n    +    private long lastWaterMarkTs = 0;<br/>\n    +<br/>\n    +    public WaterMarkEventGenerator(WindowManager<T> windowManager, int interval,<br/>\n    +   int eventTsLag, Set<GlobalStreamId> inputStreams) </p>\n{\n    +this.windowManager = windowManager;\n    +streamToTs = new ConcurrentHashMap<>();\n    +executorService = Executors.newSingleThreadScheduledExecutor();\n    +this.executorFuture = executorService.scheduleAtFixedRate(this, interval, interval, TimeUnit.MILLISECONDS);\n    +this.eventTsLag = eventTsLag;\n    +this.inputStreams = inputStreams;\n    +    }\n<p>    +<br/>\n    +    public void track(GlobalStreamId stream, long ts) {<br/>\n    +Long currentVal = streamToTs.get(stream);<br/>\n    +if (currentVal == null || ts > currentVal) </p>\n{\n    +    streamToTs.put(stream, ts);\n    +}\n<p>    +checkFailures();<br/>\n    +    }<br/>\n    +<br/>\n    +    @Override<br/>\n    +    public void run() {<br/>\n    +try {<br/>\n    +    long waterMarkTs = computeWaterMarkTs();<br/>\n    +    if (waterMarkTs > lastWaterMarkTs) </p>\n{\n    +this.windowManager.add(new WaterMarkEvent<T>(waterMarkTs - eventTsLag));\n    +lastWaterMarkTs = waterMarkTs;\n    +    }\n<p>    +} catch (Throwable th) </p>\n{\n    +    LOG.error(\"Failed while processing watermark event \", th);\n    +    throw th;\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    /**<br/>\n    +     * Computes the min ts across all streams.<br/>\n    +     */<br/>\n    +    private long computeWaterMarkTs() {<br/>\n    +long ts = Long.MIN_VALUE;<br/>\n    +// only if some data has arrived on each input stream<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    The minimum of the latest event timestamps across all input streams (minus the lag) is considered as the watermark timestamp. This is so that if events from one of the streams is delayed more, we don't treat all events from that stream as late events.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612628679/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164690576","html_url":"https://github.com/apache/storm/pull/936#issuecomment-164690576","issue_url":"https://api.github.com/repos/apache/storm/issues/936","id":164690576,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDY5MDU3Ng==","user":{"login":"arunmahadevan","id":6792890,"node_id":"MDQ6VXNlcjY3OTI4OTA=","avatar_url":"https://avatars.githubusercontent.com/u/6792890?v=4","gravatar_id":"","url":"https://api.github.com/users/arunmahadevan","html_url":"https://github.com/arunmahadevan","followers_url":"https://api.github.com/users/arunmahadevan/followers","following_url":"https://api.github.com/users/arunmahadevan/following{/other_user}","gists_url":"https://api.github.com/users/arunmahadevan/gists{/gist_id}","starred_url":"https://api.github.com/users/arunmahadevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arunmahadevan/subscriptions","organizations_url":"https://api.github.com/users/arunmahadevan/orgs","repos_url":"https://api.github.com/users/arunmahadevan/repos","events_url":"https://api.github.com/users/arunmahadevan/events{/privacy}","received_events_url":"https://api.github.com/users/arunmahadevan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T08:57:20Z","updated_at":"2015-12-15T08:57:20Z","author_association":"CONTRIBUTOR","body":"Good to add some notes in the README.md to explain the internals at a high level and the usage with the different config options. May be add an example topology in the storm-starter as well.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164690576/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631007","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631007","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631007,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMDc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T08:57:21Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#issuecomment-164690576\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#issuecomment-164690576</a></p>\n\n<p>    Good to add some notes in the README.md to explain the internals at a high level and the usage with the different config options. May be add an example topology in the storm-starter as well.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631007/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631013","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631013","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631013,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMTM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T09:02:12Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47611203\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47611203</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java &#8212;<br/>\n    @@ -0,0 +1,654 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +import java.io.IOException;<br/>\n    +import java.lang.reflect.Constructor;<br/>\n    +import java.util.Collection;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Timer;<br/>\n    +import java.util.TimerTask;<br/>\n    +import java.util.concurrent.LinkedBlockingQueue;<br/>\n    +import java.util.concurrent.atomic.AtomicBoolean;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import org.apache.hadoop.conf.Configuration;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.apache.storm.hdfs.common.security.HdfsSecurityUtil;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +<br/>\n    +public class HdfsSpout extends BaseRichSpout {<br/>\n    +<br/>\n    +  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);<br/>\n    +<br/>\n    +  private Path sourceDirPath;<br/>\n    +  private Path archiveDirPath;<br/>\n    +  private Path badFilesDirPath;<br/>\n    +  private Path lockDirPath;<br/>\n    +<br/>\n    +  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;<br/>\n    +  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;<br/>\n    +  private int maxDuplicates = Configs.DEFAULT_MAX_DUPLICATES;<br/>\n    +  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;<br/>\n    +  private boolean clocksInSync = true;<br/>\n    +<br/>\n    +  private ProgressTracker tracker = new ProgressTracker();<br/>\n    +<br/>\n    +  private FileSystem hdfs;<br/>\n    +  private FileReader reader;<br/>\n    +<br/>\n    +  private SpoutOutputCollector collector;<br/>\n    +  HashMap<MessageId, List<Object> > inflight = new HashMap<>();<br/>\n    +  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();<br/>\n    +<br/>\n    +  private String inprogress_suffix = \".inprogress\";<br/>\n    +  private String ignoreSuffix = \".ignore\";<br/>\n    +<br/>\n    +  private Configuration hdfsConfig;<br/>\n    +  private String readerType;<br/>\n    +<br/>\n    +  private Map conf = null;<br/>\n    +  private FileLock lock;<br/>\n    +  private String spoutId = null;<br/>\n    +<br/>\n    +  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;<br/>\n    +  private long lastExpiredLockTime = 0;<br/>\n    +<br/>\n    +  private long tupleCounter = 0;<br/>\n    +  private boolean ackEnabled = false;<br/>\n    +  private int acksSinceLastCommit = 0 ;<br/>\n    +  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);<br/>\n    +  private final Timer commitTimer = new Timer();<br/>\n    +  private boolean fileReadCompletely = false;<br/>\n    +<br/>\n    +  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs kerberos configs<br/>\n    +<br/>\n    +  public HdfsSpout() </p>\n{\n    +  }\n<p>    +<br/>\n    +  public Path getLockDirPath() </p>\n{\n    +    return lockDirPath;\n    +  }\n<p>    +<br/>\n    +  public SpoutOutputCollector getCollector() </p>\n{\n    +    return collector;\n    +  }\n<p>    +<br/>\n    +  public HdfsSpout withConfigKey(String configKey)</p>\n{\n    +    this.configKey = configKey;\n    +    return this;\n    +  }\n<p>    +<br/>\n    +  public void nextTuple() {<br/>\n    +    LOG.debug(\"Next Tuple\");<br/>\n    +    // 1) First re-emit any previously failed tuples (from retryList)<br/>\n    +    if (!retryList.isEmpty()) </p>\n{\n    +      LOG.debug(\"Sending from retry list\");\n    +      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();\n    +      emitData(pair.getValue(), pair.getKey());\n    +      return;\n    +    }\n<p>    +<br/>\n    +    if( ackEnabled  &&  tracker.size()>=maxDuplicates ) {<br/>\n    +      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +<br/>\n    +       \"Progress tracker size has reached limit {}\"<br/>\n    +      , maxDuplicates);<br/>\n    +      // Don't emit anything .. allow configured spout wait strategy to kick in<br/>\n    +      return;<br/>\n    +    }<br/>\n    +<br/>\n    +    // 2) If no failed tuples, then send tuples from hdfs<br/>\n    +    while (true) {<br/>\n    +      try {<br/>\n    +// 3) Select a new file if one is not open already<br/>\n    +if (reader == null) {<br/>\n    +  reader = pickNextFile();<br/>\n    +  if (reader == null) </p>\n{\n    +    LOG.info(\"Currently no new files to process under : \" + sourceDirPath);\n    +    return;\n    +  }\n<p>    +}<br/>\n    +<br/>\n    +// 4) Read record from file, emit to collector and record progress<br/>\n    +List<Object> tuple = reader.next();<br/>\n    +if (tuple != null) {<br/>\n    +  fileReadCompletely= false;<br/>\n    +  ++tupleCounter;<br/>\n    +  MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());<br/>\n    +  emitData(tuple, msgId);<br/>\n    +<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    ++acksSinceLastCommit; // assume message is immediately acked in non-ack mode\n    +    commitProgress(reader.getFileOffset());\n    +  }\n<p> else </p>\n{\n    +    commitProgress(tracker.getCommitPosition());\n    +  }\n<p>    +  return;<br/>\n    +} else {<br/>\n    +  fileReadCompletely = true;<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    markFileAsDone(reader.getFilePath());\n    +  }\n<p>    +}<br/>\n    +      } catch (IOException e) </p>\n{\n    +LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);\n    +// don't emit anything .. allow configured spout wait strategy to kick in\n    +return;\n    +      }\n<p> catch (ParseException e) </p>\n{\n    +LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader) +\n    +\". Skipping remainder of file.\", e);\n    +markFileAsBad(reader.getFilePath());\n    +// note: Unfortunately not emitting anything here due to parse error\n    +// will trigger the configured spout wait strategy which is unnecessary\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  // will commit progress into lock file if commit threshold is reached<br/>\n    +  private void commitProgress(FileOffset position) {<br/>\n    +    if ( lock!=null && canCommitNow() ) {<br/>\n    +      try </p>\n{\n    +lock.heartbeat(position.toString());\n    +acksSinceLastCommit = 0;\n    +commitTimeElapsed.set(false);\n    +setupCommitElapseTimer();\n    +      }\n<p> catch (IOException e) </p>\n{\n    +LOG.error(\"Unable to commit progress Will retry later.\", e);\n    +      }\n<p>    +    }<br/>\n    +  }<br/>\n    +<br/>\n    +  private void setupCommitElapseTimer() {<br/>\n    +    if(commitFrequencySec<=0)<br/>\n    +      return;<br/>\n    +    TimerTask timerTask = new TimerTask() {<br/>\n    +      @Override<br/>\n    +      public void run() {<br/>\n    +commitTimeElapsed.set(false);<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Should not commitTimeElapsed be set to true?  </p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631013/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631017","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631017","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631017,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMTc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T09:03:03Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47611293\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47611293</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java &#8212;<br/>\n    @@ -0,0 +1,654 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +import java.io.IOException;<br/>\n    +import java.lang.reflect.Constructor;<br/>\n    +import java.util.Collection;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Timer;<br/>\n    +import java.util.TimerTask;<br/>\n    +import java.util.concurrent.LinkedBlockingQueue;<br/>\n    +import java.util.concurrent.atomic.AtomicBoolean;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import org.apache.hadoop.conf.Configuration;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.apache.storm.hdfs.common.security.HdfsSecurityUtil;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +<br/>\n    +public class HdfsSpout extends BaseRichSpout {<br/>\n    +<br/>\n    +  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);<br/>\n    +<br/>\n    +  private Path sourceDirPath;<br/>\n    +  private Path archiveDirPath;<br/>\n    +  private Path badFilesDirPath;<br/>\n    +  private Path lockDirPath;<br/>\n    +<br/>\n    +  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;<br/>\n    +  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;<br/>\n    +  private int maxDuplicates = Configs.DEFAULT_MAX_DUPLICATES;<br/>\n    +  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;<br/>\n    +  private boolean clocksInSync = true;<br/>\n    +<br/>\n    +  private ProgressTracker tracker = new ProgressTracker();<br/>\n    +<br/>\n    +  private FileSystem hdfs;<br/>\n    +  private FileReader reader;<br/>\n    +<br/>\n    +  private SpoutOutputCollector collector;<br/>\n    +  HashMap<MessageId, List<Object> > inflight = new HashMap<>();<br/>\n    +  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();<br/>\n    +<br/>\n    +  private String inprogress_suffix = \".inprogress\";<br/>\n    +  private String ignoreSuffix = \".ignore\";<br/>\n    +<br/>\n    +  private Configuration hdfsConfig;<br/>\n    +  private String readerType;<br/>\n    +<br/>\n    +  private Map conf = null;<br/>\n    +  private FileLock lock;<br/>\n    +  private String spoutId = null;<br/>\n    +<br/>\n    +  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;<br/>\n    +  private long lastExpiredLockTime = 0;<br/>\n    +<br/>\n    +  private long tupleCounter = 0;<br/>\n    +  private boolean ackEnabled = false;<br/>\n    +  private int acksSinceLastCommit = 0 ;<br/>\n    +  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);<br/>\n    +  private final Timer commitTimer = new Timer();<br/>\n    +  private boolean fileReadCompletely = false;<br/>\n    +<br/>\n    +  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs kerberos configs<br/>\n    +<br/>\n    +  public HdfsSpout() </p>\n{\n    +  }\n<p>    +<br/>\n    +  public Path getLockDirPath() </p>\n{\n    +    return lockDirPath;\n    +  }\n<p>    +<br/>\n    +  public SpoutOutputCollector getCollector() </p>\n{\n    +    return collector;\n    +  }\n<p>    +<br/>\n    +  public HdfsSpout withConfigKey(String configKey)</p>\n{\n    +    this.configKey = configKey;\n    +    return this;\n    +  }\n<p>    +<br/>\n    +  public void nextTuple() {<br/>\n    +    LOG.debug(\"Next Tuple\");<br/>\n    +    // 1) First re-emit any previously failed tuples (from retryList)<br/>\n    +    if (!retryList.isEmpty()) </p>\n{\n    +      LOG.debug(\"Sending from retry list\");\n    +      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();\n    +      emitData(pair.getValue(), pair.getKey());\n    +      return;\n    +    }\n<p>    +<br/>\n    +    if( ackEnabled  &&  tracker.size()>=maxDuplicates ) {<br/>\n    +      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +<br/>\n    +       \"Progress tracker size has reached limit {}\"<br/>\n    +      , maxDuplicates);<br/>\n    +      // Don't emit anything .. allow configured spout wait strategy to kick in<br/>\n    +      return;<br/>\n    +    }<br/>\n    +<br/>\n    +    // 2) If no failed tuples, then send tuples from hdfs<br/>\n    +    while (true) {<br/>\n    +      try {<br/>\n    +// 3) Select a new file if one is not open already<br/>\n    +if (reader == null) {<br/>\n    +  reader = pickNextFile();<br/>\n    +  if (reader == null) </p>\n{\n    +    LOG.info(\"Currently no new files to process under : \" + sourceDirPath);\n    +    return;\n    +  }\n<p>    +}<br/>\n    +<br/>\n    +// 4) Read record from file, emit to collector and record progress<br/>\n    +List<Object> tuple = reader.next();<br/>\n    +if (tuple != null) {<br/>\n    +  fileReadCompletely= false;<br/>\n    +  ++tupleCounter;<br/>\n    +  MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());<br/>\n    +  emitData(tuple, msgId);<br/>\n    +<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    ++acksSinceLastCommit; // assume message is immediately acked in non-ack mode\n    +    commitProgress(reader.getFileOffset());\n    +  }\n<p> else </p>\n{\n    +    commitProgress(tracker.getCommitPosition());\n    +  }\n<p>    +  return;<br/>\n    +} else {<br/>\n    +  fileReadCompletely = true;<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    markFileAsDone(reader.getFilePath());\n    +  }\n<p>    +}<br/>\n    +      } catch (IOException e) </p>\n{\n    +LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);\n    +// don't emit anything .. allow configured spout wait strategy to kick in\n    +return;\n    +      }\n<p> catch (ParseException e) </p>\n{\n    +LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader) +\n    +\". Skipping remainder of file.\", e);\n    +markFileAsBad(reader.getFilePath());\n    +// note: Unfortunately not emitting anything here due to parse error\n    +// will trigger the configured spout wait strategy which is unnecessary\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  // will commit progress into lock file if commit threshold is reached<br/>\n    +  private void commitProgress(FileOffset position) {<br/>\n    +    if ( lock!=null && canCommitNow() ) {<br/>\n    +      try </p>\n{\n    +lock.heartbeat(position.toString());\n    +acksSinceLastCommit = 0;\n    +commitTimeElapsed.set(false);\n    +setupCommitElapseTimer();\n    +      }\n<p> catch (IOException e) </p>\n{\n    +LOG.error(\"Unable to commit progress Will retry later.\", e);\n    +      }\n<p>    +    }<br/>\n    +  }<br/>\n    +<br/>\n    +  private void setupCommitElapseTimer() {<br/>\n    +    if(commitFrequencySec<=0)<br/>\n    +      return;<br/>\n    +    TimerTask timerTask = new TimerTask() {<br/>\n    +      @Override<br/>\n    +      public void run() </p>\n{\n    +commitTimeElapsed.set(false);\n    +      }\n<p>    +    };<br/>\n    +    commitTimer.schedule(timerTask, commitFrequencySec * 1000);<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +  private static String getFileProgress(FileReader reader) </p>\n{\n    +    return reader.getFilePath() + \" \" + reader.getFileOffset();\n    +  }\n<p>    +<br/>\n    +  private void markFileAsDone(Path filePath) {<br/>\n    +    fileReadCompletely = false;<br/>\n    +    try </p>\n{\n    +      renameCompletedFile(reader.getFilePath());\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to archive completed file\" + filePath, e);\n    +    }\n<p>    +    unlockAndCloseReader();<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  private void markFileAsBad(Path file) {<br/>\n    +    String fileName = file.toString();<br/>\n    +    String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));<br/>\n    +    String originalName = new Path(fileNameMinusSuffix).getName();<br/>\n    +    Path  newFile = new Path( badFilesDirPath + Path.SEPARATOR + originalName);<br/>\n    +<br/>\n    +    LOG.info(\"Moving bad file to \" + newFile);<br/>\n    +    try {<br/>\n    +      if (!hdfs.rename(file, newFile) ) </p>\n{ // seems this can fail by returning false or throwing exception\n    +throw new IOException(\"Move failed for bad file: \" + file); // convert false ret value to exception\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.warn(\"Error moving bad file: \" + file + \". to destination :  \" + newFile);\n    +    }\n<p>    +<br/>\n    +    unlockAndCloseReader();<br/>\n    +  }<br/>\n    +<br/>\n    +  private void unlockAndCloseReader() {<br/>\n    +    reader.close();<br/>\n    +    reader = null;<br/>\n    +    try </p>\n{\n    +      lock.release();\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to delete lock file : \" + this.lock.getLockFile(), e);\n    +    }\n<p>    +    lock = null;<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +<br/>\n    +  protected void emitData(List<Object> tuple, MessageId id) {<br/>\n    +    LOG.debug(\"Emitting - {}\", id);<br/>\n    +    this.collector.emit(tuple, id);<br/>\n    +    inflight.put(id, tuple);<br/>\n    +  }<br/>\n    +<br/>\n    +  public void open(Map conf, TopologyContext context,  SpoutOutputCollector collector) {<br/>\n    +    this.conf = conf;<br/>\n    +    final String FILE_SYSTEM = \"filesystem\";<br/>\n    +    LOG.info(\"Opening\");<br/>\n    +    this.collector = collector;<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    this.tupleCounter = 0;<br/>\n    +<br/>\n    +    for( Object k : conf.keySet() ) {<br/>\n    +      String key = k.toString();<br/>\n    +      if( ! FILE_SYSTEM.equalsIgnoreCase( key ) ) </p>\n{ // to support unit test only\n    +String val = conf.get(key).toString();\n    +LOG.info(\"Config setting : \" + key + \" = \" + val);\n    +this.hdfsConfig.set(key, val);\n    +      }\n<p>    +      else<br/>\n    +this.hdfs = (FileSystem) conf.get(key);<br/>\n    +<br/>\n    +      if(key.equalsIgnoreCase(Configs.READER_TYPE)) </p>\n{\n    +readerType = conf.get(key).toString();\n    +checkValidReader(readerType);\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    // - Hdfs configs<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    Map<String, Object> map = (Map<String, Object>)conf.get(this.configKey);<br/>\n    +    if(map != null){<br/>\n    +      for(String key : map.keySet())</p>\n{\n    +this.hdfsConfig.set(key, String.valueOf(map.get(key)));\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    try </p>\n{\n    +      HdfsSecurityUtil.login(conf, hdfsConfig);\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Failed to open \" + sourceDirPath);\n    +      throw new RuntimeException(e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; source dir config<br/>\n    +    if ( !conf.containsKey(Configs.SOURCE_DIR) ) </p>\n{\n    +      LOG.error(Configs.SOURCE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.SOURCE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.sourceDirPath = new Path( conf.get(Configs.SOURCE_DIR).toString() );<br/>\n    +<br/>\n    +    // &#8211; archive dir config<br/>\n    +    if ( !conf.containsKey(Configs.ARCHIVE_DIR) ) </p>\n{\n    +      LOG.error(Configs.ARCHIVE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.ARCHIVE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.archiveDirPath = new Path( conf.get(Configs.ARCHIVE_DIR).toString() );<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(archiveDirPath)) {<br/>\n    +if(! hdfs.isDirectory(archiveDirPath) ) </p>\n{\n    +  LOG.error(\"Archive directory is a file. \" + archiveDirPath);\n    +  throw new RuntimeException(\"Archive directory is a file. \" + archiveDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(archiveDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create archive directory. \" + archiveDirPath);\n    +throw new RuntimeException(\"Unable to create archive directory \" + archiveDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(\"Unable to create archive directory \", e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; bad files dir config<br/>\n    +    if ( !conf.containsKey(Configs.BAD_DIR) ) </p>\n{\n    +      LOG.error(Configs.BAD_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.BAD_DIR + \" setting is required\");\n    +    }\n<p>    +<br/>\n    +    this.badFilesDirPath = new Path(conf.get(Configs.BAD_DIR).toString());<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(badFilesDirPath)) {<br/>\n    +if(! hdfs.isDirectory(badFilesDirPath) ) </p>\n{\n    +  LOG.error(\"Bad files directory is a file: \" + badFilesDirPath);\n    +  throw new RuntimeException(\"Bad files directory is a file: \" + badFilesDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(badFilesDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create directory for bad files: \" + badFilesDirPath);\n    +throw new RuntimeException(\"Unable to create a directory for bad files: \" + badFilesDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(e.getMessage(), e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; ignore filename suffix<br/>\n    +    if ( conf.containsKey(Configs.IGNORE_SUFFIX) ) </p>\n{\n    +      this.ignoreSuffix = conf.get(Configs.IGNORE_SUFFIX).toString();\n    +    }\n<p>    +<br/>\n    +    // &#8211; lock dir config<br/>\n    +    String lockDir = !conf.containsKey(Configs.LOCK_DIR) ? getDefaultLockDir(sourceDirPath) : conf.get(Configs.LOCK_DIR).toString() ;<br/>\n    +    this.lockDirPath = new Path(lockDir);<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(lockDirPath)) {<br/>\n    +if(! hdfs.isDirectory(lockDirPath) ) </p>\n{\n    +  LOG.error(\"Lock directory is a file: \" + lockDirPath);\n    +  throw new RuntimeException(\"Lock directory is a file: \" + lockDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(lockDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create lock directory: \" + lockDirPath);\n    +throw new RuntimeException(\"Unable to create lock directory: \" + lockDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create lock dir: \" + lockDirPath, e);\n    +      throw new RuntimeException(e.getMessage(), e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; lock timeout<br/>\n    +    if( conf.get(Configs.LOCK_TIMEOUT) !=null )<br/>\n    +      this.lockTimeoutSec =  Integer.parseInt(conf.get(Configs.LOCK_TIMEOUT).toString());<br/>\n    +<br/>\n    +    // &#8211; enable/disable ACKing<br/>\n    +    Object ackers = conf.get(Config.TOPOLOGY_ACKER_EXECUTORS);<br/>\n    +    if( ackers!=null )<br/>\n    +      this.ackEnabled = ( Integer.parseInt( ackers.toString() ) > 0 );<br/>\n    +    else<br/>\n    +      this.ackEnabled = false;<br/>\n    +<br/>\n    +    // &#8211; commit frequency - count<br/>\n    +    if( conf.get(Configs.COMMIT_FREQ_COUNT) != null )<br/>\n    +      commitFrequencyCount = Integer.parseInt( conf.get(Configs.COMMIT_FREQ_COUNT).toString() );<br/>\n    +<br/>\n    +    // &#8211; commit frequency - seconds<br/>\n    +    if( conf.get(Configs.COMMIT_FREQ_SEC) != null )<br/>\n    +      commitFrequencySec = Integer.parseInt( conf.get(Configs.COMMIT_FREQ_SEC).toString() );<br/>\n    +<br/>\n    +    // &#8211; max duplicate<br/>\n    +    if( conf.get(Configs.MAX_DUPLICATE) !=null )<br/>\n    +      maxDuplicates = Integer.parseInt( conf.get(Configs.MAX_DUPLICATE).toString() );<br/>\n    +<br/>\n    +    // &#8211; clocks in sync<br/>\n    +    if( conf.get(Configs.CLOCKS_INSYNC) !=null )<br/>\n    +      clocksInSync = Boolean.parseBoolean(conf.get(Configs.CLOCKS_INSYNC).toString());<br/>\n    +<br/>\n    +    // &#8211; spout id<br/>\n    +    spoutId = context.getThisComponentId();<br/>\n    +<br/>\n    +    // setup timer for commit elapse time tracking<br/>\n    +    setupCommitElapseTimer();<br/>\n    +  }<br/>\n    +<br/>\n    +  private String getDefaultLockDir(Path sourceDirPath) </p>\n{\n    +    return sourceDirPath.toString() + Path.SEPARATOR + Configs.DEFAULT_LOCK_DIR;\n    +  }\n<p>    +<br/>\n    +  private static void checkValidReader(String readerType) {<br/>\n    +    if(readerType.equalsIgnoreCase(Configs.TEXT)  || readerType.equalsIgnoreCase(Configs.SEQ) )<br/>\n    +      return;<br/>\n    +    try </p>\n{\n    +      Class<?> classType = Class.forName(readerType);\n    +      classType.getConstructor(FileSystem.class, Path.class, Map.class);\n    +      return;\n    +    }\n<p> catch (ClassNotFoundException e) </p>\n{\n    +      LOG.error(readerType + \" not found in classpath.\", e);\n    +      throw new IllegalArgumentException(readerType + \" not found in classpath.\", e);\n    +    }\n<p> catch (NoSuchMethodException e) </p>\n{\n    +      LOG.error(readerType + \" is missing the expected constructor for Readers.\", e);\n    +      throw new IllegalArgumentException(readerType + \" is missing the expected constuctor for Readers.\");\n    +    }\n<p>    +  }<br/>\n    +<br/>\n    +  @Override<br/>\n    +  public void ack(Object msgId) {<br/>\n    +    MessageId id = (MessageId) msgId;<br/>\n    +    inflight.remove(id);<br/>\n    +    ++acksSinceLastCommit;<br/>\n    +    tracker.recordAckedOffset(id.offset);<br/>\n    +    commitProgress(tracker.getCommitPosition());<br/>\n    +    if(fileReadCompletely) </p>\n{\n    +      markFileAsDone(reader.getFilePath());\n    +      reader = null;\n    +    }\n<p>    +    super.ack(msgId);<br/>\n    +  }<br/>\n    +<br/>\n    +  private boolean canCommitNow() {<br/>\n    +    if( acksSinceLastCommit >= commitFrequencyCount )<br/>\n    +      return true;<br/>\n    +    return commitTimeElapsed.get();<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    commitTimeelapsed is never set to `true` anywhere?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631017/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631019","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631019","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631019,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMTk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T09:03:50Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47611380\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47611380</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java &#8212;<br/>\n    @@ -0,0 +1,654 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +import java.io.IOException;<br/>\n    +import java.lang.reflect.Constructor;<br/>\n    +import java.util.Collection;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Timer;<br/>\n    +import java.util.TimerTask;<br/>\n    +import java.util.concurrent.LinkedBlockingQueue;<br/>\n    +import java.util.concurrent.atomic.AtomicBoolean;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import org.apache.hadoop.conf.Configuration;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.apache.storm.hdfs.common.security.HdfsSecurityUtil;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +<br/>\n    +public class HdfsSpout extends BaseRichSpout {<br/>\n    +<br/>\n    +  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);<br/>\n    +<br/>\n    +  private Path sourceDirPath;<br/>\n    +  private Path archiveDirPath;<br/>\n    +  private Path badFilesDirPath;<br/>\n    +  private Path lockDirPath;<br/>\n    +<br/>\n    +  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;<br/>\n    +  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;<br/>\n    +  private int maxDuplicates = Configs.DEFAULT_MAX_DUPLICATES;<br/>\n    +  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;<br/>\n    +  private boolean clocksInSync = true;<br/>\n    +<br/>\n    +  private ProgressTracker tracker = new ProgressTracker();<br/>\n    +<br/>\n    +  private FileSystem hdfs;<br/>\n    +  private FileReader reader;<br/>\n    +<br/>\n    +  private SpoutOutputCollector collector;<br/>\n    +  HashMap<MessageId, List<Object> > inflight = new HashMap<>();<br/>\n    +  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();<br/>\n    +<br/>\n    +  private String inprogress_suffix = \".inprogress\";<br/>\n    +  private String ignoreSuffix = \".ignore\";<br/>\n    +<br/>\n    +  private Configuration hdfsConfig;<br/>\n    +  private String readerType;<br/>\n    +<br/>\n    +  private Map conf = null;<br/>\n    +  private FileLock lock;<br/>\n    +  private String spoutId = null;<br/>\n    +<br/>\n    +  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;<br/>\n    +  private long lastExpiredLockTime = 0;<br/>\n    +<br/>\n    +  private long tupleCounter = 0;<br/>\n    +  private boolean ackEnabled = false;<br/>\n    +  private int acksSinceLastCommit = 0 ;<br/>\n    +  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);<br/>\n    +  private final Timer commitTimer = new Timer();<br/>\n    +  private boolean fileReadCompletely = false;<br/>\n    +<br/>\n    +  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs kerberos configs<br/>\n    +<br/>\n    +  public HdfsSpout() </p>\n{\n    +  }\n<p>    +<br/>\n    +  public Path getLockDirPath() </p>\n{\n    +    return lockDirPath;\n    +  }\n<p>    +<br/>\n    +  public SpoutOutputCollector getCollector() </p>\n{\n    +    return collector;\n    +  }\n<p>    +<br/>\n    +  public HdfsSpout withConfigKey(String configKey)</p>\n{\n    +    this.configKey = configKey;\n    +    return this;\n    +  }\n<p>    +<br/>\n    +  public void nextTuple() {<br/>\n    +    LOG.debug(\"Next Tuple\");<br/>\n    +    // 1) First re-emit any previously failed tuples (from retryList)<br/>\n    +    if (!retryList.isEmpty()) </p>\n{\n    +      LOG.debug(\"Sending from retry list\");\n    +      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();\n    +      emitData(pair.getValue(), pair.getKey());\n    +      return;\n    +    }\n<p>    +<br/>\n    +    if( ackEnabled  &&  tracker.size()>=maxDuplicates ) {<br/>\n    +      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +<br/>\n    +       \"Progress tracker size has reached limit {}\"<br/>\n    +      , maxDuplicates);<br/>\n    +      // Don't emit anything .. allow configured spout wait strategy to kick in<br/>\n    +      return;<br/>\n    +    }<br/>\n    +<br/>\n    +    // 2) If no failed tuples, then send tuples from hdfs<br/>\n    +    while (true) {<br/>\n    +      try {<br/>\n    +// 3) Select a new file if one is not open already<br/>\n    +if (reader == null) {<br/>\n    +  reader = pickNextFile();<br/>\n    +  if (reader == null) </p>\n{\n    +    LOG.info(\"Currently no new files to process under : \" + sourceDirPath);\n    +    return;\n    +  }\n<p>    +}<br/>\n    +<br/>\n    +// 4) Read record from file, emit to collector and record progress<br/>\n    +List<Object> tuple = reader.next();<br/>\n    +if (tuple != null) {<br/>\n    +  fileReadCompletely= false;<br/>\n    +  ++tupleCounter;<br/>\n    +  MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());<br/>\n    +  emitData(tuple, msgId);<br/>\n    +<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    ++acksSinceLastCommit; // assume message is immediately acked in non-ack mode\n    +    commitProgress(reader.getFileOffset());\n    +  }\n<p> else </p>\n{\n    +    commitProgress(tracker.getCommitPosition());\n    +  }\n<p>    +  return;<br/>\n    +} else {<br/>\n    +  fileReadCompletely = true;<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    markFileAsDone(reader.getFilePath());\n    +  }\n<p>    +}<br/>\n    +      } catch (IOException e) </p>\n{\n    +LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);\n    +// don't emit anything .. allow configured spout wait strategy to kick in\n    +return;\n    +      }\n<p> catch (ParseException e) </p>\n{\n    +LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader) +\n    +\". Skipping remainder of file.\", e);\n    +markFileAsBad(reader.getFilePath());\n    +// note: Unfortunately not emitting anything here due to parse error\n    +// will trigger the configured spout wait strategy which is unnecessary\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  // will commit progress into lock file if commit threshold is reached<br/>\n    +  private void commitProgress(FileOffset position) {<br/>\n    +    if ( lock!=null && canCommitNow() ) {<br/>\n    +      try </p>\n{\n    +lock.heartbeat(position.toString());\n    +acksSinceLastCommit = 0;\n    +commitTimeElapsed.set(false);\n    +setupCommitElapseTimer();\n    +      }\n<p> catch (IOException e) </p>\n{\n    +LOG.error(\"Unable to commit progress Will retry later.\", e);\n    +      }\n<p>    +    }<br/>\n    +  }<br/>\n    +<br/>\n    +  private void setupCommitElapseTimer() {<br/>\n    +    if(commitFrequencySec<=0)<br/>\n    +      return;<br/>\n    +    TimerTask timerTask = new TimerTask() {<br/>\n    +      @Override<br/>\n    +      public void run() </p>\n{\n    +commitTimeElapsed.set(false);\n    +      }\n<p>    +    };<br/>\n    +    commitTimer.schedule(timerTask, commitFrequencySec * 1000);<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +  private static String getFileProgress(FileReader reader) </p>\n{\n    +    return reader.getFilePath() + \" \" + reader.getFileOffset();\n    +  }\n<p>    +<br/>\n    +  private void markFileAsDone(Path filePath) {<br/>\n    +    fileReadCompletely = false;<br/>\n    +    try </p>\n{\n    +      renameCompletedFile(reader.getFilePath());\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to archive completed file\" + filePath, e);\n    +    }\n<p>    +    unlockAndCloseReader();<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  private void markFileAsBad(Path file) {<br/>\n    +    String fileName = file.toString();<br/>\n    +    String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));<br/>\n    +    String originalName = new Path(fileNameMinusSuffix).getName();<br/>\n    +    Path  newFile = new Path( badFilesDirPath + Path.SEPARATOR + originalName);<br/>\n    +<br/>\n    +    LOG.info(\"Moving bad file to \" + newFile);<br/>\n    +    try {<br/>\n    +      if (!hdfs.rename(file, newFile) ) </p>\n{ // seems this can fail by returning false or throwing exception\n    +throw new IOException(\"Move failed for bad file: \" + file); // convert false ret value to exception\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.warn(\"Error moving bad file: \" + file + \". to destination :  \" + newFile);\n    +    }\n<p>    +<br/>\n    +    unlockAndCloseReader();<br/>\n    +  }<br/>\n    +<br/>\n    +  private void unlockAndCloseReader() {<br/>\n    +    reader.close();<br/>\n    +    reader = null;<br/>\n    +    try </p>\n{\n    +      lock.release();\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to delete lock file : \" + this.lock.getLockFile(), e);\n    +    }\n<p>    +    lock = null;<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +<br/>\n    +  protected void emitData(List<Object> tuple, MessageId id) {<br/>\n    +    LOG.debug(\"Emitting - {}\", id);<br/>\n    +    this.collector.emit(tuple, id);<br/>\n    +    inflight.put(id, tuple);<br/>\n    +  }<br/>\n    +<br/>\n    +  public void open(Map conf, TopologyContext context,  SpoutOutputCollector collector) {<br/>\n    +    this.conf = conf;<br/>\n    +    final String FILE_SYSTEM = \"filesystem\";<br/>\n    +    LOG.info(\"Opening\");<br/>\n    +    this.collector = collector;<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    this.tupleCounter = 0;<br/>\n    +<br/>\n    +    for( Object k : conf.keySet() ) {<br/>\n    +      String key = k.toString();<br/>\n    +      if( ! FILE_SYSTEM.equalsIgnoreCase( key ) ) </p>\n{ // to support unit test only\n    +String val = conf.get(key).toString();\n    +LOG.info(\"Config setting : \" + key + \" = \" + val);\n    +this.hdfsConfig.set(key, val);\n    +      }\n<p>    +      else<br/>\n    +this.hdfs = (FileSystem) conf.get(key);<br/>\n    +<br/>\n    +      if(key.equalsIgnoreCase(Configs.READER_TYPE)) </p>\n{\n    +readerType = conf.get(key).toString();\n    +checkValidReader(readerType);\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    // - Hdfs configs<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    Map<String, Object> map = (Map<String, Object>)conf.get(this.configKey);<br/>\n    +    if(map != null){<br/>\n    +      for(String key : map.keySet())</p>\n{\n    +this.hdfsConfig.set(key, String.valueOf(map.get(key)));\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    try </p>\n{\n    +      HdfsSecurityUtil.login(conf, hdfsConfig);\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Failed to open \" + sourceDirPath);\n    +      throw new RuntimeException(e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; source dir config<br/>\n    +    if ( !conf.containsKey(Configs.SOURCE_DIR) ) </p>\n{\n    +      LOG.error(Configs.SOURCE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.SOURCE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.sourceDirPath = new Path( conf.get(Configs.SOURCE_DIR).toString() );<br/>\n    +<br/>\n    +    // &#8211; archive dir config<br/>\n    +    if ( !conf.containsKey(Configs.ARCHIVE_DIR) ) </p>\n{\n    +      LOG.error(Configs.ARCHIVE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.ARCHIVE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.archiveDirPath = new Path( conf.get(Configs.ARCHIVE_DIR).toString() );<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(archiveDirPath)) {<br/>\n    +if(! hdfs.isDirectory(archiveDirPath) ) </p>\n{\n    +  LOG.error(\"Archive directory is a file. \" + archiveDirPath);\n    +  throw new RuntimeException(\"Archive directory is a file. \" + archiveDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(archiveDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create archive directory. \" + archiveDirPath);\n    +throw new RuntimeException(\"Unable to create archive directory \" + archiveDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(\"Unable to create archive directory \", e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; bad files dir config<br/>\n    +    if ( !conf.containsKey(Configs.BAD_DIR) ) </p>\n{\n    +      LOG.error(Configs.BAD_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.BAD_DIR + \" setting is required\");\n    +    }\n<p>    +<br/>\n    +    this.badFilesDirPath = new Path(conf.get(Configs.BAD_DIR).toString());<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(badFilesDirPath)) {<br/>\n    +if(! hdfs.isDirectory(badFilesDirPath) ) </p>\n{\n    +  LOG.error(\"Bad files directory is a file: \" + badFilesDirPath);\n    +  throw new RuntimeException(\"Bad files directory is a file: \" + badFilesDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(badFilesDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create directory for bad files: \" + badFilesDirPath);\n    +throw new RuntimeException(\"Unable to create a directory for bad files: \" + badFilesDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(e.getMessage(), e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; ignore filename suffix<br/>\n    +    if ( conf.containsKey(Configs.IGNORE_SUFFIX) ) </p>\n{\n    +      this.ignoreSuffix = conf.get(Configs.IGNORE_SUFFIX).toString();\n    +    }\n<p>    +<br/>\n    +    // &#8211; lock dir config<br/>\n    +    String lockDir = !conf.containsKey(Configs.LOCK_DIR) ? getDefaultLockDir(sourceDirPath) : conf.get(Configs.LOCK_DIR).toString() ;<br/>\n    +    this.lockDirPath = new Path(lockDir);<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(lockDirPath)) {<br/>\n    +if(! hdfs.isDirectory(lockDirPath) ) </p>\n{\n    +  LOG.error(\"Lock directory is a file: \" + lockDirPath);\n    +  throw new RuntimeException(\"Lock directory is a file: \" + lockDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(lockDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create lock directory: \" + lockDirPath);\n    +throw new RuntimeException(\"Unable to create lock directory: \" + lockDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create lock dir: \" + lockDirPath, e);\n    +      throw new RuntimeException(e.getMessage(), e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; lock timeout<br/>\n    +    if( conf.get(Configs.LOCK_TIMEOUT) !=null )<br/>\n    +      this.lockTimeoutSec =  Integer.parseInt(conf.get(Configs.LOCK_TIMEOUT).toString());<br/>\n    +<br/>\n    +    // &#8211; enable/disable ACKing<br/>\n    +    Object ackers = conf.get(Config.TOPOLOGY_ACKER_EXECUTORS);<br/>\n    +    if( ackers!=null )<br/>\n    +      this.ackEnabled = ( Integer.parseInt( ackers.toString() ) > 0 );<br/>\n    +    else<br/>\n    +      this.ackEnabled = false;<br/>\n    +<br/>\n    +    // &#8211; commit frequency - count<br/>\n    +    if( conf.get(Configs.COMMIT_FREQ_COUNT) != null )<br/>\n    +      commitFrequencyCount = Integer.parseInt( conf.get(Configs.COMMIT_FREQ_COUNT).toString() );<br/>\n    +<br/>\n    +    // &#8211; commit frequency - seconds<br/>\n    +    if( conf.get(Configs.COMMIT_FREQ_SEC) != null )<br/>\n    +      commitFrequencySec = Integer.parseInt( conf.get(Configs.COMMIT_FREQ_SEC).toString() );<br/>\n    +<br/>\n    +    // &#8211; max duplicate<br/>\n    +    if( conf.get(Configs.MAX_DUPLICATE) !=null )<br/>\n    +      maxDuplicates = Integer.parseInt( conf.get(Configs.MAX_DUPLICATE).toString() );<br/>\n    +<br/>\n    +    // &#8211; clocks in sync<br/>\n    +    if( conf.get(Configs.CLOCKS_INSYNC) !=null )<br/>\n    +      clocksInSync = Boolean.parseBoolean(conf.get(Configs.CLOCKS_INSYNC).toString());<br/>\n    +<br/>\n    +    // &#8211; spout id<br/>\n    +    spoutId = context.getThisComponentId();<br/>\n    +<br/>\n    +    // setup timer for commit elapse time tracking<br/>\n    +    setupCommitElapseTimer();<br/>\n    +  }<br/>\n    +<br/>\n    +  private String getDefaultLockDir(Path sourceDirPath) </p>\n{\n    +    return sourceDirPath.toString() + Path.SEPARATOR + Configs.DEFAULT_LOCK_DIR;\n    +  }\n<p>    +<br/>\n    +  private static void checkValidReader(String readerType) {<br/>\n    +    if(readerType.equalsIgnoreCase(Configs.TEXT)  || readerType.equalsIgnoreCase(Configs.SEQ) )<br/>\n    +      return;<br/>\n    +    try </p>\n{\n    +      Class<?> classType = Class.forName(readerType);\n    +      classType.getConstructor(FileSystem.class, Path.class, Map.class);\n    +      return;\n    +    }\n<p> catch (ClassNotFoundException e) </p>\n{\n    +      LOG.error(readerType + \" not found in classpath.\", e);\n    +      throw new IllegalArgumentException(readerType + \" not found in classpath.\", e);\n    +    }\n<p> catch (NoSuchMethodException e) </p>\n{\n    +      LOG.error(readerType + \" is missing the expected constructor for Readers.\", e);\n    +      throw new IllegalArgumentException(readerType + \" is missing the expected constuctor for Readers.\");\n    +    }\n<p>    +  }<br/>\n    +<br/>\n    +  @Override<br/>\n    +  public void ack(Object msgId) {<br/>\n    +    MessageId id = (MessageId) msgId;<br/>\n    +    inflight.remove(id);<br/>\n    +    ++acksSinceLastCommit;<br/>\n    +    tracker.recordAckedOffset(id.offset);<br/>\n    +    commitProgress(tracker.getCommitPosition());<br/>\n    +    if(fileReadCompletely) </p>\n{\n    +      markFileAsDone(reader.getFilePath());\n    +      reader = null;\n    +    }\n<p>    +    super.ack(msgId);<br/>\n    +  }<br/>\n    +<br/>\n    +  private boolean canCommitNow() {<br/>\n    +    if( acksSinceLastCommit >= commitFrequencyCount )<br/>\n    +      return true;<br/>\n    +    return commitTimeElapsed.get();<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    commitTimeElapsed would never return true. It seems setupCommitElapseTimer should set that value to true whenever timerTask is run.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631019/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631024","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631024","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631024,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMjQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T09:10:25Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47612027\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47612027</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java &#8212;<br/>\n    @@ -0,0 +1,654 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +import java.io.IOException;<br/>\n    +import java.lang.reflect.Constructor;<br/>\n    +import java.util.Collection;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Timer;<br/>\n    +import java.util.TimerTask;<br/>\n    +import java.util.concurrent.LinkedBlockingQueue;<br/>\n    +import java.util.concurrent.atomic.AtomicBoolean;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import org.apache.hadoop.conf.Configuration;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.apache.storm.hdfs.common.security.HdfsSecurityUtil;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +<br/>\n    +public class HdfsSpout extends BaseRichSpout {<br/>\n    +<br/>\n    +  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);<br/>\n    +<br/>\n    +  private Path sourceDirPath;<br/>\n    +  private Path archiveDirPath;<br/>\n    +  private Path badFilesDirPath;<br/>\n    +  private Path lockDirPath;<br/>\n    +<br/>\n    +  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;<br/>\n    +  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;<br/>\n    +  private int maxDuplicates = Configs.DEFAULT_MAX_DUPLICATES;<br/>\n    +  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;<br/>\n    +  private boolean clocksInSync = true;<br/>\n    +<br/>\n    +  private ProgressTracker tracker = new ProgressTracker();<br/>\n    +<br/>\n    +  private FileSystem hdfs;<br/>\n    +  private FileReader reader;<br/>\n    +<br/>\n    +  private SpoutOutputCollector collector;<br/>\n    +  HashMap<MessageId, List<Object> > inflight = new HashMap<>();<br/>\n    +  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();<br/>\n    +<br/>\n    +  private String inprogress_suffix = \".inprogress\";<br/>\n    +  private String ignoreSuffix = \".ignore\";<br/>\n    +<br/>\n    +  private Configuration hdfsConfig;<br/>\n    +  private String readerType;<br/>\n    +<br/>\n    +  private Map conf = null;<br/>\n    +  private FileLock lock;<br/>\n    +  private String spoutId = null;<br/>\n    +<br/>\n    +  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;<br/>\n    +  private long lastExpiredLockTime = 0;<br/>\n    +<br/>\n    +  private long tupleCounter = 0;<br/>\n    +  private boolean ackEnabled = false;<br/>\n    +  private int acksSinceLastCommit = 0 ;<br/>\n    +  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);<br/>\n    +  private final Timer commitTimer = new Timer();<br/>\n    +  private boolean fileReadCompletely = false;<br/>\n    +<br/>\n    +  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs kerberos configs<br/>\n    +<br/>\n    +  public HdfsSpout() </p>\n{\n    +  }\n<p>    +<br/>\n    +  public Path getLockDirPath() </p>\n{\n    +    return lockDirPath;\n    +  }\n<p>    +<br/>\n    +  public SpoutOutputCollector getCollector() </p>\n{\n    +    return collector;\n    +  }\n<p>    +<br/>\n    +  public HdfsSpout withConfigKey(String configKey)</p>\n{\n    +    this.configKey = configKey;\n    +    return this;\n    +  }\n<p>    +<br/>\n    +  public void nextTuple() {<br/>\n    +    LOG.debug(\"Next Tuple\");<br/>\n    +    // 1) First re-emit any previously failed tuples (from retryList)<br/>\n    +    if (!retryList.isEmpty()) </p>\n{\n    +      LOG.debug(\"Sending from retry list\");\n    +      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();\n    +      emitData(pair.getValue(), pair.getKey());\n    +      return;\n    +    }\n<p>    +<br/>\n    +    if( ackEnabled  &&  tracker.size()>=maxDuplicates ) {<br/>\n    +      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +<br/>\n    +       \"Progress tracker size has reached limit {}\"<br/>\n    +      , maxDuplicates);<br/>\n    +      // Don't emit anything .. allow configured spout wait strategy to kick in<br/>\n    +      return;<br/>\n    +    }<br/>\n    +<br/>\n    +    // 2) If no failed tuples, then send tuples from hdfs<br/>\n    +    while (true) {<br/>\n    +      try {<br/>\n    +// 3) Select a new file if one is not open already<br/>\n    +if (reader == null) {<br/>\n    +  reader = pickNextFile();<br/>\n    +  if (reader == null) </p>\n{\n    +    LOG.info(\"Currently no new files to process under : \" + sourceDirPath);\n    +    return;\n    +  }\n<p>    +}<br/>\n    +<br/>\n    +// 4) Read record from file, emit to collector and record progress<br/>\n    +List<Object> tuple = reader.next();<br/>\n    +if (tuple != null) {<br/>\n    +  fileReadCompletely= false;<br/>\n    +  ++tupleCounter;<br/>\n    +  MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());<br/>\n    +  emitData(tuple, msgId);<br/>\n    +<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    ++acksSinceLastCommit; // assume message is immediately acked in non-ack mode\n    +    commitProgress(reader.getFileOffset());\n    +  }\n<p> else </p>\n{\n    +    commitProgress(tracker.getCommitPosition());\n    +  }\n<p>    +  return;<br/>\n    +} else {<br/>\n    +  fileReadCompletely = true;<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    markFileAsDone(reader.getFilePath());\n    +  }\n<p>    +}<br/>\n    +      } catch (IOException e) </p>\n{\n    +LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);\n    +// don't emit anything .. allow configured spout wait strategy to kick in\n    +return;\n    +      }\n<p> catch (ParseException e) </p>\n{\n    +LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader) +\n    +\". Skipping remainder of file.\", e);\n    +markFileAsBad(reader.getFilePath());\n    +// note: Unfortunately not emitting anything here due to parse error\n    +// will trigger the configured spout wait strategy which is unnecessary\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  // will commit progress into lock file if commit threshold is reached<br/>\n    +  private void commitProgress(FileOffset position) {<br/>\n    +    if ( lock!=null && canCommitNow() ) {<br/>\n    +      try {<br/>\n    +lock.heartbeat(position.toString());<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    position parameter can be null, it should be handled.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631024/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631030","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631030","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631030,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMzA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T09:11:09Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47612096\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47612096</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java &#8212;<br/>\n    @@ -0,0 +1,654 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +import java.io.IOException;<br/>\n    +import java.lang.reflect.Constructor;<br/>\n    +import java.util.Collection;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Timer;<br/>\n    +import java.util.TimerTask;<br/>\n    +import java.util.concurrent.LinkedBlockingQueue;<br/>\n    +import java.util.concurrent.atomic.AtomicBoolean;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import org.apache.hadoop.conf.Configuration;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.apache.storm.hdfs.common.security.HdfsSecurityUtil;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +<br/>\n    +public class HdfsSpout extends BaseRichSpout {<br/>\n    +<br/>\n    +  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);<br/>\n    +<br/>\n    +  private Path sourceDirPath;<br/>\n    +  private Path archiveDirPath;<br/>\n    +  private Path badFilesDirPath;<br/>\n    +  private Path lockDirPath;<br/>\n    +<br/>\n    +  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;<br/>\n    +  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;<br/>\n    +  private int maxDuplicates = Configs.DEFAULT_MAX_DUPLICATES;<br/>\n    +  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;<br/>\n    +  private boolean clocksInSync = true;<br/>\n    +<br/>\n    +  private ProgressTracker tracker = new ProgressTracker();<br/>\n    +<br/>\n    +  private FileSystem hdfs;<br/>\n    +  private FileReader reader;<br/>\n    +<br/>\n    +  private SpoutOutputCollector collector;<br/>\n    +  HashMap<MessageId, List<Object> > inflight = new HashMap<>();<br/>\n    +  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();<br/>\n    +<br/>\n    +  private String inprogress_suffix = \".inprogress\";<br/>\n    +  private String ignoreSuffix = \".ignore\";<br/>\n    +<br/>\n    +  private Configuration hdfsConfig;<br/>\n    +  private String readerType;<br/>\n    +<br/>\n    +  private Map conf = null;<br/>\n    +  private FileLock lock;<br/>\n    +  private String spoutId = null;<br/>\n    +<br/>\n    +  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;<br/>\n    +  private long lastExpiredLockTime = 0;<br/>\n    +<br/>\n    +  private long tupleCounter = 0;<br/>\n    +  private boolean ackEnabled = false;<br/>\n    +  private int acksSinceLastCommit = 0 ;<br/>\n    +  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);<br/>\n    +  private final Timer commitTimer = new Timer();<br/>\n    +  private boolean fileReadCompletely = false;<br/>\n    +<br/>\n    +  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs kerberos configs<br/>\n    +<br/>\n    +  public HdfsSpout() </p>\n{\n    +  }\n<p>    +<br/>\n    +  public Path getLockDirPath() </p>\n{\n    +    return lockDirPath;\n    +  }\n<p>    +<br/>\n    +  public SpoutOutputCollector getCollector() </p>\n{\n    +    return collector;\n    +  }\n<p>    +<br/>\n    +  public HdfsSpout withConfigKey(String configKey)</p>\n{\n    +    this.configKey = configKey;\n    +    return this;\n    +  }\n<p>    +<br/>\n    +  public void nextTuple() {<br/>\n    +    LOG.debug(\"Next Tuple\");<br/>\n    +    // 1) First re-emit any previously failed tuples (from retryList)<br/>\n    +    if (!retryList.isEmpty()) </p>\n{\n    +      LOG.debug(\"Sending from retry list\");\n    +      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();\n    +      emitData(pair.getValue(), pair.getKey());\n    +      return;\n    +    }\n<p>    +<br/>\n    +    if( ackEnabled  &&  tracker.size()>=maxDuplicates ) {<br/>\n    +      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +<br/>\n    +       \"Progress tracker size has reached limit {}\"<br/>\n    +      , maxDuplicates);<br/>\n    +      // Don't emit anything .. allow configured spout wait strategy to kick in<br/>\n    +      return;<br/>\n    +    }<br/>\n    +<br/>\n    +    // 2) If no failed tuples, then send tuples from hdfs<br/>\n    +    while (true) {<br/>\n    +      try {<br/>\n    +// 3) Select a new file if one is not open already<br/>\n    +if (reader == null) {<br/>\n    +  reader = pickNextFile();<br/>\n    +  if (reader == null) </p>\n{\n    +    LOG.info(\"Currently no new files to process under : \" + sourceDirPath);\n    +    return;\n    +  }\n<p>    +}<br/>\n    +<br/>\n    +// 4) Read record from file, emit to collector and record progress<br/>\n    +List<Object> tuple = reader.next();<br/>\n    +if (tuple != null) {<br/>\n    +  fileReadCompletely= false;<br/>\n    +  ++tupleCounter;<br/>\n    +  MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());<br/>\n    +  emitData(tuple, msgId);<br/>\n    +<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    ++acksSinceLastCommit; // assume message is immediately acked in non-ack mode\n    +    commitProgress(reader.getFileOffset());\n    +  }\n<p> else </p>\n{\n    +    commitProgress(tracker.getCommitPosition());\n    +  }\n<p>    +  return;<br/>\n    +} else {<br/>\n    +  fileReadCompletely = true;<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    markFileAsDone(reader.getFilePath());\n    +  }\n<p>    +}<br/>\n    +      } catch (IOException e) </p>\n{\n    +LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);\n    +// don't emit anything .. allow configured spout wait strategy to kick in\n    +return;\n    +      }\n<p> catch (ParseException e) </p>\n{\n    +LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader) +\n    +\". Skipping remainder of file.\", e);\n    +markFileAsBad(reader.getFilePath());\n    +// note: Unfortunately not emitting anything here due to parse error\n    +// will trigger the configured spout wait strategy which is unnecessary\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  // will commit progress into lock file if commit threshold is reached<br/>\n    +  private void commitProgress(FileOffset position) {<br/>\n    +    if ( lock!=null && canCommitNow() ) {<br/>\n    +      try </p>\n{\n    +lock.heartbeat(position.toString());\n    +acksSinceLastCommit = 0;\n    +commitTimeElapsed.set(false);\n    +setupCommitElapseTimer();\n    +      }\n<p> catch (IOException e) </p>\n{\n    +LOG.error(\"Unable to commit progress Will retry later.\", e);\n    +      }\n<p>    +    }<br/>\n    +  }<br/>\n    +<br/>\n    +  private void setupCommitElapseTimer() {<br/>\n    +    if(commitFrequencySec<=0)<br/>\n    +      return;<br/>\n    +    TimerTask timerTask = new TimerTask() {<br/>\n    +      @Override<br/>\n    +      public void run() </p>\n{\n    +commitTimeElapsed.set(false);\n    +      }\n<p>    +    };<br/>\n    +    commitTimer.schedule(timerTask, commitFrequencySec * 1000);<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +  private static String getFileProgress(FileReader reader) </p>\n{\n    +    return reader.getFilePath() + \" \" + reader.getFileOffset();\n    +  }\n<p>    +<br/>\n    +  private void markFileAsDone(Path filePath) {<br/>\n    +    fileReadCompletely = false;<br/>\n    +    try </p>\n{\n    +      renameCompletedFile(reader.getFilePath());\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to archive completed file\" + filePath, e);\n    +    }\n<p>    +    unlockAndCloseReader();<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  private void markFileAsBad(Path file) {<br/>\n    +    String fileName = file.toString();<br/>\n    +    String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));<br/>\n    +    String originalName = new Path(fileNameMinusSuffix).getName();<br/>\n    +    Path  newFile = new Path( badFilesDirPath + Path.SEPARATOR + originalName);<br/>\n    +<br/>\n    +    LOG.info(\"Moving bad file to \" + newFile);<br/>\n    +    try {<br/>\n    +      if (!hdfs.rename(file, newFile) ) </p>\n{ // seems this can fail by returning false or throwing exception\n    +throw new IOException(\"Move failed for bad file: \" + file); // convert false ret value to exception\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.warn(\"Error moving bad file: \" + file + \". to destination :  \" + newFile);\n    +    }\n<p>    +<br/>\n    +    unlockAndCloseReader();<br/>\n    +  }<br/>\n    +<br/>\n    +  private void unlockAndCloseReader() {<br/>\n    +    reader.close();<br/>\n    +    reader = null;<br/>\n    +    try </p>\n{\n    +      lock.release();\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to delete lock file : \" + this.lock.getLockFile(), e);\n    +    }\n<p>    +    lock = null;<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +<br/>\n    +  protected void emitData(List<Object> tuple, MessageId id) {<br/>\n    +    LOG.debug(\"Emitting - {}\", id);<br/>\n    +    this.collector.emit(tuple, id);<br/>\n    +    inflight.put(id, tuple);<br/>\n    +  }<br/>\n    +<br/>\n    +  public void open(Map conf, TopologyContext context,  SpoutOutputCollector collector) {<br/>\n    +    this.conf = conf;<br/>\n    +    final String FILE_SYSTEM = \"filesystem\";<br/>\n    +    LOG.info(\"Opening\");<br/>\n    +    this.collector = collector;<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    this.tupleCounter = 0;<br/>\n    +<br/>\n    +    for( Object k : conf.keySet() ) {<br/>\n    +      String key = k.toString();<br/>\n    +      if( ! FILE_SYSTEM.equalsIgnoreCase( key ) ) </p>\n{ // to support unit test only\n    +String val = conf.get(key).toString();\n    +LOG.info(\"Config setting : \" + key + \" = \" + val);\n    +this.hdfsConfig.set(key, val);\n    +      }\n<p>    +      else<br/>\n    +this.hdfs = (FileSystem) conf.get(key);<br/>\n    +<br/>\n    +      if(key.equalsIgnoreCase(Configs.READER_TYPE)) </p>\n{\n    +readerType = conf.get(key).toString();\n    +checkValidReader(readerType);\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    // - Hdfs configs<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    Map<String, Object> map = (Map<String, Object>)conf.get(this.configKey);<br/>\n    +    if(map != null){<br/>\n    +      for(String key : map.keySet())</p>\n{\n    +this.hdfsConfig.set(key, String.valueOf(map.get(key)));\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    try </p>\n{\n    +      HdfsSecurityUtil.login(conf, hdfsConfig);\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Failed to open \" + sourceDirPath);\n    +      throw new RuntimeException(e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; source dir config<br/>\n    +    if ( !conf.containsKey(Configs.SOURCE_DIR) ) </p>\n{\n    +      LOG.error(Configs.SOURCE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.SOURCE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.sourceDirPath = new Path( conf.get(Configs.SOURCE_DIR).toString() );<br/>\n    +<br/>\n    +    // &#8211; archive dir config<br/>\n    +    if ( !conf.containsKey(Configs.ARCHIVE_DIR) ) </p>\n{\n    +      LOG.error(Configs.ARCHIVE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.ARCHIVE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.archiveDirPath = new Path( conf.get(Configs.ARCHIVE_DIR).toString() );<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(archiveDirPath)) {<br/>\n    +if(! hdfs.isDirectory(archiveDirPath) ) </p>\n{\n    +  LOG.error(\"Archive directory is a file. \" + archiveDirPath);\n    +  throw new RuntimeException(\"Archive directory is a file. \" + archiveDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(archiveDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create archive directory. \" + archiveDirPath);\n    +throw new RuntimeException(\"Unable to create archive directory \" + archiveDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(\"Unable to create archive directory \", e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; bad files dir config<br/>\n    +    if ( !conf.containsKey(Configs.BAD_DIR) ) </p>\n{\n    +      LOG.error(Configs.BAD_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.BAD_DIR + \" setting is required\");\n    +    }\n<p>    +<br/>\n    +    this.badFilesDirPath = new Path(conf.get(Configs.BAD_DIR).toString());<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(badFilesDirPath)) {<br/>\n    +if(! hdfs.isDirectory(badFilesDirPath) ) </p>\n{\n    +  LOG.error(\"Bad files directory is a file: \" + badFilesDirPath);\n    +  throw new RuntimeException(\"Bad files directory is a file: \" + badFilesDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(badFilesDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create directory for bad files: \" + badFilesDirPath);\n    +throw new RuntimeException(\"Unable to create a directory for bad files: \" + badFilesDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(e.getMessage(), e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; ignore filename suffix<br/>\n    +    if ( conf.containsKey(Configs.IGNORE_SUFFIX) ) </p>\n{\n    +      this.ignoreSuffix = conf.get(Configs.IGNORE_SUFFIX).toString();\n    +    }\n<p>    +<br/>\n    +    // &#8211; lock dir config<br/>\n    +    String lockDir = !conf.containsKey(Configs.LOCK_DIR) ? getDefaultLockDir(sourceDirPath) : conf.get(Configs.LOCK_DIR).toString() ;<br/>\n    +    this.lockDirPath = new Path(lockDir);<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(lockDirPath)) {<br/>\n    +if(! hdfs.isDirectory(lockDirPath) ) </p>\n{\n    +  LOG.error(\"Lock directory is a file: \" + lockDirPath);\n    +  throw new RuntimeException(\"Lock directory is a file: \" + lockDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(lockDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create lock directory: \" + lockDirPath);\n    +throw new RuntimeException(\"Unable to create lock directory: \" + lockDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create lock dir: \" + lockDirPath, e);\n    +      throw new RuntimeException(e.getMessage(), e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; lock timeout<br/>\n    +    if( conf.get(Configs.LOCK_TIMEOUT) !=null )<br/>\n    +      this.lockTimeoutSec =  Integer.parseInt(conf.get(Configs.LOCK_TIMEOUT).toString());<br/>\n    +<br/>\n    +    // &#8211; enable/disable ACKing<br/>\n    +    Object ackers = conf.get(Config.TOPOLOGY_ACKER_EXECUTORS);<br/>\n    +    if( ackers!=null )<br/>\n    +      this.ackEnabled = ( Integer.parseInt( ackers.toString() ) > 0 );<br/>\n    +    else<br/>\n    +      this.ackEnabled = false;<br/>\n    +<br/>\n    +    // &#8211; commit frequency - count<br/>\n    +    if( conf.get(Configs.COMMIT_FREQ_COUNT) != null )<br/>\n    +      commitFrequencyCount = Integer.parseInt( conf.get(Configs.COMMIT_FREQ_COUNT).toString() );<br/>\n    +<br/>\n    +    // &#8211; commit frequency - seconds<br/>\n    +    if( conf.get(Configs.COMMIT_FREQ_SEC) != null )<br/>\n    +      commitFrequencySec = Integer.parseInt( conf.get(Configs.COMMIT_FREQ_SEC).toString() );<br/>\n    +<br/>\n    +    // &#8211; max duplicate<br/>\n    +    if( conf.get(Configs.MAX_DUPLICATE) !=null )<br/>\n    +      maxDuplicates = Integer.parseInt( conf.get(Configs.MAX_DUPLICATE).toString() );<br/>\n    +<br/>\n    +    // &#8211; clocks in sync<br/>\n    +    if( conf.get(Configs.CLOCKS_INSYNC) !=null )<br/>\n    +      clocksInSync = Boolean.parseBoolean(conf.get(Configs.CLOCKS_INSYNC).toString());<br/>\n    +<br/>\n    +    // &#8211; spout id<br/>\n    +    spoutId = context.getThisComponentId();<br/>\n    +<br/>\n    +    // setup timer for commit elapse time tracking<br/>\n    +    setupCommitElapseTimer();<br/>\n    +  }<br/>\n    +<br/>\n    +  private String getDefaultLockDir(Path sourceDirPath) </p>\n{\n    +    return sourceDirPath.toString() + Path.SEPARATOR + Configs.DEFAULT_LOCK_DIR;\n    +  }\n<p>    +<br/>\n    +  private static void checkValidReader(String readerType) {<br/>\n    +    if(readerType.equalsIgnoreCase(Configs.TEXT)  || readerType.equalsIgnoreCase(Configs.SEQ) )<br/>\n    +      return;<br/>\n    +    try </p>\n{\n    +      Class<?> classType = Class.forName(readerType);\n    +      classType.getConstructor(FileSystem.class, Path.class, Map.class);\n    +      return;\n    +    }\n<p> catch (ClassNotFoundException e) </p>\n{\n    +      LOG.error(readerType + \" not found in classpath.\", e);\n    +      throw new IllegalArgumentException(readerType + \" not found in classpath.\", e);\n    +    }\n<p> catch (NoSuchMethodException e) </p>\n{\n    +      LOG.error(readerType + \" is missing the expected constructor for Readers.\", e);\n    +      throw new IllegalArgumentException(readerType + \" is missing the expected constuctor for Readers.\");\n    +    }\n<p>    +  }<br/>\n    +<br/>\n    +  @Override<br/>\n    +  public void ack(Object msgId) {<br/>\n    +    MessageId id = (MessageId) msgId;<br/>\n    +    inflight.remove(id);<br/>\n    +    ++acksSinceLastCommit;<br/>\n    +    tracker.recordAckedOffset(id.offset);<br/>\n    +    commitProgress(tracker.getCommitPosition());<br/>\n    +    if(fileReadCompletely) </p>\n{\n    +      markFileAsDone(reader.getFilePath());\n    +      reader = null;\n    +    }\n<p>    +    super.ack(msgId);<br/>\n    +  }<br/>\n    +<br/>\n    +  private boolean canCommitNow() {<br/>\n    +    if( acksSinceLastCommit >= commitFrequencyCount )<br/>\n    +      return true;<br/>\n    +    return commitTimeElapsed.get();<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Th timer itself is not required since the commit is performed synchronously (whenever nextTuple, ack is invoked). I don't see the commits being triggered asynchronously from the timer.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631030/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631034","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631034","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631034,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMzQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T09:54:51Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47616335\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47616335</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java &#8212;<br/>\n    @@ -0,0 +1,654 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +import java.io.IOException;<br/>\n    +import java.lang.reflect.Constructor;<br/>\n    +import java.util.Collection;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Timer;<br/>\n    +import java.util.TimerTask;<br/>\n    +import java.util.concurrent.LinkedBlockingQueue;<br/>\n    +import java.util.concurrent.atomic.AtomicBoolean;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import org.apache.hadoop.conf.Configuration;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.apache.storm.hdfs.common.security.HdfsSecurityUtil;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +<br/>\n    +public class HdfsSpout extends BaseRichSpout {<br/>\n    +<br/>\n    +  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);<br/>\n    +<br/>\n    +  private Path sourceDirPath;<br/>\n    +  private Path archiveDirPath;<br/>\n    +  private Path badFilesDirPath;<br/>\n    +  private Path lockDirPath;<br/>\n    +<br/>\n    +  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;<br/>\n    +  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;<br/>\n    +  private int maxDuplicates = Configs.DEFAULT_MAX_DUPLICATES;<br/>\n    +  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;<br/>\n    +  private boolean clocksInSync = true;<br/>\n    +<br/>\n    +  private ProgressTracker tracker = new ProgressTracker();<br/>\n    +<br/>\n    +  private FileSystem hdfs;<br/>\n    +  private FileReader reader;<br/>\n    +<br/>\n    +  private SpoutOutputCollector collector;<br/>\n    +  HashMap<MessageId, List<Object> > inflight = new HashMap<>();<br/>\n    +  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();<br/>\n    +<br/>\n    +  private String inprogress_suffix = \".inprogress\";<br/>\n    +  private String ignoreSuffix = \".ignore\";<br/>\n    +<br/>\n    +  private Configuration hdfsConfig;<br/>\n    +  private String readerType;<br/>\n    +<br/>\n    +  private Map conf = null;<br/>\n    +  private FileLock lock;<br/>\n    +  private String spoutId = null;<br/>\n    +<br/>\n    +  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;<br/>\n    +  private long lastExpiredLockTime = 0;<br/>\n    +<br/>\n    +  private long tupleCounter = 0;<br/>\n    +  private boolean ackEnabled = false;<br/>\n    +  private int acksSinceLastCommit = 0 ;<br/>\n    +  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);<br/>\n    +  private final Timer commitTimer = new Timer();<br/>\n    +  private boolean fileReadCompletely = false;<br/>\n    +<br/>\n    +  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs kerberos configs<br/>\n    +<br/>\n    +  public HdfsSpout() </p>\n{\n    +  }\n<p>    +<br/>\n    +  public Path getLockDirPath() </p>\n{\n    +    return lockDirPath;\n    +  }\n<p>    +<br/>\n    +  public SpoutOutputCollector getCollector() </p>\n{\n    +    return collector;\n    +  }\n<p>    +<br/>\n    +  public HdfsSpout withConfigKey(String configKey)</p>\n{\n    +    this.configKey = configKey;\n    +    return this;\n    +  }\n<p>    +<br/>\n    +  public void nextTuple() {<br/>\n    +    LOG.debug(\"Next Tuple\");<br/>\n    +    // 1) First re-emit any previously failed tuples (from retryList)<br/>\n    +    if (!retryList.isEmpty()) </p>\n{\n    +      LOG.debug(\"Sending from retry list\");\n    +      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();\n    +      emitData(pair.getValue(), pair.getKey());\n    +      return;\n    +    }\n<p>    +<br/>\n    +    if( ackEnabled  &&  tracker.size()>=maxDuplicates ) {<br/>\n    +      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +<br/>\n    +       \"Progress tracker size has reached limit {}\"<br/>\n    +      , maxDuplicates);<br/>\n    +      // Don't emit anything .. allow configured spout wait strategy to kick in<br/>\n    +      return;<br/>\n    +    }<br/>\n    +<br/>\n    +    // 2) If no failed tuples, then send tuples from hdfs<br/>\n    +    while (true) {<br/>\n    +      try {<br/>\n    +// 3) Select a new file if one is not open already<br/>\n    +if (reader == null) {<br/>\n    +  reader = pickNextFile();<br/>\n    +  if (reader == null) </p>\n{\n    +    LOG.info(\"Currently no new files to process under : \" + sourceDirPath);\n    +    return;\n    +  }\n<p>    +}<br/>\n    +<br/>\n    +// 4) Read record from file, emit to collector and record progress<br/>\n    +List<Object> tuple = reader.next();<br/>\n    +if (tuple != null) {<br/>\n    +  fileReadCompletely= false;<br/>\n    +  ++tupleCounter;<br/>\n    +  MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());<br/>\n    +  emitData(tuple, msgId);<br/>\n    +<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    ++acksSinceLastCommit; // assume message is immediately acked in non-ack mode\n    +    commitProgress(reader.getFileOffset());\n    +  }\n<p> else </p>\n{\n    +    commitProgress(tracker.getCommitPosition());\n    +  }\n<p>    +  return;<br/>\n    +} else {<br/>\n    +  fileReadCompletely = true;<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    markFileAsDone(reader.getFilePath());\n    +  }\n<p>    &#8212; End diff &#8211;</p>\n\n<p>    if ackEnabled is true then fileReadCompletely is set as false on line no:142 after reading the first record from the next file. This makes the earlier file is always opened and it is not moved to archive dir.  ack() method has code to markFIleAsDone() when fileReadCompletely is true but it may not get triggered because fileReadCompletely is set as false.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631034/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631037","html_url":"https://github.com/apache/storm/issues/5037#issuecomment-2612631037","issue_url":"https://api.github.com/repos/apache/storm/issues/5037","id":2612631037,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzEwMzc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T09:57:48Z","updated_at":"2025-01-24T14:14:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/936#discussion_r47616643\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/936#discussion_r47616643</a></p>\n\n<p>    &#8212; Diff: external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java &#8212;<br/>\n    @@ -0,0 +1,654 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + * <p/><br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + * <p/><br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +<br/>\n    +package org.apache.storm.hdfs.spout;<br/>\n    +<br/>\n    +import java.io.IOException;<br/>\n    +import java.lang.reflect.Constructor;<br/>\n    +import java.util.Collection;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Timer;<br/>\n    +import java.util.TimerTask;<br/>\n    +import java.util.concurrent.LinkedBlockingQueue;<br/>\n    +import java.util.concurrent.atomic.AtomicBoolean;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import org.apache.hadoop.conf.Configuration;<br/>\n    +import org.apache.hadoop.fs.FileSystem;<br/>\n    +import org.apache.hadoop.fs.Path;<br/>\n    +import org.apache.storm.hdfs.common.HdfsUtils;<br/>\n    +import org.apache.storm.hdfs.common.security.HdfsSecurityUtil;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +<br/>\n    +public class HdfsSpout extends BaseRichSpout {<br/>\n    +<br/>\n    +  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);<br/>\n    +<br/>\n    +  private Path sourceDirPath;<br/>\n    +  private Path archiveDirPath;<br/>\n    +  private Path badFilesDirPath;<br/>\n    +  private Path lockDirPath;<br/>\n    +<br/>\n    +  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;<br/>\n    +  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;<br/>\n    +  private int maxDuplicates = Configs.DEFAULT_MAX_DUPLICATES;<br/>\n    +  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;<br/>\n    +  private boolean clocksInSync = true;<br/>\n    +<br/>\n    +  private ProgressTracker tracker = new ProgressTracker();<br/>\n    +<br/>\n    +  private FileSystem hdfs;<br/>\n    +  private FileReader reader;<br/>\n    +<br/>\n    +  private SpoutOutputCollector collector;<br/>\n    +  HashMap<MessageId, List<Object> > inflight = new HashMap<>();<br/>\n    +  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();<br/>\n    +<br/>\n    +  private String inprogress_suffix = \".inprogress\";<br/>\n    +  private String ignoreSuffix = \".ignore\";<br/>\n    +<br/>\n    +  private Configuration hdfsConfig;<br/>\n    +  private String readerType;<br/>\n    +<br/>\n    +  private Map conf = null;<br/>\n    +  private FileLock lock;<br/>\n    +  private String spoutId = null;<br/>\n    +<br/>\n    +  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;<br/>\n    +  private long lastExpiredLockTime = 0;<br/>\n    +<br/>\n    +  private long tupleCounter = 0;<br/>\n    +  private boolean ackEnabled = false;<br/>\n    +  private int acksSinceLastCommit = 0 ;<br/>\n    +  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);<br/>\n    +  private final Timer commitTimer = new Timer();<br/>\n    +  private boolean fileReadCompletely = false;<br/>\n    +<br/>\n    +  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs kerberos configs<br/>\n    +<br/>\n    +  public HdfsSpout() </p>\n{\n    +  }\n<p>    +<br/>\n    +  public Path getLockDirPath() </p>\n{\n    +    return lockDirPath;\n    +  }\n<p>    +<br/>\n    +  public SpoutOutputCollector getCollector() </p>\n{\n    +    return collector;\n    +  }\n<p>    +<br/>\n    +  public HdfsSpout withConfigKey(String configKey)</p>\n{\n    +    this.configKey = configKey;\n    +    return this;\n    +  }\n<p>    +<br/>\n    +  public void nextTuple() {<br/>\n    +    LOG.debug(\"Next Tuple\");<br/>\n    +    // 1) First re-emit any previously failed tuples (from retryList)<br/>\n    +    if (!retryList.isEmpty()) </p>\n{\n    +      LOG.debug(\"Sending from retry list\");\n    +      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();\n    +      emitData(pair.getValue(), pair.getKey());\n    +      return;\n    +    }\n<p>    +<br/>\n    +    if( ackEnabled  &&  tracker.size()>=maxDuplicates ) {<br/>\n    +      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +<br/>\n    +       \"Progress tracker size has reached limit {}\"<br/>\n    +      , maxDuplicates);<br/>\n    +      // Don't emit anything .. allow configured spout wait strategy to kick in<br/>\n    +      return;<br/>\n    +    }<br/>\n    +<br/>\n    +    // 2) If no failed tuples, then send tuples from hdfs<br/>\n    +    while (true) {<br/>\n    +      try {<br/>\n    +// 3) Select a new file if one is not open already<br/>\n    +if (reader == null) {<br/>\n    +  reader = pickNextFile();<br/>\n    +  if (reader == null) </p>\n{\n    +    LOG.info(\"Currently no new files to process under : \" + sourceDirPath);\n    +    return;\n    +  }\n<p>    +}<br/>\n    +<br/>\n    +// 4) Read record from file, emit to collector and record progress<br/>\n    +List<Object> tuple = reader.next();<br/>\n    +if (tuple != null) {<br/>\n    +  fileReadCompletely= false;<br/>\n    +  ++tupleCounter;<br/>\n    +  MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());<br/>\n    +  emitData(tuple, msgId);<br/>\n    +<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    ++acksSinceLastCommit; // assume message is immediately acked in non-ack mode\n    +    commitProgress(reader.getFileOffset());\n    +  }\n<p> else </p>\n{\n    +    commitProgress(tracker.getCommitPosition());\n    +  }\n<p>    +  return;<br/>\n    +} else {<br/>\n    +  fileReadCompletely = true;<br/>\n    +  if(!ackEnabled) </p>\n{\n    +    markFileAsDone(reader.getFilePath());\n    +  }\n<p>    +}<br/>\n    +      } catch (IOException e) </p>\n{\n    +LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);\n    +// don't emit anything .. allow configured spout wait strategy to kick in\n    +return;\n    +      }\n<p> catch (ParseException e) </p>\n{\n    +LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader) +\n    +\". Skipping remainder of file.\", e);\n    +markFileAsBad(reader.getFilePath());\n    +// note: Unfortunately not emitting anything here due to parse error\n    +// will trigger the configured spout wait strategy which is unnecessary\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  // will commit progress into lock file if commit threshold is reached<br/>\n    +  private void commitProgress(FileOffset position) {<br/>\n    +    if ( lock!=null && canCommitNow() ) {<br/>\n    +      try </p>\n{\n    +lock.heartbeat(position.toString());\n    +acksSinceLastCommit = 0;\n    +commitTimeElapsed.set(false);\n    +setupCommitElapseTimer();\n    +      }\n<p> catch (IOException e) </p>\n{\n    +LOG.error(\"Unable to commit progress Will retry later.\", e);\n    +      }\n<p>    +    }<br/>\n    +  }<br/>\n    +<br/>\n    +  private void setupCommitElapseTimer() {<br/>\n    +    if(commitFrequencySec<=0)<br/>\n    +      return;<br/>\n    +    TimerTask timerTask = new TimerTask() {<br/>\n    +      @Override<br/>\n    +      public void run() </p>\n{\n    +commitTimeElapsed.set(false);\n    +      }\n<p>    +    };<br/>\n    +    commitTimer.schedule(timerTask, commitFrequencySec * 1000);<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +  private static String getFileProgress(FileReader reader) </p>\n{\n    +    return reader.getFilePath() + \" \" + reader.getFileOffset();\n    +  }\n<p>    +<br/>\n    +  private void markFileAsDone(Path filePath) {<br/>\n    +    fileReadCompletely = false;<br/>\n    +    try </p>\n{\n    +      renameCompletedFile(reader.getFilePath());\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to archive completed file\" + filePath, e);\n    +    }\n<p>    +    unlockAndCloseReader();<br/>\n    +<br/>\n    +  }<br/>\n    +<br/>\n    +  private void markFileAsBad(Path file) {<br/>\n    +    String fileName = file.toString();<br/>\n    +    String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));<br/>\n    +    String originalName = new Path(fileNameMinusSuffix).getName();<br/>\n    +    Path  newFile = new Path( badFilesDirPath + Path.SEPARATOR + originalName);<br/>\n    +<br/>\n    +    LOG.info(\"Moving bad file to \" + newFile);<br/>\n    +    try {<br/>\n    +      if (!hdfs.rename(file, newFile) ) </p>\n{ // seems this can fail by returning false or throwing exception\n    +throw new IOException(\"Move failed for bad file: \" + file); // convert false ret value to exception\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.warn(\"Error moving bad file: \" + file + \". to destination :  \" + newFile);\n    +    }\n<p>    +<br/>\n    +    unlockAndCloseReader();<br/>\n    +  }<br/>\n    +<br/>\n    +  private void unlockAndCloseReader() {<br/>\n    +    reader.close();<br/>\n    +    reader = null;<br/>\n    +    try </p>\n{\n    +      lock.release();\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to delete lock file : \" + this.lock.getLockFile(), e);\n    +    }\n<p>    +    lock = null;<br/>\n    +  }<br/>\n    +<br/>\n    +<br/>\n    +<br/>\n    +  protected void emitData(List<Object> tuple, MessageId id) {<br/>\n    +    LOG.debug(\"Emitting - {}\", id);<br/>\n    +    this.collector.emit(tuple, id);<br/>\n    +    inflight.put(id, tuple);<br/>\n    +  }<br/>\n    +<br/>\n    +  public void open(Map conf, TopologyContext context,  SpoutOutputCollector collector) {<br/>\n    +    this.conf = conf;<br/>\n    +    final String FILE_SYSTEM = \"filesystem\";<br/>\n    +    LOG.info(\"Opening\");<br/>\n    +    this.collector = collector;<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    this.tupleCounter = 0;<br/>\n    +<br/>\n    +    for( Object k : conf.keySet() ) {<br/>\n    +      String key = k.toString();<br/>\n    +      if( ! FILE_SYSTEM.equalsIgnoreCase( key ) ) </p>\n{ // to support unit test only\n    +String val = conf.get(key).toString();\n    +LOG.info(\"Config setting : \" + key + \" = \" + val);\n    +this.hdfsConfig.set(key, val);\n    +      }\n<p>    +      else<br/>\n    +this.hdfs = (FileSystem) conf.get(key);<br/>\n    +<br/>\n    +      if(key.equalsIgnoreCase(Configs.READER_TYPE)) </p>\n{\n    +readerType = conf.get(key).toString();\n    +checkValidReader(readerType);\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    // - Hdfs configs<br/>\n    +    this.hdfsConfig = new Configuration();<br/>\n    +    Map<String, Object> map = (Map<String, Object>)conf.get(this.configKey);<br/>\n    +    if(map != null){<br/>\n    +      for(String key : map.keySet())</p>\n{\n    +this.hdfsConfig.set(key, String.valueOf(map.get(key)));\n    +      }\n<p>    +    }<br/>\n    +<br/>\n    +    try </p>\n{\n    +      HdfsSecurityUtil.login(conf, hdfsConfig);\n    +    }\n<p> catch (IOException e) </p>\n{\n    +      LOG.error(\"Failed to open \" + sourceDirPath);\n    +      throw new RuntimeException(e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; source dir config<br/>\n    +    if ( !conf.containsKey(Configs.SOURCE_DIR) ) </p>\n{\n    +      LOG.error(Configs.SOURCE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.SOURCE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.sourceDirPath = new Path( conf.get(Configs.SOURCE_DIR).toString() );<br/>\n    +<br/>\n    +    // &#8211; archive dir config<br/>\n    +    if ( !conf.containsKey(Configs.ARCHIVE_DIR) ) </p>\n{\n    +      LOG.error(Configs.ARCHIVE_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.ARCHIVE_DIR + \" setting is required\");\n    +    }\n<p>    +    this.archiveDirPath = new Path( conf.get(Configs.ARCHIVE_DIR).toString() );<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(archiveDirPath)) {<br/>\n    +if(! hdfs.isDirectory(archiveDirPath) ) </p>\n{\n    +  LOG.error(\"Archive directory is a file. \" + archiveDirPath);\n    +  throw new RuntimeException(\"Archive directory is a file. \" + archiveDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(archiveDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create archive directory. \" + archiveDirPath);\n    +throw new RuntimeException(\"Unable to create archive directory \" + archiveDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(\"Unable to create archive directory \", e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; bad files dir config<br/>\n    +    if ( !conf.containsKey(Configs.BAD_DIR) ) </p>\n{\n    +      LOG.error(Configs.BAD_DIR + \" setting is required\");\n    +      throw new RuntimeException(Configs.BAD_DIR + \" setting is required\");\n    +    }\n<p>    +<br/>\n    +    this.badFilesDirPath = new Path(conf.get(Configs.BAD_DIR).toString());<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(badFilesDirPath)) {<br/>\n    +if(! hdfs.isDirectory(badFilesDirPath) ) </p>\n{\n    +  LOG.error(\"Bad files directory is a file: \" + badFilesDirPath);\n    +  throw new RuntimeException(\"Bad files directory is a file: \" + badFilesDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(badFilesDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create directory for bad files: \" + badFilesDirPath);\n    +throw new RuntimeException(\"Unable to create a directory for bad files: \" + badFilesDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create archive dir \", e);\n    +      throw new RuntimeException(e.getMessage(), e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; ignore filename suffix<br/>\n    +    if ( conf.containsKey(Configs.IGNORE_SUFFIX) ) </p>\n{\n    +      this.ignoreSuffix = conf.get(Configs.IGNORE_SUFFIX).toString();\n    +    }\n<p>    +<br/>\n    +    // &#8211; lock dir config<br/>\n    +    String lockDir = !conf.containsKey(Configs.LOCK_DIR) ? getDefaultLockDir(sourceDirPath) : conf.get(Configs.LOCK_DIR).toString() ;<br/>\n    +    this.lockDirPath = new Path(lockDir);<br/>\n    +<br/>\n    +    try {<br/>\n    +      if(hdfs.exists(lockDirPath)) {<br/>\n    +if(! hdfs.isDirectory(lockDirPath) ) </p>\n{\n    +  LOG.error(\"Lock directory is a file: \" + lockDirPath);\n    +  throw new RuntimeException(\"Lock directory is a file: \" + lockDirPath);\n    +}\n<p>    +      } else if(! hdfs.mkdirs(lockDirPath) ) </p>\n{\n    +LOG.error(\"Unable to create lock directory: \" + lockDirPath);\n    +throw new RuntimeException(\"Unable to create lock directory: \" + lockDirPath);\n    +      }\n<p>    +    } catch (IOException e) </p>\n{\n    +      LOG.error(\"Unable to create lock dir: \" + lockDirPath, e);\n    +      throw new RuntimeException(e.getMessage(), e);\n    +    }\n<p>    +<br/>\n    +    // &#8211; lock timeout<br/>\n    +    if( conf.get(Configs.LOCK_TIMEOUT) !=null )<br/>\n    +      this.lockTimeoutSec =  Integer.parseInt(conf.get(Configs.LOCK_TIMEOUT).toString());<br/>\n    +<br/>\n    +    // &#8211; enable/disable ACKing<br/>\n    +    Object ackers = conf.get(Config.TOPOLOGY_ACKER_EXECUTORS);<br/>\n    +    if( ackers!=null )<br/>\n    +      this.ackEnabled = ( Integer.parseInt( ackers.toString() ) > 0 );<br/>\n    +    else<br/>\n    +      this.ackEnabled = false;<br/>\n    +<br/>\n    +    // &#8211; commit frequency - count<br/>\n    +    if( conf.get(Configs.COMMIT_FREQ_COUNT) != null )<br/>\n    +      commitFrequencyCount = Integer.parseInt( conf.get(Configs.COMMIT_FREQ_COUNT).toString() );<br/>\n    +<br/>\n    +    // &#8211; commit frequency - seconds<br/>\n    +    if( conf.get(Configs.COMMIT_FREQ_SEC) != null )<br/>\n    +      commitFrequencySec = Integer.parseInt( conf.get(Configs.COMMIT_FREQ_SEC).toString() );<br/>\n    +<br/>\n    +    // &#8211; max duplicate<br/>\n    +    if( conf.get(Configs.MAX_DUPLICATE) !=null )<br/>\n    +      maxDuplicates = Integer.parseInt( conf.get(Configs.MAX_DUPLICATE).toString() );<br/>\n    +<br/>\n    +    // &#8211; clocks in sync<br/>\n    +    if( conf.get(Configs.CLOCKS_INSYNC) !=null )<br/>\n    +      clocksInSync = Boolean.parseBoolean(conf.get(Configs.CLOCKS_INSYNC).toString());<br/>\n    +<br/>\n    +    // &#8211; spout id<br/>\n    +    spoutId = context.getThisComponentId();<br/>\n    +<br/>\n    +    // setup timer for commit elapse time tracking<br/>\n    +    setupCommitElapseTimer();<br/>\n    +  }<br/>\n    +<br/>\n    +  private String getDefaultLockDir(Path sourceDirPath) </p>\n{\n    +    return sourceDirPath.toString() + Path.SEPARATOR + Configs.DEFAULT_LOCK_DIR;\n    +  }\n<p>    +<br/>\n    +  private static void checkValidReader(String readerType) {<br/>\n    +    if(readerType.equalsIgnoreCase(Configs.TEXT)  || readerType.equalsIgnoreCase(Configs.SEQ) )<br/>\n    +      return;<br/>\n    +    try </p>\n{\n    +      Class<?> classType = Class.forName(readerType);\n    +      classType.getConstructor(FileSystem.class, Path.class, Map.class);\n    +      return;\n    +    }\n<p> catch (ClassNotFoundException e) </p>\n{\n    +      LOG.error(readerType + \" not found in classpath.\", e);\n    +      throw new IllegalArgumentException(readerType + \" not found in classpath.\", e);\n    +    }\n<p> catch (NoSuchMethodException e) </p>\n{\n    +      LOG.error(readerType + \" is missing the expected constructor for Readers.\", e);\n    +      throw new IllegalArgumentException(readerType + \" is missing the expected constuctor for Readers.\");\n    +    }\n<p>    +  }<br/>\n    +<br/>\n    +  @Override<br/>\n    +  public void ack(Object msgId) {<br/>\n    +    MessageId id = (MessageId) msgId;<br/>\n    +    inflight.remove(id);<br/>\n    +    ++acksSinceLastCommit;<br/>\n    +    tracker.recordAckedOffset(id.offset);<br/>\n    +    commitProgress(tracker.getCommitPosition());<br/>\n    +    if(fileReadCompletely) </p>\n{\n    +      markFileAsDone(reader.getFilePath());\n    +      reader = null;\n    +    }\n<p>    &#8212; End diff &#8211;</p>\n\n<p>    tracker is not reset for the new file/lock. So, existing tracker may have offsets of earlier file read which may give wrong state of the system.</p>\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612631037/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164766816","html_url":"https://github.com/apache/storm/pull/930#issuecomment-164766816","issue_url":"https://api.github.com/repos/apache/storm/issues/930","id":164766816,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDc2NjgxNg==","user":{"login":"satishd","id":2577761,"node_id":"MDQ6VXNlcjI1Nzc3NjE=","avatar_url":"https://avatars.githubusercontent.com/u/2577761?v=4","gravatar_id":"","url":"https://api.github.com/users/satishd","html_url":"https://github.com/satishd","followers_url":"https://api.github.com/users/satishd/followers","following_url":"https://api.github.com/users/satishd/following{/other_user}","gists_url":"https://api.github.com/users/satishd/gists{/gist_id}","starred_url":"https://api.github.com/users/satishd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishd/subscriptions","organizations_url":"https://api.github.com/users/satishd/orgs","repos_url":"https://api.github.com/users/satishd/repos","events_url":"https://api.github.com/users/satishd/events{/privacy}","received_events_url":"https://api.github.com/users/satishd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T13:37:30Z","updated_at":"2015-12-15T13:37:30Z","author_association":"MEMBER","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164766816/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627462","html_url":"https://github.com/apache/storm/issues/5017#issuecomment-2612627462","issue_url":"https://api.github.com/repos/apache/storm/issues/5017","id":2612627462,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjc0NjI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T13:37:32Z","updated_at":"2025-01-24T14:12:32Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user satishd commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/930#issuecomment-164766816\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/930#issuecomment-164766816</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627462/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686612","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686612","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686612,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2MTI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T14:35:29Z","updated_at":"2025-01-24T14:38:26Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#discussion_r47642414\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#discussion_r47642414</a></p>\n\n<p>    &#8212; Diff: storm-core/test/clj/backtype/storm/security/auth/ThriftClient_test.clj &#8212;<br/>\n    @@ -20,26 +20,26 @@<br/>\n       (:import <span class=\"error\">&#91;org.apache.thrift.transport TTransportException&#93;</span>)<br/>\n     )</p>\n\n<p>    +(def TIMEOUT (Integer. (* 3 1000)))<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    > Are we going from 30 seconds to 3000 miliseconds here? Shouldn't it be 30000?</p>\n\n<p>    30 seconds is an extremely long timeout for a thrift response.  Most of the time, 30ms is enough.  If the thrift server does not reply within 3 full seconds, I am done waiting for it.</p>\n\n\n<p>    > How about adding a comment describing why the choice of 3000 in this particular test?</p>\n\n<p>    Yes, I can add a shorter comment.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686612/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164782473","html_url":"https://github.com/apache/storm/pull/941#issuecomment-164782473","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":164782473,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDc4MjQ3Mw==","user":{"login":"d2r","id":905298,"node_id":"MDQ6VXNlcjkwNTI5OA==","avatar_url":"https://avatars.githubusercontent.com/u/905298?v=4","gravatar_id":"","url":"https://api.github.com/users/d2r","html_url":"https://github.com/d2r","followers_url":"https://api.github.com/users/d2r/followers","following_url":"https://api.github.com/users/d2r/following{/other_user}","gists_url":"https://api.github.com/users/d2r/gists{/gist_id}","starred_url":"https://api.github.com/users/d2r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/d2r/subscriptions","organizations_url":"https://api.github.com/users/d2r/orgs","repos_url":"https://api.github.com/users/d2r/repos","events_url":"https://api.github.com/users/d2r/events{/privacy}","received_events_url":"https://api.github.com/users/d2r/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T14:35:41Z","updated_at":"2015-12-15T14:35:41Z","author_association":"NONE","body":"> Can not we have this constant in a common file as (3 secs or 30 secs) used in other places like auth_test.clj, drpc_auth_test.clj, nimbus_auth_test.clj?\n\nSure, I will do that.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164782473/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686615","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686615","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686615,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2MTU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T14:35:42Z","updated_at":"2025-01-24T14:38:26Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-164782473\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-164782473</a></p>\n\n<p>    > Can not we have this constant in a common file as (3 secs or 30 secs) used in other places like auth_test.clj, drpc_auth_test.clj, nimbus_auth_test.clj?</p>\n\n<p>    Sure, I will do that.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686615/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164794521","html_url":"https://github.com/apache/storm/pull/930#issuecomment-164794521","issue_url":"https://api.github.com/repos/apache/storm/issues/930","id":164794521,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDc5NDUyMQ==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:15:42Z","updated_at":"2015-12-15T15:15:42Z","author_association":"CONTRIBUTOR","body":"Thanks @hmcl merged into master.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164794521/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627475","html_url":"https://github.com/apache/storm/issues/5017#issuecomment-2612627475","issue_url":"https://api.github.com/repos/apache/storm/issues/5017","id":2612627475,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjc0NzU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:15:43Z","updated_at":"2025-01-24T14:12:33Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/930#issuecomment-164794521\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/930#issuecomment-164794521</a></p>\n\n<p>    Thanks @hmcl merged into master.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627475/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627480","html_url":"https://github.com/apache/storm/issues/5017#issuecomment-2612627480","issue_url":"https://api.github.com/repos/apache/storm/issues/5017","id":2612627480,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjc0ODA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:15:44Z","updated_at":"2025-01-24T14:12:33Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user asfgit closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/930\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/930</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612627480/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164794633","html_url":"https://github.com/apache/storm/pull/941#issuecomment-164794633","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":164794633,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDc5NDYzMw==","user":{"login":"unsleepy22","id":631361,"node_id":"MDQ6VXNlcjYzMTM2MQ==","avatar_url":"https://avatars.githubusercontent.com/u/631361?v=4","gravatar_id":"","url":"https://api.github.com/users/unsleepy22","html_url":"https://github.com/unsleepy22","followers_url":"https://api.github.com/users/unsleepy22/followers","following_url":"https://api.github.com/users/unsleepy22/following{/other_user}","gists_url":"https://api.github.com/users/unsleepy22/gists{/gist_id}","starred_url":"https://api.github.com/users/unsleepy22/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/unsleepy22/subscriptions","organizations_url":"https://api.github.com/users/unsleepy22/orgs","repos_url":"https://api.github.com/users/unsleepy22/repos","events_url":"https://api.github.com/users/unsleepy22/events{/privacy}","received_events_url":"https://api.github.com/users/unsleepy22/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:16:09Z","updated_at":"2015-12-15T15:16:09Z","author_association":"NONE","body":"Hi @d2r , could you change 3 secs a little longer? We've met the case in production that for a large topology(say, 800+ workers, 4000+ tasks), this may timeout.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164794633/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686619","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686619","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686619,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2MTk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:16:45Z","updated_at":"2025-01-24T14:38:26Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-164794633\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-164794633</a></p>\n\n<p>    Hi @d2r , could you change 3 secs a little longer? We've met the case in production that for a large topology(say, 800+ workers, 4000+ tasks), this may timeout.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686619/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164796992","html_url":"https://github.com/apache/storm/pull/941#issuecomment-164796992","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":164796992,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDc5Njk5Mg==","user":{"login":"d2r","id":905298,"node_id":"MDQ6VXNlcjkwNTI5OA==","avatar_url":"https://avatars.githubusercontent.com/u/905298?v=4","gravatar_id":"","url":"https://api.github.com/users/d2r","html_url":"https://github.com/d2r","followers_url":"https://api.github.com/users/d2r/followers","following_url":"https://api.github.com/users/d2r/following{/other_user}","gists_url":"https://api.github.com/users/d2r/gists{/gist_id}","starred_url":"https://api.github.com/users/d2r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/d2r/subscriptions","organizations_url":"https://api.github.com/users/d2r/orgs","repos_url":"https://api.github.com/users/d2r/repos","events_url":"https://api.github.com/users/d2r/events{/privacy}","received_events_url":"https://api.github.com/users/d2r/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:23:32Z","updated_at":"2015-12-15T15:23:32Z","author_association":"NONE","body":"> Hi @d2r , could you change 3 secs a little longer? We've met the case in production that for a large topology(say, 800+ workers, 4000+ tasks), this may timeout.\n\n@unsleepy22, these are timeouts for simple tests.  Normally much, much shorter timeouts work.  It's just that on single-core environments, like that of our travis-ci, some of the clients cannot quite reply in 30 milliseconds.  This should not affect any deployment in production; it is merely for running tests.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164796992/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686622","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686622","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686622,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2MjI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:23:34Z","updated_at":"2025-01-24T14:38:26Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-164796992\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-164796992</a></p>\n\n<p>    > Hi @d2r , could you change 3 secs a little longer? We've met the case in production that for a large topology(say, 800+ workers, 4000+ tasks), this may timeout.</p>\n\n<p>    @unsleepy22, these are timeouts for simple tests.  Normally much, much shorter timeouts work.  It's just that on single-core environments, like that of our travis-ci, some of the clients cannot quite reply in 30 milliseconds.  This should not affect any deployment in production; it is merely for running tests.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686622/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164797150","html_url":"https://github.com/apache/storm/pull/941#issuecomment-164797150","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":164797150,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDc5NzE1MA==","user":{"login":"d2r","id":905298,"node_id":"MDQ6VXNlcjkwNTI5OA==","avatar_url":"https://avatars.githubusercontent.com/u/905298?v=4","gravatar_id":"","url":"https://api.github.com/users/d2r","html_url":"https://github.com/d2r","followers_url":"https://api.github.com/users/d2r/followers","following_url":"https://api.github.com/users/d2r/following{/other_user}","gists_url":"https://api.github.com/users/d2r/gists{/gist_id}","starred_url":"https://api.github.com/users/d2r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/d2r/subscriptions","organizations_url":"https://api.github.com/users/d2r/orgs","repos_url":"https://api.github.com/users/d2r/repos","events_url":"https://api.github.com/users/d2r/events{/privacy}","received_events_url":"https://api.github.com/users/d2r/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:24:07Z","updated_at":"2015-12-15T15:24:07Z","author_association":"NONE","body":"Review comments addressed\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164797150/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686626","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686626","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686626,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2MjY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:24:08Z","updated_at":"2025-01-24T14:38:26Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-164797150\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-164797150</a></p>\n\n<p>    Review comments addressed</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686626/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164797217","html_url":"https://github.com/apache/storm/pull/941#issuecomment-164797217","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":164797217,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDc5NzIxNw==","user":{"login":"unsleepy22","id":631361,"node_id":"MDQ6VXNlcjYzMTM2MQ==","avatar_url":"https://avatars.githubusercontent.com/u/631361?v=4","gravatar_id":"","url":"https://api.github.com/users/unsleepy22","html_url":"https://github.com/unsleepy22","followers_url":"https://api.github.com/users/unsleepy22/followers","following_url":"https://api.github.com/users/unsleepy22/following{/other_user}","gists_url":"https://api.github.com/users/unsleepy22/gists{/gist_id}","starred_url":"https://api.github.com/users/unsleepy22/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/unsleepy22/subscriptions","organizations_url":"https://api.github.com/users/unsleepy22/orgs","repos_url":"https://api.github.com/users/unsleepy22/repos","events_url":"https://api.github.com/users/unsleepy22/events{/privacy}","received_events_url":"https://api.github.com/users/unsleepy22/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:24:24Z","updated_at":"2015-12-15T15:24:24Z","author_association":"NONE","body":"@d2r thanks, +1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164797217/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686631","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686631","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686631,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2MzE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:24:25Z","updated_at":"2025-01-24T14:38:27Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-164797217\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-164797217</a></p>\n\n<p>    @d2r thanks, +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686631/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685717","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685717","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685717,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3MTc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T15:35:30Z","updated_at":"2025-01-24T14:38:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47651609\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47651609</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,246 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.LocalCluster;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.blobstore.BlobStoreAclHandler;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.IOException;<br/>\n    +import java.io.InputStreamReader;<br/>\n    +import java.util.Arrays;<br/>\n    +import java.util.HashMap;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static NimbusBlobStore store = new NimbusBlobStore(); // Client API to invoke blob store API functionality<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    store -> clientStore?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685717/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164813457","html_url":"https://github.com/apache/storm/pull/941#issuecomment-164813457","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":164813457,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDgxMzQ1Nw==","user":{"login":"d2r","id":905298,"node_id":"MDQ6VXNlcjkwNTI5OA==","avatar_url":"https://avatars.githubusercontent.com/u/905298?v=4","gravatar_id":"","url":"https://api.github.com/users/d2r","html_url":"https://github.com/d2r","followers_url":"https://api.github.com/users/d2r/followers","following_url":"https://api.github.com/users/d2r/following{/other_user}","gists_url":"https://api.github.com/users/d2r/gists{/gist_id}","starred_url":"https://api.github.com/users/d2r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/d2r/subscriptions","organizations_url":"https://api.github.com/users/d2r/orgs","repos_url":"https://api.github.com/users/d2r/repos","events_url":"https://api.github.com/users/d2r/events{/privacy}","received_events_url":"https://api.github.com/users/d2r/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:15:34Z","updated_at":"2015-12-15T16:15:34Z","author_association":"NONE","body":"1 kafka test timed out:\n\n```\nTests run: 18, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 30.722 sec <<< FAILURE! - in storm.kafka.KafkaUtilsTest\ngetOffsetFromConfigAndFroceFromStart(storm.kafka.KafkaUtilsTest)  Time elapsed: 2.732 sec  <<< FAILURE!\njava.lang.AssertionError: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 1000 ms.\n    at org.junit.Assert.fail(Assert.java:88)\n    at storm.kafka.KafkaUtilsTest.createTopicAndSendMessage(KafkaUtilsTest.java:246)\n    at storm.kafka.KafkaUtilsTest.createTopicAndSendMessage(KafkaUtilsTest.java:228)\n    at storm.kafka.KafkaUtilsTest.getOffsetFromConfigAndFroceFromStart(KafkaUtilsTest.java:132)\n```\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164813457/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686636","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686636","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686636,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2MzY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:15:35Z","updated_at":"2025-01-24T14:38:27Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-164813457\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-164813457</a></p>\n\n<p>    1 kafka test timed out:<br/>\n    ```<br/>\n    Tests run: 18, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 30.722 sec <<< FAILURE! - in storm.kafka.KafkaUtilsTest<br/>\n    getOffsetFromConfigAndFroceFromStart(storm.kafka.KafkaUtilsTest)  Time elapsed: 2.732 sec  <<< FAILURE!<br/>\n    java.lang.AssertionError: org.apache.kafka.common.errors.TimeoutException: Failed to update metadata after 1000 ms.<br/>\n    \tat org.junit.Assert.fail(Assert.java:88)<br/>\n    \tat storm.kafka.KafkaUtilsTest.createTopicAndSendMessage(KafkaUtilsTest.java:246)<br/>\n    \tat storm.kafka.KafkaUtilsTest.createTopicAndSendMessage(KafkaUtilsTest.java:228)<br/>\n    \tat storm.kafka.KafkaUtilsTest.getOffsetFromConfigAndFroceFromStart(KafkaUtilsTest.java:132)<br/>\n    ```</p>\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686636/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688824","html_url":"https://github.com/apache/storm/issues/5227#issuecomment-2612688824","issue_url":"https://api.github.com/repos/apache/storm/issues/5227","id":2612688824,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg4MjQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:51:31Z","updated_at":"2025-01-24T14:39:25Z","author_association":"COLLABORATOR","body":"Subtask of parent task STORM-915","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688824/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612628684","html_url":"https://github.com/apache/storm/issues/5025#issuecomment-2612628684","issue_url":"https://api.github.com/repos/apache/storm/issues/5025","id":2612628684,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjg2ODQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:51:42Z","updated_at":"2025-01-24T14:13:04Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/900#discussion_r47663680\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/900#discussion_r47663680</a></p>\n\n<p>    &#8212; Diff: storm-core/src/jvm/backtype/storm/Config.java &#8212;<br/>\n    @@ -1697,19 +1697,45 @@<br/>\n public static final String TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS = \"topology.bolts.window.length.duration.ms\";</p>\n\n<p> /*</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>* Bolt-specific configuration for windowed bolts to specifiy the sliding interval as a count of number of tuples.<br/>\n    +     * Bolt-specific configuration for windowed bolts to specify the sliding interval as a count of number of tuples.<br/>\n  */<br/>\n @isInteger<br/>\n @isPositiveNumber<br/>\n public static final String TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT = \"topology.bolts.window.sliding.interval.count\";</li>\n</ul>\n\n\n<p> /*</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>* Bolt-specific configuration for windowed bolts to specifiy the sliding interval in time duration.<br/>\n    +     * Bolt-specific configuration for windowed bolts to specify the sliding interval in time duration.<br/>\n  */<br/>\n @isInteger<br/>\n @isPositiveNumber<br/>\n public static final String TOPOLOGY_BOLTS_SLIDING_INTERVAL_DURATION_MS = \"topology.bolts.window.sliding.interval.duration.ms\";</li>\n</ul>\n\n\n<p>    +    /*<br/>\n    +     * Bolt-specific configuration for windowed bolts to specify the name of the field in the tuple that holds<br/>\n    +     * the timestamp (e.g. the ts when the tuple was actually generated). If this config is specified and the<br/>\n    +     * field is not present in the incoming tuple, a java.lang.IllegalArgumentException will be thrown.<br/>\n    +     */<br/>\n    +    @isString<br/>\n    +    public static final String TOPOLOGY_BOLTS_TUPLE_TIMESTAMP_FIELD_NAME = \"topology.bolts.tuple.timestamp.field.name\";<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    any reason for this config to have BOLT in them , can't we name it as topology.tuple.timestamp.field.name</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612628684/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164825084","html_url":"https://github.com/apache/storm/pull/938#issuecomment-164825084","issue_url":"https://api.github.com/repos/apache/storm/issues/938","id":164825084,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDgyNTA4NA==","user":{"login":"d2r","id":905298,"node_id":"MDQ6VXNlcjkwNTI5OA==","avatar_url":"https://avatars.githubusercontent.com/u/905298?v=4","gravatar_id":"","url":"https://api.github.com/users/d2r","html_url":"https://github.com/d2r","followers_url":"https://api.github.com/users/d2r/followers","following_url":"https://api.github.com/users/d2r/following{/other_user}","gists_url":"https://api.github.com/users/d2r/gists{/gist_id}","starred_url":"https://api.github.com/users/d2r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/d2r/subscriptions","organizations_url":"https://api.github.com/users/d2r/orgs","repos_url":"https://api.github.com/users/d2r/repos","events_url":"https://api.github.com/users/d2r/events{/privacy}","received_events_url":"https://api.github.com/users/d2r/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:52:39Z","updated_at":"2015-12-15T16:52:39Z","author_association":"NONE","body":"- JDK7 storm-core: drpc-auth-test race lost binding ephemeral port\n  `\n  118390 [Thread-1037] ERROR b.s.s.a.ThriftServer - ThriftServer is being stopped due to: org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:52747.\n  `\n- JDK8 storm-core: nimbus-auth-test failed, likely will be fixed by #941 \n- JDK7 !storm-core: org.apache.storm.cassandra.DynamicStatementBuilderTest:\n  \n  ```\n  java.lang.AssertionError: Cassandra daemon did not start within timeout\n  ```\n  \n  Created [STORM-1392](https://issues.apache.org/jira/browse/STORM-1392)\n\nNone of these is related to this PR.  I will close and re-open this PR to start a new test run.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164825084/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687559","html_url":"https://github.com/apache/storm/issues/5218#issuecomment-2612687559","issue_url":"https://api.github.com/repos/apache/storm/issues/5218","id":2612687559,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODc1NTk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:52:40Z","updated_at":"2025-01-24T14:38:51Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/938#issuecomment-164825084\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/938#issuecomment-164825084</a></p>\n\n<ul>\n\t<li>JDK7 storm-core: drpc-auth-test race lost binding ephemeral port<br/>\n```<br/>\n118390 <span class=\"error\">&#91;Thread-1037&#93;</span> ERROR b.s.s.a.ThriftServer - ThriftServer is being stopped due to: org.apache.thrift.transport.TTransportException: Could not create ServerSocket on address 0.0.0.0/0.0.0.0:52747.<br/>\n```</li>\n\t<li>JDK8 storm-core: nimbus-auth-test failed, likely will be fixed by #941</li>\n</ul>\n\n\n<ul>\n\t<li>JDK7 !storm-core: org.apache.storm.cassandra.DynamicStatementBuilderTest:<br/>\n```<br/>\njava.lang.AssertionError: Cassandra daemon did not start within timeout<br/>\n```<br/>\n      Created <a href=\"https://issues.apache.org/jira/browse/STORM-1392\" title=\"Storm Cassandra Test Timeouts\" class=\"issue-link\" data-issue-key=\"STORM-1392\"><del>STORM-1392</del></a>(<a href=\"https://issues.apache.org/jira/browse/STORM-1392\" class=\"external-link\" rel=\"nofollow\">https://issues.apache.org/jira/browse/STORM-1392</a>)</li>\n</ul>\n\n\n<p>    None of these is related to this PR.  I will close and re-open this PR to start a new test run.</p>\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687559/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687564","html_url":"https://github.com/apache/storm/issues/5218#issuecomment-2612687564","issue_url":"https://api.github.com/repos/apache/storm/issues/5218","id":2612687564,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODc1NjQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:52:40Z","updated_at":"2025-01-24T14:38:51Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/938\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/938</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687564/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687569","html_url":"https://github.com/apache/storm/issues/5218#issuecomment-2612687569","issue_url":"https://api.github.com/repos/apache/storm/issues/5218","id":2612687569,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODc1Njk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:52:43Z","updated_at":"2025-01-24T14:38:51Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>GitHub user d2r reopened a pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/938\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/938</a></p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1383\" title=\"Supervisors should not crash if nimbus is unavailable\" class=\"issue-link\" data-issue-key=\"STORM-1383\"><del>STORM-1383</del></a> Avoid supervisor crashing if nimbus is unavailable</p>\n\n<ul>\n\t<li>Adds a new exception to differentiate the failure to find a nimbus leader from a network failure.</li>\n\t<li>Since blobs are scanned for updates periodically, do not crash the supervisor if nimbus is not available.</li>\n\t<li>Do not crash the supervisor if nimbus is unavailable when downloading topology resources for launch.</li>\n</ul>\n\n\n\n<p>You can merge this pull request into a Git repository by running:</p>\n\n<p>    $ git pull <a href=\"https://github.com/d2r/storm\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/d2r/storm</a> storm-1383-nimbus-supvor-crash-loop</p>\n\n<p>Alternatively you can review and apply these changes as the patch at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/938.patch\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/938.patch</a></p>\n\n<p>To close this pull request, make a commit to your master/trunk branch<br/>\nwith (at least) the following in the commit message:</p>\n\n<p>    This closes #938</p>\n\n<hr />\n<p>commit 8cf9116ee3e635849bce2623f31ac42fde3918f7<br/>\nAuthor: Derek Dagit <derekd@yahoo-inc.com><br/>\nDate:   2015-12-14T21:03:32Z</p>\n\n<p>    Avoid supervisor crashing if nimbus is unavailable</p>\n\n<hr />","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612687569/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612628686","html_url":"https://github.com/apache/storm/issues/5025#issuecomment-2612628686","issue_url":"https://api.github.com/repos/apache/storm/issues/5025","id":2612628686,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjg2ODY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T16:56:47Z","updated_at":"2025-01-24T14:13:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/900#discussion_r47664435\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/900#discussion_r47664435</a></p>\n\n<p>    &#8212; Diff: storm-core/src/jvm/backtype/storm/Config.java &#8212;<br/>\n    @@ -1697,19 +1697,45 @@<br/>\n public static final String TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS = \"topology.bolts.window.length.duration.ms\";</p>\n\n<p> /*</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>* Bolt-specific configuration for windowed bolts to specifiy the sliding interval as a count of number of tuples.<br/>\n    +     * Bolt-specific configuration for windowed bolts to specify the sliding interval as a count of number of tuples.<br/>\n  */<br/>\n @isInteger<br/>\n @isPositiveNumber<br/>\n public static final String TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT = \"topology.bolts.window.sliding.interval.count\";</li>\n</ul>\n\n\n<p> /*</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>* Bolt-specific configuration for windowed bolts to specifiy the sliding interval in time duration.<br/>\n    +     * Bolt-specific configuration for windowed bolts to specify the sliding interval in time duration.<br/>\n  */<br/>\n @isInteger<br/>\n @isPositiveNumber<br/>\n public static final String TOPOLOGY_BOLTS_SLIDING_INTERVAL_DURATION_MS = \"topology.bolts.window.sliding.interval.duration.ms\";</li>\n</ul>\n\n\n<p>    +    /*<br/>\n    +     * Bolt-specific configuration for windowed bolts to specify the name of the field in the tuple that holds<br/>\n    +     * the timestamp (e.g. the ts when the tuple was actually generated). If this config is specified and the<br/>\n    +     * field is not present in the incoming tuple, a java.lang.IllegalArgumentException will be thrown.<br/>\n    +     */<br/>\n    +    @isString<br/>\n    +    public static final String TOPOLOGY_BOLTS_TUPLE_TIMESTAMP_FIELD_NAME = \"topology.bolts.tuple.timestamp.field.name\";<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    @harshach BOLT was added to emphasize that this is a bolt specific configuration (used for windowed bolts) similar to the other params above this.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612628686/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685360","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685360","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685360,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUzNjA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T17:15:35Z","updated_at":"2025-01-24T14:37:57Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47667287\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47667287</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,736 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blobstore implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +<br/>\n    +```<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +```<br/>\n    +<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blobstore. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blobstore implementations. The replication in HDFS blobstore is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbuses. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blobstore allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blobstore, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional blobstore Implementation Details<br/>\n    +blobstore uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blobstore reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    comma needed in the following:<br/>\n    Once the topology is launched and the relevant blobs have been created, the supervisor downloads blobs related to the storm.conf, storm.ser </p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685360/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685367","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685367","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685367,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUzNjc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T17:17:59Z","updated_at":"2025-01-24T14:37:57Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47667610\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47667610</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,732 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Comma needed:<br/>\n    In most cases, the </p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685367/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685372","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685372","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685372,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUzNzI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T17:18:53Z","updated_at":"2025-01-24T14:37:57Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47667733\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47667733</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,736 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blobstore implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +<br/>\n    +```<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +```<br/>\n    +<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blobstore. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blobstore implementations. The replication in HDFS blobstore is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbuses. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blobstore allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blobstore, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional blobstore Implementation Details<br/>\n    +blobstore uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blobstore reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blobstore implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blobstore is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blobstore directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the process that does supervision. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    comma:<br/>\n    Under these circumstances, the topologies</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685372/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685381","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685381","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685381,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUzODE=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T17:19:51Z","updated_at":"2025-01-24T14:37:57Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47667869\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47667869</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,736 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blobstore implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +<br/>\n    +```<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +```<br/>\n    +<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blobstore. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blobstore implementations. The replication in HDFS blobstore is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbuses. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blobstore allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blobstore, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional blobstore Implementation Details<br/>\n    +blobstore uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blobstore reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blobstore implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blobstore is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blobstore directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the process that does supervision. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Comma:</p>\n\n<p>    With this project we intend, to resolve this problem</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685381/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685384","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685384","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685384,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUzODQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T17:24:22Z","updated_at":"2025-01-24T14:37:57Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47668508\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47668508</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,736 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blobstore implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +<br/>\n    +```<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +```<br/>\n    +<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blobstore. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blobstore implementations. The replication in HDFS blobstore is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbuses. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blobstore allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blobstore, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional blobstore Implementation Details<br/>\n    +blobstore uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blobstore reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blobstore implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blobstore is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blobstore directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    +nimbus failure is transient and it is restarted by the process that does supervision. However sometimes when disks fail and networks <br/>\n    +partitions occur, nimbus goes down. Under these circumstances the topologies run normally but no new topologies can be <br/>\n    +submitted, no existing topologies can be killed/deactivated/activated and if a supervisor node fails then the <br/>\n    +reassignments are not performed resulting in performance degradation or topology failures. With this project we intend <br/>\n    +to resolve this problem by running nimbus in a primary backup mode to guarantee that even if a nimbus server fails one <br/>\n    +of the backups will take over. <br/>\n    +<br/>\n    +### Requirements for Highly Available Nimbus:<br/>\n    +* Increase overall availability of nimbus.<br/>\n    +* Allow nimbus hosts to leave and join the cluster at will any time. A newly joined host should auto catch up and join <br/>\n    +the list of potential leaders automatically. <br/>\n    +* No topology resubmissions required in case of nimbus fail overs.<br/>\n    +* No active topology should ever be lost. <br/>\n    +<br/>\n    +#### Leader Election:<br/>\n    +The nimbus server will use the following interface:<br/>\n    +<br/>\n    +```java<br/>\n    +public interface ILeaderElector </p>\n{\n    +    /**\n    +     * queue up for leadership lock. The call returns immediately and the caller     \n    +     * must check isLeader() to perform any leadership action.\n    +     */\n    +    void addToLeaderLockQueue();\n    +\n    +    /**\n    +     * Removes the caller from the leader lock queue. If the caller is leader\n    +     * also releases the lock.\n    +     */\n    +    void removeFromLeaderLockQueue();\n    +\n    +    /**\n    +     *\n    +     * @return true if the caller currently has the leader lock.\n    +     */\n    +    boolean isLeader();\n    +\n    +    /**\n    +     *\n    +     * @return the current leader's address , throws exception if noone has has    lock.\n    +     */\n    +    InetSocketAddress getLeaderAddress();\n    +\n    +    /**\n    +     * \n    +     * @return list of current nimbus addresses, includes leader.\n    +     */\n    +    List<InetSocketAddress> getAllNimbusAddresses();\n    +}\n<p>    +```<br/>\n    +Once a nimbus comes up it calls addToLeaderLockQueue() function. The leader election code selects a leader from the queue.<br/>\n    +If the topology code, jar or config blobs are missing, it would download the blobs from any other nimbus which is up and running.<br/>\n    +<br/>\n    +The first implementation will be Zookeeper based. If the zookeeper connection is lost/reset resulting in loss of lock<br/>\n    +or the spot in queue the implementation will take care of updating the state such that isLeader() will reflect the <br/>\n    +current status. The leader like actions must finish in less than minimumOf(connectionTimeout, SessionTimeout) to ensure<br/>\n    +the lock was held by nimbus for the entire duration of the action (Not sure if we want to just state this expectation <br/>\n    +and ensure that zk configurations are set high enough which will result in higher failover time or we actually want to <br/>\n    +create some sort of rollback mechanism for all actions, the second option needs a lot of code). If a nimbus that is not <br/>\n    +leader receives a request that only a leader can perform it will throw a RunTimeException.<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Comma:<br/>\n    receives a request that only a leader can perform, it will throw a RunTimeException.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685384/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164898677","html_url":"https://github.com/apache/storm/pull/921#issuecomment-164898677","issue_url":"https://api.github.com/repos/apache/storm/issues/921","id":164898677,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDg5ODY3Nw==","user":{"login":"rfarivar","id":8742608,"node_id":"MDQ6VXNlcjg3NDI2MDg=","avatar_url":"https://avatars.githubusercontent.com/u/8742608?v=4","gravatar_id":"","url":"https://api.github.com/users/rfarivar","html_url":"https://github.com/rfarivar","followers_url":"https://api.github.com/users/rfarivar/followers","following_url":"https://api.github.com/users/rfarivar/following{/other_user}","gists_url":"https://api.github.com/users/rfarivar/gists{/gist_id}","starred_url":"https://api.github.com/users/rfarivar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rfarivar/subscriptions","organizations_url":"https://api.github.com/users/rfarivar/orgs","repos_url":"https://api.github.com/users/rfarivar/repos","events_url":"https://api.github.com/users/rfarivar/events{/privacy}","received_events_url":"https://api.github.com/users/rfarivar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T21:16:01Z","updated_at":"2015-12-15T21:16:01Z","author_association":"CONTRIBUTOR","body":"Done with this review pass. Mostly formatting issues, no major problem I can detect with the logic. \n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164898677/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688937","html_url":"https://github.com/apache/storm/issues/5228#issuecomment-2612688937","issue_url":"https://api.github.com/repos/apache/storm/issues/5228","id":2612688937,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg5Mzc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T22:13:34Z","updated_at":"2025-01-24T14:39:29Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>GitHub user zhuoliu opened a pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/949\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/949</a></p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1393\" title=\"Sort out storm.log.dir configure and add documentation about logs\" class=\"issue-link\" data-issue-key=\"STORM-1393\"><del>STORM-1393</del></a> Update the storm.log.dir function, add doc for logs</p>\n\n<p>    Currently, we have reorganized logs in <a href=\"https://issues.apache.org/jira/browse/STORM-901\" title=\"Worker Artifacts Directory\" class=\"issue-link\" data-issue-key=\"STORM-901\"><del>STORM-901</del></a> and <a href=\"https://issues.apache.org/jira/browse/STORM-1387\" title=\"Make workers-artifacts directory configurable and default to storm.log.dir/workers-artifacts\" class=\"issue-link\" data-issue-key=\"STORM-1387\"><del>STORM-1387</del></a>, it is preferable for us to document the changes out for avoiding confusion to users.</p>\n\n<p>    Also, the util/LOG-DIR and the way supervisor to get storm.log.dir in worker-launch is inaccurate since it does not take the storm-conf into account. We should fix it.</p>\n\n\n<p>You can merge this pull request into a Git repository by running:</p>\n\n<p>    $ git pull <a href=\"https://github.com/zhuoliu/storm\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/zhuoliu/storm</a> 1393</p>\n\n<p>Alternatively you can review and apply these changes as the patch at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/949.patch\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/949.patch</a></p>\n\n<p>To close this pull request, make a commit to your master/trunk branch<br/>\nwith (at least) the following in the commit message:</p>\n\n<p>    This closes #949</p>\n\n<hr />\n<p>commit 288d6054baa0c32f682bfe8ff71f89f3a8f8d54a<br/>\nAuthor: zhuol <zhuol@yahoo-inc.com><br/>\nDate:   2015-12-15T22:01:39Z</p>\n\n<p>    <a href=\"https://issues.apache.org/jira/browse/STORM-1393\" title=\"Sort out storm.log.dir configure and add documentation about logs\" class=\"issue-link\" data-issue-key=\"STORM-1393\"><del>STORM-1393</del></a> Update the storm.log.dir function, add doc for logs</p>\n\n<hr />","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688937/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688945","html_url":"https://github.com/apache/storm/issues/5228#issuecomment-2612688945","issue_url":"https://api.github.com/repos/apache/storm/issues/5228","id":2612688945,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg5NDU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T22:25:53Z","updated_at":"2025-01-24T14:39:29Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user rfarivar commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/949#discussion_r47708800\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/949#discussion_r47708800</a></p>\n\n<p>    &#8212; Diff: docs/documentation/Logs.md &#8212;<br/>\n    @@ -0,0 +1,30 @@<br/>\n    +---<br/>\n    +title: Storm Logs<br/>\n    +layout: documentation<br/>\n    +documentation: true<br/>\n    +---<br/>\n    +Logs in Storm are essential for tracking the status, operations, error messages and debug information for all the <br/>\n    +daemons (nimbus, supervisor, logviewer, drpc, ui) and topologies' workers.<br/>\n    +<br/>\n    +### Location of the Logs<br/>\n    +All the daemon logs are placed under ${storm.log.dir} directory, which user can set in System property or<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Grammar. <br/>\n    which user can set in System property  --> which \"a\" user can set in \"the\" System property</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688945/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164920140","html_url":"https://github.com/apache/storm/pull/946#issuecomment-164920140","issue_url":"https://api.github.com/repos/apache/storm/issues/946","id":164920140,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDkyMDE0MA==","user":{"login":"rfarivar","id":8742608,"node_id":"MDQ6VXNlcjg3NDI2MDg=","avatar_url":"https://avatars.githubusercontent.com/u/8742608?v=4","gravatar_id":"","url":"https://api.github.com/users/rfarivar","html_url":"https://github.com/rfarivar","followers_url":"https://api.github.com/users/rfarivar/followers","following_url":"https://api.github.com/users/rfarivar/following{/other_user}","gists_url":"https://api.github.com/users/rfarivar/gists{/gist_id}","starred_url":"https://api.github.com/users/rfarivar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rfarivar/subscriptions","organizations_url":"https://api.github.com/users/rfarivar/orgs","repos_url":"https://api.github.com/users/rfarivar/repos","events_url":"https://api.github.com/users/rfarivar/events{/privacy}","received_events_url":"https://api.github.com/users/rfarivar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T22:40:48Z","updated_at":"2015-12-15T22:40:48Z","author_association":"CONTRIBUTOR","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164920140/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688448","html_url":"https://github.com/apache/storm/issues/5224#issuecomment-2612688448","issue_url":"https://api.github.com/repos/apache/storm/issues/5224","id":2612688448,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg0NDg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T22:40:50Z","updated_at":"2025-01-24T14:39:15Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user rfarivar commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/946#issuecomment-164920140\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/946#issuecomment-164920140</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688448/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164935737","html_url":"https://github.com/apache/storm/pull/921#issuecomment-164935737","issue_url":"https://api.github.com/repos/apache/storm/issues/921","id":164935737,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDkzNTczNw==","user":{"login":"jerrypeng","id":3613359,"node_id":"MDQ6VXNlcjM2MTMzNTk=","avatar_url":"https://avatars.githubusercontent.com/u/3613359?v=4","gravatar_id":"","url":"https://api.github.com/users/jerrypeng","html_url":"https://github.com/jerrypeng","followers_url":"https://api.github.com/users/jerrypeng/followers","following_url":"https://api.github.com/users/jerrypeng/following{/other_user}","gists_url":"https://api.github.com/users/jerrypeng/gists{/gist_id}","starred_url":"https://api.github.com/users/jerrypeng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jerrypeng/subscriptions","organizations_url":"https://api.github.com/users/jerrypeng/orgs","repos_url":"https://api.github.com/users/jerrypeng/repos","events_url":"https://api.github.com/users/jerrypeng/events{/privacy}","received_events_url":"https://api.github.com/users/jerrypeng/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-15T23:33:33Z","updated_at":"2015-12-15T23:33:33Z","author_association":"CONTRIBUTOR","body":"@rfarivar thanks for your review! much appreciated!\n@harshach do you have any comments regarding my PR\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164935737/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685394","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685394","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685394,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODUzOTQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T02:37:15Z","updated_at":"2025-01-24T14:37:57Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47730566\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47730566</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,732 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    @rfarivar changed to use blobstore</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685394/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685400","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685400","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685400,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU0MDA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T02:40:06Z","updated_at":"2025-01-24T14:37:57Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#discussion_r47730714\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#discussion_r47730714</a></p>\n\n<p>    &#8212; Diff: docs/documentation/distcache-blobstore.md &#8212;<br/>\n    @@ -0,0 +1,732 @@<br/>\n    +# Storm Distributed Cache API<br/>\n    +<br/>\n    +The distributed cache feature in storm is used to efficiently distribute files<br/>\n    +(or blobs, which is the equivalent terminology for a file in the distributed<br/>\n    +cache and is used interchangeably in this document) that are large and can<br/>\n    +change during the lifetime of a topology, such as geo-location data,<br/>\n    +dictionaries, etc. Typical use cases include phrase recognition, entity<br/>\n    +extraction, document classification, URL re-writing, location/address detection<br/>\n    +and so forth. Such files may be several KB to several GB in size. For small<br/>\n    +datasets that don't need dynamic updates, including them in the topology jar<br/>\n    +could be fine. But for large files, the startup times could become very large.<br/>\n    +In these cases, the distributed cache feature can provide fast topology startup,<br/>\n    +especially if the files were previously downloaded for the same submitter and<br/>\n    +are still in the cache. This is useful with frequent deployments, sometimes few<br/>\n    +times a day with updated jars, because the large cached files will remain available<br/>\n    +without changes. The large cached blobs that do not change frequently will<br/>\n    +remain available in the distributed cache.<br/>\n    +<br/>\n    +At the starting time of a topology, the user specifies the set of files the<br/>\n    +topology needs. Once a topology is running, the user at any time can request for<br/>\n    +any file in the distributed cache to be updated with a newer version. The<br/>\n    +updating of blobs happens in an eventual consistency model. If the topology<br/>\n    +needs to know what version of a file it has access to, it is the responsibility<br/>\n    +of the user to find this information out. The files are stored in a cache with<br/>\n    +Least-Recently Used (LRU) eviction policy, where the supervisor decides which<br/>\n    +cached files are no longer needed and can delete them to free disk space. The<br/>\n    +blobs can be compressed, and the user can request the blobs to be uncompressed<br/>\n    +before it accesses them.<br/>\n    +<br/>\n    +## Motivation for Distributed Cache<br/>\n    +* Allows sharing blobs among topologies.<br/>\n    +* Allows updating the blobs from the command line.<br/>\n    +<br/>\n    +## Distributed Cache Implementations<br/>\n    +The current BlobStore interface has the following two implementations<br/>\n    +* LocalFsBlobStore<br/>\n    +* HdfsBlobStore<br/>\n    +<br/>\n    +Appendix A contains the interface for blob store implementation.<br/>\n    +<br/>\n    +## LocalFsBlobStore<br/>\n    +!<span class=\"error\">&#91;LocalFsBlobStore&#93;</span>(images/local_blobstore.png)<br/>\n    +<br/>\n    +Local file system implementation of Blobstore can be depicted in the above timeline diagram.<br/>\n    +<br/>\n    +There are several stages from blob creation to blob download and corresponding execution of a topology. <br/>\n    +The main stages can be depicted as follows<br/>\n    +<br/>\n    +### Blob Creation Command<br/>\n    +Blobs in the blobstore can be created through command line using the following command.<br/>\n    +storm blobstore create --file README.txt --acl o::rwa --repl-fctr 4 key1<br/>\n    +The above command creates a blob with a key name key1 corresponding to the file README.txt. <br/>\n    +The access given to all users being read, write and admin with a replication factor of 4.<br/>\n    +<br/>\n    +### Topology Submission and Blob Mapping<br/>\n    +Users can submit their topology with the following command. The command includes the <br/>\n    +topology map configuration. The configuration holds two keys key1 and key2 with the <br/>\n    +key key1 having a local file name mapping named blob_file and it is not compressed.<br/>\n    +<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar <br/>\n    +storm.starter.clj.word_count test_topo -c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    +<br/>\n    +### Blob Creation Process<br/>\n    +The creation of the blob takes place through the interface ClientBlobStore. Appendix B contains the ClientBlobStore interface. <br/>\n    +The concrete implementation of this interface is the  NimbusBlobStore. In the case of local file system the client makes a <br/>\n    +call to the nimbus to create the blobs within the local file system. The nimbus uses the local file system implementation to create these blobs. <br/>\n    +When a user submits a topology, the jar, configuration and code files are uploaded as blobs with the help of blob store. <br/>\n    +Also, all the other blobs specified by the topology are mapped to it with the help of topology.blobstore.map configuration.<br/>\n    +<br/>\n    +### Blob Download by the Supervisor<br/>\n    +Finally, the blobs corresponding to a topology are downloaded by the supervisor once it receives the assignments from the nimbus through <br/>\n    +the same NimbusBlobStore thrift client that uploaded the blobs. The supervisor downloads the code, jar and conf blobs by calling the <br/>\n    +NimbusBlobStore client directly while the blobs specified in the topology.blobstore.map are downloaded and mapped locally with the help <br/>\n    +of the Localizer. The Localizer talks to the NimbusBlobStore thrift client to download the blobs and adds the blob compression and local <br/>\n    +blob name mapping logic to suit the implementation of a topology. Once all the blobs have been downloaded the workers are launched to run <br/>\n    +the topologies.<br/>\n    +<br/>\n    +## HdfsBlobStore<br/>\n    +!<span class=\"error\">&#91;HdfsBlobStore&#93;</span>(images/hdfs_blobstore.png)<br/>\n    +<br/>\n    +The HdfsBlobStore functionality has a similar implementation and blob creation and download procedure barring how the replication <br/>\n    +is handled in the two blob store implementations. The replication in HDFS blob store is obvious as HDFS is equipped to handle replication <br/>\n    +and it requires no state to be stored inside the zookeeper. On the other hand, the local file system blobstore requires the state to be <br/>\n    +stored on the zookeeper in order for it to work with nimbus HA. Nimbus HA allows the local filesystem to implement the replication feature <br/>\n    +seamlessly by storing the state in the zookeeper about the running topologies and syncing the blobs on various nimbodes. On the supervisors <br/>\n    +end, the supervisor and localizer talks to HdfsBlobStore through HdfsClientBlobStore implementation.<br/>\n    +<br/>\n    +## Additional Features and Documentation<br/>\n    +```<br/>\n    +storm jar /home/y/lib/storm-starter/current/storm-starter-jar-with-dependencies.jar storm.starter.clj.word_count test_topo <br/>\n    +-c topology.blobstore.map='{\"key1\":</p>\n{\"localname\":\"blob_file\", \"uncompress\":\"false\"}\n<p>,\"key2\":{}}'<br/>\n    +```<br/>\n    + <br/>\n    +### Compression<br/>\n    +The blob store allows the user to specify the uncompress configuration to true or false. This configuration can be specified <br/>\n    +in the topology.blobstore.map mentioned in the above command. This allows the user to upload a compressed file like a tarball/zip. <br/>\n    +In local file system blob store, the compressed blobs are stored on the nimbus node. The localizer code takes the responsibility to <br/>\n    +uncompress the blob and store it on the supervisor node. Symbolic links to the blobs on the supervisor node are created within the worker <br/>\n    +before the execution starts.<br/>\n    +<br/>\n    +### Local File Name Mapping<br/>\n    +Apart from compression the blobstore helps to give the blob a name that can be used by the workers. The localizer takes <br/>\n    +the responsibility of mapping the blob to a local name on the supervisor node.<br/>\n    +<br/>\n    +## Additional Blob Store Implementation Details<br/>\n    +Blob store uses a hashing function to create the blobs based on the key. The blobs are generally stored inside the directory specified by <br/>\n    +the blobstore.dir configuration. By default, it is stored under storm.local.dir/nimbus/blobs for local file system and a similar path on <br/>\n    +hdfs file system.<br/>\n    +<br/>\n    +Once a file is submitted, the blob store reads the configs and creates a metadata for the blob with all the access control details. The metadata <br/>\n    +is generally used for authorization while accessing the blobs. The blob key and version contribute to the hash code and there by the directory <br/>\n    +under storm.local.dir/nimbus/blobs/data where the data is placed. The blobs are generally placed in a positive number directory like 193,822 etc.<br/>\n    +<br/>\n    +Once the topology is launched and the relevant blobs have been created the supervisor downloads blobs related to the storm.conf, storm.ser <br/>\n    +and storm.code first and all the blobs uploaded by the command line separately using the localizer to uncompress and map them to a local name <br/>\n    +specified in the topology.blobstore.map configuration. The supervisor periodically updates blobs by checking for the change of version. <br/>\n    +This allows updating the blobs on the fly and thereby making it a very useful feature.<br/>\n    +<br/>\n    +For a local file system, the distributed cache on the supervisor node is set to 10240 MB as a soft limit and the clean up code attempts <br/>\n    +to clean anything over the soft limit every 600 seconds based on LRU policy.<br/>\n    +<br/>\n    +The HDFS blob store implementation handles load better by removing the burden on the nimbus to store the blobs, which avoids it becoming a bottleneck. Moreover, it provides seamless replication of blobs. On the other hand, the local file system blob store is not very efficient in <br/>\n    +replicating the blobs and is limited by the number of nimbuses. Moreover, the supervisor talks to the HDFS blob store directly without the <br/>\n    +involvement of the nimbus and thereby reduces the load and dependency on nimbus.<br/>\n    +<br/>\n    +## Highly Available Nimbus<br/>\n    +### Problem Statement:<br/>\n    +Currently the storm master aka nimbus, is a process that runs on a single machine under supervision. In most cases the <br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    @revans2 removed nimbus-ha-design doc and consolidated into blobstore itself</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685400/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164968324","html_url":"https://github.com/apache/storm/pull/945#issuecomment-164968324","issue_url":"https://api.github.com/repos/apache/storm/issues/945","id":164968324,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDk2ODMyNA==","user":{"login":"redsanket","id":8295799,"node_id":"MDQ6VXNlcjgyOTU3OTk=","avatar_url":"https://avatars.githubusercontent.com/u/8295799?v=4","gravatar_id":"","url":"https://api.github.com/users/redsanket","html_url":"https://github.com/redsanket","followers_url":"https://api.github.com/users/redsanket/followers","following_url":"https://api.github.com/users/redsanket/following{/other_user}","gists_url":"https://api.github.com/users/redsanket/gists{/gist_id}","starred_url":"https://api.github.com/users/redsanket/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/redsanket/subscriptions","organizations_url":"https://api.github.com/users/redsanket/orgs","repos_url":"https://api.github.com/users/redsanket/repos","events_url":"https://api.github.com/users/redsanket/events{/privacy}","received_events_url":"https://api.github.com/users/redsanket/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T02:41:27Z","updated_at":"2015-12-16T02:41:27Z","author_association":"NONE","body":"@revan2 @rfarivar @jerrypeng @unsleepy22  Addressed your concerns\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164968324/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685407","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685407","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685407,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU0MDc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T02:41:28Z","updated_at":"2025-01-24T14:37:58Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#issuecomment-164968324\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#issuecomment-164968324</a></p>\n\n<p>    @revan2 @rfarivar @jerrypeng @unsleepy22  Addressed your concerns</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685407/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164977788","html_url":"https://github.com/apache/storm/pull/934#issuecomment-164977788","issue_url":"https://api.github.com/repos/apache/storm/issues/934","id":164977788,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDk3Nzc4OA==","user":{"login":"redsanket","id":8295799,"node_id":"MDQ6VXNlcjgyOTU3OTk=","avatar_url":"https://avatars.githubusercontent.com/u/8295799?v=4","gravatar_id":"","url":"https://api.github.com/users/redsanket","html_url":"https://github.com/redsanket","followers_url":"https://api.github.com/users/redsanket/followers","following_url":"https://api.github.com/users/redsanket/following{/other_user}","gists_url":"https://api.github.com/users/redsanket/gists{/gist_id}","starred_url":"https://api.github.com/users/redsanket/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/redsanket/subscriptions","organizations_url":"https://api.github.com/users/redsanket/orgs","repos_url":"https://api.github.com/users/redsanket/repos","events_url":"https://api.github.com/users/redsanket/events{/privacy}","received_events_url":"https://api.github.com/users/redsanket/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T03:25:02Z","updated_at":"2015-12-16T03:25:02Z","author_association":"NONE","body":"@revans2 concerns addressed\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164977788/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685722","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685722","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685722,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3MjI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T03:25:04Z","updated_at":"2025-01-24T14:38:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user redsanket commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#issuecomment-164977788\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#issuecomment-164977788</a></p>\n\n<p>    @revans2 concerns addressed</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685722/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/164978151","html_url":"https://github.com/apache/storm/pull/941#issuecomment-164978151","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":164978151,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NDk3ODE1MQ==","user":{"login":"d2r","id":905298,"node_id":"MDQ6VXNlcjkwNTI5OA==","avatar_url":"https://avatars.githubusercontent.com/u/905298?v=4","gravatar_id":"","url":"https://api.github.com/users/d2r","html_url":"https://github.com/d2r","followers_url":"https://api.github.com/users/d2r/followers","following_url":"https://api.github.com/users/d2r/following{/other_user}","gists_url":"https://api.github.com/users/d2r/gists{/gist_id}","starred_url":"https://api.github.com/users/d2r/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/d2r/subscriptions","organizations_url":"https://api.github.com/users/d2r/orgs","repos_url":"https://api.github.com/users/d2r/repos","events_url":"https://api.github.com/users/d2r/events{/privacy}","received_events_url":"https://api.github.com/users/d2r/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T03:27:19Z","updated_at":"2015-12-16T03:27:19Z","author_association":"NONE","body":"Both test failures look unrelated, both are !storm-core:\n\n```\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-redis: groups/excludedGroups require TestNG or JUnit48+ on project test classpath -> [Help 1]\n\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-metrics: groups/excludedGroups require TestNG or JUnit48+ on project test classpath -> [Help 1]\n```\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/164978151/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686639","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686639","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686639,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2Mzk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T03:27:21Z","updated_at":"2025-01-24T14:38:27Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user d2r commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-164978151\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-164978151</a></p>\n\n<p>    Both test failures look unrelated, both are !storm-core:<br/>\n    ```<br/>\n    <span class=\"error\">&#91;ERROR&#93;</span> Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-redis: groups/excludedGroups require TestNG or JUnit48+ on project test classpath -> <span class=\"error\">&#91;Help 1&#93;</span></p>\n\n<p>    <span class=\"error\">&#91;ERROR&#93;</span> Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project storm-metrics: groups/excludedGroups require TestNG or JUnit48+ on project test classpath -> <span class=\"error\">&#91;Help 1&#93;</span><br/>\n    ```</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686639/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612439278","html_url":"https://github.com/apache/storm/issues/3972#issuecomment-2612439278","issue_url":"https://api.github.com/repos/apache/storm/issues/3972","id":2612439278,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI0MzkyNzg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T05:13:49Z","updated_at":"2025-01-24T12:40:14Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fogetti\">fogetti</a>:</i>\n<p>Hi folks. I experience the same bug in 0.10.0. In my case the supervisor survives but the topology dies and it's getting rebalanced to another supervisor.</p>\n\n<p>This is what I see in the supervisor log:</p>\n<div class=\"code panel\" style=\"border-width: 1px;\"><div class=\"codeContent panelContent\">\n<pre class=\"code-java\">2015-12-16 12:56:27.746 b.s.u.Utils [INFO] Using defaults.yaml from resources\n2015-12-16 12:56:27.784 b.s.u.Utils [INFO] Using storm.yaml from resources\n2015-12-16 12:56:27.841 b.s.d.supervisor [INFO] Launching worker with command: <span class=\"code-quote\">'java'</span> <span class=\"code-quote\">'-cp'</span> <span class=\"code-quote\">'/home/fogetti/downloads/apache-storm-0.10.0/lib/kryo-2.21.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-over-slf4j-1.6.6.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-slf4j-impl-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/minlog-1.2.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/asm-4.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-core-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/reflectasm-1.07-shaded.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-api-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/hadoop-auth-2.4.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/slf4j-api-1.7.7.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/clojure-1.6.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/servlet-api-2.5.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/disruptor-2.10.4.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/storm-core-0.10.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/conf:/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/stormjar.jar'</span> <span class=\"code-quote\">'-Dlogfile.name=phish-storm-topology-2-1450237054-worker-6700.log'</span> <span class=\"code-quote\">'-Dstorm.home=/home/fogetti/downloads/apache-storm-0.10.0'</span> <span class=\"code-quote\">'-Dstorm.id=phish-storm-topology-2-1450237054'</span> <span class=\"code-quote\">'-Dworker.id=d3819964-7671-447d-8763-827ab5cd6140'</span> <span class=\"code-quote\">'-Dworker.port=6700'</span> <span class=\"code-quote\">'-Dstorm.log.dir=/home/fogetti/downloads/apache-storm-0.10.0/logs'</span> <span class=\"code-quote\">'-Dlog4j.configurationFile=/home/fogetti/downloads/apache-storm-0.10.0/log4j2/worker.xml'</span> <span class=\"code-quote\">'backtype.storm.LogWriter'</span> <span class=\"code-quote\">'java'</span> <span class=\"code-quote\">'-server'</span> <span class=\"code-quote\">'-Xmx768m'</span> <span class=\"code-quote\">'-Djava.library.path=/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/resources/Linux-amd64:/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/resources:/usr'</span> <span class=\"code-quote\">'-Dlogfile.name=phish-storm-topology-2-1450237054-worker-6700.log'</span> <span class=\"code-quote\">'-Dstorm.home=/home/fogetti/downloads/apache-storm-0.10.0'</span> <span class=\"code-quote\">'-Dstorm.conf.file='</span> <span class=\"code-quote\">'-Dstorm.options='</span> <span class=\"code-quote\">'-Dstorm.log.dir=/home/fogetti/downloads/apache-storm-0.10.0/logs'</span> <span class=\"code-quote\">'-Dlogging.sensitivity=S3'</span> <span class=\"code-quote\">'-Dlog4j.configurationFile=/home/fogetti/downloads/apache-storm-0.10.0/log4j2/worker.xml'</span> <span class=\"code-quote\">'-Dstorm.id=phish-storm-topology-2-1450237054'</span> <span class=\"code-quote\">'-Dworker.id=d3819964-7671-447d-8763-827ab5cd6140'</span> <span class=\"code-quote\">'-Dworker.port=6700'</span> <span class=\"code-quote\">'-cp'</span> <span class=\"code-quote\">'/home/fogetti/downloads/apache-storm-0.10.0/lib/kryo-2.21.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-over-slf4j-1.6.6.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-slf4j-impl-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/minlog-1.2.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/asm-4.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-core-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/reflectasm-1.07-shaded.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/log4j-api-2.1.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/hadoop-auth-2.4.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/slf4j-api-1.7.7.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/clojure-1.6.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/servlet-api-2.5.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/disruptor-2.10.4.jar:/home/fogetti/downloads/apache-storm-0.10.0/lib/storm-core-0.10.0.jar:/home/fogetti/downloads/apache-storm-0.10.0/conf:/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/stormjar.jar'</span> <span class=\"code-quote\">'backtype.storm.daemon.worker'</span> <span class=\"code-quote\">'phish-storm-topology-2-1450237054'</span> <span class=\"code-quote\">'6c528751-1a10-4c33-bd54-a1ec9cb26d86'</span> <span class=\"code-quote\">'6700'</span> <span class=\"code-quote\">'d3819964-7671-447d-8763-827ab5cd6140'</span>\n2015-12-16 12:56:27.888 b.s.config [INFO] SET worker-user d3819964-7671-447d-8763-827ab5cd6140 \n2015-12-16 12:56:27.948 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:28.169 b.s.d.supervisor [INFO] Removing code <span class=\"code-keyword\">for</span> storm id phish-storm-topology-2-1450237054\n2015-12-16 12:56:28.454 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:28.956 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:29.457 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:29.957 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:30.458 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:30.960 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:31.463 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:31.964 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:32.465 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:32.966 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:33.467 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:33.968 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:34.469 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:34.970 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:35.471 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:35.972 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:36.473 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:36.974 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:37.076 b.s.d.supervisor [INFO] Worker <span class=\"code-object\">Process</span> d3819964-7671-447d-8763-827ab5cd6140 exited with code: 13\n2015-12-16 12:56:37.475 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:56:37.976 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n...\n2015-12-16 12:58:23.196 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:58:23.697 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:58:24.198 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:58:24.699 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:58:25.200 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:58:25.701 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:58:26.202 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:58:26.702 b.s.d.supervisor [INFO] d3819964-7671-447d-8763-827ab5cd6140 still hasn't started\n2015-12-16 12:58:27.203 b.s.d.supervisor [INFO] Worker d3819964-7671-447d-8763-827ab5cd6140 failed to start\n2015-12-16 12:58:27.206 b.s.d.supervisor [INFO] Shutting down and clearing state <span class=\"code-keyword\">for</span> id d3819964-7671-447d-8763-827ab5cd6140. Current supervisor time: 1450238307. State: :not-started, Heartbeat: nil\n2015-12-16 12:58:27.207 b.s.d.supervisor [INFO] Shutting down 6c528751-1a10-4c33-bd54-a1ec9cb26d86:d3819964-7671-447d-8763-827ab5cd6140\n2015-12-16 12:58:27.208 b.s.config [INFO] GET worker-user d3819964-7671-447d-8763-827ab5cd6140\n2015-12-16 12:58:27.259 b.s.util [INFO] Error when trying to kill 5076. <span class=\"code-object\">Process</span> is probably already dead.\n2015-12-16 12:58:27.260 b.s.d.supervisor [INFO] Sleep 1 seconds <span class=\"code-keyword\">for</span> execution of cleanup threads on worker.\n2015-12-16 12:58:28.266 b.s.util [INFO] Error when trying to kill 5076. <span class=\"code-object\">Process</span> is probably already dead.\n2015-12-16 12:58:28.275 b.s.config [INFO] REMOVE worker-user d3819964-7671-447d-8763-827ab5cd6140\n2015-12-16 12:58:28.276 b.s.d.supervisor [INFO] Shut down 6c528751-1a10-4c33-bd54-a1ec9cb26d86:d3819964-7671-447d-8763-827ab5cd6140\n</pre>\n</div></div>\n\n<p>And this is what I see in the worker log:</p>\n<div class=\"code panel\" style=\"border-width: 1px;\"><div class=\"codeContent panelContent\">\n<pre class=\"code-java\">2015-12-16 12:56:36.702 b.s.u.Utils [INFO] Using defaults.yaml from resources\n2015-12-16 12:56:36.769 b.s.u.Utils [INFO] Using storm.yaml from resources\n2015-12-16 12:56:36.921 b.s.d.worker [INFO] Launching worker <span class=\"code-keyword\">for</span> phish-storm-topology-2-1450237054 on 6c528751-1a10-4c33-bd54-a1ec9cb26d86:6700 with id d3819964-7671-447d-8763-827ab5cd6140 and conf {<span class=\"code-quote\">\"topology.builtin.metrics.bucket.size.secs\"</span> 60, <span class=\"code-quote\">\"nimbus.childopts\"</span> <span class=\"code-quote\">\"-Xmx1024m\"</span>, <span class=\"code-quote\">\"ui.filter.params\"</span> nil, <span class=\"code-quote\">\"storm.cluster.mode\"</span> <span class=\"code-quote\">\"distributed\"</span>, <span class=\"code-quote\">\"storm.messaging.netty.client_worker_threads\"</span> 1, <span class=\"code-quote\">\"supervisor.run.worker.as.user\"</span> <span class=\"code-keyword\">false</span>, <span class=\"code-quote\">\"topology.max.task.parallelism\"</span> nil, <span class=\"code-quote\">\"zmq.threads\"</span> 1, <span class=\"code-quote\">\"storm.group.mapping.service\"</span> <span class=\"code-quote\">\"backtype.storm.security.auth.ShellBasedGroupsMapping\"</span>, <span class=\"code-quote\">\"transactional.zookeeper.root\"</span> <span class=\"code-quote\">\"/transactional\"</span>, <span class=\"code-quote\">\"topology.sleep.spout.wait.strategy.time.ms\"</span> 1, <span class=\"code-quote\">\"drpc.invocations.port\"</span> 3773, <span class=\"code-quote\">\"topology.multilang.serializer\"</span> <span class=\"code-quote\">\"backtype.storm.multilang.JsonSerializer\"</span>, <span class=\"code-quote\">\"storm.messaging.netty.server_worker_threads\"</span> 1, <span class=\"code-quote\">\"topology.max.error.report.per.interval\"</span> 5, <span class=\"code-quote\">\"storm.thrift.transport\"</span> <span class=\"code-quote\">\"backtype.storm.security.auth.SimpleTransportPlugin\"</span>, <span class=\"code-quote\">\"zmq.hwm\"</span> 0, <span class=\"code-quote\">\"storm.principal.tolocal\"</span> <span class=\"code-quote\">\"backtype.storm.security.auth.DefaultPrincipalToLocal\"</span>, <span class=\"code-quote\">\"supervisor.worker.shutdown.sleep.secs\"</span> 1, <span class=\"code-quote\">\"storm.zookeeper.retry.times\"</span> 5, <span class=\"code-quote\">\"ui.actions.enabled\"</span> <span class=\"code-keyword\">true</span>, <span class=\"code-quote\">\"zmq.linger.millis\"</span> 5000, <span class=\"code-quote\">\"supervisor.enable\"</span> <span class=\"code-keyword\">true</span>, <span class=\"code-quote\">\"topology.stats.sample.rate\"</span> 0.05, <span class=\"code-quote\">\"storm.messaging.netty.min_wait_ms\"</span> 100, <span class=\"code-quote\">\"storm.zookeeper.port\"</span> 2181, <span class=\"code-quote\">\"supervisor.heartbeat.frequency.secs\"</span> 5, <span class=\"code-quote\">\"topology.enable.message.timeouts\"</span> <span class=\"code-keyword\">true</span>, <span class=\"code-quote\">\"drpc.worker.threads\"</span> 64, <span class=\"code-quote\">\"drpc.queue.size\"</span> 128, <span class=\"code-quote\">\"drpc.https.keystore.password\"</span> <span class=\"code-quote\">\"\", \"</span>logviewer.port<span class=\"code-quote\">\" 8000, \"</span>nimbus.reassign<span class=\"code-quote\">\" <span class=\"code-keyword\">true</span>, \"</span>topology.executor.send.buffer.size<span class=\"code-quote\">\" 1024, \"</span>topology.spout.wait.strategy<span class=\"code-quote\">\" \"</span>backtype.storm.spout.SleepSpoutWaitStrategy<span class=\"code-quote\">\", \"</span>ui.host<span class=\"code-quote\">\" \"</span>0.0.0.0<span class=\"code-quote\">\", \"</span>storm.nimbus.retry.interval.millis<span class=\"code-quote\">\" 2000, \"</span>nimbus.inbox.jar.expiration.secs<span class=\"code-quote\">\" 3600, \"</span>dev.zookeeper.path<span class=\"code-quote\">\" \"</span>/tmp/dev-storm-zookeeper<span class=\"code-quote\">\", \"</span>topology.acker.executors<span class=\"code-quote\">\" nil, \"</span>topology.fall.back.on.java.serialization<span class=\"code-quote\">\" <span class=\"code-keyword\">true</span>, \"</span>storm.zookeeper.servers<span class=\"code-quote\">\" [\"</span>dimebag<span class=\"code-quote\">\" \"</span>petrucci<span class=\"code-quote\">\" \"</span>hetfield<span class=\"code-quote\">\"], \"</span>nimbus.thrift.threads<span class=\"code-quote\">\" 64, \"</span>logviewer.cleanup.age.mins<span class=\"code-quote\">\" 10080, \"</span>topology.worker.childopts<span class=\"code-quote\">\" nil, \"</span>topology.classpath<span class=\"code-quote\">\" nil, \"</span>supervisor.monitor.frequency.secs<span class=\"code-quote\">\" 3, \"</span>nimbus.credential.renewers.freq.secs<span class=\"code-quote\">\" 600, \"</span>topology.skip.missing.kryo.registrations<span class=\"code-quote\">\" <span class=\"code-keyword\">false</span>, \"</span>drpc.authorizer.acl.filename<span class=\"code-quote\">\" \"</span>drpc-auth-acl.yaml<span class=\"code-quote\">\", \"</span>storm.group.mapping.service.cache.duration.secs<span class=\"code-quote\">\" 120, \"</span>topology.testing.always.<span class=\"code-keyword\">try</span>.serialize<span class=\"code-quote\">\" <span class=\"code-keyword\">false</span>, \"</span>nimbus.monitor.freq.secs<span class=\"code-quote\">\" 10, \"</span>supervisor.supervisors<span class=\"code-quote\">\" [], \"</span>topology.tasks<span class=\"code-quote\">\" nil, \"</span>topology.bolts.outgoing.overflow.buffer.enable<span class=\"code-quote\">\" <span class=\"code-keyword\">false</span>, \"</span>storm.messaging.netty.socket.backlog<span class=\"code-quote\">\" 500, \"</span>topology.workers<span class=\"code-quote\">\" 1, \"</span>storm.local.dir<span class=\"code-quote\">\" \"</span>/home/fogetti/downloads/apache-storm-0.10.0/storm-local<span class=\"code-quote\">\", \"</span>worker.childopts<span class=\"code-quote\">\" \"</span>-Xmx768m<span class=\"code-quote\">\", \"</span>storm.auth.simple-white-list.users<span class=\"code-quote\">\" [], \"</span>topology.message.timeout.secs<span class=\"code-quote\">\" 30, \"</span>topology.state.synchronization.timeout.secs<span class=\"code-quote\">\" 60, \"</span>topology.tuple.serializer<span class=\"code-quote\">\" \"</span>backtype.storm.serialization.types.ListDelegateSerializer<span class=\"code-quote\">\", \"</span>supervisor.supervisors.commands<span class=\"code-quote\">\" [], \"</span>logviewer.childopts<span class=\"code-quote\">\" \"</span>-Xmx128m<span class=\"code-quote\">\", \"</span>topology.environment<span class=\"code-quote\">\" nil, \"</span>topology.debug<span class=\"code-quote\">\" <span class=\"code-keyword\">false</span>, \"</span>storm.messaging.netty.max_retries<span class=\"code-quote\">\" 300, \"</span>ui.childopts<span class=\"code-quote\">\" \"</span>-Xmx768m<span class=\"code-quote\">\", \"</span>storm.zookeeper.session.timeout<span class=\"code-quote\">\" 20000, \"</span>drpc.childopts<span class=\"code-quote\">\" \"</span>-Xmx768m<span class=\"code-quote\">\", \"</span>drpc.http.creds.plugin<span class=\"code-quote\">\" \"</span>backtype.storm.security.auth.DefaultHttpCredentialsPlugin<span class=\"code-quote\">\", \"</span>storm.zookeeper.connection.timeout<span class=\"code-quote\">\" 15000, \"</span>storm.zookeeper.auth.user<span class=\"code-quote\">\" nil, \"</span>storm.meta.serialization.delegate<span class=\"code-quote\">\" \"</span>backtype.storm.serialization.GzipThriftSerializationDelegate<span class=\"code-quote\">\", \"</span>topology.max.spout.pending<span class=\"code-quote\">\" nil, \"</span>nimbus.supervisor.timeout.secs<span class=\"code-quote\">\" 60, \"</span>nimbus.task.timeout.secs<span class=\"code-quote\">\" 30, \"</span>drpc.port<span class=\"code-quote\">\" 3772, \"</span>storm.zookeeper.retry.intervalceiling.millis<span class=\"code-quote\">\" 30000, \"</span>nimbus.thrift.port<span class=\"code-quote\">\" 6627, \"</span>storm.auth.simple-acl.admins<span class=\"code-quote\">\" [], \"</span>storm.nimbus.retry.times<span class=\"code-quote\">\" 5, \"</span>supervisor.worker.start.timeout.secs<span class=\"code-quote\">\" 120, \"</span>storm.zookeeper.retry.interval<span class=\"code-quote\">\" 1000, \"</span>logs.users<span class=\"code-quote\">\" nil, \"</span>transactional.zookeeper.port<span class=\"code-quote\">\" nil, \"</span>drpc.max_buffer_size<span class=\"code-quote\">\" 1048576, \"</span>task.credentials.poll.secs<span class=\"code-quote\">\" 30, \"</span>drpc.https.keystore.type<span class=\"code-quote\">\" \"</span>JKS<span class=\"code-quote\">\", \"</span>topology.worker.receiver.thread.count<span class=\"code-quote\">\" 1, \"</span>supervisor.slots.ports<span class=\"code-quote\">\" [6700 6701], \"</span>topology.transfer.buffer.size<span class=\"code-quote\">\" 1024, \"</span>topology.worker.shared.thread.pool.size<span class=\"code-quote\">\" 4, \"</span>drpc.authorizer.acl.strict<span class=\"code-quote\">\" <span class=\"code-keyword\">false</span>, \"</span>nimbus.file.copy.expiration.secs<span class=\"code-quote\">\" 600, \"</span>topology.executor.receive.buffer.size<span class=\"code-quote\">\" 1024, \"</span>nimbus.task.launch.secs<span class=\"code-quote\">\" 120, \"</span>storm.local.mode.zmq<span class=\"code-quote\">\" <span class=\"code-keyword\">false</span>, \"</span>storm.messaging.netty.buffer_size<span class=\"code-quote\">\" 5242880, \"</span>worker.heartbeat.frequency.secs<span class=\"code-quote\">\" 1, \"</span>ui.http.creds.plugin<span class=\"code-quote\">\" \"</span>backtype.storm.security.auth.DefaultHttpCredentialsPlugin<span class=\"code-quote\">\", \"</span>storm.zookeeper.root<span class=\"code-quote\">\" \"</span>/storm<span class=\"code-quote\">\", \"</span>topology.tick.tuple.freq.secs<span class=\"code-quote\">\" nil, \"</span>drpc.https.port<span class=\"code-quote\">\" -1, \"</span>task.refresh.poll.secs<span class=\"code-quote\">\" 10, \"</span>task.heartbeat.frequency.secs<span class=\"code-quote\">\" 3, \"</span>storm.messaging.netty.max_wait_ms<span class=\"code-quote\">\" 1000, \"</span>nimbus.impersonation.authorizer<span class=\"code-quote\">\" \"</span>backtype.storm.security.auth.authorizer.ImpersonationAuthorizer<span class=\"code-quote\">\", \"</span>drpc.http.port<span class=\"code-quote\">\" 3774, \"</span>topology.error.throttle.interval.secs<span class=\"code-quote\">\" 10, \"</span>storm.messaging.transport<span class=\"code-quote\">\" \"</span>backtype.storm.messaging.netty.Context<span class=\"code-quote\">\", \"</span>storm.messaging.netty.authentication<span class=\"code-quote\">\" <span class=\"code-keyword\">false</span>, \"</span>topology.kryo.factory<span class=\"code-quote\">\" \"</span>backtype.storm.serialization.DefaultKryoFactory<span class=\"code-quote\">\", \"</span>worker.gc.childopts<span class=\"code-quote\">\" \"</span><span class=\"code-quote\">\", \"</span>nimbus.topology.validator<span class=\"code-quote\">\" \"</span>backtype.storm.nimbus.DefaultTopologyValidator<span class=\"code-quote\">\", \"</span>nimbus.cleanup.inbox.freq.secs<span class=\"code-quote\">\" 600, \"</span>ui.users<span class=\"code-quote\">\" nil, \"</span>transactional.zookeeper.servers<span class=\"code-quote\">\" nil, \"</span>supervisor.worker.timeout.secs<span class=\"code-quote\">\" 30, \"</span>storm.zookeeper.auth.password<span class=\"code-quote\">\" nil, \"</span>supervisor.childopts<span class=\"code-quote\">\" \"</span>-Xmx256m<span class=\"code-quote\">\", \"</span>ui.filter<span class=\"code-quote\">\" nil, \"</span>ui.header.buffer.bytes<span class=\"code-quote\">\" 4096, \"</span>topology.disruptor.wait.timeout.millis<span class=\"code-quote\">\" 1000, \"</span>storm.nimbus.retry.intervalceiling.millis<span class=\"code-quote\">\" 60000, \"</span>topology.trident.batch.emit.interval.millis<span class=\"code-quote\">\" 500, \"</span>topology.disruptor.wait.strategy<span class=\"code-quote\">\" \"</span>com.lmax.disruptor.BlockingWaitStrategy<span class=\"code-quote\">\", \"</span>storm.auth.simple-acl.users<span class=\"code-quote\">\" [], \"</span>drpc.invocations.threads<span class=\"code-quote\">\" 64, \"</span>java.library.path<span class=\"code-quote\">\" \"</span>/usr<span class=\"code-quote\">\", \"</span>ui.port<span class=\"code-quote\">\" 8080, \"</span>storm.messaging.netty.transfer.batch.size<span class=\"code-quote\">\" 262144, \"</span>logviewer.appender.name<span class=\"code-quote\">\" \"</span>A1<span class=\"code-quote\">\", \"</span>nimbus.thrift.max_buffer_size<span class=\"code-quote\">\" 1048576, \"</span>nimbus.host<span class=\"code-quote\">\" \"</span>dimebag<span class=\"code-quote\">\", \"</span>storm.auth.simple-acl.users.commands<span class=\"code-quote\">\" [], \"</span>drpc.request.timeout.secs\" 600}\n2015-12-16 12:56:36.947 b.s.util [DEBUG] Touching file at /home/fogetti/downloads/apache-storm-0.10.0/storm-local/workers/d3819964-7671-447d-8763-827ab5cd6140/pids/5076\n2015-12-16 12:56:36.962 b.s.d.worker [ERROR] Error on initialization of server mk-worker\njava.io.FileNotFoundException: File <span class=\"code-quote\">'/home/fogetti/downloads/apache-storm-0.10.0/storm-local/supervisor/stormdist/phish-storm-topology-2-1450237054/stormconf.ser'</span> does not exist\nat org.apache.storm.shade.org.apache.commons.io.FileUtils.openInputStream(FileUtils.java:299) ~[storm-core-0.10.0.jar:0.10.0]\nat org.apache.storm.shade.org.apache.commons.io.FileUtils.readFileToByteArray(FileUtils.java:1763) ~[storm-core-0.10.0.jar:0.10.0]\nat backtype.storm.config$read_supervisor_storm_conf.invoke(config.clj:222) ~[storm-core-0.10.0.jar:0.10.0]\nat backtype.storm.daemon.worker$fn__7098$exec_fn__1236__auto____7099.invoke(worker.clj:418) ~[storm-core-0.10.0.jar:0.10.0]\nat clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.6.0.jar:?]\nat clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.6.0.jar:?]\nat clojure.core$apply.invoke(core.clj:624) ~[clojure-1.6.0.jar:?]\nat backtype.storm.daemon.worker$fn__7098$mk_worker__7175.doInvoke(worker.clj:409) [storm-core-0.10.0.jar:0.10.0]\nat clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:?]\nat backtype.storm.daemon.worker$_main.invoke(worker.clj:542) [storm-core-0.10.0.jar:0.10.0]\nat clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:?]\nat clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?]\nat backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.0.jar:0.10.0]\n2015-12-16 12:56:36.984 b.s.util [ERROR] Halting process: (<span class=\"code-quote\">\"Error on initialization\"</span>)\njava.lang.RuntimeException: (<span class=\"code-quote\">\"Error on initialization\"</span>)\nat backtype.storm.util$exit_process_BANG_.doInvoke(util.clj:336) [storm-core-0.10.0.jar:0.10.0]\nat clojure.lang.RestFn.invoke(RestFn.java:423) [clojure-1.6.0.jar:?]\nat backtype.storm.daemon.worker$fn__7098$mk_worker__7175.doInvoke(worker.clj:409) [storm-core-0.10.0.jar:0.10.0]\nat clojure.lang.RestFn.invoke(RestFn.java:512) [clojure-1.6.0.jar:?]\nat backtype.storm.daemon.worker$_main.invoke(worker.clj:542) [storm-core-0.10.0.jar:0.10.0]\nat clojure.lang.AFn.applyToHelper(AFn.java:165) [clojure-1.6.0.jar:?]\nat clojure.lang.AFn.applyTo(AFn.java:144) [clojure-1.6.0.jar:?]\nat backtype.storm.daemon.worker.main(Unknown Source) [storm-core-0.10.0.jar:0.10.0]\n</pre>\n</div></div>\n\n<p>And this is what I see in the nimbus log:</p>\n<div class=\"code panel\" style=\"border-width: 1px;\"><div class=\"codeContent panelContent\">\n<pre class=\"code-java\">2015-12-16 12:52:26.929 b.s.d.nimbus [INFO] Delaying event :<span class=\"code-keyword\">do</span>-rebalance <span class=\"code-keyword\">for</span> 10 secs <span class=\"code-keyword\">for</span> phish-storm-topology-2-1450237054\n2015-12-16 12:52:27.257 b.s.d.nimbus [INFO] [req 29] Access from:  principal: op:getTopologyConf\n2015-12-16 12:52:27.290 b.s.d.nimbus [INFO] [req 28] Access from:  principal: op:getTopologyInfo\n2015-12-16 12:52:27.376 b.s.d.nimbus [INFO] [req 34] Access from:  principal: op:getTopology\n2015-12-16 12:52:27.378 b.s.d.nimbus [INFO] [req 30] Access from:  principal: op:getTopologyConf\n2015-12-16 12:52:28.505 b.s.d.nimbus [INFO] [req 32] Access from:  principal: op:getClusterInfo\n2015-12-16 12:52:28.510 b.s.d.nimbus [INFO] [req 31] Access from:  principal: op:getClusterInfo\n2015-12-16 12:52:28.524 b.s.d.nimbus [INFO] [req 38] Access from:  principal: op:getClusterInfo\n2015-12-16 12:52:28.529 b.s.d.nimbus [INFO] [req 33] Access from:  principal: op:getNimbusConf\n2015-12-16 12:52:37.055 b.s.s.EvenScheduler [INFO] Available slots: ([<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6701] [<span class=\"code-quote\">\"6c528751-1a10-4c33-bd54-a1ec9cb26d86\"</span> 6700] [\"6c528751-1a10-4c33-bd54-a1\nec9cb26d86\" 6701])\n2015-12-16 12:52:37.057 b.s.d.nimbus [INFO] Setting <span class=\"code-keyword\">new</span> assignment <span class=\"code-keyword\">for</span> topology id phish-storm-topology-2-1450237054: #backtype.storm.daemon.common.Assignment{:master-code-dir \"/home/fogetti/downloads/apache-storm-0.10.0/storm-local/ni\nmbus/stormdist/phish-storm-topology-2-1450237054<span class=\"code-quote\">\", :node->host {\"</span>6c528751-1a10-4c33-bd54-a1ec9cb26d86<span class=\"code-quote\">\" \"</span>petrucci<span class=\"code-quote\">\", \"</span>2c80eadd-a7bd-4470-9ba5-d17be81c94ae<span class=\"code-quote\">\" \"</span>hetfield<span class=\"code-quote\">\"}, :executor->node+port {[8 8] [\"</span>6c528751-1a10-4c33-bd54-a1ec9cb26d86\" \n6700], [7 7] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700], [5 6] [<span class=\"code-quote\">\"6c528751-1a10-4c33-bd54-a1ec9cb26d86\"</span> 6700], [3 4] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700], [9 10] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700], [2 2] [<span class=\"code-quote\">\"6c528751-1a10-4c33-bd54-a1ec9cb26d86\"</span> 6700], [1 1] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700]}, :executor->start-time-secs {[9 10] 1450237957, [7 7] 1450237957, [3 4] 1450237957, [1 1] 1450237957, [8 8] 1450237957, [5 6] 1450237957, [2 2] 1450237957}}\n...\n2015-12-16 12:54:41.453 b.s.d.nimbus [INFO] Executor phish-storm-topology-2-1450237054:[8 8] not alive\n2015-12-16 12:54:41.453 b.s.d.nimbus [INFO] Executor phish-storm-topology-2-1450237054:[2 2] not alive\n2015-12-16 12:54:41.453 b.s.d.nimbus [INFO] Executor phish-storm-topology-2-1450237054:[5 6] not alive\n2015-12-16 12:54:41.457 b.s.s.EvenScheduler [INFO] Available slots: ([<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6701] [<span class=\"code-quote\">\"6c528751-1a10-4c33-bd54-a1ec9cb26d86\"</span> 6701])\n2015-12-16 12:54:41.457 b.s.d.nimbus [INFO] Reassigning phish-storm-topology-2-1450237054 to 2 slots\n2015-12-16 12:54:41.457 b.s.d.nimbus [INFO] Reassign executors: [[8 8] [5 6] [2 2]]\n2015-12-16 12:54:41.462 b.s.d.nimbus [INFO] Setting <span class=\"code-keyword\">new</span> assignment <span class=\"code-keyword\">for</span> topology id phish-storm-topology-2-1450237054: #backtype.storm.daemon.common.Assignment{:master-code-dir <span class=\"code-quote\">\"/home/fogetti/downloads/apache-storm-0.10.0/storm-local/nimbus/stormdist/phish-storm-topology-2-1450237054\"</span>, :node->host {<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> <span class=\"code-quote\">\"hetfield\"</span>}, :executor->node+port {[8 8] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6701], [7 7] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700], [5 6] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6701], [3 4] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700], [9 10] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700], [2 2] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6701], [1 1] [<span class=\"code-quote\">\"2c80eadd-a7bd-4470-9ba5-d17be81c94ae\"</span> 6700]}, :executor->start-time-secs {[7 7] 1450237957, [1 1] 1450237957, [8 8] 1450238081, [2 2] 1450238081, [9 10] 1450237957, [3 4] 1450237957, [5 6] 1450238081}}\n</pre>\n</div></div>\n\n<p>Hope it helps.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612439278/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165011040","html_url":"https://github.com/apache/storm/pull/945#issuecomment-165011040","issue_url":"https://api.github.com/repos/apache/storm/issues/945","id":165011040,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTAxMTA0MA==","user":{"login":"jerrypeng","id":3613359,"node_id":"MDQ6VXNlcjM2MTMzNTk=","avatar_url":"https://avatars.githubusercontent.com/u/3613359?v=4","gravatar_id":"","url":"https://api.github.com/users/jerrypeng","html_url":"https://github.com/jerrypeng","followers_url":"https://api.github.com/users/jerrypeng/followers","following_url":"https://api.github.com/users/jerrypeng/following{/other_user}","gists_url":"https://api.github.com/users/jerrypeng/gists{/gist_id}","starred_url":"https://api.github.com/users/jerrypeng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jerrypeng/subscriptions","organizations_url":"https://api.github.com/users/jerrypeng/orgs","repos_url":"https://api.github.com/users/jerrypeng/repos","events_url":"https://api.github.com/users/jerrypeng/events{/privacy}","received_events_url":"https://api.github.com/users/jerrypeng/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T06:13:52Z","updated_at":"2015-12-16T06:13:52Z","author_association":"CONTRIBUTOR","body":"LGTM +1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165011040/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685416","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685416","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685416,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU0MTY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T06:13:54Z","updated_at":"2025-01-24T14:37:58Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#issuecomment-165011040\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#issuecomment-165011040</a></p>\n\n<p>    LGTM +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685416/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688455","html_url":"https://github.com/apache/storm/issues/5224#issuecomment-2612688455","issue_url":"https://api.github.com/repos/apache/storm/issues/5224","id":2612688455,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg0NTU=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T06:26:09Z","updated_at":"2025-01-24T14:39:15Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user asfgit closed the pull request at:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/946\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/946</a></p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688455/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165014091","html_url":"https://github.com/apache/storm/pull/946#issuecomment-165014091","issue_url":"https://api.github.com/repos/apache/storm/issues/946","id":165014091,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTAxNDA5MQ==","user":{"login":"arunmahadevan","id":6792890,"node_id":"MDQ6VXNlcjY3OTI4OTA=","avatar_url":"https://avatars.githubusercontent.com/u/6792890?v=4","gravatar_id":"","url":"https://api.github.com/users/arunmahadevan","html_url":"https://github.com/arunmahadevan","followers_url":"https://api.github.com/users/arunmahadevan/followers","following_url":"https://api.github.com/users/arunmahadevan/following{/other_user}","gists_url":"https://api.github.com/users/arunmahadevan/gists{/gist_id}","starred_url":"https://api.github.com/users/arunmahadevan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/arunmahadevan/subscriptions","organizations_url":"https://api.github.com/users/arunmahadevan/orgs","repos_url":"https://api.github.com/users/arunmahadevan/repos","events_url":"https://api.github.com/users/arunmahadevan/events{/privacy}","received_events_url":"https://api.github.com/users/arunmahadevan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T06:30:44Z","updated_at":"2015-12-16T06:30:44Z","author_association":"CONTRIBUTOR","body":"Thanks @satishd merged to master.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165014091/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688458","html_url":"https://github.com/apache/storm/issues/5224#issuecomment-2612688458","issue_url":"https://api.github.com/repos/apache/storm/issues/5224","id":2612688458,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg0NTg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T06:30:45Z","updated_at":"2025-01-24T14:39:15Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user arunmahadevan commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/946#issuecomment-165014091\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/946#issuecomment-165014091</a></p>\n\n<p>    Thanks @satishd merged to master.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688458/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685730","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685730","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685730,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3MzA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T06:38:17Z","updated_at":"2025-01-24T14:38:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47741284\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47741284</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,288 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.blobstore.BlobStoreAclHandler;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.BufferedWriter;<br/>\n    +import java.io.File;<br/>\n    +import java.io.FileReader;<br/>\n    +import java.io.FileWriter;<br/>\n    +import java.io.IOException;<br/>\n    +import java.util.HashSet;<br/>\n    +import java.util.Iterator;<br/>\n    +import java.util.LinkedList;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +import java.util.Set;<br/>\n    +import java.util.StringTokenizer;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static ClientBlobStore store; // Client API to invoke blob store API functionality<br/>\n    +    private static String key = \"key\";<br/>\n    +    private static String fileName = \"blacklist.txt\";<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreAPIWordCountTopology.class);<br/>\n    +<br/>\n    +    public static void prepare() </p>\n{\n    +Config conf = new Config();\n    +conf.putAll(Utils.readStormConfig());\n    +store = Utils.getClientBlobStore(conf);\n    +    }\n<p>    +<br/>\n    +    // Spout implementation<br/>\n    +    public static class RandomSentenceSpout extends BaseRichSpout {<br/>\n    +SpoutOutputCollector _collector;<br/>\n    +<br/>\n    +@Override<br/>\n    +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) </p>\n{\n    +    _collector = collector;\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void nextTuple() </p>\n{\n    +    Utils.sleep(100);\n    +    _collector.emit(new Values(getRandomSentence()));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void ack(Object id) </p>\n{\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void fail(Object id) {    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"sentence\"));\n    +}\n<p>    +<br/>\n    +    }<br/>\n    +<br/>\n    +    // Bolt implementation<br/>\n    +    public static class SplitSentence extends ShellBolt implements IRichBolt {<br/>\n    +<br/>\n    +public SplitSentence() </p>\n{\n    +    super(\"python\", \"splitsentence.py\");\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"word\"));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public Map<String, Object> getComponentConfiguration() </p>\n{\n    +    return null;\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    public static class FilterWords extends BaseBasicBolt {<br/>\n    +String fileName = \"blacklist.txt\";<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    can we just re use the fileName variable defined at the top?</p>\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685730/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685734","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685734","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685734,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3MzQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T06:44:02Z","updated_at":"2025-01-24T14:38:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47741484\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47741484</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,288 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.blobstore.BlobStoreAclHandler;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.BufferedWriter;<br/>\n    +import java.io.File;<br/>\n    +import java.io.FileReader;<br/>\n    +import java.io.FileWriter;<br/>\n    +import java.io.IOException;<br/>\n    +import java.util.HashSet;<br/>\n    +import java.util.Iterator;<br/>\n    +import java.util.LinkedList;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +import java.util.Set;<br/>\n    +import java.util.StringTokenizer;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static ClientBlobStore store; // Client API to invoke blob store API functionality<br/>\n    +    private static String key = \"key\";<br/>\n    +    private static String fileName = \"blacklist.txt\";<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreAPIWordCountTopology.class);<br/>\n    +<br/>\n    +    public static void prepare() </p>\n{\n    +Config conf = new Config();\n    +conf.putAll(Utils.readStormConfig());\n    +store = Utils.getClientBlobStore(conf);\n    +    }\n<p>    +<br/>\n    +    // Spout implementation<br/>\n    +    public static class RandomSentenceSpout extends BaseRichSpout {<br/>\n    +SpoutOutputCollector _collector;<br/>\n    +<br/>\n    +@Override<br/>\n    +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) </p>\n{\n    +    _collector = collector;\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void nextTuple() </p>\n{\n    +    Utils.sleep(100);\n    +    _collector.emit(new Values(getRandomSentence()));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void ack(Object id) </p>\n{\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void fail(Object id) {    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"sentence\"));\n    +}\n<p>    +<br/>\n    +    }<br/>\n    +<br/>\n    +    // Bolt implementation<br/>\n    +    public static class SplitSentence extends ShellBolt implements IRichBolt {<br/>\n    +<br/>\n    +public SplitSentence() </p>\n{\n    +    super(\"python\", \"splitsentence.py\");\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"word\"));\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public Map<String, Object> getComponentConfiguration() {\n    +    return null;\n    +}<br/>\n    +    }<br/>\n    +<br/>\n    +    public static class FilterWords extends BaseBasicBolt {<br/>\n    +String fileName = \"blacklist.txt\";<br/>\n    +@Override<br/>\n    +public void execute(Tuple tuple, BasicOutputCollector collector) {<br/>\n    +    try {<br/>\n    +String word = tuple.getString(0);<br/>\n    +Set<String> wordSet = parseFile(fileName);<br/>\n    +if (!wordSet.contains(word)) {\n    +    collector.emit(new Values(word));\n    +}<br/>\n    +    } catch (IOException exp) {\n    +throw new RuntimeException(exp);\n    +    }<br/>\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) {    +    declarer.declare(new Fields(\"word\"));    +}\n<p>    +    }<br/>\n    +<br/>\n    +    public void buildAndLaunchWordCountTopology(String[] args) {<br/>\n    +TopologyBuilder builder = new TopologyBuilder();<br/>\n    +builder.setSpout(\"spout\", new RandomSentenceSpout(), 5);<br/>\n    +builder.setBolt(\"split\", new SplitSentence(), 8).shuffleGrouping(\"spout\");<br/>\n    +builder.setBolt(\"filter\", new FilterWords(), 6).shuffleGrouping(\"split\");<br/>\n    +<br/>\n    +Config conf = new Config();<br/>\n    +conf.setDebug(true);<br/>\n    +try </p>\n{\n    +    conf.setNumWorkers(3);\n    +    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());\n    +}\n<p> catch (InvalidTopologyException | AuthorizationException | AlreadyAliveException exp) </p>\n{\n    +    throw new RuntimeException(exp);\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    // Equivalent create command on command line<br/>\n    +    // storm blobstore create --file blacklist.txt --acl o::rwa key<br/>\n    +    private static void createBlobWithContent(String blobKey, ClientBlobStore clientBlobStore, File file)<br/>\n    +    throws AuthorizationException, KeyAlreadyExistsException, IOException,KeyNotFoundException </p>\n{\n    +String stringBlobACL = \"o::rwa\";\n    +AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);\n    +List<AccessControl> acls = new LinkedList<AccessControl>();\n    +acls.add(blobACL); // more ACLs can be added here\n    +SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);\n    +AtomicOutputStream blobStream = clientBlobStore.createBlob(blobKey,settableBlobMeta);\n    +blobStream.write(readFile(file).toString().getBytes());\n    +blobStream.close();\n    +    }\n<p>    +<br/>\n    +    // Equivalent update command on command line<br/>\n    +    // storm blobstore update --file blacklist.txt key<br/>\n    +    private static void updateBlobWithContent(String blobKey, ClientBlobStore clientBlobStore, File file)<br/>\n    +    throws KeyNotFoundException, AuthorizationException, IOException </p>\n{\n    +AtomicOutputStream blobOutputStream = clientBlobStore.updateBlob(blobKey);\n    +blobOutputStream.write(readFile(file).toString().getBytes());\n    +blobOutputStream.close();\n    +    }\n<p>    +<br/>\n    +    private static String getRandomSentence() {<br/>\n    +String[] sentences = new String[]</p>\n{ \"the cow jumped over the moon\", \"an apple a day keeps the doctor away\",\n    +\"four score and seven years ago\", \"snow white and the seven dwarfs\", \"i am at two with nature\" }\n<p>;<br/>\n    +String sentence = sentences<span class=\"error\">&#91;new Random().nextInt(sentences.length)&#93;</span>;<br/>\n    +return sentence;<br/>\n    +    }<br/>\n    +<br/>\n    +    private static Set<String> getRandomWordSet() {<br/>\n    +Set<String> randomWordSet = new HashSet<>();<br/>\n    +Random random = new Random();<br/>\n    +String[] words = new String[]</p>\n{ \"cow\", \"jumped\", \"over\", \"the\", \"moon\", \"apple\", \"day\", \"doctor\", \"away\",\n    +\"four\", \"seven\", \"ago\", \"snow\", \"white\", \"seven\", \"dwarfs\", \"nature\", \"two\" }\n<p>;<br/>\n    +// Choosing atmost 5 words to update the blacklist file for filtering<br/>\n    +for (int i=0; i<5; i++) </p>\n{\n    +    randomWordSet.add(words[random.nextInt(words.length)]);\n    +}\n<p>    +return randomWordSet;<br/>\n    +    }<br/>\n    +<br/>\n    +    private static Set<String> parseFile(String fileName) throws IOException {<br/>\n    +File file = new File(fileName);<br/>\n    +Set<String> wordSet = new HashSet<>();<br/>\n    +if (!file.exists()) </p>\n{\n    +    return wordSet;\n    +}\n<p>    +StringTokenizer tokens = new StringTokenizer(readFile(file).toString(), \"\\r\\n\");<br/>\n    +while (tokens.hasMoreElements()) </p>\n{\n    +    wordSet.add(tokens.nextToken());\n    +}\n<p>    +LOG.info(\"parseFile {}\", wordSet);<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    perhaps debug?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685734/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685743","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685743","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685743,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3NDM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T06:45:19Z","updated_at":"2025-01-24T14:38:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47741557\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47741557</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,288 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.blobstore.BlobStoreAclHandler;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.BufferedWriter;<br/>\n    +import java.io.File;<br/>\n    +import java.io.FileReader;<br/>\n    +import java.io.FileWriter;<br/>\n    +import java.io.IOException;<br/>\n    +import java.util.HashSet;<br/>\n    +import java.util.Iterator;<br/>\n    +import java.util.LinkedList;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +import java.util.Set;<br/>\n    +import java.util.StringTokenizer;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static ClientBlobStore store; // Client API to invoke blob store API functionality<br/>\n    +    private static String key = \"key\";<br/>\n    +    private static String fileName = \"blacklist.txt\";<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreAPIWordCountTopology.class);<br/>\n    +<br/>\n    +    public static void prepare() </p>\n{\n    +Config conf = new Config();\n    +conf.putAll(Utils.readStormConfig());\n    +store = Utils.getClientBlobStore(conf);\n    +    }\n<p>    +<br/>\n    +    // Spout implementation<br/>\n    +    public static class RandomSentenceSpout extends BaseRichSpout {<br/>\n    +SpoutOutputCollector _collector;<br/>\n    +<br/>\n    +@Override<br/>\n    +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) </p>\n{\n    +    _collector = collector;\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void nextTuple() </p>\n{\n    +    Utils.sleep(100);\n    +    _collector.emit(new Values(getRandomSentence()));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void ack(Object id) </p>\n{\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void fail(Object id) {    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"sentence\"));\n    +}\n<p>    +<br/>\n    +    }<br/>\n    +<br/>\n    +    // Bolt implementation<br/>\n    +    public static class SplitSentence extends ShellBolt implements IRichBolt {<br/>\n    +<br/>\n    +public SplitSentence() </p>\n{\n    +    super(\"python\", \"splitsentence.py\");\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"word\"));\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public Map<String, Object> getComponentConfiguration() {\n    +    return null;\n    +}<br/>\n    +    }<br/>\n    +<br/>\n    +    public static class FilterWords extends BaseBasicBolt {<br/>\n    +String fileName = \"blacklist.txt\";<br/>\n    +@Override<br/>\n    +public void execute(Tuple tuple, BasicOutputCollector collector) {<br/>\n    +    try {<br/>\n    +String word = tuple.getString(0);<br/>\n    +Set<String> wordSet = parseFile(fileName);<br/>\n    +if (!wordSet.contains(word)) {\n    +    collector.emit(new Values(word));\n    +}<br/>\n    +    } catch (IOException exp) {\n    +throw new RuntimeException(exp);\n    +    }<br/>\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) {    +    declarer.declare(new Fields(\"word\"));    +}\n<p>    +    }<br/>\n    +<br/>\n    +    public void buildAndLaunchWordCountTopology(String[] args) {<br/>\n    +TopologyBuilder builder = new TopologyBuilder();<br/>\n    +builder.setSpout(\"spout\", new RandomSentenceSpout(), 5);<br/>\n    +builder.setBolt(\"split\", new SplitSentence(), 8).shuffleGrouping(\"spout\");<br/>\n    +builder.setBolt(\"filter\", new FilterWords(), 6).shuffleGrouping(\"split\");<br/>\n    +<br/>\n    +Config conf = new Config();<br/>\n    +conf.setDebug(true);<br/>\n    +try </p>\n{\n    +    conf.setNumWorkers(3);\n    +    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());\n    +}\n<p> catch (InvalidTopologyException | AuthorizationException | AlreadyAliveException exp) </p>\n{\n    +    throw new RuntimeException(exp);\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    // Equivalent create command on command line<br/>\n    +    // storm blobstore create --file blacklist.txt --acl o::rwa key<br/>\n    +    private static void createBlobWithContent(String blobKey, ClientBlobStore clientBlobStore, File file)<br/>\n    +    throws AuthorizationException, KeyAlreadyExistsException, IOException,KeyNotFoundException </p>\n{\n    +String stringBlobACL = \"o::rwa\";\n    +AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);\n    +List<AccessControl> acls = new LinkedList<AccessControl>();\n    +acls.add(blobACL); // more ACLs can be added here\n    +SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);\n    +AtomicOutputStream blobStream = clientBlobStore.createBlob(blobKey,settableBlobMeta);\n    +blobStream.write(readFile(file).toString().getBytes());\n    +blobStream.close();\n    +    }\n<p>    +<br/>\n    +    // Equivalent update command on command line<br/>\n    +    // storm blobstore update --file blacklist.txt key<br/>\n    +    private static void updateBlobWithContent(String blobKey, ClientBlobStore clientBlobStore, File file)<br/>\n    +    throws KeyNotFoundException, AuthorizationException, IOException </p>\n{\n    +AtomicOutputStream blobOutputStream = clientBlobStore.updateBlob(blobKey);\n    +blobOutputStream.write(readFile(file).toString().getBytes());\n    +blobOutputStream.close();\n    +    }\n<p>    +<br/>\n    +    private static String getRandomSentence() {<br/>\n    +String[] sentences = new String[]</p>\n{ \"the cow jumped over the moon\", \"an apple a day keeps the doctor away\",\n    +\"four score and seven years ago\", \"snow white and the seven dwarfs\", \"i am at two with nature\" }\n<p>;<br/>\n    +String sentence = sentences<span class=\"error\">&#91;new Random().nextInt(sentences.length)&#93;</span>;<br/>\n    +return sentence;<br/>\n    +    }<br/>\n    +<br/>\n    +    private static Set<String> getRandomWordSet() {<br/>\n    +Set<String> randomWordSet = new HashSet<>();<br/>\n    +Random random = new Random();<br/>\n    +String[] words = new String[]</p>\n{ \"cow\", \"jumped\", \"over\", \"the\", \"moon\", \"apple\", \"day\", \"doctor\", \"away\",\n    +\"four\", \"seven\", \"ago\", \"snow\", \"white\", \"seven\", \"dwarfs\", \"nature\", \"two\" }\n<p>;<br/>\n    +// Choosing atmost 5 words to update the blacklist file for filtering<br/>\n    +for (int i=0; i<5; i++) </p>\n{\n    +    randomWordSet.add(words[random.nextInt(words.length)]);\n    +}\n<p>    +return randomWordSet;<br/>\n    +    }<br/>\n    +<br/>\n    +    private static Set<String> parseFile(String fileName) throws IOException {<br/>\n    +File file = new File(fileName);<br/>\n    +Set<String> wordSet = new HashSet<>();<br/>\n    +if (!file.exists()) </p>\n{\n    +    return wordSet;\n    +}\n<p>    +StringTokenizer tokens = new StringTokenizer(readFile(file).toString(), \"\\r\\n\");<br/>\n    +while (tokens.hasMoreElements()) </p>\n{\n    +    wordSet.add(tokens.nextToken());\n    +}\n<p>    +LOG.info(\"parseFile {}\", wordSet);<br/>\n    +return wordSet;<br/>\n    +    }<br/>\n    +<br/>\n    +    private static StringBuilder readFile(File file) throws IOException {<br/>\n    +String line;<br/>\n    +StringBuilder fileContent = new StringBuilder();<br/>\n    +// Do not use canonical file name here<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    I don't quite understand how the file is being found when providing the filename and not the path.  Can you please provide me with some clarification.  Thanks!</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685743/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685752","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685752","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685752,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3NTI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T14:24:53Z","updated_at":"2025-01-24T14:38:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user revans2 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47780773\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47780773</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,288 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.blobstore.BlobStoreAclHandler;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.BufferedWriter;<br/>\n    +import java.io.File;<br/>\n    +import java.io.FileReader;<br/>\n    +import java.io.FileWriter;<br/>\n    +import java.io.IOException;<br/>\n    +import java.util.HashSet;<br/>\n    +import java.util.Iterator;<br/>\n    +import java.util.LinkedList;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +import java.util.Set;<br/>\n    +import java.util.StringTokenizer;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static ClientBlobStore store; // Client API to invoke blob store API functionality<br/>\n    +    private static String key = \"key\";<br/>\n    +    private static String fileName = \"blacklist.txt\";<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreAPIWordCountTopology.class);<br/>\n    +<br/>\n    +    public static void prepare() </p>\n{\n    +Config conf = new Config();\n    +conf.putAll(Utils.readStormConfig());\n    +store = Utils.getClientBlobStore(conf);\n    +    }\n<p>    +<br/>\n    +    // Spout implementation<br/>\n    +    public static class RandomSentenceSpout extends BaseRichSpout {<br/>\n    +SpoutOutputCollector _collector;<br/>\n    +<br/>\n    +@Override<br/>\n    +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) </p>\n{\n    +    _collector = collector;\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void nextTuple() </p>\n{\n    +    Utils.sleep(100);\n    +    _collector.emit(new Values(getRandomSentence()));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void ack(Object id) </p>\n{\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void fail(Object id) {    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"sentence\"));\n    +}\n<p>    +<br/>\n    +    }<br/>\n    +<br/>\n    +    // Bolt implementation<br/>\n    +    public static class SplitSentence extends ShellBolt implements IRichBolt {<br/>\n    +<br/>\n    +public SplitSentence() </p>\n{\n    +    super(\"python\", \"splitsentence.py\");\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"word\"));\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public Map<String, Object> getComponentConfiguration() {\n    +    return null;\n    +}<br/>\n    +    }<br/>\n    +<br/>\n    +    public static class FilterWords extends BaseBasicBolt {<br/>\n    +String fileName = \"blacklist.txt\";<br/>\n    +@Override<br/>\n    +public void execute(Tuple tuple, BasicOutputCollector collector) {<br/>\n    +    try {<br/>\n    +String word = tuple.getString(0);<br/>\n    +Set<String> wordSet = parseFile(fileName);<br/>\n    +if (!wordSet.contains(word)) {\n    +    collector.emit(new Values(word));\n    +}<br/>\n    +    } catch (IOException exp) {\n    +throw new RuntimeException(exp);\n    +    }<br/>\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) {    +    declarer.declare(new Fields(\"word\"));    +}\n<p>    +    }<br/>\n    +<br/>\n    +    public void buildAndLaunchWordCountTopology(String[] args) {<br/>\n    +TopologyBuilder builder = new TopologyBuilder();<br/>\n    +builder.setSpout(\"spout\", new RandomSentenceSpout(), 5);<br/>\n    +builder.setBolt(\"split\", new SplitSentence(), 8).shuffleGrouping(\"spout\");<br/>\n    +builder.setBolt(\"filter\", new FilterWords(), 6).shuffleGrouping(\"split\");<br/>\n    +<br/>\n    +Config conf = new Config();<br/>\n    +conf.setDebug(true);<br/>\n    +try {<br/>\n    +    conf.setNumWorkers(3);<br/>\n    +    StormSubmitter.submitTopologyWithProgressBar(args<span class=\"error\">&#91;0&#93;</span>, conf, builder.createTopology());<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Can we have a default topology name if args<span class=\"error\">&#91;0&#93;</span> is not set?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685752/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685760","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685760","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685760,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3NjA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T14:26:56Z","updated_at":"2025-01-24T14:38:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user revans2 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47781014\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47781014</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,288 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.blobstore.BlobStoreAclHandler;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.BufferedWriter;<br/>\n    +import java.io.File;<br/>\n    +import java.io.FileReader;<br/>\n    +import java.io.FileWriter;<br/>\n    +import java.io.IOException;<br/>\n    +import java.util.HashSet;<br/>\n    +import java.util.Iterator;<br/>\n    +import java.util.LinkedList;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +import java.util.Set;<br/>\n    +import java.util.StringTokenizer;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static ClientBlobStore store; // Client API to invoke blob store API functionality<br/>\n    +    private static String key = \"key\";<br/>\n    +    private static String fileName = \"blacklist.txt\";<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreAPIWordCountTopology.class);<br/>\n    +<br/>\n    +    public static void prepare() </p>\n{\n    +Config conf = new Config();\n    +conf.putAll(Utils.readStormConfig());\n    +store = Utils.getClientBlobStore(conf);\n    +    }\n<p>    +<br/>\n    +    // Spout implementation<br/>\n    +    public static class RandomSentenceSpout extends BaseRichSpout {<br/>\n    +SpoutOutputCollector _collector;<br/>\n    +<br/>\n    +@Override<br/>\n    +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) </p>\n{\n    +    _collector = collector;\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void nextTuple() </p>\n{\n    +    Utils.sleep(100);\n    +    _collector.emit(new Values(getRandomSentence()));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void ack(Object id) </p>\n{\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void fail(Object id) {    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"sentence\"));\n    +}\n<p>    +<br/>\n    +    }<br/>\n    +<br/>\n    +    // Bolt implementation<br/>\n    +    public static class SplitSentence extends ShellBolt implements IRichBolt {<br/>\n    +<br/>\n    +public SplitSentence() </p>\n{\n    +    super(\"python\", \"splitsentence.py\");\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"word\"));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public Map<String, Object> getComponentConfiguration() </p>\n{\n    +    return null;\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    public static class FilterWords extends BaseBasicBolt {<br/>\n    +String fileName = \"blacklist.txt\";<br/>\n    +@Override<br/>\n    +public void execute(Tuple tuple, BasicOutputCollector collector) {<br/>\n    +    try {<br/>\n    +String word = tuple.getString(0);<br/>\n    +Set<String> wordSet = parseFile(fileName);<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Can we please cache the wordSet and only update it once every 5 seconds?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685760/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685767","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685767","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685767,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3Njc=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T14:30:13Z","updated_at":"2025-01-24T14:38:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user revans2 commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#discussion_r47781387\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#discussion_r47781387</a></p>\n\n<p>    &#8212; Diff: examples/storm-starter/src/jvm/storm/starter/BlobStoreAPIWordCountTopology.java &#8212;<br/>\n    @@ -0,0 +1,288 @@<br/>\n    +/**<br/>\n    + * Licensed to the Apache Software Foundation (ASF) under one<br/>\n    + * or more contributor license agreements.  See the NOTICE file<br/>\n    + * distributed with this work for additional information<br/>\n    + * regarding copyright ownership.  The ASF licenses this file<br/>\n    + * to you under the Apache License, Version 2.0 (the<br/>\n    + * \"License\"); you may not use this file except in compliance<br/>\n    + * with the License.  You may obtain a copy of the License at<br/>\n    + *<br/>\n    + * <a href=\"http://www.apache.org/licenses/LICENSE-2.0\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">http://www.apache.org/licenses/LICENSE-2.0</a><br/>\n    + *<br/>\n    + * Unless required by applicable law or agreed to in writing, software<br/>\n    + * distributed under the License is distributed on an \"AS IS\" BASIS,<br/>\n    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.<br/>\n    + * See the License for the specific language governing permissions and<br/>\n    + * limitations under the License.<br/>\n    + */<br/>\n    +package storm.starter;<br/>\n    +<br/>\n    +import backtype.storm.Config;<br/>\n    +import backtype.storm.StormSubmitter;<br/>\n    +import backtype.storm.blobstore.AtomicOutputStream;<br/>\n    +import backtype.storm.blobstore.ClientBlobStore;<br/>\n    +import backtype.storm.blobstore.InputStreamWithMeta;<br/>\n    +import backtype.storm.blobstore.NimbusBlobStore;<br/>\n    +<br/>\n    +import backtype.storm.generated.AccessControl;<br/>\n    +import backtype.storm.generated.AccessControlType;<br/>\n    +import backtype.storm.generated.AlreadyAliveException;<br/>\n    +import backtype.storm.generated.AuthorizationException;<br/>\n    +import backtype.storm.generated.InvalidTopologyException;<br/>\n    +import backtype.storm.generated.KeyAlreadyExistsException;<br/>\n    +import backtype.storm.generated.KeyNotFoundException;<br/>\n    +import backtype.storm.generated.SettableBlobMeta;<br/>\n    +import backtype.storm.spout.SpoutOutputCollector;<br/>\n    +import backtype.storm.task.ShellBolt;<br/>\n    +import backtype.storm.task.TopologyContext;<br/>\n    +import backtype.storm.topology.BasicOutputCollector;<br/>\n    +import backtype.storm.topology.IRichBolt;<br/>\n    +import backtype.storm.topology.OutputFieldsDeclarer;<br/>\n    +import backtype.storm.topology.TopologyBuilder;<br/>\n    +import backtype.storm.topology.base.BaseBasicBolt;<br/>\n    +import backtype.storm.topology.base.BaseRichSpout;<br/>\n    +import backtype.storm.blobstore.BlobStoreAclHandler;<br/>\n    +import backtype.storm.tuple.Fields;<br/>\n    +import backtype.storm.tuple.Tuple;<br/>\n    +import backtype.storm.tuple.Values;<br/>\n    +import backtype.storm.utils.Utils;<br/>\n    +import org.slf4j.Logger;<br/>\n    +import org.slf4j.LoggerFactory;<br/>\n    +<br/>\n    +import java.io.BufferedReader;<br/>\n    +import java.io.BufferedWriter;<br/>\n    +import java.io.File;<br/>\n    +import java.io.FileReader;<br/>\n    +import java.io.FileWriter;<br/>\n    +import java.io.IOException;<br/>\n    +import java.util.HashSet;<br/>\n    +import java.util.Iterator;<br/>\n    +import java.util.LinkedList;<br/>\n    +import java.util.List;<br/>\n    +import java.util.Map;<br/>\n    +import java.util.Random;<br/>\n    +import java.util.Set;<br/>\n    +import java.util.StringTokenizer;<br/>\n    +<br/>\n    +public class BlobStoreAPIWordCountTopology {<br/>\n    +    private static ClientBlobStore store; // Client API to invoke blob store API functionality<br/>\n    +    private static String key = \"key\";<br/>\n    +    private static String fileName = \"blacklist.txt\";<br/>\n    +    private static final Logger LOG = LoggerFactory.getLogger(BlobStoreAPIWordCountTopology.class);<br/>\n    +<br/>\n    +    public static void prepare() </p>\n{\n    +Config conf = new Config();\n    +conf.putAll(Utils.readStormConfig());\n    +store = Utils.getClientBlobStore(conf);\n    +    }\n<p>    +<br/>\n    +    // Spout implementation<br/>\n    +    public static class RandomSentenceSpout extends BaseRichSpout {<br/>\n    +SpoutOutputCollector _collector;<br/>\n    +<br/>\n    +@Override<br/>\n    +public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) </p>\n{\n    +    _collector = collector;\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void nextTuple() </p>\n{\n    +    Utils.sleep(100);\n    +    _collector.emit(new Values(getRandomSentence()));\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void ack(Object id) </p>\n{\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void fail(Object id) {    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"sentence\"));\n    +}\n<p>    +<br/>\n    +    }<br/>\n    +<br/>\n    +    // Bolt implementation<br/>\n    +    public static class SplitSentence extends ShellBolt implements IRichBolt {<br/>\n    +<br/>\n    +public SplitSentence() </p>\n{\n    +    super(\"python\", \"splitsentence.py\");\n    +}\n<p>    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) </p>\n{\n    +    declarer.declare(new Fields(\"word\"));\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public Map<String, Object> getComponentConfiguration() {\n    +    return null;\n    +}<br/>\n    +    }<br/>\n    +<br/>\n    +    public static class FilterWords extends BaseBasicBolt {<br/>\n    +String fileName = \"blacklist.txt\";<br/>\n    +@Override<br/>\n    +public void execute(Tuple tuple, BasicOutputCollector collector) {<br/>\n    +    try {<br/>\n    +String word = tuple.getString(0);<br/>\n    +Set<String> wordSet = parseFile(fileName);<br/>\n    +if (!wordSet.contains(word)) {\n    +    collector.emit(new Values(word));\n    +}<br/>\n    +    } catch (IOException exp) {\n    +throw new RuntimeException(exp);\n    +    }<br/>\n    +}<br/>\n    +<br/>\n    +@Override<br/>\n    +public void declareOutputFields(OutputFieldsDeclarer declarer) {    +    declarer.declare(new Fields(\"word\"));    +}\n<p>    +    }<br/>\n    +<br/>\n    +    public void buildAndLaunchWordCountTopology(String[] args) {<br/>\n    +TopologyBuilder builder = new TopologyBuilder();<br/>\n    +builder.setSpout(\"spout\", new RandomSentenceSpout(), 5);<br/>\n    +builder.setBolt(\"split\", new SplitSentence(), 8).shuffleGrouping(\"spout\");<br/>\n    +builder.setBolt(\"filter\", new FilterWords(), 6).shuffleGrouping(\"split\");<br/>\n    +<br/>\n    +Config conf = new Config();<br/>\n    +conf.setDebug(true);<br/>\n    +try </p>\n{\n    +    conf.setNumWorkers(3);\n    +    StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());\n    +}\n<p> catch (InvalidTopologyException | AuthorizationException | AlreadyAliveException exp) </p>\n{\n    +    throw new RuntimeException(exp);\n    +}\n<p>    +    }<br/>\n    +<br/>\n    +    // Equivalent create command on command line<br/>\n    +    // storm blobstore create --file blacklist.txt --acl o::rwa key<br/>\n    +    private static void createBlobWithContent(String blobKey, ClientBlobStore clientBlobStore, File file)<br/>\n    +    throws AuthorizationException, KeyAlreadyExistsException, IOException,KeyNotFoundException </p>\n{\n    +String stringBlobACL = \"o::rwa\";\n    +AccessControl blobACL = BlobStoreAclHandler.parseAccessControl(stringBlobACL);\n    +List<AccessControl> acls = new LinkedList<AccessControl>();\n    +acls.add(blobACL); // more ACLs can be added here\n    +SettableBlobMeta settableBlobMeta = new SettableBlobMeta(acls);\n    +AtomicOutputStream blobStream = clientBlobStore.createBlob(blobKey,settableBlobMeta);\n    +blobStream.write(readFile(file).toString().getBytes());\n    +blobStream.close();\n    +    }\n<p>    +<br/>\n    +    // Equivalent update command on command line<br/>\n    +    // storm blobstore update --file blacklist.txt key<br/>\n    +    private static void updateBlobWithContent(String blobKey, ClientBlobStore clientBlobStore, File file)<br/>\n    +    throws KeyNotFoundException, AuthorizationException, IOException </p>\n{\n    +AtomicOutputStream blobOutputStream = clientBlobStore.updateBlob(blobKey);\n    +blobOutputStream.write(readFile(file).toString().getBytes());\n    +blobOutputStream.close();\n    +    }\n<p>    +<br/>\n    +    private static String getRandomSentence() {<br/>\n    +String[] sentences = new String[]</p>\n{ \"the cow jumped over the moon\", \"an apple a day keeps the doctor away\",\n    +\"four score and seven years ago\", \"snow white and the seven dwarfs\", \"i am at two with nature\" }\n<p>;<br/>\n    +String sentence = sentences<span class=\"error\">&#91;new Random().nextInt(sentences.length)&#93;</span>;<br/>\n    +return sentence;<br/>\n    +    }<br/>\n    +<br/>\n    +    private static Set<String> getRandomWordSet() {<br/>\n    +Set<String> randomWordSet = new HashSet<>();<br/>\n    +Random random = new Random();<br/>\n    +String[] words = new String[]</p>\n{ \"cow\", \"jumped\", \"over\", \"the\", \"moon\", \"apple\", \"day\", \"doctor\", \"away\",\n    +\"four\", \"seven\", \"ago\", \"snow\", \"white\", \"seven\", \"dwarfs\", \"nature\", \"two\" }\n<p>;<br/>\n    +// Choosing atmost 5 words to update the blacklist file for filtering<br/>\n    +for (int i=0; i<5; i++) </p>\n{\n    +    randomWordSet.add(words[random.nextInt(words.length)]);\n    +}\n<p>    +return randomWordSet;<br/>\n    +    }<br/>\n    +<br/>\n    +    private static Set<String> parseFile(String fileName) throws IOException {<br/>\n    +File file = new File(fileName);<br/>\n    +Set<String> wordSet = new HashSet<>();<br/>\n    +if (!file.exists()) </p>\n{\n    +    return wordSet;\n    +}\n<p>    +StringTokenizer tokens = new StringTokenizer(readFile(file).toString(), \"\\r\\n\");<br/>\n    +while (tokens.hasMoreElements()) </p>\n{\n    +    wordSet.add(tokens.nextToken());\n    +}\n<p>    +LOG.info(\"parseFile {}\", wordSet);<br/>\n    +return wordSet;<br/>\n    +    }<br/>\n    +<br/>\n    +    private static StringBuilder readFile(File file) throws IOException {<br/>\n    +String line;<br/>\n    +StringBuilder fileContent = new StringBuilder();<br/>\n    +// Do not use canonical file name here<br/>\n    +BufferedReader br = new BufferedReader(new FileReader(file));<br/>\n    +while ((line = br.readLine()) != null) </p>\n{\n    +    fileContent.append(line);\n    +    fileContent.append(System.lineSeparator());\n    +}\n<p>    +return fileContent;<br/>\n    +    }<br/>\n    +<br/>\n    +    // Creating a blacklist file to read from the disk<br/>\n    +    public static File createFile(String fileName) throws IOException {<br/>\n    +File file = null;<br/>\n    +file = new File(fileName);<br/>\n    +if (!file.exists()) </p>\n{\n    +    file.createNewFile();\n    +}\n<p>    +writeToFile(file, getRandomWordSet());<br/>\n    +LOG.info(readFile(file).toString());<br/>\n    +return file;<br/>\n    +    }<br/>\n    +<br/>\n    +    // Updating a blacklist file periodically with random words<br/>\n    +    public static File updateFile(File file) throws IOException </p>\n{\n    +writeToFile(file, getRandomWordSet());\n    +return file;\n    +    }\n<p>    +<br/>\n    +    // Writing random words to be blacklisted<br/>\n    +    public static void writeToFile(File file, Set<String> content) throws IOException{<br/>\n    +FileWriter fw = new FileWriter(file, false);<br/>\n    +BufferedWriter bw = new BufferedWriter(fw);<br/>\n    +Iterator<String> iter = content.iterator();<br/>\n    +while(iter.hasNext()) </p>\n{\n    +    bw.write(iter.next());\n    +    bw.write(System.lineSeparator());\n    +}\n<p>    +bw.close();<br/>\n    +    }<br/>\n    +<br/>\n    +    public static void main(String[] args) {<br/>\n    +prepare();<br/>\n    +BlobStoreAPIWordCountTopology wc = new BlobStoreAPIWordCountTopology();<br/>\n    +try {<br/>\n    +    File file = createFile(fileName);<br/>\n    +    // Creating blob again before launching topology<br/>\n    +    createBlobWithContent(key, store, file);<br/>\n    &#8212; End diff &#8211;</p>\n\n<p>    Are we going to get errors if we run this again?  The key will already exist.  Would it be good to delete the blob at the end of the example so others can also run the example?</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685767/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165126313","html_url":"https://github.com/apache/storm/pull/934#issuecomment-165126313","issue_url":"https://api.github.com/repos/apache/storm/issues/934","id":165126313,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTEyNjMxMw==","user":{"login":"revans2","id":3441321,"node_id":"MDQ6VXNlcjM0NDEzMjE=","avatar_url":"https://avatars.githubusercontent.com/u/3441321?v=4","gravatar_id":"","url":"https://api.github.com/users/revans2","html_url":"https://github.com/revans2","followers_url":"https://api.github.com/users/revans2/followers","following_url":"https://api.github.com/users/revans2/following{/other_user}","gists_url":"https://api.github.com/users/revans2/gists{/gist_id}","starred_url":"https://api.github.com/users/revans2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/revans2/subscriptions","organizations_url":"https://api.github.com/users/revans2/orgs","repos_url":"https://api.github.com/users/revans2/repos","events_url":"https://api.github.com/users/revans2/events{/privacy}","received_events_url":"https://api.github.com/users/revans2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T14:30:41Z","updated_at":"2015-12-16T14:30:41Z","author_association":"CONTRIBUTOR","body":"Looks good to me,  Just a few minor comments.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165126313/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685773","html_url":"https://github.com/apache/storm/issues/5208#issuecomment-2612685773","issue_url":"https://api.github.com/repos/apache/storm/issues/5208","id":2612685773,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU3NzM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T14:30:43Z","updated_at":"2025-01-24T14:38:06Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user revans2 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/934#issuecomment-165126313\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/934#issuecomment-165126313</a></p>\n\n<p>    Looks good to me,  Just a few minor comments.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685773/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165134453","html_url":"https://github.com/apache/storm/pull/941#issuecomment-165134453","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":165134453,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTEzNDQ1Mw==","user":{"login":"revans2","id":3441321,"node_id":"MDQ6VXNlcjM0NDEzMjE=","avatar_url":"https://avatars.githubusercontent.com/u/3441321?v=4","gravatar_id":"","url":"https://api.github.com/users/revans2","html_url":"https://github.com/revans2","followers_url":"https://api.github.com/users/revans2/followers","following_url":"https://api.github.com/users/revans2/following{/other_user}","gists_url":"https://api.github.com/users/revans2/gists{/gist_id}","starred_url":"https://api.github.com/users/revans2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/revans2/subscriptions","organizations_url":"https://api.github.com/users/revans2/orgs","repos_url":"https://api.github.com/users/revans2/repos","events_url":"https://api.github.com/users/revans2/events{/privacy}","received_events_url":"https://api.github.com/users/revans2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T15:02:55Z","updated_at":"2015-12-16T15:02:55Z","author_association":"CONTRIBUTOR","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165134453/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686646","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686646","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686646,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2NDY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T15:02:57Z","updated_at":"2025-01-24T14:38:27Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user revans2 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-165134453\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-165134453</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686646/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165142316","html_url":"https://github.com/apache/storm/pull/945#issuecomment-165142316","issue_url":"https://api.github.com/repos/apache/storm/issues/945","id":165142316,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE0MjMxNg==","user":{"login":"zhuoliu","id":11683054,"node_id":"MDQ6VXNlcjExNjgzMDU0","avatar_url":"https://avatars.githubusercontent.com/u/11683054?v=4","gravatar_id":"","url":"https://api.github.com/users/zhuoliu","html_url":"https://github.com/zhuoliu","followers_url":"https://api.github.com/users/zhuoliu/followers","following_url":"https://api.github.com/users/zhuoliu/following{/other_user}","gists_url":"https://api.github.com/users/zhuoliu/gists{/gist_id}","starred_url":"https://api.github.com/users/zhuoliu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhuoliu/subscriptions","organizations_url":"https://api.github.com/users/zhuoliu/orgs","repos_url":"https://api.github.com/users/zhuoliu/repos","events_url":"https://api.github.com/users/zhuoliu/events{/privacy}","received_events_url":"https://api.github.com/users/zhuoliu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T15:28:50Z","updated_at":"2015-12-16T15:28:50Z","author_association":"NONE","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165142316/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685424","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685424","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685424,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU0MjQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T15:28:52Z","updated_at":"2025-01-24T14:37:58Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user zhuoliu commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#issuecomment-165142316\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#issuecomment-165142316</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685424/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165144228","html_url":"https://github.com/apache/storm/pull/945#issuecomment-165144228","issue_url":"https://api.github.com/repos/apache/storm/issues/945","id":165144228,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE0NDIyOA==","user":{"login":"unsleepy22","id":631361,"node_id":"MDQ6VXNlcjYzMTM2MQ==","avatar_url":"https://avatars.githubusercontent.com/u/631361?v=4","gravatar_id":"","url":"https://api.github.com/users/unsleepy22","html_url":"https://github.com/unsleepy22","followers_url":"https://api.github.com/users/unsleepy22/followers","following_url":"https://api.github.com/users/unsleepy22/following{/other_user}","gists_url":"https://api.github.com/users/unsleepy22/gists{/gist_id}","starred_url":"https://api.github.com/users/unsleepy22/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/unsleepy22/subscriptions","organizations_url":"https://api.github.com/users/unsleepy22/orgs","repos_url":"https://api.github.com/users/unsleepy22/repos","events_url":"https://api.github.com/users/unsleepy22/events{/privacy}","received_events_url":"https://api.github.com/users/unsleepy22/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T15:32:17Z","updated_at":"2015-12-16T15:32:17Z","author_association":"NONE","body":"LGTM +1\nA minor suggestion, is it better to quote blob commands in `Configuration` section like the above parts in order to keep consistent? but it's up to you.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165144228/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685430","html_url":"https://github.com/apache/storm/issues/5207#issuecomment-2612685430","issue_url":"https://api.github.com/repos/apache/storm/issues/5207","id":2612685430,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODU0MzA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T15:32:19Z","updated_at":"2025-01-24T14:37:58Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user unsleepy22 commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/945#issuecomment-165144228\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/945#issuecomment-165144228</a></p>\n\n<p>    LGTM +1<br/>\n    A minor suggestion, is it better to quote blob commands in `Configuration` section like the above parts in order to keep consistent? but it's up to you.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612685430/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165164884","html_url":"https://github.com/apache/storm/pull/900#issuecomment-165164884","issue_url":"https://api.github.com/repos/apache/storm/issues/900","id":165164884,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE2NDg4NA==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:27:18Z","updated_at":"2015-12-16T16:27:18Z","author_association":"CONTRIBUTOR","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165164884/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612628693","html_url":"https://github.com/apache/storm/issues/5025#issuecomment-2612628693","issue_url":"https://api.github.com/repos/apache/storm/issues/5025","id":2612628693,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2Mjg2OTM=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:27:20Z","updated_at":"2025-01-24T14:13:05Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/900#issuecomment-165164884\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/900#issuecomment-165164884</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612628693/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165164975","html_url":"https://github.com/apache/storm/pull/941#issuecomment-165164975","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":165164975,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE2NDk3NQ==","user":{"login":"jerrypeng","id":3613359,"node_id":"MDQ6VXNlcjM2MTMzNTk=","avatar_url":"https://avatars.githubusercontent.com/u/3613359?v=4","gravatar_id":"","url":"https://api.github.com/users/jerrypeng","html_url":"https://github.com/jerrypeng","followers_url":"https://api.github.com/users/jerrypeng/followers","following_url":"https://api.github.com/users/jerrypeng/following{/other_user}","gists_url":"https://api.github.com/users/jerrypeng/gists{/gist_id}","starred_url":"https://api.github.com/users/jerrypeng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jerrypeng/subscriptions","organizations_url":"https://api.github.com/users/jerrypeng/orgs","repos_url":"https://api.github.com/users/jerrypeng/repos","events_url":"https://api.github.com/users/jerrypeng/events{/privacy}","received_events_url":"https://api.github.com/users/jerrypeng/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:27:36Z","updated_at":"2015-12-16T16:27:36Z","author_association":"CONTRIBUTOR","body":"+1 and verified nimbus auth unit tests have no failures running on a machine with 1 core\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165164975/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686654","html_url":"https://github.com/apache/storm/issues/5212#issuecomment-2612686654","issue_url":"https://api.github.com/repos/apache/storm/issues/5212","id":2612686654,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODY2NTQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:27:37Z","updated_at":"2025-01-24T14:38:27Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user jerrypeng commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/941#issuecomment-165164975\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/941#issuecomment-165164975</a></p>\n\n<p>    +1 and verified nimbus auth unit tests have no failures running on a machine with 1 core</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612686654/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165166066","html_url":"https://github.com/apache/storm/pull/893#issuecomment-165166066","issue_url":"https://api.github.com/repos/apache/storm/issues/893","id":165166066,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE2NjA2Ng==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:31:36Z","updated_at":"2015-12-16T16:31:36Z","author_association":"CONTRIBUTOR","body":"@arunmahadevan I might be overlooking something but if the default tick tuple is at 1secs and this will cause the tuples to be flushed much more frequently without even reaching to batchsize. This behavior can create lot of small files in hdfs and also for hive.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165166066/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634449","html_url":"https://github.com/apache/storm/issues/5057#issuecomment-2612634449","issue_url":"https://api.github.com/repos/apache/storm/issues/5057","id":2612634449,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzQ0NDk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:31:38Z","updated_at":"2025-01-24T14:15:36Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/893#issuecomment-165166066\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/893#issuecomment-165166066</a></p>\n\n<p>    @arunmahadevan I might be overlooking something but if the default tick tuple is at 1secs and this will cause the tuples to be flushed much more frequently without even reaching to batchsize. This behavior can create lot of small files in hdfs and also for hive.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634449/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165166518","html_url":"https://github.com/apache/storm/pull/893#issuecomment-165166518","issue_url":"https://api.github.com/repos/apache/storm/issues/893","id":165166518,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE2NjUxOA==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:32:27Z","updated_at":"2015-12-16T16:32:27Z","author_association":"CONTRIBUTOR","body":"I think the default should be at least 15secs than 1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165166518/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634458","html_url":"https://github.com/apache/storm/issues/5057#issuecomment-2612634458","issue_url":"https://api.github.com/repos/apache/storm/issues/5057","id":2612634458,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzQ0NTg=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:32:29Z","updated_at":"2025-01-24T14:15:36Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/893#issuecomment-165166518\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/893#issuecomment-165166518</a></p>\n\n<p>    I think the default should be at least 15secs than 1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634458/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165168520","html_url":"https://github.com/apache/storm/pull/893#issuecomment-165168520","issue_url":"https://api.github.com/repos/apache/storm/issues/893","id":165168520,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE2ODUyMA==","user":{"login":"dossett","id":7207582,"node_id":"MDQ6VXNlcjcyMDc1ODI=","avatar_url":"https://avatars.githubusercontent.com/u/7207582?v=4","gravatar_id":"","url":"https://api.github.com/users/dossett","html_url":"https://github.com/dossett","followers_url":"https://api.github.com/users/dossett/followers","following_url":"https://api.github.com/users/dossett/following{/other_user}","gists_url":"https://api.github.com/users/dossett/gists{/gist_id}","starred_url":"https://api.github.com/users/dossett/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dossett/subscriptions","organizations_url":"https://api.github.com/users/dossett/orgs","repos_url":"https://api.github.com/users/dossett/repos","events_url":"https://api.github.com/users/dossett/events{/privacy}","received_events_url":"https://api.github.com/users/dossett/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:36:19Z","updated_at":"2015-12-16T16:36:19Z","author_association":"CONTRIBUTOR","body":"@harshach I don't believe flushing would force early file rotations.  On second thought, I do agree that 15 seconds would be better than 1 second.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165168520/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634466","html_url":"https://github.com/apache/storm/issues/5057#issuecomment-2612634466","issue_url":"https://api.github.com/repos/apache/storm/issues/5057","id":2612634466,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzQ0NjY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:36:21Z","updated_at":"2025-01-24T14:15:36Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user dossett commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/893#issuecomment-165168520\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/893#issuecomment-165168520</a></p>\n\n<p>    @harshach I don't believe flushing would force early file rotations.  On second thought, I do agree that 15 seconds would be better than 1 second.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634466/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165169297","html_url":"https://github.com/apache/storm/pull/893#issuecomment-165169297","issue_url":"https://api.github.com/repos/apache/storm/issues/893","id":165169297,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE2OTI5Nw==","user":{"login":"harshach","id":38649,"node_id":"MDQ6VXNlcjM4NjQ5","avatar_url":"https://avatars.githubusercontent.com/u/38649?v=4","gravatar_id":"","url":"https://api.github.com/users/harshach","html_url":"https://github.com/harshach","followers_url":"https://api.github.com/users/harshach/followers","following_url":"https://api.github.com/users/harshach/following{/other_user}","gists_url":"https://api.github.com/users/harshach/gists{/gist_id}","starred_url":"https://api.github.com/users/harshach/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/harshach/subscriptions","organizations_url":"https://api.github.com/users/harshach/orgs","repos_url":"https://api.github.com/users/harshach/repos","events_url":"https://api.github.com/users/harshach/events{/privacy}","received_events_url":"https://api.github.com/users/harshach/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:38:59Z","updated_at":"2015-12-16T16:38:59Z","author_association":"CONTRIBUTOR","body":"@dossett It does for hive. It creates small files , which eventually will be merged by compactor. But its not good to have small files especially if your compactor runs like every hour.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165169297/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634469","html_url":"https://github.com/apache/storm/issues/5057#issuecomment-2612634469","issue_url":"https://api.github.com/repos/apache/storm/issues/5057","id":2612634469,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzQ0Njk=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:39:00Z","updated_at":"2025-01-24T14:15:36Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user harshach commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/893#issuecomment-165169297\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/893#issuecomment-165169297</a></p>\n\n<p>    @dossett It does for hive. It creates small files , which eventually will be merged by compactor. But its not good to have small files especially if your compactor runs like every hour.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634469/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165169501","html_url":"https://github.com/apache/storm/pull/893#issuecomment-165169501","issue_url":"https://api.github.com/repos/apache/storm/issues/893","id":165169501,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE2OTUwMQ==","user":{"login":"dossett","id":7207582,"node_id":"MDQ6VXNlcjcyMDc1ODI=","avatar_url":"https://avatars.githubusercontent.com/u/7207582?v=4","gravatar_id":"","url":"https://api.github.com/users/dossett","html_url":"https://github.com/dossett","followers_url":"https://api.github.com/users/dossett/followers","following_url":"https://api.github.com/users/dossett/following{/other_user}","gists_url":"https://api.github.com/users/dossett/gists{/gist_id}","starred_url":"https://api.github.com/users/dossett/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dossett/subscriptions","organizations_url":"https://api.github.com/users/dossett/orgs","repos_url":"https://api.github.com/users/dossett/repos","events_url":"https://api.github.com/users/dossett/events{/privacy}","received_events_url":"https://api.github.com/users/dossett/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:39:42Z","updated_at":"2015-12-16T16:39:42Z","author_association":"CONTRIBUTOR","body":"@harshach Ah, thanks, I was only thinking of the hdfs bolt side.\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165169501/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634472","html_url":"https://github.com/apache/storm/issues/5057#issuecomment-2612634472","issue_url":"https://api.github.com/repos/apache/storm/issues/5057","id":2612634472,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2MzQ0NzI=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T16:39:44Z","updated_at":"2025-01-24T14:15:36Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user dossett commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/893#issuecomment-165169501\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/893#issuecomment-165169501</a></p>\n\n<p>    @harshach Ah, thanks, I was only thinking of the hdfs bolt side.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612634472/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165178423","html_url":"https://github.com/apache/storm/pull/943#issuecomment-165178423","issue_url":"https://api.github.com/repos/apache/storm/issues/943","id":165178423,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE3ODQyMw==","user":{"login":"rfarivar","id":8742608,"node_id":"MDQ6VXNlcjg3NDI2MDg=","avatar_url":"https://avatars.githubusercontent.com/u/8742608?v=4","gravatar_id":"","url":"https://api.github.com/users/rfarivar","html_url":"https://github.com/rfarivar","followers_url":"https://api.github.com/users/rfarivar/followers","following_url":"https://api.github.com/users/rfarivar/following{/other_user}","gists_url":"https://api.github.com/users/rfarivar/gists{/gist_id}","starred_url":"https://api.github.com/users/rfarivar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rfarivar/subscriptions","organizations_url":"https://api.github.com/users/rfarivar/orgs","repos_url":"https://api.github.com/users/rfarivar/repos","events_url":"https://api.github.com/users/rfarivar/events{/privacy}","received_events_url":"https://api.github.com/users/rfarivar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T17:10:04Z","updated_at":"2015-12-16T17:10:04Z","author_association":"CONTRIBUTOR","body":"+1\n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165178423/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688334","html_url":"https://github.com/apache/storm/issues/5223#issuecomment-2612688334","issue_url":"https://api.github.com/repos/apache/storm/issues/5223","id":2612688334,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODgzMzQ=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T17:10:06Z","updated_at":"2025-01-24T14:39:11Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user rfarivar commented on the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/943#issuecomment-165178423\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/943#issuecomment-165178423</a></p>\n\n<p>    +1</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688334/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688950","html_url":"https://github.com/apache/storm/issues/5228#issuecomment-2612688950","issue_url":"https://api.github.com/repos/apache/storm/issues/5228","id":2612688950,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg5NTA=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T17:12:35Z","updated_at":"2025-01-24T14:39:29Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user knusbaum commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/949#discussion_r47804454\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/949#discussion_r47804454</a></p>\n\n<p>    &#8212; Diff: storm-core/src/clj/backtype/storm/config.clj &#8212;<br/>\n    @@ -258,8 +264,8 @@<br/>\n  (if workers-artifacts-dir<br/>\n    (if (is-absolute-path? workers-artifacts-dir)<br/>\n      workers-artifacts-dir</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>(str backtype.storm.util/LOG-DIR file-path-separator workers-artifacts-dir))</li>\n\t<li>(str backtype.storm.util/LOG-DIR file-path-separator \"workers-artifacts\"))))<br/>\n    + (str backtype.storm.config/LOG-DIR file-path-separator workers-artifacts-dir))<br/>\n    +       (str backtype.storm.config/LOG-DIR file-path-separator \"workers-artifacts\"))))\n\t<ul class=\"alternate\" type=\"square\">\n\t\t<li>\n\t\t<ul class=\"alternate\" type=\"square\">\n\t\t\t<li>End diff &#8211;</li>\n\t\t</ul>\n\t\t</li>\n\t</ul>\n\t</li>\n</ul>\n\n\n<p>    We can just remove the `backtype.storm.config/` from these. They don't need an explicit namespace because they're in the same namespace.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688950/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688956","html_url":"https://github.com/apache/storm/issues/5228#issuecomment-2612688956","issue_url":"https://api.github.com/repos/apache/storm/issues/5228","id":2612688956,"node_id":"MDEyOklzc3VlQ29tbWVudDI2MTI2ODg5NTY=","user":{"login":"jira-importer","id":99357308,"node_id":"U_kgDOBewSfA","avatar_url":"https://avatars.githubusercontent.com/u/99357308?v=4","gravatar_id":"","url":"https://api.github.com/users/jira-importer","html_url":"https://github.com/jira-importer","followers_url":"https://api.github.com/users/jira-importer/followers","following_url":"https://api.github.com/users/jira-importer/following{/other_user}","gists_url":"https://api.github.com/users/jira-importer/gists{/gist_id}","starred_url":"https://api.github.com/users/jira-importer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jira-importer/subscriptions","organizations_url":"https://api.github.com/users/jira-importer/orgs","repos_url":"https://api.github.com/users/jira-importer/repos","events_url":"https://api.github.com/users/jira-importer/events{/privacy}","received_events_url":"https://api.github.com/users/jira-importer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T17:12:51Z","updated_at":"2025-01-24T14:39:29Z","author_association":"COLLABORATOR","body":"<i><a href=\"https://issues.apache.org/jira/secure/ViewProfile.jspa?name=githubbot\">githubbot</a>:</i>\n<p>Github user knusbaum commented on a diff in the pull request:</p>\n\n<p>    <a href=\"https://github.com/apache/storm/pull/949#discussion_r47804494\" class=\"external-link\" target=\"_blank\" rel=\"nofollow noopener\">https://github.com/apache/storm/pull/949#discussion_r47804494</a></p>\n\n<p>    &#8212; Diff: storm-core/src/clj/backtype/storm/daemon/supervisor.clj &#8212;<br/>\n    @@ -1022,7 +1022,7 @@<br/>\n       storm-home (System/getProperty \"storm.home\")<br/>\n       storm-options (System/getProperty \"storm.options\")<br/>\n       storm-conf-file (System/getProperty \"storm.conf.file\")</p>\n<ul class=\"alternate\" type=\"square\">\n\t<li>storm-log-dir (or (System/getProperty \"storm.log.dir\") (str storm-home file-path-separator \"logs\"))<br/>\n    +  storm-log-dir backtype.storm.config/LOG-DIR\n\t<ul class=\"alternate\" type=\"square\">\n\t\t<li>\n\t\t<ul class=\"alternate\" type=\"square\">\n\t\t\t<li>End diff &#8211;</li>\n\t\t</ul>\n\t\t</li>\n\t</ul>\n\t</li>\n</ul>\n\n\n<p>    Here as well.</p>","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/2612688956/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/storm/issues/comments/165180783","html_url":"https://github.com/apache/storm/pull/941#issuecomment-165180783","issue_url":"https://api.github.com/repos/apache/storm/issues/941","id":165180783,"node_id":"MDEyOklzc3VlQ29tbWVudDE2NTE4MDc4Mw==","user":{"login":"rfarivar","id":8742608,"node_id":"MDQ6VXNlcjg3NDI2MDg=","avatar_url":"https://avatars.githubusercontent.com/u/8742608?v=4","gravatar_id":"","url":"https://api.github.com/users/rfarivar","html_url":"https://github.com/rfarivar","followers_url":"https://api.github.com/users/rfarivar/followers","following_url":"https://api.github.com/users/rfarivar/following{/other_user}","gists_url":"https://api.github.com/users/rfarivar/gists{/gist_id}","starred_url":"https://api.github.com/users/rfarivar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rfarivar/subscriptions","organizations_url":"https://api.github.com/users/rfarivar/orgs","repos_url":"https://api.github.com/users/rfarivar/repos","events_url":"https://api.github.com/users/rfarivar/events{/privacy}","received_events_url":"https://api.github.com/users/rfarivar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2015-12-16T17:15:46Z","updated_at":"2015-12-16T17:15:46Z","author_association":"CONTRIBUTOR","body":"It's an unlikely scenario and I might be paranoid, but what if the tests were running on a cluster under heavy traffic? For instance, what if the tests were to be run on OpenStack, and the test just happened to run when there is unrelated heavy traffic (say, Hadoop shuffle) pressuring the switches? \n","reactions":{"url":"https://api.github.com/repos/apache/storm/issues/comments/165180783/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]