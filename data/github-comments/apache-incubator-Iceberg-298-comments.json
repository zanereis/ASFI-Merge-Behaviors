[{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845240644","html_url":"https://github.com/apache/iceberg/issues/9127#issuecomment-1845240644","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9127","id":1845240644,"node_id":"IC_kwDOCW7NX85t_CNE","user":{"login":"Fokko","id":1134248,"node_id":"MDQ6VXNlcjExMzQyNDg=","avatar_url":"https://avatars.githubusercontent.com/u/1134248?v=4","gravatar_id":"","url":"https://api.github.com/users/Fokko","html_url":"https://github.com/Fokko","followers_url":"https://api.github.com/users/Fokko/followers","following_url":"https://api.github.com/users/Fokko/following{/other_user}","gists_url":"https://api.github.com/users/Fokko/gists{/gist_id}","starred_url":"https://api.github.com/users/Fokko/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Fokko/subscriptions","organizations_url":"https://api.github.com/users/Fokko/orgs","repos_url":"https://api.github.com/users/Fokko/repos","events_url":"https://api.github.com/users/Fokko/events{/privacy}","received_events_url":"https://api.github.com/users/Fokko/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T12:17:08Z","updated_at":"2023-12-07T12:17:08Z","author_association":"CONTRIBUTOR","body":"Yes, I think that makes a lot of sense. @bitsondatadev is also working on updating the [roadmap](https://iceberg.apache.org/roadmap/), it might be a good idea to link to that so we have one single place where people can see what's going on ðŸ‘ ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845240644/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845247046","html_url":"https://github.com/apache/iceberg/pull/8909#issuecomment-1845247046","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8909","id":1845247046,"node_id":"IC_kwDOCW7NX85t_DxG","user":{"login":"ajantha-bhat","id":5889404,"node_id":"MDQ6VXNlcjU4ODk0MDQ=","avatar_url":"https://avatars.githubusercontent.com/u/5889404?v=4","gravatar_id":"","url":"https://api.github.com/users/ajantha-bhat","html_url":"https://github.com/ajantha-bhat","followers_url":"https://api.github.com/users/ajantha-bhat/followers","following_url":"https://api.github.com/users/ajantha-bhat/following{/other_user}","gists_url":"https://api.github.com/users/ajantha-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/ajantha-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ajantha-bhat/subscriptions","organizations_url":"https://api.github.com/users/ajantha-bhat/orgs","repos_url":"https://api.github.com/users/ajantha-bhat/repos","events_url":"https://api.github.com/users/ajantha-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/ajantha-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T12:21:08Z","updated_at":"2023-12-07T12:21:08Z","author_association":"MEMBER","body":"Note: I am squashing the commits into one (since I am having hard time rebasing with 8 commits). ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845247046/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845262257","html_url":"https://github.com/apache/iceberg/pull/9230#issuecomment-1845262257","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9230","id":1845262257,"node_id":"IC_kwDOCW7NX85t_Hex","user":{"login":"nastra","id":271029,"node_id":"MDQ6VXNlcjI3MTAyOQ==","avatar_url":"https://avatars.githubusercontent.com/u/271029?v=4","gravatar_id":"","url":"https://api.github.com/users/nastra","html_url":"https://github.com/nastra","followers_url":"https://api.github.com/users/nastra/followers","following_url":"https://api.github.com/users/nastra/following{/other_user}","gists_url":"https://api.github.com/users/nastra/gists{/gist_id}","starred_url":"https://api.github.com/users/nastra/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nastra/subscriptions","organizations_url":"https://api.github.com/users/nastra/orgs","repos_url":"https://api.github.com/users/nastra/repos","events_url":"https://api.github.com/users/nastra/events{/privacy}","received_events_url":"https://api.github.com/users/nastra/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T12:31:36Z","updated_at":"2023-12-07T12:31:36Z","author_association":"CONTRIBUTOR","body":"@ConeyLiu could you also please take a look at this?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845262257/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845334573","html_url":"https://github.com/apache/iceberg/pull/9239#issuecomment-1845334573","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9239","id":1845334573,"node_id":"IC_kwDOCW7NX85t_ZIt","user":{"login":"nastra","id":271029,"node_id":"MDQ6VXNlcjI3MTAyOQ==","avatar_url":"https://avatars.githubusercontent.com/u/271029?v=4","gravatar_id":"","url":"https://api.github.com/users/nastra","html_url":"https://github.com/nastra","followers_url":"https://api.github.com/users/nastra/followers","following_url":"https://api.github.com/users/nastra/following{/other_user}","gists_url":"https://api.github.com/users/nastra/gists{/gist_id}","starred_url":"https://api.github.com/users/nastra/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nastra/subscriptions","organizations_url":"https://api.github.com/users/nastra/orgs","repos_url":"https://api.github.com/users/nastra/repos","events_url":"https://api.github.com/users/nastra/events{/privacy}","received_events_url":"https://api.github.com/users/nastra/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T13:21:32Z","updated_at":"2023-12-07T13:21:32Z","author_association":"CONTRIBUTOR","body":"thanks @zhongyujiang for adding this","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845334573/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845411543","html_url":"https://github.com/apache/iceberg/pull/6887#issuecomment-1845411543","issue_url":"https://api.github.com/repos/apache/iceberg/issues/6887","id":1845411543,"node_id":"IC_kwDOCW7NX85t_r7X","user":{"login":"ajantha-bhat","id":5889404,"node_id":"MDQ6VXNlcjU4ODk0MDQ=","avatar_url":"https://avatars.githubusercontent.com/u/5889404?v=4","gravatar_id":"","url":"https://api.github.com/users/ajantha-bhat","html_url":"https://github.com/ajantha-bhat","followers_url":"https://api.github.com/users/ajantha-bhat/followers","following_url":"https://api.github.com/users/ajantha-bhat/following{/other_user}","gists_url":"https://api.github.com/users/ajantha-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/ajantha-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ajantha-bhat/subscriptions","organizations_url":"https://api.github.com/users/ajantha-bhat/orgs","repos_url":"https://api.github.com/users/ajantha-bhat/repos","events_url":"https://api.github.com/users/ajantha-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/ajantha-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T14:11:24Z","updated_at":"2023-12-07T14:11:24Z","author_association":"MEMBER","body":"@nastra: Thanks for the review. I have replied to comments. \r\n\r\nI think no new code modifications needed. Please check again. Thanks.  ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845411543/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845461829","html_url":"https://github.com/apache/iceberg/pull/9224#issuecomment-1845461829","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9224","id":1845461829,"node_id":"IC_kwDOCW7NX85t_4NF","user":{"login":"yyy1000","id":69566909,"node_id":"MDQ6VXNlcjY5NTY2OTA5","avatar_url":"https://avatars.githubusercontent.com/u/69566909?v=4","gravatar_id":"","url":"https://api.github.com/users/yyy1000","html_url":"https://github.com/yyy1000","followers_url":"https://api.github.com/users/yyy1000/followers","following_url":"https://api.github.com/users/yyy1000/following{/other_user}","gists_url":"https://api.github.com/users/yyy1000/gists{/gist_id}","starred_url":"https://api.github.com/users/yyy1000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yyy1000/subscriptions","organizations_url":"https://api.github.com/users/yyy1000/orgs","repos_url":"https://api.github.com/users/yyy1000/repos","events_url":"https://api.github.com/users/yyy1000/events{/privacy}","received_events_url":"https://api.github.com/users/yyy1000/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T14:40:27Z","updated_at":"2023-12-07T14:40:27Z","author_association":"CONTRIBUTOR","body":"> > Hi, @nk1506 It seems that my code can't pass the unit tests. Is it because of the default waiting time is not enough? Do you have any ideas?\r\n> \r\n> Please refer [here](https://github.com/apache/iceberg/blob/6a9d3c77977baff4295ee2dde0150d73c8c46af1/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogLock.java#L146) . Also it is advisable to set the `atMost` too.\r\n\r\nOh, got it. I should use condition 'or' but not 'and'. :) Now the unit test can run in my local env.\r\nAwaitliity use default timeout of 10 seconds, and I think it is enough in this case. https://github.com/awaitility/awaitility/wiki/Usage#defaults\r\nSo adding a 'atMost' seems not necessary here. What do you think?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845461829/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845465132","html_url":"https://github.com/apache/iceberg/pull/9192#issuecomment-1845465132","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9192","id":1845465132,"node_id":"IC_kwDOCW7NX85t_5As","user":{"login":"ConeyLiu","id":12733256,"node_id":"MDQ6VXNlcjEyNzMzMjU2","avatar_url":"https://avatars.githubusercontent.com/u/12733256?v=4","gravatar_id":"","url":"https://api.github.com/users/ConeyLiu","html_url":"https://github.com/ConeyLiu","followers_url":"https://api.github.com/users/ConeyLiu/followers","following_url":"https://api.github.com/users/ConeyLiu/following{/other_user}","gists_url":"https://api.github.com/users/ConeyLiu/gists{/gist_id}","starred_url":"https://api.github.com/users/ConeyLiu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ConeyLiu/subscriptions","organizations_url":"https://api.github.com/users/ConeyLiu/orgs","repos_url":"https://api.github.com/users/ConeyLiu/repos","events_url":"https://api.github.com/users/ConeyLiu/events{/privacy}","received_events_url":"https://api.github.com/users/ConeyLiu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T14:42:15Z","updated_at":"2023-12-07T14:42:15Z","author_association":"CONTRIBUTOR","body":"@tmnd1991 thanks for your contribution. I think this is a valid fix.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845465132/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845537180","html_url":"https://github.com/apache/iceberg/issues/9127#issuecomment-1845537180","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9127","id":1845537180,"node_id":"IC_kwDOCW7NX85uAKmc","user":{"login":"bitsondatadev","id":8547669,"node_id":"MDQ6VXNlcjg1NDc2Njk=","avatar_url":"https://avatars.githubusercontent.com/u/8547669?v=4","gravatar_id":"","url":"https://api.github.com/users/bitsondatadev","html_url":"https://github.com/bitsondatadev","followers_url":"https://api.github.com/users/bitsondatadev/followers","following_url":"https://api.github.com/users/bitsondatadev/following{/other_user}","gists_url":"https://api.github.com/users/bitsondatadev/gists{/gist_id}","starred_url":"https://api.github.com/users/bitsondatadev/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bitsondatadev/subscriptions","organizations_url":"https://api.github.com/users/bitsondatadev/orgs","repos_url":"https://api.github.com/users/bitsondatadev/repos","events_url":"https://api.github.com/users/bitsondatadev/events{/privacy}","received_events_url":"https://api.github.com/users/bitsondatadev/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T15:19:19Z","updated_at":"2023-12-07T15:19:19Z","author_association":"COLLABORATOR","body":"@ronkorving Let me know if you'd like to raise the PR, or I can do it as well. For now pointing to the roadmap page will be the best. Eventually, I'd like there to be something less static that exists directly in GitHub for this. A main project board or possibly a wiki with links to individual projects.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845537180/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845560669","html_url":"https://github.com/apache/iceberg/pull/9147#issuecomment-1845560669","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9147","id":1845560669,"node_id":"IC_kwDOCW7NX85uAQVd","user":{"login":"irshadcc","id":24606617,"node_id":"MDQ6VXNlcjI0NjA2NjE3","avatar_url":"https://avatars.githubusercontent.com/u/24606617?v=4","gravatar_id":"","url":"https://api.github.com/users/irshadcc","html_url":"https://github.com/irshadcc","followers_url":"https://api.github.com/users/irshadcc/followers","following_url":"https://api.github.com/users/irshadcc/following{/other_user}","gists_url":"https://api.github.com/users/irshadcc/gists{/gist_id}","starred_url":"https://api.github.com/users/irshadcc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/irshadcc/subscriptions","organizations_url":"https://api.github.com/users/irshadcc/orgs","repos_url":"https://api.github.com/users/irshadcc/repos","events_url":"https://api.github.com/users/irshadcc/events{/privacy}","received_events_url":"https://api.github.com/users/irshadcc/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T15:32:35Z","updated_at":"2023-12-09T12:59:40Z","author_association":"CONTRIBUTOR","body":"> Thanks for raising this @irshadcc, this looks good to me. I've left two small comments, could you take a peek at those? Thanks for fixing this! ðŸ™Œ\r\n\r\nI've added the Javadoc and removed the empty line. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845560669/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845659813","html_url":"https://github.com/apache/iceberg/pull/8980#issuecomment-1845659813","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8980","id":1845659813,"node_id":"IC_kwDOCW7NX85uAoil","user":{"login":"cccs-jc","id":56140112,"node_id":"MDQ6VXNlcjU2MTQwMTEy","avatar_url":"https://avatars.githubusercontent.com/u/56140112?v=4","gravatar_id":"","url":"https://api.github.com/users/cccs-jc","html_url":"https://github.com/cccs-jc","followers_url":"https://api.github.com/users/cccs-jc/followers","following_url":"https://api.github.com/users/cccs-jc/following{/other_user}","gists_url":"https://api.github.com/users/cccs-jc/gists{/gist_id}","starred_url":"https://api.github.com/users/cccs-jc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cccs-jc/subscriptions","organizations_url":"https://api.github.com/users/cccs-jc/orgs","repos_url":"https://api.github.com/users/cccs-jc/repos","events_url":"https://api.github.com/users/cccs-jc/events{/privacy}","received_events_url":"https://api.github.com/users/cccs-jc/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T16:31:21Z","updated_at":"2023-12-07T16:31:21Z","author_association":"CONTRIBUTOR","body":"> @cccs-jc i mean let's have changes for 3.5 with it's test only in 3.5 and we can backport the change with it's test in lower spark version like 3.4 and 3.3, 3.4 test failures are expected right as we don't have changes for SparkMicrobatch stream for 3.4 in it.\r\n> \r\n> Also i would request to revert the change in core for Microbatch.java if we don't have coverage for it as i am unsure when would that fail (may be some legacy handling)\r\n> \r\n> Apologies for getting being late in getting back at this.\r\n\r\nKeeping the `+ existingFilesCount();` in the SparkMicrobatch.java makes no sense to me.\r\n\r\nWhat is the purpose of adding that to the currentFileIndex ?\r\n\r\nThe way I understand it currentFileIndex is a position of the added files. So we want to only count the added files (addedFilesCount()). These are the files that you want a streaming job to consume.\r\n\r\nCan you explain what is the purpose of using `existingFilesCount` here ?\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845659813/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845761731","html_url":"https://github.com/apache/iceberg/pull/9192#issuecomment-1845761731","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9192","id":1845761731,"node_id":"IC_kwDOCW7NX85uBBbD","user":{"login":"tmnd1991","id":7031242,"node_id":"MDQ6VXNlcjcwMzEyNDI=","avatar_url":"https://avatars.githubusercontent.com/u/7031242?v=4","gravatar_id":"","url":"https://api.github.com/users/tmnd1991","html_url":"https://github.com/tmnd1991","followers_url":"https://api.github.com/users/tmnd1991/followers","following_url":"https://api.github.com/users/tmnd1991/following{/other_user}","gists_url":"https://api.github.com/users/tmnd1991/gists{/gist_id}","starred_url":"https://api.github.com/users/tmnd1991/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmnd1991/subscriptions","organizations_url":"https://api.github.com/users/tmnd1991/orgs","repos_url":"https://api.github.com/users/tmnd1991/repos","events_url":"https://api.github.com/users/tmnd1991/events{/privacy}","received_events_url":"https://api.github.com/users/tmnd1991/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T17:17:43Z","updated_at":"2023-12-07T17:17:43Z","author_association":"NONE","body":"cc @nastra @dramaticlly @advancedxy for review\r\nthanks","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845761731/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845762590","html_url":"https://github.com/apache/iceberg/pull/9233#issuecomment-1845762590","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9233","id":1845762590,"node_id":"IC_kwDOCW7NX85uBBoe","user":{"login":"tmnd1991","id":7031242,"node_id":"MDQ6VXNlcjcwMzEyNDI=","avatar_url":"https://avatars.githubusercontent.com/u/7031242?v=4","gravatar_id":"","url":"https://api.github.com/users/tmnd1991","html_url":"https://github.com/tmnd1991","followers_url":"https://api.github.com/users/tmnd1991/followers","following_url":"https://api.github.com/users/tmnd1991/following{/other_user}","gists_url":"https://api.github.com/users/tmnd1991/gists{/gist_id}","starred_url":"https://api.github.com/users/tmnd1991/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmnd1991/subscriptions","organizations_url":"https://api.github.com/users/tmnd1991/orgs","repos_url":"https://api.github.com/users/tmnd1991/repos","events_url":"https://api.github.com/users/tmnd1991/events{/privacy}","received_events_url":"https://api.github.com/users/tmnd1991/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T17:18:02Z","updated_at":"2023-12-07T17:18:02Z","author_association":"NONE","body":"cc @nastra @dramaticlly @advancedxy for review\r\nthanks","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845762590/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845791782","html_url":"https://github.com/apache/iceberg/pull/8502#issuecomment-1845791782","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8502","id":1845791782,"node_id":"IC_kwDOCW7NX85uBIwm","user":{"login":"ajantha-bhat","id":5889404,"node_id":"MDQ6VXNlcjU4ODk0MDQ=","avatar_url":"https://avatars.githubusercontent.com/u/5889404?v=4","gravatar_id":"","url":"https://api.github.com/users/ajantha-bhat","html_url":"https://github.com/ajantha-bhat","followers_url":"https://api.github.com/users/ajantha-bhat/followers","following_url":"https://api.github.com/users/ajantha-bhat/following{/other_user}","gists_url":"https://api.github.com/users/ajantha-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/ajantha-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ajantha-bhat/subscriptions","organizations_url":"https://api.github.com/users/ajantha-bhat/orgs","repos_url":"https://api.github.com/users/ajantha-bhat/repos","events_url":"https://api.github.com/users/ajantha-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/ajantha-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T17:33:52Z","updated_at":"2023-12-07T17:33:52Z","author_association":"MEMBER","body":"@aokolnychyi: Thanks for the review. \r\n\r\n- I see that most of the questions or comments related to why puffin stats followed that style. We can fix in this PR for partition stats and later back port to puffin too.  \r\n- I didn't add expire snapshots and remove orphan files test code (but have test case of RemoveSnapshots API) as I was thinking to keep the scope of PR to non-Spark. I will have a PR that depends on this PR which tests these function. So, this PR can be merged. \r\n\r\nGot a little busy week. I will finish addressing comments and the follow up Spark PR for expire snapshots and remove orphan files by Monday. \r\n\r\nMeanwhile you can also review the independent PR (Util for partition stats reading and writing) : https://github.com/apache/iceberg/pull/9170","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845791782/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845793619","html_url":"https://github.com/apache/iceberg/pull/9242#issuecomment-1845793619","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9242","id":1845793619,"node_id":"IC_kwDOCW7NX85uBJNT","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T17:34:52Z","updated_at":"2023-12-07T17:34:52Z","author_association":"MEMBER","body":"Changed the title to match the contents","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845793619/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845885416","html_url":"https://github.com/apache/iceberg/pull/8398#issuecomment-1845885416","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8398","id":1845885416,"node_id":"IC_kwDOCW7NX85uBfno","user":{"login":"jackye1995","id":29823233,"node_id":"MDQ6VXNlcjI5ODIzMjMz","avatar_url":"https://avatars.githubusercontent.com/u/29823233?v=4","gravatar_id":"","url":"https://api.github.com/users/jackye1995","html_url":"https://github.com/jackye1995","followers_url":"https://api.github.com/users/jackye1995/followers","following_url":"https://api.github.com/users/jackye1995/following{/other_user}","gists_url":"https://api.github.com/users/jackye1995/gists{/gist_id}","starred_url":"https://api.github.com/users/jackye1995/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jackye1995/subscriptions","organizations_url":"https://api.github.com/users/jackye1995/orgs","repos_url":"https://api.github.com/users/jackye1995/repos","events_url":"https://api.github.com/users/jackye1995/events{/privacy}","received_events_url":"https://api.github.com/users/jackye1995/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T18:22:37Z","updated_at":"2023-12-07T18:22:37Z","author_association":"CONTRIBUTOR","body":"Most looks good to me, CI seems to fail with unrelated issue, try to retrigger","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845885416/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845955767","html_url":"https://github.com/apache/iceberg/pull/9211#issuecomment-1845955767","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9211","id":1845955767,"node_id":"IC_kwDOCW7NX85uBwy3","user":{"login":"stevenzwu","id":1545663,"node_id":"MDQ6VXNlcjE1NDU2NjM=","avatar_url":"https://avatars.githubusercontent.com/u/1545663?v=4","gravatar_id":"","url":"https://api.github.com/users/stevenzwu","html_url":"https://github.com/stevenzwu","followers_url":"https://api.github.com/users/stevenzwu/followers","following_url":"https://api.github.com/users/stevenzwu/following{/other_user}","gists_url":"https://api.github.com/users/stevenzwu/gists{/gist_id}","starred_url":"https://api.github.com/users/stevenzwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/stevenzwu/subscriptions","organizations_url":"https://api.github.com/users/stevenzwu/orgs","repos_url":"https://api.github.com/users/stevenzwu/repos","events_url":"https://api.github.com/users/stevenzwu/events{/privacy}","received_events_url":"https://api.github.com/users/stevenzwu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T19:13:41Z","updated_at":"2023-12-07T19:13:41Z","author_association":"CONTRIBUTOR","body":"thanks @rodmeneses for the contribution and @pvary for following up with the Flink community","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845955767/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":1,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845978489","html_url":"https://github.com/apache/iceberg/issues/7111#issuecomment-1845978489","issue_url":"https://api.github.com/repos/apache/iceberg/issues/7111","id":1845978489,"node_id":"IC_kwDOCW7NX85uB2V5","user":{"login":"jackye1995","id":29823233,"node_id":"MDQ6VXNlcjI5ODIzMjMz","avatar_url":"https://avatars.githubusercontent.com/u/29823233?v=4","gravatar_id":"","url":"https://api.github.com/users/jackye1995","html_url":"https://github.com/jackye1995","followers_url":"https://api.github.com/users/jackye1995/followers","following_url":"https://api.github.com/users/jackye1995/following{/other_user}","gists_url":"https://api.github.com/users/jackye1995/gists{/gist_id}","starred_url":"https://api.github.com/users/jackye1995/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jackye1995/subscriptions","organizations_url":"https://api.github.com/users/jackye1995/orgs","repos_url":"https://api.github.com/users/jackye1995/repos","events_url":"https://api.github.com/users/jackye1995/events{/privacy}","received_events_url":"https://api.github.com/users/jackye1995/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T19:30:56Z","updated_at":"2023-12-07T19:30:56Z","author_association":"CONTRIBUTOR","body":"Related devlist discussion: https://lists.apache.org/thread/9swm4jo8jjlj5q2jxct5cwo4yv2m8tcd","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845978489/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845981450","html_url":"https://github.com/apache/iceberg/pull/8398#issuecomment-1845981450","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8398","id":1845981450,"node_id":"IC_kwDOCW7NX85uB3EK","user":{"login":"jackye1995","id":29823233,"node_id":"MDQ6VXNlcjI5ODIzMjMz","avatar_url":"https://avatars.githubusercontent.com/u/29823233?v=4","gravatar_id":"","url":"https://api.github.com/users/jackye1995","html_url":"https://github.com/jackye1995","followers_url":"https://api.github.com/users/jackye1995/followers","following_url":"https://api.github.com/users/jackye1995/following{/other_user}","gists_url":"https://api.github.com/users/jackye1995/gists{/gist_id}","starred_url":"https://api.github.com/users/jackye1995/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jackye1995/subscriptions","organizations_url":"https://api.github.com/users/jackye1995/orgs","repos_url":"https://api.github.com/users/jackye1995/repos","events_url":"https://api.github.com/users/jackye1995/events{/privacy}","received_events_url":"https://api.github.com/users/jackye1995/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T19:33:12Z","updated_at":"2023-12-07T19:33:12Z","author_association":"CONTRIBUTOR","body":"CI seems to be passing now after retry. Given the fact that this issue was breaking CI, I will go ahead to merge it directly. Thanks for the fix! ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845981450/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845986735","html_url":"https://github.com/apache/iceberg/issues/9030#issuecomment-1845986735","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9030","id":1845986735,"node_id":"IC_kwDOCW7NX85uB4Wv","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T19:37:21Z","updated_at":"2023-12-07T21:06:45Z","author_association":"NONE","body":"@whymed \r\n@zhangbutao \r\n@zyx199693 \r\n\r\nHello,\r\n\r\nAlmost I have the same problem when I tried to execute the query: spark.sql(\"CREATE TABLE catalog_hive.iceberg_db.table (name STRING) USING iceberg;\").show()\r\n**I get this error:**\r\n\r\n23/12/07 19:08:15 WARN metastore: Failed to connect to the MetaStore Server...\r\n23/12/07 19:08:16 WARN metastore: Failed to connect to the MetaStore Server...\r\n23/12/07 19:08:17 WARN metastore: Failed to connect to the MetaStore Server...\r\n\r\n---------------------------------------------------------------------------\r\nPy4JJavaError                             Traceback (most recent call last)\r\nCell In[5], line 39\r\n     37 spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg_db;\")\r\n     38 ## Create a Table\r\n---> 39 spark.sql(\"CREATE TABLE catalog_hive.iceberg_db.table (name STRING) USING iceberg;\").show()\r\n     40 ## Insert Some Data\r\n     41 spark.sql(\"INSERT INTO catalog_hive.iceberg_db.table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n   1032     sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n   1033 try:\r\n-> 1034     return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n   1035 finally:\r\n   1036     if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args)\r\n   1315 command = proto.CALL_COMMAND_NAME +\\\r\n   1316     self.command_header +\\\r\n   1317     args_command +\\\r\n   1318     proto.END_COMMAND_PART\r\n   1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n   1322     answer, self.gateway_client, self.target_id, self.name)\r\n   1324 for temp_arg in temp_args:\r\n   1325     temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception.<locals>.deco(*a, **kw)\r\n    188 def deco(*a: Any, **kw: Any) -> Any:\r\n    189     try:\r\n--> 190         return f(*a, **kw)\r\n    191     except Py4JJavaError as e:\r\n    192         converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n    325 if answer[1] == REFERENCE_TYPE:\r\n--> 326     raise Py4JJavaError(\r\n    327         \"An error occurred while calling {0}{1}{2}.\\n\".\r\n    328         format(target_id, \".\", name), value)\r\n    329 else:\r\n    330     raise Py4JError(\r\n    331         \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n    332         format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o47.sql.\r\n: org.apache.iceberg.hive.RuntimeMetaException: Failed to connect to Hive Metastore\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:84)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\n\tat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\n\tat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\n\tat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\n\t... 63 more\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\n\t... 75 more\r\nCaused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: nessie\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\n\tat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\n\tat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\n\tat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.UnknownHostException: nessie\r\n\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\r\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\n\tat java.base/java.net.Socket.connect(Socket.java:609)\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\r\n\t... 82 more\r\n)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:527)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\r\n\t... 80 more\r\n\r\n\r\n\r\nDid you solve the problem ?\r\nPlease can you give me an idea to solve this problem ?\r\nthanks in advance.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1845986735/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846120664","html_url":"https://github.com/apache/iceberg/issues/3131#issuecomment-1846120664","issue_url":"https://api.github.com/repos/apache/iceberg/issues/3131","id":1846120664,"node_id":"IC_kwDOCW7NX85uCZDY","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T21:13:04Z","updated_at":"2023-12-07T21:25:18Z","author_association":"NONE","body":"@jackye1995 \r\n@qq240035000 \r\n@bvinayakumar \r\nHello,\r\nFirst of all, I'm sorry for any inconvenience.\r\n\r\nAlmost I have the same problem when I tried to execute this script:\r\n\r\nimport os\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\n\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\", \"OQm04sIzCakGYugOqBOV\")\r\nAWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\", \"PPlNPddfFWnUdxWdKq5BKoNfkjuRz8fjCQLi4b4I\")\r\nAWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\", \"http://minioserver:9000\")\r\n\r\nconf = (\r\n    pyspark.SparkConf()\r\n    .setAppName(\"app_name\")\r\n    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178\")\r\n    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\r\n    .set(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\r\n    .set(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\r\n    .set(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\r\n    .set(\"spark.sql.catalog.iceberg.warehouse\", WAREHOUSE)\r\n    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n    .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\r\n    .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\r\n    .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\r\n    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n\r\n)\r\n\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\nspark.sql(\"CREATE TABLE iceberg.tab (name string) USING iceberg;\")\r\n\r\n\r\n\r\n---------------------------------------------------------------------------\r\nPy4JJavaError                             Traceback (most recent call last)\r\nCell In[4], line 2\r\n      1 ## Run a Query to create a table\r\n----> 2 spark.sql(\"CREATE TABLE iceberg.tab (name string) USING iceberg;\")\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n   1032     sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n   1033 try:\r\n-> 1034     return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n   1035 finally:\r\n   1036     if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args)\r\n   1315 command = proto.CALL_COMMAND_NAME +\\\r\n   1316     self.command_header +\\\r\n   1317     args_command +\\\r\n   1318     proto.END_COMMAND_PART\r\n   1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n   1322     answer, self.gateway_client, self.target_id, self.name)\r\n   1324 for temp_arg in temp_args:\r\n   1325     temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception.<locals>.deco(*a, **kw)\r\n    188 def deco(*a: Any, **kw: Any) -> Any:\r\n    189     try:\r\n--> 190         return f(*a, **kw)\r\n    191     except Py4JJavaError as e:\r\n    192         converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n    325 if answer[1] == REFERENCE_TYPE:\r\n--> 326     raise Py4JJavaError(\r\n    327         \"An error occurred while calling {0}{1}{2}.\\n\".\r\n    328         format(target_id, \".\", name), value)\r\n    329 else:\r\n    330     raise Py4JError(\r\n    331         \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n    332         format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: XBB5TDMY7SQWVSJF, Extended Request ID: gURxVAOKwZtlNuTuUULrgbShWeSzXJLlMScCRqbjMWnfy5/Rsux0sDbXXn/DJO0jh7KLo23Emuw=)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)\r\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:95)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:245)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\r\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:167)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:175)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)\r\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)\r\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:5438)\r\n\tat org.apache.iceberg.aws.s3.BaseS3File.getObjectMetadata(BaseS3File.java:85)\r\n\tat org.apache.iceberg.aws.s3.BaseS3File.exists(BaseS3File.java:70)\r\n\tat org.apache.iceberg.aws.s3.S3OutputFile.exists(S3OutputFile.java:33)\r\n\tat org.apache.iceberg.aws.s3.S3OutputFile.create(S3OutputFile.java:57)\r\n\tat org.apache.iceberg.TableMetadataParser.internalWrite(TableMetadataParser.java:120)\r\n\tat org.apache.iceberg.TableMetadataParser.write(TableMetadataParser.java:114)\r\n\tat org.apache.iceberg.hadoop.HadoopTableOperations.commit(HadoopTableOperations.java:155)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:174)\r\n\tat org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\r\n\tat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:192)\r\n\tat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\r\n\r\nDid you solve the problem ?\r\nPlease can you give me an idea to solve this problem ?\r\nThanks in advance.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846120664/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846146418","html_url":"https://github.com/apache/iceberg/pull/9242#issuecomment-1846146418","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9242","id":1846146418,"node_id":"IC_kwDOCW7NX85uCfVy","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-07T21:34:50Z","updated_at":"2023-12-07T21:34:50Z","author_association":"MEMBER","body":"Thanks @puchengy for the pr!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846146418/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846330182","html_url":"https://github.com/apache/iceberg/issues/9030#issuecomment-1846330182","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9030","id":1846330182,"node_id":"IC_kwDOCW7NX85uDMNG","user":{"login":"whymed","id":6046226,"node_id":"MDQ6VXNlcjYwNDYyMjY=","avatar_url":"https://avatars.githubusercontent.com/u/6046226?v=4","gravatar_id":"","url":"https://api.github.com/users/whymed","html_url":"https://github.com/whymed","followers_url":"https://api.github.com/users/whymed/followers","following_url":"https://api.github.com/users/whymed/following{/other_user}","gists_url":"https://api.github.com/users/whymed/gists{/gist_id}","starred_url":"https://api.github.com/users/whymed/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/whymed/subscriptions","organizations_url":"https://api.github.com/users/whymed/orgs","repos_url":"https://api.github.com/users/whymed/repos","events_url":"https://api.github.com/users/whymed/events{/privacy}","received_events_url":"https://api.github.com/users/whymed/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T00:50:01Z","updated_at":"2023-12-08T00:50:46Z","author_association":"NONE","body":"Hi, I was not able to fix the issue...\r\nI ended ditching hive/iceberg and doing ETL using R/sparklyr/spark and L directly on the same source DB AWS RDS instance but in a diferent DBs from the transactional one.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846330182/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846456239","html_url":"https://github.com/apache/iceberg/issues/9229#issuecomment-1846456239","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9229","id":1846456239,"node_id":"IC_kwDOCW7NX85uDq-v","user":{"login":"yeunghl-shoalter","id":105259054,"node_id":"U_kgDOBkYgLg","avatar_url":"https://avatars.githubusercontent.com/u/105259054?v=4","gravatar_id":"","url":"https://api.github.com/users/yeunghl-shoalter","html_url":"https://github.com/yeunghl-shoalter","followers_url":"https://api.github.com/users/yeunghl-shoalter/followers","following_url":"https://api.github.com/users/yeunghl-shoalter/following{/other_user}","gists_url":"https://api.github.com/users/yeunghl-shoalter/gists{/gist_id}","starred_url":"https://api.github.com/users/yeunghl-shoalter/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yeunghl-shoalter/subscriptions","organizations_url":"https://api.github.com/users/yeunghl-shoalter/orgs","repos_url":"https://api.github.com/users/yeunghl-shoalter/repos","events_url":"https://api.github.com/users/yeunghl-shoalter/events{/privacy}","received_events_url":"https://api.github.com/users/yeunghl-shoalter/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T02:24:53Z","updated_at":"2023-12-08T02:24:53Z","author_association":"NONE","body":"It is found that Flink CDC MySQL can only provide event with source operation timestamp in precision of second. This is not an issue of converting Flink data into Iceberg data.\r\n```\r\n    \"source\": {\r\n      \"version\": \"1.9.7.Final\",\r\n      \"connector\": \"mysql\",\r\n      \"name\": \"mysql_binlog_source\",\r\n      \"ts_ms\": 1701938009000,\r\n      \"snapshot\": \"false\",\r\n      \"db\": \"inventory\",\r\n      \"sequence\": null,\r\n      \"table\": \"customers\",\r\n      \"server_id\": 223344,\r\n      \"gtid\": null,\r\n      \"file\": \"mysql-bin.000003\",\r\n      \"pos\": 4014,\r\n      \"row\": 0,\r\n      \"thread\": 14,\r\n      \"query\": null\r\n    }\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846456239/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846628756","html_url":"https://github.com/apache/iceberg/issues/8953#issuecomment-1846628756","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8953","id":1846628756,"node_id":"IC_kwDOCW7NX85uEVGU","user":{"login":"amogh-jahagirdar","id":87500546,"node_id":"MDQ6VXNlcjg3NTAwNTQ2","avatar_url":"https://avatars.githubusercontent.com/u/87500546?v=4","gravatar_id":"","url":"https://api.github.com/users/amogh-jahagirdar","html_url":"https://github.com/amogh-jahagirdar","followers_url":"https://api.github.com/users/amogh-jahagirdar/followers","following_url":"https://api.github.com/users/amogh-jahagirdar/following{/other_user}","gists_url":"https://api.github.com/users/amogh-jahagirdar/gists{/gist_id}","starred_url":"https://api.github.com/users/amogh-jahagirdar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amogh-jahagirdar/subscriptions","organizations_url":"https://api.github.com/users/amogh-jahagirdar/orgs","repos_url":"https://api.github.com/users/amogh-jahagirdar/repos","events_url":"https://api.github.com/users/amogh-jahagirdar/events{/privacy}","received_events_url":"https://api.github.com/users/amogh-jahagirdar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T06:44:12Z","updated_at":"2023-12-08T06:45:06Z","author_association":"CONTRIBUTOR","body":"Ok I actually looked at the history of these changes now https://github.com/apache/iceberg/pull/5214 was never merged but followed by https://github.com/apache/iceberg/pull/6569/files which actually applied the change and would've been released in 1.2.0. I think your suspicion is correct @github-raphael-douyere \r\n\r\nThe goal for including the query ID looks to be to identify which spark job actually performed the write; previously there would've been a new UUID per write, and we would've avoided files stepping on each other. \r\n\r\nLet me try and get a reproducible example,  (we would want one anyways for verifying whatever fix we do actually works) ideally we can get the best of both worlds. I think to do that some combination of the query ID + the hostname + the thread ID would be truly unique and enable better debugging (at the cost of a really long filename :) ).","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846628756/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846634916","html_url":"https://github.com/apache/iceberg/issues/8953#issuecomment-1846634916","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8953","id":1846634916,"node_id":"IC_kwDOCW7NX85uEWmk","user":{"login":"amogh-jahagirdar","id":87500546,"node_id":"MDQ6VXNlcjg3NTAwNTQ2","avatar_url":"https://avatars.githubusercontent.com/u/87500546?v=4","gravatar_id":"","url":"https://api.github.com/users/amogh-jahagirdar","html_url":"https://github.com/amogh-jahagirdar","followers_url":"https://api.github.com/users/amogh-jahagirdar/followers","following_url":"https://api.github.com/users/amogh-jahagirdar/following{/other_user}","gists_url":"https://api.github.com/users/amogh-jahagirdar/gists{/gist_id}","starred_url":"https://api.github.com/users/amogh-jahagirdar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amogh-jahagirdar/subscriptions","organizations_url":"https://api.github.com/users/amogh-jahagirdar/orgs","repos_url":"https://api.github.com/users/amogh-jahagirdar/repos","events_url":"https://api.github.com/users/amogh-jahagirdar/events{/privacy}","received_events_url":"https://api.github.com/users/amogh-jahagirdar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T06:52:26Z","updated_at":"2023-12-08T18:45:12Z","author_association":"CONTRIBUTOR","body":"I also want to verify why fileCount doesn't really cover the uniqueness right now, the only other way would be if it's in a different thread (and both threads just end up having a file count of 1, as in the example) or if there's another way a writer is created per task + partition ID.\r\n\r\n\r\nWe also aren't considering the epochId when constructing the streaing writer. That seems a bit suspicious to me since I think it's possible that across epochs there would be the same task + partition ID. \r\n\r\nWe also could just add another random UUID after the query ID as @github-raphael-douyere  mentioned. That may be the most practical solution to be defensive against any assumptions on Spark behavior.\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846634916/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846738796","html_url":"https://github.com/apache/iceberg/issues/9030#issuecomment-1846738796","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9030","id":1846738796,"node_id":"IC_kwDOCW7NX85uEv9s","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T08:11:33Z","updated_at":"2023-12-08T08:11:33Z","author_association":"NONE","body":"@whymed \r\nHi,\r\nThank you so much.\r\ncan you please explain this part more to me ?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846738796/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846764267","html_url":"https://github.com/apache/iceberg/pull/9233#issuecomment-1846764267","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9233","id":1846764267,"node_id":"IC_kwDOCW7NX85uE2Lr","user":{"login":"tmnd1991","id":7031242,"node_id":"MDQ6VXNlcjcwMzEyNDI=","avatar_url":"https://avatars.githubusercontent.com/u/7031242?v=4","gravatar_id":"","url":"https://api.github.com/users/tmnd1991","html_url":"https://github.com/tmnd1991","followers_url":"https://api.github.com/users/tmnd1991/followers","following_url":"https://api.github.com/users/tmnd1991/following{/other_user}","gists_url":"https://api.github.com/users/tmnd1991/gists{/gist_id}","starred_url":"https://api.github.com/users/tmnd1991/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmnd1991/subscriptions","organizations_url":"https://api.github.com/users/tmnd1991/orgs","repos_url":"https://api.github.com/users/tmnd1991/repos","events_url":"https://api.github.com/users/tmnd1991/events{/privacy}","received_events_url":"https://api.github.com/users/tmnd1991/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T08:32:02Z","updated_at":"2023-12-08T08:32:02Z","author_association":"NONE","body":"> How do you determine that the SystemFunctions are not pushed down?\r\n> \r\n> Spark will push down predicate(which includes predicates containing system functions) through join(except for full outer join), see: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala#L1912 . So I don't think you need to handle joins specifically in ReplaceStaticInvoke.\r\n\r\nThanks @advancedxy , now, that explains a lot of what I was observing happening in my project.\r\nDuring a MERGE (which is 2 joins, one LeftSemi + one FullOuter) I was observing that the first join was correctly pruning the partitions, while the secondo one, was not. Adding this patch though helps pruning more partitions, this is because the batch scan on the target table cannot prune partitions because the file names (collected as a result of the first join) are not known when performing physical planning. I think we should limit the replacement to the \"full outer\" case, what do you think?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846764267/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846842066","html_url":"https://github.com/apache/iceberg/pull/9165#issuecomment-1846842066","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9165","id":1846842066,"node_id":"IC_kwDOCW7NX85uFJLS","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T09:22:45Z","updated_at":"2023-12-08T09:22:45Z","author_association":"CONTRIBUTOR","body":"Thanks for reviewing, @flyrain!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846842066/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846867489","html_url":"https://github.com/apache/iceberg/pull/9250#issuecomment-1846867489","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9250","id":1846867489,"node_id":"IC_kwDOCW7NX85uFPYh","user":{"login":"bknbkn","id":67318028,"node_id":"MDQ6VXNlcjY3MzE4MDI4","avatar_url":"https://avatars.githubusercontent.com/u/67318028?v=4","gravatar_id":"","url":"https://api.github.com/users/bknbkn","html_url":"https://github.com/bknbkn","followers_url":"https://api.github.com/users/bknbkn/followers","following_url":"https://api.github.com/users/bknbkn/following{/other_user}","gists_url":"https://api.github.com/users/bknbkn/gists{/gist_id}","starred_url":"https://api.github.com/users/bknbkn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bknbkn/subscriptions","organizations_url":"https://api.github.com/users/bknbkn/orgs","repos_url":"https://api.github.com/users/bknbkn/repos","events_url":"https://api.github.com/users/bknbkn/events{/privacy}","received_events_url":"https://api.github.com/users/bknbkn/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T09:43:20Z","updated_at":"2023-12-08T09:43:20Z","author_association":"CONTRIBUTOR","body":"cc @rdblue @ajantha-bhat ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846867489/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846875009","html_url":"https://github.com/apache/iceberg/issues/9064#issuecomment-1846875009","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9064","id":1846875009,"node_id":"IC_kwDOCW7NX85uFROB","user":{"login":"zhongyujiang","id":42907416,"node_id":"MDQ6VXNlcjQyOTA3NDE2","avatar_url":"https://avatars.githubusercontent.com/u/42907416?v=4","gravatar_id":"","url":"https://api.github.com/users/zhongyujiang","html_url":"https://github.com/zhongyujiang","followers_url":"https://api.github.com/users/zhongyujiang/followers","following_url":"https://api.github.com/users/zhongyujiang/following{/other_user}","gists_url":"https://api.github.com/users/zhongyujiang/gists{/gist_id}","starred_url":"https://api.github.com/users/zhongyujiang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhongyujiang/subscriptions","organizations_url":"https://api.github.com/users/zhongyujiang/orgs","repos_url":"https://api.github.com/users/zhongyujiang/repos","events_url":"https://api.github.com/users/zhongyujiang/events{/privacy}","received_events_url":"https://api.github.com/users/zhongyujiang/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T09:49:04Z","updated_at":"2023-12-08T09:49:04Z","author_association":"CONTRIBUTOR","body":"Hi @danielcweeks, thank you for opening this. We used to have this need in real use cases, so I think this can be really helpful! \r\nIn addition, we have also encountered cases where we need to convert a column from nullable to not-null (the column contains no nulls). So I wonder whether we can also consider supporting promote a column from nullable to not-null when we can make sure that the column has no nulls?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846875009/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846896443","html_url":"https://github.com/apache/iceberg/pull/8909#issuecomment-1846896443","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8909","id":1846896443,"node_id":"IC_kwDOCW7NX85uFWc7","user":{"login":"ajantha-bhat","id":5889404,"node_id":"MDQ6VXNlcjU4ODk0MDQ=","avatar_url":"https://avatars.githubusercontent.com/u/5889404?v=4","gravatar_id":"","url":"https://api.github.com/users/ajantha-bhat","html_url":"https://github.com/ajantha-bhat","followers_url":"https://api.github.com/users/ajantha-bhat/followers","following_url":"https://api.github.com/users/ajantha-bhat/following{/other_user}","gists_url":"https://api.github.com/users/ajantha-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/ajantha-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ajantha-bhat/subscriptions","organizations_url":"https://api.github.com/users/ajantha-bhat/orgs","repos_url":"https://api.github.com/users/ajantha-bhat/repos","events_url":"https://api.github.com/users/ajantha-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/ajantha-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T10:04:47Z","updated_at":"2023-12-08T10:04:47Z","author_association":"MEMBER","body":"@nastra: I have addressed the comments. Thanks for the review.  ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846896443/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846903866","html_url":"https://github.com/apache/iceberg/pull/9150#issuecomment-1846903866","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9150","id":1846903866,"node_id":"IC_kwDOCW7NX85uFYQ6","user":{"login":"nastra","id":271029,"node_id":"MDQ6VXNlcjI3MTAyOQ==","avatar_url":"https://avatars.githubusercontent.com/u/271029?v=4","gravatar_id":"","url":"https://api.github.com/users/nastra","html_url":"https://github.com/nastra","followers_url":"https://api.github.com/users/nastra/followers","following_url":"https://api.github.com/users/nastra/following{/other_user}","gists_url":"https://api.github.com/users/nastra/gists{/gist_id}","starred_url":"https://api.github.com/users/nastra/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nastra/subscriptions","organizations_url":"https://api.github.com/users/nastra/orgs","repos_url":"https://api.github.com/users/nastra/repos","events_url":"https://api.github.com/users/nastra/events{/privacy}","received_events_url":"https://api.github.com/users/nastra/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T10:10:06Z","updated_at":"2023-12-08T10:10:06Z","author_association":"CONTRIBUTOR","body":"@gabrywu can you fix CI failures please?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846903866/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846909628","html_url":"https://github.com/apache/iceberg/pull/9233#issuecomment-1846909628","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9233","id":1846909628,"node_id":"IC_kwDOCW7NX85uFZq8","user":{"login":"advancedxy","id":807537,"node_id":"MDQ6VXNlcjgwNzUzNw==","avatar_url":"https://avatars.githubusercontent.com/u/807537?v=4","gravatar_id":"","url":"https://api.github.com/users/advancedxy","html_url":"https://github.com/advancedxy","followers_url":"https://api.github.com/users/advancedxy/followers","following_url":"https://api.github.com/users/advancedxy/following{/other_user}","gists_url":"https://api.github.com/users/advancedxy/gists{/gist_id}","starred_url":"https://api.github.com/users/advancedxy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/advancedxy/subscriptions","organizations_url":"https://api.github.com/users/advancedxy/orgs","repos_url":"https://api.github.com/users/advancedxy/repos","events_url":"https://api.github.com/users/advancedxy/events{/privacy}","received_events_url":"https://api.github.com/users/advancedxy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T10:14:43Z","updated_at":"2023-12-08T10:14:43Z","author_association":"CONTRIBUTOR","body":"> Adding this patch though helps pruning more partitions, this is because the batch scan on the target table cannot prune partitions because the file names (collected as a result of the first join) are not known when performing physical planning. I think we should limit the replacement to the \"full outer\" case, what do you think?\r\n\r\nCould you elaborate a bit more? the planning tree string/dag of Spark SQL would be helpful. \r\nIf the join type is full outer, the predicate could not be pushed down, therefore the partition pruning is unlikely to be performed. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846909628/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846982769","html_url":"https://github.com/apache/iceberg/pull/9192#issuecomment-1846982769","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9192","id":1846982769,"node_id":"IC_kwDOCW7NX85uFrhx","user":{"login":"tmnd1991","id":7031242,"node_id":"MDQ6VXNlcjcwMzEyNDI=","avatar_url":"https://avatars.githubusercontent.com/u/7031242?v=4","gravatar_id":"","url":"https://api.github.com/users/tmnd1991","html_url":"https://github.com/tmnd1991","followers_url":"https://api.github.com/users/tmnd1991/followers","following_url":"https://api.github.com/users/tmnd1991/following{/other_user}","gists_url":"https://api.github.com/users/tmnd1991/gists{/gist_id}","starred_url":"https://api.github.com/users/tmnd1991/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmnd1991/subscriptions","organizations_url":"https://api.github.com/users/tmnd1991/orgs","repos_url":"https://api.github.com/users/tmnd1991/repos","events_url":"https://api.github.com/users/tmnd1991/events{/privacy}","received_events_url":"https://api.github.com/users/tmnd1991/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T11:14:06Z","updated_at":"2023-12-08T11:14:06Z","author_association":"NONE","body":"Thanks for the review, I addressed your concerns. If I get green light I proceed to copy-paste over 3.4","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1846982769/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847038152","html_url":"https://github.com/apache/iceberg/issues/2359#issuecomment-1847038152","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2359","id":1847038152,"node_id":"IC_kwDOCW7NX85uF5DI","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T11:44:01Z","updated_at":"2023-12-08T11:44:01Z","author_association":"NONE","body":" @coolderli \r\n@RussellSpitzer \r\n@hunter-cloud09 \r\n@dixingxing0 \r\n@pvary \r\n\r\nHello everyone.\r\n\r\nI am using Hive Catalog to create Iceberg tables with Spark as the execution engine:\r\n\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName('app_name')\r\n#packages\r\n.set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\r\n#SQL Extensions\r\n.set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\r\n#Configuring Catalog\r\n.set('spark.sql.catalog.catalog_hive', 'org.apache.iceberg.spark.SparkCatalog')\r\n.set('spark.sql.catalog.catalog_hive.type', 'hive')\r\n.set('spark.sql.catalog.catalog_hive.uri', HIVE_URI)\r\n.set('spark.sql.catalog.catalog_hive.warehouse', WAREHOUSE)\r\n.set('spark.sql.catalog.catalog_hive.endpoint', AWS_S3_ENDPOINT)\r\n.set('spark.sql.catalog.catalog_hive.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\r\n.set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\r\n.set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\r\n)\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\nspark.sql(\"CREATE TABLE catalog_hive.default.table (name STRING) USING iceberg;\").show()\r\n\r\nWhen I try to run createTable command it gives me an exception:\r\n\r\nSpark Running\r\n23/12/07 16:47:16 WARN metastore: Failed to connect to the MetaStore Server...\r\n23/12/07 16:47:17 WARN metastore: Failed to connect to the MetaStore Server...\r\n23/12/07 16:47:18 WARN metastore: Failed to connect to the MetaStore Server...\r\n\r\nPy4JJavaError Traceback (most recent call last)\r\nCell In[2], line 36\r\n34 print(\"Spark Running\")\r\n35 ## Create a Table\r\n---> 36 spark.sql(\"CREATE TABLE catalog_hive.default.table (name STRING) USING iceberg;\").show()\r\n37 ## Insert Some Data\r\n38 spark.sql(\"INSERT INTO catalog_hive.default.table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n1032 sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n1033 try:\r\n-> 1034 return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n1035 finally:\r\n1036 if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.call(self, *args)\r\n1315 command = proto.CALL_COMMAND_NAME +\r\n1316 self.command_header +\r\n1317 args_command +\r\n1318 proto.END_COMMAND_PART\r\n1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n1322 answer, self.gateway_client, self.target_id, self.name)\r\n1324 for temp_arg in temp_args:\r\n1325 temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception..deco(*a, **kw)\r\n188 def deco(*a: Any, **kw: Any) -> Any:\r\n189 try:\r\n--> 190 return f(*a, **kw)\r\n191 except Py4JJavaError as e:\r\n192 converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n325 if answer[1] == REFERENCE_TYPE:\r\n--> 326 raise Py4JJavaError(\r\n327 \"An error occurred while calling {0}{1}{2}.\\n\".\r\n328 format(target_id, \".\", name), value)\r\n329 else:\r\n330 raise Py4JError(\r\n331 \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n332 format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o47.sql.\r\n: org.apache.iceberg.hive.RuntimeMetaException: Failed to connect to Hive Metastore\r\nat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:84)\r\nat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\nat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\nat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\nat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\nat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\nat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\nat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\nat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\nat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\nat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\nat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\nat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\nat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\nat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\nat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\nat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\nat org.apache.spark.sql.Dataset.(Dataset.scala:220)\r\nat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\nat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nat py4j.Gateway.invoke(Gateway.java:282)\r\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\nat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\r\nat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.(RetryingMetaStoreClient.java:83)\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\nat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\nat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\nat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\nat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\n... 63 more\r\nCaused by: java.lang.reflect.InvocationTargetException\r\nat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\nat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\n... 75 more\r\nCaused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: nessie\r\nat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\r\nat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)\r\nat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.(HiveMetaStoreClient.java:245)\r\nat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\nat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\nat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.(RetryingMetaStoreClient.java:83)\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\nat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\nat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\nat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\nat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\nat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\nat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\nat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\nat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\nat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\nat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\nat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\nat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\nat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\nat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\nat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\nat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\nat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\nat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\nat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\nat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\nat org.apache.spark.sql.Dataset.(Dataset.scala:220)\r\nat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\nat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nat py4j.Gateway.invoke(Gateway.java:282)\r\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\nat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.UnknownHostException: nessie\r\nat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\r\nat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\nat java.base/java.net.Socket.connect(Socket.java:609)\r\nat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\r\n... 82 more\r\n)\r\nat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:527)\r\nat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.(HiveMetaStoreClient.java:245)\r\n... 80 more\r\n\r\nPlease let me know what I might be doing wrong.\r\nThanks in advance.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847038152/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847058041","html_url":"https://github.com/apache/iceberg/issues/2359#issuecomment-1847058041","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2359","id":1847058041,"node_id":"IC_kwDOCW7NX85uF955","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T12:01:51Z","updated_at":"2023-12-08T12:02:08Z","author_association":"MEMBER","body":"The error says you can't connect to your metastore, so either you have the wrong address, it's not running, or you can't reach it from the where spark is running ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847058041/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847109573","html_url":"https://github.com/apache/iceberg/issues/2359#issuecomment-1847109573","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2359","id":1847109573,"node_id":"IC_kwDOCW7NX85uGKfF","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T12:47:14Z","updated_at":"2023-12-08T12:47:14Z","author_association":"NONE","body":"@RussellSpitzer \r\nfirst of all thank you very much for your answer\r\nas shown in the screenshots below:\r\n\r\n1. I configured hive metastore with the address: thrift://hive-metastore:9083\r\n2. hive metastore service is running\r\n3. and for the connection with spark I defined the .env file which contains the hive metastore address\r\n\r\n![image](https://github.com/apache/iceberg/assets/149940691/f7107f07-5a7d-4b5e-896a-cd19581daf80)\r\n\r\n\r\n![image](https://github.com/apache/iceberg/assets/149940691/2b4bd4f4-ec0f-4536-8371-49baa60e175c)\r\n\r\n\r\n\r\n![image](https://github.com/apache/iceberg/assets/149940691/778c82a8-fee2-4c08-a7da-5cea505a6235)\r\n\r\nIs there any configuration missing or to be added in the script?\r\n\r\nSincerely,\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847109573/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847179494","html_url":"https://github.com/apache/iceberg/pull/9233#issuecomment-1847179494","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9233","id":1847179494,"node_id":"IC_kwDOCW7NX85uGbjm","user":{"login":"tmnd1991","id":7031242,"node_id":"MDQ6VXNlcjcwMzEyNDI=","avatar_url":"https://avatars.githubusercontent.com/u/7031242?v=4","gravatar_id":"","url":"https://api.github.com/users/tmnd1991","html_url":"https://github.com/tmnd1991","followers_url":"https://api.github.com/users/tmnd1991/followers","following_url":"https://api.github.com/users/tmnd1991/following{/other_user}","gists_url":"https://api.github.com/users/tmnd1991/gists{/gist_id}","starred_url":"https://api.github.com/users/tmnd1991/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmnd1991/subscriptions","organizations_url":"https://api.github.com/users/tmnd1991/orgs","repos_url":"https://api.github.com/users/tmnd1991/repos","events_url":"https://api.github.com/users/tmnd1991/events{/privacy}","received_events_url":"https://api.github.com/users/tmnd1991/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T13:38:46Z","updated_at":"2023-12-08T13:38:46Z","author_association":"NONE","body":"Sure, let me add a bit of context:\r\nI have two table with the exact same schema/layout, partitioned on 3 columns:\r\n- identity(MEAS_YM)\r\n- identity(MEAS_DD)\r\n- bucket(POD, 4)\r\nThe source table (small one) has strictly a subset of partitions w/r/t the target table (big one).\r\nIn this example I will talk about a local reproducer but keep in mind we are talking about a 65TB table with 400k partitions, so every 1% improvement actually means a lot.\r\n\r\nI started running a merge statement as following, taking advantage of SPJ:\r\n```\r\nMERGE INTO target USING (SELECT * FROM source)\r\nON target.MEAS_YM = source.MEAS_YM AND target. MEAS_DD = source. MEAS_DD AND target.POD = source.POD\r\nWHEN MATCHED THEN UPDATE SET ...\r\n```\r\n\r\nThis results in the following physical plan:\r\n```\r\n== Physical Plan ==\r\nReplaceData (13)\r\n+- * Sort (12)\r\n   +- * Project (11)\r\n      +- MergeRows (10)\r\n         +- SortMergeJoin FullOuter (9)\r\n            :- * Sort (4)\r\n            :  +- * Project (3)\r\n            :     +- * ColumnarToRow (2)\r\n            :        +- BatchScan target (1)\r\n            +- * Sort (8)\r\n               +- * Project (7)\r\n                  +- * ColumnarToRow (6)\r\n                     +- BatchScan source (5)\r\n===== Subqueries =====\r\n\r\nSubquery:1 Hosting operator id = 1 Hosting Expression = _file#2274 IN subquery#2672\r\n* HashAggregate (26)\r\n+- Exchange (25)\r\n   +- * HashAggregate (24)\r\n      +- * Project (23)\r\n         +- * SortMergeJoin LeftSemi (22)\r\n            :- * Sort (17)\r\n            :  +- * Filter (16)\r\n            :     +- * ColumnarToRow (15)\r\n            :        +- BatchScan target (14)\r\n            +- * Sort (21)\r\n               +- * Filter (20)\r\n                  +- * ColumnarToRow (19)\r\n                     +- BatchScan source (18)\r\n```\r\n\r\nwith\r\n\r\n```\r\n(1) BatchScan target\r\nOutput [60]: [..., _file#2274]\r\ntarget (branch=null) [filters=, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(5) BatchScan source\r\nOutput [60]: [...]\r\nsource (branch=null) [filters=, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(14) BatchScan target\r\nOutput [8]: [..., _file#2590]\r\ntarget (branch=null) [filters=POD IS NOT NULL, MEAS_YM IS NOT NULL, MEAS_DD IS NOT NULL, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(18) BatchScan source\r\nOutput [7]: [...]\r\nsource (branch=null) [filters=POD IS NOT NULL, MEAS_YM IS NOT NULL, MEAS_DD IS NOT NULL, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n```\r\n\r\nThis was creating 33 (+10 to exchange the file names) tasks for the subquery and 33 tasks for the second join.\r\nPractically I know for sure that I hit only 25 partitions, not 33 (i.e. some files were still read even if we know upfront that they are not needed, also the `_file IN (subquery)` can't prune any file because it's dynamic. On top of that, I observed that even if files should've been excluded by Spark in post-scan filter, still the execution of the task was not as fast as I expected (i.e. close to 0ms)).\r\n\r\nTherefore, knowing exactly the partitions that I hit beforehand, I tried to help iceberg/spark a little enumerating the partitions values that are actually hit:\r\n\r\n```\r\nMERGE INTO target USING (SELECT * FROM source)\r\nON target.`POD` = source.`POD` AND target.`MEAS_YM` = source.`MEAS_YM` AND target.`MEAS_DD` = source.`MEAS_DD` AND (\r\n  (target.`meas_ym` = '202306' AND target.`meas_dd` = '02' AND system.bucket(4, target.`pod`) IN (0,2,3)) OR\r\n  (target.`meas_ym` = '202306' AND target.`meas_dd` = '01') OR \r\n  (target.`meas_ym` = '202307' AND target.`meas_dd` = '02' AND system.bucket(4, target.`pod`) IN (1,3)) OR \r\n  (target.`meas_ym` = '202306' AND target.`meas_dd` = '03') OR \r\n  (target.`meas_ym` = '202308' AND target.`meas_dd` = '01' AND system.bucket(4, target.`pod`) IN (0,1,2)) OR \r\n  (target.`meas_ym` = '202307' AND target.`meas_dd` = '03' AND system.bucket(4, target.`pod`) IN (0,1,2)) OR \r\n  (target.`meas_ym` = '202308' AND target.`meas_dd` = '03' AND system.bucket(4, target.`pod`) IN (0,3)) OR \r\n  (target.`meas_ym` = '202307' AND target.`meas_dd` = '01' AND system.bucket(4, target.`pod`) IN (0,1,2)) OR \r\n  (target.`meas_ym` = '202308' AND target.`meas_dd` = '02' AND system.bucket(4, target.`pod`) IN (3)))\r\nWHEN MATCHED THEN UPDATE SET ...\r\n```\r\n\r\nTo my surprise the plan was exactly the same...\r\n\r\nThen I fixed this issue and also #9191 locally (adding an optimiser to my spark session) and the scans actually changed:\r\n\r\n```\r\n(1) BatchScan target\r\nOutput [60]: [..., _file#2279]\r\ntarget (branch=null) [filters=((((MEAS_YM = '202306' AND ((MEAS_DD = '02' AND bucket[4](POD) IN (0, 2, 3)) OR MEAS_DD = '01')) OR ((MEAS_YM = '202307' AND MEAS_DD = '02') AND bucket[4](POD) IN (1, 3))) OR ((MEAS_YM = '202306' AND MEAS_DD = '03') OR ((MEAS_YM = '202308' AND MEAS_DD = '01') AND bucket[4](POD) IN (0, 1, 2)))) OR ((MEAS_DD = '03' AND ((MEAS_YM = '202307' AND bucket[4](POD) IN (0, 1, 2)) OR (MEAS_YM = '202308' AND bucket[4](POD) IN (0, 3)))) OR (((MEAS_YM = '202307' AND MEAS_DD = '01') AND bucket[4](POD) IN (0, 1, 2)) OR ((MEAS_YM = '202308' AND MEAS_DD = '02') AND bucket[4](POD) = 3)))), groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(5) BatchScan source\r\nOutput [60]: [...]\r\nsource (branch=null) [filters=, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(14) BatchScan target\r\nOutput [8]: [..., _file#2590]\r\ntarget (branch=null) [filters=((((MEAS_YM = '202306' AND ((MEAS_DD = '02' AND bucket[4](POD) IN (0, 2, 3)) OR MEAS_DD = '01')) OR ((MEAS_YM = '202307' AND MEAS_DD = '02') AND bucket[4](POD) IN (1, 3))) OR ((MEAS_YM = '202306' AND MEAS_DD = '03') OR ((MEAS_YM = '202308' AND MEAS_DD = '01') AND bucket[4](POD) IN (0, 1, 2)))) OR ((MEAS_DD = '03' AND ((MEAS_YM = '202307' AND bucket[4](POD) IN (0, 1, 2)) OR (MEAS_YM = '202308' AND bucket[4](POD) IN (0, 3)))) OR (((MEAS_YM = '202307' AND MEAS_DD = '01') AND bucket[4](POD) IN (0, 1, 2)) OR ((MEAS_YM = '202308' AND MEAS_DD = '02') AND bucket[4](POD) = 3)))), POD IS NOT NULL, MEAS_YM IS NOT NULL, MEAS_DD IS NOT NULL, MAGNITUDE IS NOT NULL, METER_KEY IS NOT NULL, REC_ID IS NOT NULL, COLLECT_ID IS NOT NULL, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(18) BatchScan source\r\nOutput [7]: [...]\r\nsource (branch=null) [filters=POD IS NOT NULL, MEAS_YM IS NOT NULL, MEAS_DD IS NOT NULL, MAGNITUDE IS NOT NULL, METER_KEY IS NOT NULL, REC_ID IS NOT NULL, COLLECT_ID IS NOT NULL, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n```\r\n\r\nWith this plan I obtain 25 (+10 of shuffle) + 25 tasks, hitting actually only the minimum number of partitions.\r\n\r\n----\r\n\r\nGiven the context, I think that I probably highlighted 2 \"bugs\":\r\n\r\n1. the fact that also the full-outer join condition can be used to prune partitions (fixed in this PR)\r\n2. for some reason spark is not able to detect correctly the minimum subset of hit partitions (maybe I can work on another PR for this, but I guess it's much harder and maybe part of Spark codebase)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847179494/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847296766","html_url":"https://github.com/apache/iceberg/issues/2359#issuecomment-1847296766","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2359","id":1847296766,"node_id":"IC_kwDOCW7NX85uG4L-","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T14:40:13Z","updated_at":"2023-12-08T14:40:13Z","author_association":"CONTRIBUTOR","body":"@ExplorData24: You might want to remove your AWS secrets from a public place...\r\n\r\nThis is the relevant part from the exception:\r\n```\r\nCaused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: nessie\r\n```\r\n\r\nIt says that it tries to connect to the HMS on the host `nessie` - maybe a catalog configuration error?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847296766/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847344053","html_url":"https://github.com/apache/iceberg/issues/2359#issuecomment-1847344053","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2359","id":1847344053,"node_id":"IC_kwDOCW7NX85uHDu1","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T15:08:32Z","updated_at":"2023-12-08T15:21:19Z","author_association":"NONE","body":"@pvary \r\nThank you very much for your answer\r\n\r\nIn fact, i am using \"catalog_hive\" Catalog to create Iceberg tables with Spark as the execution engine:\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\nimport os\r\n\r\n## DEFINE SENSITIVE VARIABLES\r\nHIVE_URI = os.environ.get(\"HIVE_URI\",\"thrift://hive-metastore:9083\") \r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\", \"OQm04sIzCakGYugOqBOV\")\r\nAWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\", \"PPlNPddfFWnUdxWdKq5BKoNfkjuRz8fjCQLi4b4I\")\r\nAWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\", \"http://minioserver:9000\")\r\n\r\nprint(AWS_S3_ENDPOINT)\r\nprint(HIVE_URI)\r\nprint(WAREHOUSE)\r\nconf = (\r\n    pyspark.SparkConf()\r\n        .setAppName('app_name')\r\n  \t\t#packages\r\n        .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\r\n  \t\t#SQL Extensions\r\n        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\r\n  \t\t#Configuring Catalog\r\n        .set('spark.sql.catalog.catalog_hive', 'org.apache.iceberg.spark.SparkCatalog')\r\n        .set('spark.sql.catalog.catalog_hive.type', 'hive')\r\n        .set('spark.sql.catalog.catalog_hive.uri', HIVE_URI)\r\n        .set('spark.sql.catalog.catalog_hive.warehouse.dir', WAREHOUSE) \r\n        .set('spark.sql.catalog.catalog_hive.endpoint', AWS_S3_ENDPOINT)\r\n        .set('spark.sql.catalog.catalog_hive.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\r\n        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\r\n        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\r\n        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n)\r\n## Start Spark Session\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n## Create a Table\r\nspark.sql(\"CREATE TABLE catalog_hive.defaul.tmy_table (name STRING) USING iceberg;\").show()\r\n## Insert Some Data\r\nspark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n## Query the Data\r\nspark.sql(\"SELECT * FROM catalog_hive.default.my_table;\").show()\r\n\r\nwhen I run the script again,i get this error:\r\n\r\n\r\nhttp://minioserver:9000/\r\nthrift://hive-metastore:9083\r\ns3a://warehouse/\r\n23/12/08 15:18:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\r\n23/12/08 15:18:34 DEBUG SparkSession: Ignored static SQL configurations:\r\n  spark.sql.catalogImplementation=hive\r\n  spark.sql.warehouse.dir=s3a://warehouse/\r\nSpark Running\r\n23/12/08 15:18:34 DEBUG SparkSqlParser: Parsing command: CREATE TABLE catalog_hive.default.table (name STRING) USING iceberg;\r\n23/12/08 15:18:34 INFO metastore: Trying to connect to metastore with URI http://nessie:19120/api/v1\r\n23/12/08 15:18:42 WARN metastore: Failed to connect to the MetaStore Server...\r\norg.apache.thrift.transport.TTransportException: java.net.UnknownHostException: nessie\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\n\tat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\n\tat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\n\tat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.UnknownHostException: nessie\r\n\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\r\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\n\tat java.base/java.net.Socket.connect(Socket.java:609)\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\r\n\t... 81 more\r\n23/12/08 15:18:42 INFO metastore: Waiting 1 seconds before next connection attempt.\r\n\r\n23/12/08 15:18:43 INFO metastore: Trying to connect to metastore with URI http://nessie:19120/api/v1\r\n23/12/08 15:18:43 WARN metastore: Failed to connect to the MetaStore Server...\r\norg.apache.thrift.transport.TTransportException: java.net.UnknownHostException: nessie\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\n\tat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\n\tat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\n\tat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.UnknownHostException: nessie\r\n\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\r\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\n\tat java.base/java.net.Socket.connect(Socket.java:609)\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\r\n\t... 81 more\r\n23/12/08 15:18:43 INFO metastore: Waiting 1 seconds before next connection attempt.\r\n23/12/08 15:18:44 INFO metastore: Trying to connect to metastore with URI http://nessie:19120/api/v1\r\n23/12/08 15:18:44 WARN metastore: Failed to connect to the MetaStore Server...\r\norg.apache.thrift.transport.TTransportException: java.net.UnknownHostException: nessie\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\n\tat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\n\tat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\n\tat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.UnknownHostException: nessie\r\n\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\r\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\n\tat java.base/java.net.Socket.connect(Socket.java:609)\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\r\n\t... 81 more\r\n23/12/08 15:18:44 INFO metastore: Waiting 1 seconds before next connection attempt.\r\n\r\n---------------------------------------------------------------------------\r\nPy4JJavaError                             Traceback (most recent call last)\r\nCell In[23], line 39\r\n     36 print(\"Spark Running\")\r\n     38 ## Create a Table\r\n---> 39 spark.sql(\"CREATE TABLE catalog_hive.default.table (name STRING) USING iceberg;\").show()\r\n     40 ## Insert Some Data\r\n     41 spark.sql(\"INSERT INTO catalog_hive.default.table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n   1032     sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n   1033 try:\r\n-> 1034     return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n   1035 finally:\r\n   1036     if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args)\r\n   1315 command = proto.CALL_COMMAND_NAME +\\\r\n   1316     self.command_header +\\\r\n   1317     args_command +\\\r\n   1318     proto.END_COMMAND_PART\r\n   1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n   1322     answer, self.gateway_client, self.target_id, self.name)\r\n   1324 for temp_arg in temp_args:\r\n   1325     temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception.<locals>.deco(*a, **kw)\r\n    188 def deco(*a: Any, **kw: Any) -> Any:\r\n    189     try:\r\n--> 190         return f(*a, **kw)\r\n    191     except Py4JJavaError as e:\r\n    192         converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n    325 if answer[1] == REFERENCE_TYPE:\r\n--> 326     raise Py4JJavaError(\r\n    327         \"An error occurred while calling {0}{1}{2}.\\n\".\r\n    328         format(target_id, \".\", name), value)\r\n    329 else:\r\n    330     raise Py4JError(\r\n    331         \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n    332         format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o47.sql.\r\n: org.apache.iceberg.hive.RuntimeMetaException: Failed to connect to Hive Metastore\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:84)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\n\tat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\n\tat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1742)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\n\tat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\n\t... 62 more\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\n\t... 74 more\r\nCaused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.UnknownHostException: nessie\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:226)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:478)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\r\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invokeChecked(DynMethods.java:60)\r\n\tat org.apache.iceberg.common.DynMethods$UnboundMethod.invoke(DynMethods.java:72)\r\n\tat org.apache.iceberg.common.DynMethods$StaticMethod.invoke(DynMethods.java:185)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:63)\r\n\tat org.apache.iceberg.hive.HiveClientPool.newClient(HiveClientPool.java:34)\r\n\tat org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56)\r\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51)\r\n\tat org.apache.iceberg.hive.CachedClientPool.run(CachedClientPool.java:82)\r\n\tat org.apache.iceberg.hive.HiveTableOperations.doRefresh(HiveTableOperations.java:205)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:95)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:78)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:43)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\r\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:587)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:142)\r\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:156)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.UnknownHostException: nessie\r\n\tat java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:229)\r\n\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\n\tat java.base/java.net.Socket.connect(Socket.java:609)\r\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:221)\r\n\t... 81 more\r\n)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:527)\r\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:245)\r\n\t... 79 more\r\n\r\n\r\nI don't know why, it to connect nessie and is not on hive metastore.\r\nCan you please check with me the causes of this error, is there any configuration missing or to be added ?\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847344053/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847363462","html_url":"https://github.com/apache/iceberg/pull/9243#issuecomment-1847363462","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9243","id":1847363462,"node_id":"IC_kwDOCW7NX85uHIeG","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T15:20:49Z","updated_at":"2023-12-08T15:20:49Z","author_association":"MEMBER","body":"Thanks for back-porting, If you can please follow up with a doc PR for the new arg","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847363462/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847450030","html_url":"https://github.com/apache/iceberg/pull/9233#issuecomment-1847450030","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9233","id":1847450030,"node_id":"IC_kwDOCW7NX85uHdmu","user":{"login":"advancedxy","id":807537,"node_id":"MDQ6VXNlcjgwNzUzNw==","avatar_url":"https://avatars.githubusercontent.com/u/807537?v=4","gravatar_id":"","url":"https://api.github.com/users/advancedxy","html_url":"https://github.com/advancedxy","followers_url":"https://api.github.com/users/advancedxy/followers","following_url":"https://api.github.com/users/advancedxy/following{/other_user}","gists_url":"https://api.github.com/users/advancedxy/gists{/gist_id}","starred_url":"https://api.github.com/users/advancedxy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/advancedxy/subscriptions","organizations_url":"https://api.github.com/users/advancedxy/orgs","repos_url":"https://api.github.com/users/advancedxy/repos","events_url":"https://api.github.com/users/advancedxy/events{/privacy}","received_events_url":"https://api.github.com/users/advancedxy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T16:04:08Z","updated_at":"2023-12-08T16:04:08Z","author_association":"CONTRIBUTOR","body":">\r\n```\r\n== Physical Plan ==\r\nReplaceData (13)\r\n+- * Sort (12)\r\n   +- * Project (11)\r\n      +- MergeRows (10)\r\n         +- SortMergeJoin FullOuter (9)  <---- Full Outer here\r\n```\r\n\r\nIf the join type is full outer, it means that there are NoMatchedActions. So your merge into command should have an `when not matched` clause, is that correct?\r\n\r\n>\r\n```(1) BatchScan target\r\nOutput [60]: [..., _file#2279]\r\ntarget (branch=null) [filters=((((MEAS_YM = '202306' AND ((MEAS_DD = '02' AND bucket[4](POD) IN (0, 2, 3)) OR MEAS_DD = '01')) OR ((MEAS_YM = '202307' AND MEAS_DD = '02') AND bucket[4](POD) IN (1, 3))) OR ((MEAS_YM = '202306' AND MEAS_DD = '03') OR ((MEAS_YM = '202308' AND MEAS_DD = '01') AND bucket[4](POD) IN (0, 1, 2)))) OR ((MEAS_DD = '03' AND ((MEAS_YM = '202307' AND bucket[4](POD) IN (0, 1, 2)) OR (MEAS_YM = '202308' AND bucket[4](POD) IN (0, 3)))) OR (((MEAS_YM = '202307' AND MEAS_DD = '01') AND bucket[4](POD) IN (0, 1, 2)) OR ((MEAS_YM = '202308' AND MEAS_DD = '02') AND bucket[4](POD) = 3)))), groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(5) BatchScan source\r\nOutput [60]: [...]\r\nsource (branch=null) [filters=, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(14) BatchScan target\r\nOutput [8]: [..., _file#2590]\r\ntarget (branch=null) [filters=((((MEAS_YM = '202306' AND ((MEAS_DD = '02' AND bucket[4](POD) IN (0, 2, 3)) OR MEAS_DD = '01')) OR ((MEAS_YM = '202307' AND MEAS_DD = '02') AND bucket[4](POD) IN (1, 3))) OR ((MEAS_YM = '202306' AND MEAS_DD = '03') OR ((MEAS_YM = '202308' AND MEAS_DD = '01') AND bucket[4](POD) IN (0, 1, 2)))) OR ((MEAS_DD = '03' AND ((MEAS_YM = '202307' AND bucket[4](POD) IN (0, 1, 2)) OR (MEAS_YM = '202308' AND bucket[4](POD) IN (0, 3)))) OR (((MEAS_YM = '202307' AND MEAS_DD = '01') AND bucket[4](POD) IN (0, 1, 2)) OR ((MEAS_YM = '202308' AND MEAS_DD = '02') AND bucket[4](POD) = 3)))), POD IS NOT NULL, MEAS_YM IS NOT NULL, MEAS_DD IS NOT NULL, MAGNITUDE IS NOT NULL, METER_KEY IS NOT NULL, REC_ID IS NOT NULL, COLLECT_ID IS NOT NULL, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n\r\n(18) BatchScan source\r\nOutput [7]: [...]\r\nsource (branch=null) [filters=POD IS NOT NULL, MEAS_YM IS NOT NULL, MEAS_DD IS NOT NULL, MAGNITUDE IS NOT NULL, METER_KEY IS NOT NULL, REC_ID IS NOT NULL, COLLECT_ID IS NOT NULL, groupedBy=MEAS_YM, MEAS_DD, POD_bucket]\r\n```\r\nCould you give the full plan tree or dag for this changed plan? Is the join type still full outer?  This is quite strange.  I'm not sure why Filter would be pushed down to the data source for a full outer join.  You may set `spark.sql.planChangeLog.level` to `INFO` to get which rule changes the plan, and posted related plan changes in a gist, that would help to clarify the problem.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847450030/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847513665","html_url":"https://github.com/apache/iceberg/pull/9233#issuecomment-1847513665","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9233","id":1847513665,"node_id":"IC_kwDOCW7NX85uHtJB","user":{"login":"tmnd1991","id":7031242,"node_id":"MDQ6VXNlcjcwMzEyNDI=","avatar_url":"https://avatars.githubusercontent.com/u/7031242?v=4","gravatar_id":"","url":"https://api.github.com/users/tmnd1991","html_url":"https://github.com/tmnd1991","followers_url":"https://api.github.com/users/tmnd1991/followers","following_url":"https://api.github.com/users/tmnd1991/following{/other_user}","gists_url":"https://api.github.com/users/tmnd1991/gists{/gist_id}","starred_url":"https://api.github.com/users/tmnd1991/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmnd1991/subscriptions","organizations_url":"https://api.github.com/users/tmnd1991/orgs","repos_url":"https://api.github.com/users/tmnd1991/repos","events_url":"https://api.github.com/users/tmnd1991/events{/privacy}","received_events_url":"https://api.github.com/users/tmnd1991/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T16:49:20Z","updated_at":"2023-12-08T16:49:20Z","author_association":"NONE","body":"yes sorry, thereâ€™s also a when not matched statement. i canâ€™t attach the plan, but iâ€™ll push a reproducer soon","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847513665/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847552754","html_url":"https://github.com/apache/iceberg/pull/9250#issuecomment-1847552754","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9250","id":1847552754,"node_id":"IC_kwDOCW7NX85uH2ry","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T17:19:03Z","updated_at":"2023-12-08T17:19:03Z","author_association":"CONTRIBUTOR","body":"Thanks, @bknbkn!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847552754/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847572861","html_url":"https://github.com/apache/iceberg/pull/8982#issuecomment-1847572861","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8982","id":1847572861,"node_id":"IC_kwDOCW7NX85uH7l9","user":{"login":"emkornfield","id":17869838,"node_id":"MDQ6VXNlcjE3ODY5ODM4","avatar_url":"https://avatars.githubusercontent.com/u/17869838?v=4","gravatar_id":"","url":"https://api.github.com/users/emkornfield","html_url":"https://github.com/emkornfield","followers_url":"https://api.github.com/users/emkornfield/followers","following_url":"https://api.github.com/users/emkornfield/following{/other_user}","gists_url":"https://api.github.com/users/emkornfield/gists{/gist_id}","starred_url":"https://api.github.com/users/emkornfield/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/emkornfield/subscriptions","organizations_url":"https://api.github.com/users/emkornfield/orgs","repos_url":"https://api.github.com/users/emkornfield/repos","events_url":"https://api.github.com/users/emkornfield/events{/privacy}","received_events_url":"https://api.github.com/users/emkornfield/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T17:36:10Z","updated_at":"2023-12-08T17:36:10Z","author_association":"CONTRIBUTOR","body":"@aokolnychyi did my changes address your feedback properly?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847572861/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847573238","html_url":"https://github.com/apache/iceberg/pull/8981#issuecomment-1847573238","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8981","id":1847573238,"node_id":"IC_kwDOCW7NX85uH7r2","user":{"login":"emkornfield","id":17869838,"node_id":"MDQ6VXNlcjE3ODY5ODM4","avatar_url":"https://avatars.githubusercontent.com/u/17869838?v=4","gravatar_id":"","url":"https://api.github.com/users/emkornfield","html_url":"https://github.com/emkornfield","followers_url":"https://api.github.com/users/emkornfield/followers","following_url":"https://api.github.com/users/emkornfield/following{/other_user}","gists_url":"https://api.github.com/users/emkornfield/gists{/gist_id}","starred_url":"https://api.github.com/users/emkornfield/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/emkornfield/subscriptions","organizations_url":"https://api.github.com/users/emkornfield/orgs","repos_url":"https://api.github.com/users/emkornfield/repos","events_url":"https://api.github.com/users/emkornfield/events{/privacy}","received_events_url":"https://api.github.com/users/emkornfield/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T17:36:32Z","updated_at":"2023-12-08T17:36:32Z","author_association":"CONTRIBUTOR","body":"@Fokko or @rdblue would you mind taking a look?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847573238/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847647392","html_url":"https://github.com/apache/iceberg/pull/9253#issuecomment-1847647392","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9253","id":1847647392,"node_id":"IC_kwDOCW7NX85uINyg","user":{"login":"puchengy","id":8072956,"node_id":"MDQ6VXNlcjgwNzI5NTY=","avatar_url":"https://avatars.githubusercontent.com/u/8072956?v=4","gravatar_id":"","url":"https://api.github.com/users/puchengy","html_url":"https://github.com/puchengy","followers_url":"https://api.github.com/users/puchengy/followers","following_url":"https://api.github.com/users/puchengy/following{/other_user}","gists_url":"https://api.github.com/users/puchengy/gists{/gist_id}","starred_url":"https://api.github.com/users/puchengy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/puchengy/subscriptions","organizations_url":"https://api.github.com/users/puchengy/orgs","repos_url":"https://api.github.com/users/puchengy/repos","events_url":"https://api.github.com/users/puchengy/events{/privacy}","received_events_url":"https://api.github.com/users/puchengy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T18:33:47Z","updated_at":"2023-12-08T18:33:47Z","author_association":"CONTRIBUTOR","body":"@RussellSpitzer Do you know how to format the doc and how to test the doc locally? I can't find the doc, Thanks.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847647392/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847830279","html_url":"https://github.com/apache/iceberg/pull/9248#issuecomment-1847830279","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9248","id":1847830279,"node_id":"IC_kwDOCW7NX85uI6cH","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T20:52:35Z","updated_at":"2023-12-08T20:52:35Z","author_association":"CONTRIBUTOR","body":"Thanks, @RussellSpitzer!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847830279/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847835239","html_url":"https://github.com/apache/iceberg/pull/9212#issuecomment-1847835239","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9212","id":1847835239,"node_id":"IC_kwDOCW7NX85uI7pn","user":{"login":"stevenzwu","id":1545663,"node_id":"MDQ6VXNlcjE1NDU2NjM=","avatar_url":"https://avatars.githubusercontent.com/u/1545663?v=4","gravatar_id":"","url":"https://api.github.com/users/stevenzwu","html_url":"https://github.com/stevenzwu","followers_url":"https://api.github.com/users/stevenzwu/followers","following_url":"https://api.github.com/users/stevenzwu/following{/other_user}","gists_url":"https://api.github.com/users/stevenzwu/gists{/gist_id}","starred_url":"https://api.github.com/users/stevenzwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/stevenzwu/subscriptions","organizations_url":"https://api.github.com/users/stevenzwu/orgs","repos_url":"https://api.github.com/users/stevenzwu/repos","events_url":"https://api.github.com/users/stevenzwu/events{/privacy}","received_events_url":"https://api.github.com/users/stevenzwu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T20:57:56Z","updated_at":"2023-12-08T20:57:56Z","author_association":"CONTRIBUTOR","body":"thanks @pvary for the review","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847835239/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847839526","html_url":"https://github.com/apache/iceberg/issues/9124#issuecomment-1847839526","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9124","id":1847839526,"node_id":"IC_kwDOCW7NX85uI8sm","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T21:02:34Z","updated_at":"2023-12-08T21:18:42Z","author_association":"NONE","body":"@singhpk234 \r\n@lognoel \r\n@dacort \r\n@electrum \r\n@martint \r\nHello.\r\n\r\nI am using Hive Catalog to create Iceberg tables with Spark as the execution engine:\r\n\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\nimport os\r\n#DEFINE SENSITIVE VARIABLES\r\nHIVE_URI = os.environ.get(\"HIVE_URI\",\"thrift://hive-metastore:9083\")\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\", \"xxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\", \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\", \"http://minioserver:9000\")\r\n\r\nprint(AWS_S3_ENDPOINT)\r\nprint(HIVE_URI)\r\nprint(WAREHOUSE)\r\nconf = (\r\npyspark.SparkConf()\r\n       .setAppName('app_name')\r\n#packages\r\n       .set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\r\n#SQL Extensions\r\n       .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\r\n#Configuring Catalog\r\n       .set('spark.sql.catalog.catalog_hive', 'org.apache.iceberg.spark.SparkCatalog')\r\n       .set('spark.sql.catalog.catalog_hive.type', 'hive')\r\n       .set('spark.sql.catalog.catalog_hive.uri', HIVE_URI)\r\n       .set('spark.sql.catalog.catalog_hive.warehouse.dir', WAREHOUSE)\r\n       .set('spark.sql.catalog.catalog_hive.endpoint', AWS_S3_ENDPOINT)\r\n       .set('spark.sql.catalog.catalog_hive.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\r\n       .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\r\n       .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\r\n       .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n)\r\n\r\n#Start Spark Session\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\n#Create a Table\r\nspark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n\r\n#Insert Some Data\r\nspark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('ns'), ('nd'), ('Ja')\").show()\r\n\r\n#Query the Data\r\nspark.sql(\"SELECT * FROM catalog_hive.default.my_table;\").show()\r\n\r\nWhen I try to run createTable command it gives me an exception:\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\n\r\n---------------------------------------------------------------------------\r\nPy4JJavaError                             Traceback (most recent call last)\r\nCell In[4], line 38\r\n     35 print(\"Spark Running\")\r\n     37 #Create a Table\r\n---> 38 spark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n     40 #Insert Some Data\r\n     41 spark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n   1032     sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n   1033 try:\r\n-> 1034     return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n   1035 finally:\r\n   1036     if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args)\r\n   1315 command = proto.CALL_COMMAND_NAME +\\\r\n   1316     self.command_header +\\\r\n   1317     args_command +\\\r\n   1318     proto.END_COMMAND_PART\r\n   1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n   1322     answer, self.gateway_client, self.target_id, self.name)\r\n   1324 for temp_arg in temp_args:\r\n   1325     temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception.<locals>.deco(*a, **kw)\r\n    188 def deco(*a: Any, **kw: Any) -> Any:\r\n    189     try:\r\n--> 190         return f(*a, **kw)\r\n    191     except Py4JJavaError as e:\r\n    192         converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n    324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n    325 if answer[1] == REFERENCE_TYPE:\r\n--> 326     raise Py4JJavaError(\r\n    327         \"An error occurred while calling {0}{1}{2}.\\n\".\r\n    328         format(target_id, \".\", name), value)\r\n    329 else:\r\n    330     raise Py4JError(\r\n    331         \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n    332         format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 2MBCRA6QRAF6SMBQ, Extended Request ID: s41ibIYx6fFDoMXiRK+8TRNkUT/GsiwqEzR5X2Drq9cY213HQkX19/PxSacQwo+SPX8eAqTNy7k=)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)\r\n\tat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)\r\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:95)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:245)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\r\n\tat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n\tat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n\tat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:167)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:175)\r\n\tat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)\r\n\tat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n\tat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)\r\n\tat software.amazon.awssdk.services.s3.DefaultS3Client.putObject(DefaultS3Client.java:9325)\r\n\tat org.apache.iceberg.aws.s3.S3OutputStream.completeUploads(S3OutputStream.java:422)\r\n\tat org.apache.iceberg.aws.s3.S3OutputStream.close(S3OutputStream.java:267)\r\n\tat java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341)\r\n\tat java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161)\r\n\tat java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:255)\r\n\tat org.apache.iceberg.TableMetadataParser.internalWrite(TableMetadataParser.java:127)\r\n\tat org.apache.iceberg.TableMetadataParser.overwrite(TableMetadataParser.java:110)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadata(BaseMetastoreTableOperations.java:162)\r\n\tat org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:234)\r\n\tat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:133)\r\n\tat org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:174)\r\n\tat org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n\tat org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\r\n\tat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:192)\r\n\tat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:99)\r\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\r\n\r\nEven if I use a hadoop type iceberg catalog, I always get the same error:  \"\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 5KRB8NP7R6TJ2JC4, Extended Request ID: aHgN8vSpD8xf5/Mu9u34rRJF7fcKWlupodDS67WAuQJ5+pTiyWqltK51IJADZKYXEcTSqGbgl2Y=)\"\r\n\r\nFor this script:\r\nimport os\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\n\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"xxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"xxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\",\"http://minioserver:9000\")\r\n\r\nconf = (\r\n    pyspark.SparkConf()\r\n    .setAppName(\"app_name\")\r\n    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178\")\r\n    .set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\r\n    .set(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\r\n    .set(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\r\n    .set(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\r\n    .set(\"spark.sql.catalog.iceberg.warehouse\", WAREHOUSE)\r\n    .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n    .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\r\n    .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\r\n    .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\r\n    .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n    ##.set(\"spark.hadoop.fs.s3a.requester.pays.enabled\", \"true\")\r\n)\r\n\r\n\r\n## Start Spark Session\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n## Run a Query to create a table\r\nspark.sql(\"CREATE TABLE iceberg.tab (name string) USING iceberg;\")\r\n\r\n**NB:**   (AWS_ACCESS_KEY and AWS_SECRET_KEy, AWS _S3_ENDPOINT and S3A://warehouse / are correct and I have already tested them for read data from minio.\r\n\r\nAny thoughts on what I might be missing. Thank you!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847839526/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847845419","html_url":"https://github.com/apache/iceberg/issues/9124#issuecomment-1847845419","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9124","id":1847845419,"node_id":"IC_kwDOCW7NX85uI-Ir","user":{"login":"lognoel","id":104791283,"node_id":"U_kgDOBj788w","avatar_url":"https://avatars.githubusercontent.com/u/104791283?v=4","gravatar_id":"","url":"https://api.github.com/users/lognoel","html_url":"https://github.com/lognoel","followers_url":"https://api.github.com/users/lognoel/followers","following_url":"https://api.github.com/users/lognoel/following{/other_user}","gists_url":"https://api.github.com/users/lognoel/gists{/gist_id}","starred_url":"https://api.github.com/users/lognoel/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lognoel/subscriptions","organizations_url":"https://api.github.com/users/lognoel/orgs","repos_url":"https://api.github.com/users/lognoel/repos","events_url":"https://api.github.com/users/lognoel/events{/privacy}","received_events_url":"https://api.github.com/users/lognoel/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T21:08:43Z","updated_at":"2023-12-08T21:09:07Z","author_association":"NONE","body":"@ExplorData24 \r\n\r\nyou're leaking your AWS creds in the comment, I suggest you redact them immediately","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847845419/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847855484","html_url":"https://github.com/apache/iceberg/issues/9124#issuecomment-1847855484","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9124","id":1847855484,"node_id":"IC_kwDOCW7NX85uJAl8","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T21:19:35Z","updated_at":"2023-12-08T21:19:35Z","author_association":"NONE","body":"@lognoel \r\nI deleted them thank you very much.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847855484/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847856969","html_url":"https://github.com/apache/iceberg/issues/8419#issuecomment-1847856969","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8419","id":1847856969,"node_id":"IC_kwDOCW7NX85uJA9J","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T21:21:07Z","updated_at":"2023-12-08T21:21:07Z","author_association":"NONE","body":"@palanik1 \r\n@di2mot \r\n@maulanaady \r\n@RussellSpitzer \r\n@dacort \r\nHello.\r\n\r\nI am using Hive Catalog to create Iceberg tables with Spark as the execution engine:\r\n\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\nimport os\r\n#DEFINE SENSITIVE VARIABLES\r\nHIVE_URI = os.environ.get(\"HIVE_URI\",\"thrift://hive-metastore:9083\")\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\", \"xxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\", \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\", \"http://minioserver:9000/\")\r\n\r\nprint(AWS_S3_ENDPOINT)\r\nprint(HIVE_URI)\r\nprint(WAREHOUSE)\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName('app_name')\r\n#packages\r\n.set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\r\n#SQL Extensions\r\n.set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\r\n#Configuring Catalog\r\n.set('spark.sql.catalog.catalog_hive', 'org.apache.iceberg.spark.SparkCatalog')\r\n.set('spark.sql.catalog.catalog_hive.type', 'hive')\r\n.set('spark.sql.catalog.catalog_hive.uri', HIVE_URI)\r\n.set('spark.sql.catalog.catalog_hive.warehouse.dir', WAREHOUSE)\r\n.set('spark.sql.catalog.catalog_hive.endpoint', AWS_S3_ENDPOINT)\r\n.set('spark.sql.catalog.catalog_hive.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\r\n.set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\r\n.set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\r\n.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n)\r\n\r\n#Start Spark Session\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\n#Create a Table\r\nspark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n\r\n#Insert Some Data\r\nspark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('ns'), ('nd'), ('Ja')\").show()\r\n\r\n#Query the Data\r\nspark.sql(\"SELECT * FROM catalog_hive.default.my_table;\").show()\r\n\r\nWhen I try to run createTable command it gives me an exception:\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\n\r\nPy4JJavaError Traceback (most recent call last)\r\nCell In[4], line 38\r\n35 print(\"Spark Running\")\r\n37 #Create a Table\r\n---> 38 spark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n40 #Insert Some Data\r\n41 spark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n1032 sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n1033 try:\r\n-> 1034 return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n1035 finally:\r\n1036 if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.call(self, *args)\r\n1315 command = proto.CALL_COMMAND_NAME +\r\n1316 self.command_header +\r\n1317 args_command +\r\n1318 proto.END_COMMAND_PART\r\n1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n1322 answer, self.gateway_client, self.target_id, self.name)\r\n1324 for temp_arg in temp_args:\r\n1325 temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception..deco(*a, **kw)\r\n188 def deco(*a: Any, **kw: Any) -> Any:\r\n189 try:\r\n--> 190 return f(*a, **kw)\r\n191 except Py4JJavaError as e:\r\n192 converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n325 if answer[1] == REFERENCE_TYPE:\r\n--> 326 raise Py4JJavaError(\r\n327 \"An error occurred while calling {0}{1}{2}.\\n\".\r\n328 format(target_id, \".\", name), value)\r\n329 else:\r\n330 raise Py4JError(\r\n331 \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n332 format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 2MBCRA6QRAF6SMBQ, Extended Request ID: s41ibIYx6fFDoMXiRK+8TRNkUT/GsiwqEzR5X2Drq9cY213HQkX19/PxSacQwo+SPX8eAqTNy7k=)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)\r\nat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:95)\r\nat software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:245)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\r\nat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\nat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:167)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:175)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)\r\nat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\nat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)\r\nat software.amazon.awssdk.services.s3.DefaultS3Client.putObject(DefaultS3Client.java:9325)\r\nat org.apache.iceberg.aws.s3.S3OutputStream.completeUploads(S3OutputStream.java:422)\r\nat org.apache.iceberg.aws.s3.S3OutputStream.close(S3OutputStream.java:267)\r\nat java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341)\r\nat java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161)\r\nat java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:255)\r\nat org.apache.iceberg.TableMetadataParser.internalWrite(TableMetadataParser.java:127)\r\nat org.apache.iceberg.TableMetadataParser.overwrite(TableMetadataParser.java:110)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadata(BaseMetastoreTableOperations.java:162)\r\nat org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:234)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:133)\r\nat org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:174)\r\nat org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\nat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\nat org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\r\nat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:192)\r\nat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:99)\r\nat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\nat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\nat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\nat org.apache.spark.sql.Dataset.(Dataset.scala:220)\r\nat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\nat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nat py4j.Gateway.invoke(Gateway.java:282)\r\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\nat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\n\r\nEven if I use a hadoop type iceberg catalog, I always get the same error: \"\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 5KRB8NP7R6TJ2JC4, Extended Request ID: aHgN8vSpD8xf5/Mu9u34rRJF7fcKWlupodDS67WAuQJ5+pTiyWqltK51IJADZKYXEcTSqGbgl2Y=)\"\r\n\r\nFor this script:\r\nimport os\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\n\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"xxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"xxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\",\"http://minioserver:9000/\")\r\n\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName(\"app_name\")\r\n.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178\")\r\n.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\r\n.set(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\r\n.set(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\r\n.set(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\r\n.set(\"spark.sql.catalog.iceberg.warehouse\", WAREHOUSE)\r\n.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\r\n.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\r\n.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\r\n.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n##.set(\"spark.hadoop.fs.s3a.requester.pays.enabled\", \"true\")\r\n)\r\n\r\n\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\nspark.sql(\"CREATE TABLE iceberg.tab (name string) USING iceberg;\")\r\n\r\nNB: (AWS_ACCESS_KEY and AWS_SECRET_KEy, AWS _S3_ENDPOINT and S3A://warehouse / are correct and I have already tested them for read data from minio.\r\n\r\nAny thoughts on what I might be missing. Thank you!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847856969/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847860494","html_url":"https://github.com/apache/iceberg/issues/2685#issuecomment-1847860494","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2685","id":1847860494,"node_id":"IC_kwDOCW7NX85uJB0O","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T21:24:42Z","updated_at":"2023-12-08T21:24:42Z","author_association":"NONE","body":"@aimenglin \r\n@bitsondatadev \r\n@marton-bod \r\n@dacort \r\n@electrum \r\nHello.\r\n\r\nI am using Hive Catalog to create Iceberg tables with Spark as the execution engine:\r\n\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\nimport os\r\n#DEFINE SENSITIVE VARIABLES\r\nHIVE_URI = os.environ.get(\"HIVE_URI\",\"thrift://hive-metastore:9083\")\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\", \"xxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\", \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\", \"http://minioserver:9000/\")\r\n\r\nprint(AWS_S3_ENDPOINT)\r\nprint(HIVE_URI)\r\nprint(WAREHOUSE)\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName('app_name')\r\n#packages\r\n.set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\r\n#SQL Extensions\r\n.set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\r\n#Configuring Catalog\r\n.set('spark.sql.catalog.catalog_hive', 'org.apache.iceberg.spark.SparkCatalog')\r\n.set('spark.sql.catalog.catalog_hive.type', 'hive')\r\n.set('spark.sql.catalog.catalog_hive.uri', HIVE_URI)\r\n.set('spark.sql.catalog.catalog_hive.warehouse.dir', WAREHOUSE)\r\n.set('spark.sql.catalog.catalog_hive.endpoint', AWS_S3_ENDPOINT)\r\n.set('spark.sql.catalog.catalog_hive.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\r\n.set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\r\n.set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\r\n.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n)\r\n\r\n#Start Spark Session\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\n#Create a Table\r\nspark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n\r\n#Insert Some Data\r\nspark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('ns'), ('nd'), ('Ja')\").show()\r\n\r\n#Query the Data\r\nspark.sql(\"SELECT * FROM catalog_hive.default.my_table;\").show()\r\n\r\nWhen I try to run createTable command it gives me an exception:\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\n\r\nPy4JJavaError Traceback (most recent call last)\r\nCell In[4], line 38\r\n35 print(\"Spark Running\")\r\n37 #Create a Table\r\n---> 38 spark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n40 #Insert Some Data\r\n41 spark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n1032 sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n1033 try:\r\n-> 1034 return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n1035 finally:\r\n1036 if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.call(self, *args)\r\n1315 command = proto.CALL_COMMAND_NAME +\r\n1316 self.command_header +\r\n1317 args_command +\r\n1318 proto.END_COMMAND_PART\r\n1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n1322 answer, self.gateway_client, self.target_id, self.name)\r\n1324 for temp_arg in temp_args:\r\n1325 temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception..deco(*a, **kw)\r\n188 def deco(*a: Any, **kw: Any) -> Any:\r\n189 try:\r\n--> 190 return f(*a, **kw)\r\n191 except Py4JJavaError as e:\r\n192 converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n325 if answer[1] == REFERENCE_TYPE:\r\n--> 326 raise Py4JJavaError(\r\n327 \"An error occurred while calling {0}{1}{2}.\\n\".\r\n328 format(target_id, \".\", name), value)\r\n329 else:\r\n330 raise Py4JError(\r\n331 \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n332 format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 2MBCRA6QRAF6SMBQ, Extended Request ID: s41ibIYx6fFDoMXiRK+8TRNkUT/GsiwqEzR5X2Drq9cY213HQkX19/PxSacQwo+SPX8eAqTNy7k=)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)\r\nat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:95)\r\nat software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:245)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\r\nat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\nat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:167)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:175)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)\r\nat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\nat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)\r\nat software.amazon.awssdk.services.s3.DefaultS3Client.putObject(DefaultS3Client.java:9325)\r\nat org.apache.iceberg.aws.s3.S3OutputStream.completeUploads(S3OutputStream.java:422)\r\nat org.apache.iceberg.aws.s3.S3OutputStream.close(S3OutputStream.java:267)\r\nat java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341)\r\nat java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161)\r\nat java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:255)\r\nat org.apache.iceberg.TableMetadataParser.internalWrite(TableMetadataParser.java:127)\r\nat org.apache.iceberg.TableMetadataParser.overwrite(TableMetadataParser.java:110)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadata(BaseMetastoreTableOperations.java:162)\r\nat org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:234)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:133)\r\nat org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:174)\r\nat org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\nat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\nat org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\r\nat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:192)\r\nat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:99)\r\nat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\nat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\nat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\nat org.apache.spark.sql.Dataset.(Dataset.scala:220)\r\nat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\nat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nat py4j.Gateway.invoke(Gateway.java:282)\r\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\nat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\n\r\nEven if I use a hadoop type iceberg catalog, I always get the same error: \"\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 5KRB8NP7R6TJ2JC4, Extended Request ID: aHgN8vSpD8xf5/Mu9u34rRJF7fcKWlupodDS67WAuQJ5+pTiyWqltK51IJADZKYXEcTSqGbgl2Y=)\"\r\n\r\nFor this script:\r\nimport os\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\n\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"xxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"xxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\",\"http://minioserver:9000/\")\r\n\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName(\"app_name\")\r\n.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178\")\r\n.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\r\n.set(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\r\n.set(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\r\n.set(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\r\n.set(\"spark.sql.catalog.iceberg.warehouse\", WAREHOUSE)\r\n.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\r\n.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\r\n.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\r\n.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n##.set(\"spark.hadoop.fs.s3a.requester.pays.enabled\", \"true\")\r\n)\r\n\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\nspark.sql(\"CREATE TABLE iceberg.tab (name string) USING iceberg;\")\r\n\r\nNB: (AWS_ACCESS_KEY and AWS_SECRET_KEy, AWS _S3_ENDPOINT and S3A://warehouse / are correct and I have already tested them for read data from minio.\r\n\r\nAny thoughts on what I might be missing. Thank you!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847860494/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847862051","html_url":"https://github.com/apache/iceberg/issues/2176#issuecomment-1847862051","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2176","id":1847862051,"node_id":"IC_kwDOCW7NX85uJCMj","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T21:26:23Z","updated_at":"2023-12-08T21:26:23Z","author_association":"NONE","body":"@adnanhb \r\n@jackye1995 \r\nHello.\r\n\r\nI am using Hive Catalog to create Iceberg tables with Spark as the execution engine:\r\n\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\nimport os\r\n#DEFINE SENSITIVE VARIABLES\r\nHIVE_URI = os.environ.get(\"HIVE_URI\",\"thrift://hive-metastore:9083\")\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\", \"xxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\", \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\", \"http://minioserver:9000/\")\r\n\r\nprint(AWS_S3_ENDPOINT)\r\nprint(HIVE_URI)\r\nprint(WAREHOUSE)\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName('app_name')\r\n#packages\r\n.set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\r\n#SQL Extensions\r\n.set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\r\n#Configuring Catalog\r\n.set('spark.sql.catalog.catalog_hive', 'org.apache.iceberg.spark.SparkCatalog')\r\n.set('spark.sql.catalog.catalog_hive.type', 'hive')\r\n.set('spark.sql.catalog.catalog_hive.uri', HIVE_URI)\r\n.set('spark.sql.catalog.catalog_hive.warehouse.dir', WAREHOUSE)\r\n.set('spark.sql.catalog.catalog_hive.endpoint', AWS_S3_ENDPOINT)\r\n.set('spark.sql.catalog.catalog_hive.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\r\n.set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\r\n.set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\r\n.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n)\r\n\r\n#Start Spark Session\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\n#Create a Table\r\nspark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n\r\n#Insert Some Data\r\nspark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('ns'), ('nd'), ('Ja')\").show()\r\n\r\n#Query the Data\r\nspark.sql(\"SELECT * FROM catalog_hive.default.my_table;\").show()\r\n\r\nWhen I try to run createTable command it gives me an exception:\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\n\r\nPy4JJavaError Traceback (most recent call last)\r\nCell In[4], line 38\r\n35 print(\"Spark Running\")\r\n37 #Create a Table\r\n---> 38 spark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n40 #Insert Some Data\r\n41 spark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n1032 sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n1033 try:\r\n-> 1034 return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n1035 finally:\r\n1036 if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.call(self, *args)\r\n1315 command = proto.CALL_COMMAND_NAME +\r\n1316 self.command_header +\r\n1317 args_command +\r\n1318 proto.END_COMMAND_PART\r\n1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n1322 answer, self.gateway_client, self.target_id, self.name)\r\n1324 for temp_arg in temp_args:\r\n1325 temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception..deco(*a, **kw)\r\n188 def deco(*a: Any, **kw: Any) -> Any:\r\n189 try:\r\n--> 190 return f(*a, **kw)\r\n191 except Py4JJavaError as e:\r\n192 converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n325 if answer[1] == REFERENCE_TYPE:\r\n--> 326 raise Py4JJavaError(\r\n327 \"An error occurred while calling {0}{1}{2}.\\n\".\r\n328 format(target_id, \".\", name), value)\r\n329 else:\r\n330 raise Py4JError(\r\n331 \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n332 format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 2MBCRA6QRAF6SMBQ, Extended Request ID: s41ibIYx6fFDoMXiRK+8TRNkUT/GsiwqEzR5X2Drq9cY213HQkX19/PxSacQwo+SPX8eAqTNy7k=)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)\r\nat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:95)\r\nat software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:245)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\r\nat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\nat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:167)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:175)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)\r\nat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\nat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)\r\nat software.amazon.awssdk.services.s3.DefaultS3Client.putObject(DefaultS3Client.java:9325)\r\nat org.apache.iceberg.aws.s3.S3OutputStream.completeUploads(S3OutputStream.java:422)\r\nat org.apache.iceberg.aws.s3.S3OutputStream.close(S3OutputStream.java:267)\r\nat java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341)\r\nat java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161)\r\nat java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:255)\r\nat org.apache.iceberg.TableMetadataParser.internalWrite(TableMetadataParser.java:127)\r\nat org.apache.iceberg.TableMetadataParser.overwrite(TableMetadataParser.java:110)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadata(BaseMetastoreTableOperations.java:162)\r\nat org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:234)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:133)\r\nat org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:174)\r\nat org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\nat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\nat org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\r\nat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:192)\r\nat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:99)\r\nat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\nat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\nat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\nat org.apache.spark.sql.Dataset.(Dataset.scala:220)\r\nat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\nat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nat py4j.Gateway.invoke(Gateway.java:282)\r\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\nat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\n\r\nEven if I use a hadoop type iceberg catalog, I always get the same error: \"\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 5KRB8NP7R6TJ2JC4, Extended Request ID: aHgN8vSpD8xf5/Mu9u34rRJF7fcKWlupodDS67WAuQJ5+pTiyWqltK51IJADZKYXEcTSqGbgl2Y=)\"\r\n\r\nFor this script:\r\nimport os\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\n\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"xxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"xxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\",\"http://minioserver:9000/\")\r\n\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName(\"app_name\")\r\n.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178\")\r\n.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\r\n.set(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\r\n.set(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\r\n.set(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\r\n.set(\"spark.sql.catalog.iceberg.warehouse\", WAREHOUSE)\r\n.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\r\n.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\r\n.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\r\n.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n##.set(\"spark.hadoop.fs.s3a.requester.pays.enabled\", \"true\")\r\n)\r\n\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\nspark.sql(\"CREATE TABLE iceberg.tab (name string) USING iceberg;\")\r\n\r\nNB: (AWS_ACCESS_KEY and AWS_SECRET_KEy, AWS _S3_ENDPOINT and S3A://warehouse / are correct and I have already tested them for read data from minio.\r\n\r\nAny thoughts on what I might be missing. Thank you!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847862051/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847892200","html_url":"https://github.com/apache/iceberg/issues/2685#issuecomment-1847892200","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2685","id":1847892200,"node_id":"IC_kwDOCW7NX85uJJjo","user":{"login":"aimenglin","id":84943281,"node_id":"MDQ6VXNlcjg0OTQzMjgx","avatar_url":"https://avatars.githubusercontent.com/u/84943281?v=4","gravatar_id":"","url":"https://api.github.com/users/aimenglin","html_url":"https://github.com/aimenglin","followers_url":"https://api.github.com/users/aimenglin/followers","following_url":"https://api.github.com/users/aimenglin/following{/other_user}","gists_url":"https://api.github.com/users/aimenglin/gists{/gist_id}","starred_url":"https://api.github.com/users/aimenglin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aimenglin/subscriptions","organizations_url":"https://api.github.com/users/aimenglin/orgs","repos_url":"https://api.github.com/users/aimenglin/repos","events_url":"https://api.github.com/users/aimenglin/events{/privacy}","received_events_url":"https://api.github.com/users/aimenglin/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T21:56:41Z","updated_at":"2023-12-08T21:56:41Z","author_association":"NONE","body":"Hi Rym,\r\n\r\nI understand you're inquiring about writing Iceberg tables using Hive.\r\nBased on my experience and experiments, it appears that directly writing to\r\nIceberg tables through Hive is not supported. However, a viable workaround\r\nis to create and populate your Iceberg tables using Apache Spark. Once\r\nthese tables are created and data is inserted via Spark, you can seamlessly\r\nread them using Hive.\r\n\r\nBest,\r\nMenglin\r\n\r\nOn Fri, Dec 8, 2023 at 1:24â€¯PM Rym ***@***.***> wrote:\r\n\r\n> @aimenglin <https://github.com/aimenglin>\r\n> @bitsondatadev <https://github.com/bitsondatadev>\r\n> @marton-bod <https://github.com/marton-bod>\r\n> @dacort <https://github.com/dacort>\r\n> @electrum <https://github.com/electrum>\r\n> Hello.\r\n>\r\n> I am using Hive Catalog to create Iceberg tables with Spark as the\r\n> execution engine:\r\n>\r\n> import pyspark\r\n> from pyspark.sql import SparkSession\r\n> import os\r\n> #DEFINE SENSITIVE VARIABLES\r\n> HIVE_URI = os.environ.get(\"HIVE_URI\",\"thrift://hive-metastore:9083\")\r\n> WAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\n> AWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\", \"xxxxxxxxxxxxxxxx\")\r\n> AWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\",\r\n> \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\r\n> AWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\", \"\r\n> http://minioserver:9000/\")\r\n>\r\n> print(AWS_S3_ENDPOINT)\r\n> print(HIVE_URI)\r\n> print(WAREHOUSE)\r\n> conf = (\r\n> pyspark.SparkConf()\r\n> .setAppName('app_name')\r\n> #packages\r\n> .set('spark.jars.packages',\r\n> 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\r\n> #SQL Extensions\r\n> .set('spark.sql.extensions',\r\n> 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\r\n> #Configuring Catalog\r\n> .set('spark.sql.catalog.catalog_hive',\r\n> 'org.apache.iceberg.spark.SparkCatalog')\r\n> .set('spark.sql.catalog.catalog_hive.type', 'hive')\r\n> .set('spark.sql.catalog.catalog_hive.uri', HIVE_URI)\r\n> .set('spark.sql.catalog.catalog_hive.warehouse.dir', WAREHOUSE)\r\n> .set('spark.sql.catalog.catalog_hive.endpoint', AWS_S3_ENDPOINT)\r\n> .set('spark.sql.catalog.catalog_hive.io-impl',\r\n> 'org.apache.iceberg.aws.s3.S3FileIO')\r\n> .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\r\n> .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\r\n> .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n> )\r\n>\r\n> #Start Spark Session\r\n> spark = SparkSession.builder.config(conf=conf).getOrCreate()\r\n> print(\"Spark Running\")\r\n>\r\n> #Create a Table\r\n> spark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING\r\n> iceberg;\").show()\r\n>\r\n> #Insert Some Data\r\n> spark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('ns'),\r\n> ('nd'), ('Ja')\").show()\r\n>\r\n> #Query the Data\r\n> spark.sql(\"SELECT * FROM catalog_hive.default.my_table;\").show()\r\n>\r\n> When I try to run createTable command it gives me an exception:\r\n> SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\r\n> SLF4J: Defaulting to no-operation (NOP) logger implementation\r\n> SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further\r\n> details.\r\n>\r\n> Py4JJavaError Traceback (most recent call last)\r\n> Cell In[4], line 38\r\n> 35 print(\"Spark Running\")\r\n> 37 #Create a Table\r\n> ---> 38 spark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name\r\n> STRING) USING iceberg;\").show()\r\n> 40 #Insert Some Data\r\n> 41 spark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('Alex\r\n> Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n>\r\n> File ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in\r\n> SparkSession.sql(self, sqlQuery, **kwargs)\r\n> 1032 sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n> 1033 try:\r\n> -> 1034 return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n> 1035 finally:\r\n> 1036 if len(kwargs) > 0:\r\n>\r\n> File ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in\r\n> JavaMember.call(self, *args)\r\n> 1315 command = proto.CALL_COMMAND_NAME +\r\n> 1316 self.command_header +\r\n> 1317 args_command +\r\n> 1318 proto.END_COMMAND_PART\r\n> 1320 answer = self.gateway_client.send_command(command)\r\n> -> 1321 return_value = get_return_value(\r\n> 1322 answer, self.gateway_client, self.target_id, self.name)\r\n> 1324 for temp_arg in temp_args:\r\n> 1325 temp_arg._detach()\r\n>\r\n> File ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in\r\n> capture_sql_exception..deco(*a, **kw)\r\n> 188 def deco(*a: Any, **kw: Any) -> Any:\r\n> 189 try:\r\n> --> 190 return f(*a, **kw)\r\n> 191 except Py4JJavaError as e:\r\n> 192 converted = convert_exception(e.java_exception)\r\n>\r\n> File ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in\r\n> get_return_value(answer, gateway_client, target_id, name)\r\n> 324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n> 325 if answer[1] == REFERENCE_TYPE:\r\n> --> 326 raise Py4JJavaError(\r\n> 327 \"An error occurred while calling {0}{1}{2}.\\n\".\r\n> 328 format(target_id, \".\", name), value)\r\n> 329 else:\r\n> 330 raise Py4JError(\r\n> 331 \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n> 332 format(target_id, \".\", name, value))\r\n>\r\n> Py4JJavaError: An error occurred while calling o49.sql.\r\n> : software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3,\r\n> Status Code: 400, Request ID: 2MBCRA6QRAF6SMBQ, Extended Request ID:\r\n> s41ibIYx6fFDoMXiRK+8TRNkUT/GsiwqEzR5X2Drq9cY213HQkX19/PxSacQwo+SPX8eAqTNy7k=)\r\n> at\r\n> software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)\r\n> at\r\n> software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)\r\n> at\r\n> software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)\r\n> at\r\n> software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)\r\n> at\r\n> software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:95)\r\n> at\r\n> software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:245)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\n> at\r\n> software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)\r\n> at\r\n> software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\n> at\r\n> software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:167)\r\n> at\r\n> software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)\r\n> at\r\n> software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:175)\r\n> at\r\n> software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)\r\n> at\r\n> software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\n> at\r\n> software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)\r\n> at\r\n> software.amazon.awssdk.services.s3.DefaultS3Client.putObject(DefaultS3Client.java:9325)\r\n> at\r\n> org.apache.iceberg.aws.s3.S3OutputStream.completeUploads(S3OutputStream.java:422)\r\n> at org.apache.iceberg.aws.s3.S3OutputStream.close(S3OutputStream.java:267)\r\n> at java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341)\r\n> at java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161)\r\n> at java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:255)\r\n> at\r\n> org.apache.iceberg.TableMetadataParser.internalWrite(TableMetadataParser.java:127)\r\n> at\r\n> org.apache.iceberg.TableMetadataParser.overwrite(TableMetadataParser.java:110)\r\n> at\r\n> org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadata(BaseMetastoreTableOperations.java:162)\r\n> at\r\n> org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:234)\r\n> at\r\n> org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:133)\r\n> at\r\n> org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:174)\r\n> at\r\n> org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\r\n> at\r\n> org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\n> at\r\n> java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\n> at\r\n> org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\n> at\r\n> org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\n> at\r\n> org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\n> at\r\n> org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\n> at\r\n> org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\r\n> at org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:192)\r\n> at org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:99)\r\n> at\r\n> org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\r\n> at\r\n> org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\n> at\r\n> org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\n> at\r\n> org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\n> at\r\n> org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n> at\r\n> org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n> at\r\n> org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n> at\r\n> org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n> at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n> at\r\n> org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n> at\r\n> org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n> at\r\n> org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n> at\r\n> org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n> at\r\n> org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n> at\r\n> org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n> at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org\r\n> $apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n> at\r\n> org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n> at\r\n> org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n> at\r\n> org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n> at\r\n> org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n> at\r\n> org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n> at\r\n> org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n> at\r\n> org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n> at\r\n> org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n> at org.apache.spark.sql.Dataset.(Dataset.scala:220)\r\n> at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\n> at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n> at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\n> at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\n> at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n> at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\n> at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native\r\n> Method)\r\n> at\r\n> java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n> at\r\n> java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n> at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n> at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n> at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n> at py4j.Gateway.invoke(Gateway.java:282)\r\n> at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n> at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n> at\r\n> py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n> at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n> at java.base/java.lang.Thread.run(Thread.java:829)\r\n>\r\n> Even if I use a hadoop type iceberg catalog, I always get the same error: \"\r\n> Py4JJavaError: An error occurred while calling o49.sql.\r\n> : software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3,\r\n> Status Code: 400, Request ID: 5KRB8NP7R6TJ2JC4, Extended Request ID:\r\n> aHgN8vSpD8xf5/Mu9u34rRJF7fcKWlupodDS67WAuQJ5+pTiyWqltK51IJADZKYXEcTSqGbgl2Y=)\"\r\n>\r\n> For this script:\r\n> import os\r\n> import pyspark\r\n> from pyspark.sql import SparkSession\r\n>\r\n> WAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\n> AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\",\r\n> \"xxxxxxxxxxxxxxxxxxxxxxx\")\r\n> AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\",\r\n> \"xxxxxxxxxxxxxxxxxxxxx\")\r\n> AWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\",\"\r\n> http://minioserver:9000/\")\r\n>\r\n> conf = (\r\n> pyspark.SparkConf()\r\n> .setAppName(\"app_name\")\r\n> .set(\"spark.jars.packages\",\r\n> \"org.apache.hadoop:hadoop-aws:3.3.1,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178\")\r\n> .set(\"spark.sql.extensions\",\r\n> \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\r\n> .set(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\r\n> .set(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\r\n> .set(\"spark.sql.catalog.iceberg.io-impl\",\r\n> \"org.apache.iceberg.aws.s3.S3FileIO\")\r\n> .set(\"spark.sql.catalog.iceberg.warehouse\", WAREHOUSE)\r\n> .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n> .set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\r\n> .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\r\n> .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\r\n> .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n> ##.set(\"spark.hadoop.fs.s3a.requester.pays.enabled\", \"true\")\r\n> )\r\n>\r\n> spark = SparkSession.builder.config(conf=conf).getOrCreate()\r\n> print(\"Spark Running\")\r\n>\r\n> spark.sql(\"CREATE TABLE iceberg.tab (name string) USING iceberg;\")\r\n>\r\n> NB: (AWS_ACCESS_KEY and AWS_SECRET_KEy, AWS _S3_ENDPOINT and\r\n> S3A://warehouse / are correct and I have already tested them for read data\r\n> from minio.\r\n>\r\n> Any thoughts on what I might be missing. Thank you!\r\n>\r\n> â€”\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/apache/iceberg/issues/2685#issuecomment-1847860494>,\r\n> or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AUICDMNA6IYJM6VIAWRNIVDYIOASNAVCNFSM46KJS63KU5DIOJSWCZC7NNSXTN2JONZXKZKDN5WW2ZLOOQ5TCOBUG44DMMBUHE2A>\r\n> .\r\n> You are receiving this because you were mentioned.Message ID:\r\n> ***@***.***>\r\n>\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847892200/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847923590","html_url":"https://github.com/apache/iceberg/issues/2685#issuecomment-1847923590","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2685","id":1847923590,"node_id":"IC_kwDOCW7NX85uJROG","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T22:31:58Z","updated_at":"2023-12-08T22:31:58Z","author_association":"NONE","body":"Hi @aimenglin.\r\nFirst of all, thank you very much for your availability. \r\n\r\nI actually followed the part **Custom Iceberg catalogs** of this link: https://iceberg.incubator.apache.org/docs/latest/hive/#custom-iceberg-catalogs to create a table in iceberg format via the hive type catalog.\r\n**However, a viable workaround is to create and populate your Iceberg tables using Apache Spark** => for this solution are you talking about an integrated catalog (writing to the local file system)!!!\r\n\r\nSincerely.\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847923590/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847976194","html_url":"https://github.com/apache/iceberg/pull/9233#issuecomment-1847976194","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9233","id":1847976194,"node_id":"IC_kwDOCW7NX85uJeEC","user":{"login":"tmnd1991","id":7031242,"node_id":"MDQ6VXNlcjcwMzEyNDI=","avatar_url":"https://avatars.githubusercontent.com/u/7031242?v=4","gravatar_id":"","url":"https://api.github.com/users/tmnd1991","html_url":"https://github.com/tmnd1991","followers_url":"https://api.github.com/users/tmnd1991/followers","following_url":"https://api.github.com/users/tmnd1991/following{/other_user}","gists_url":"https://api.github.com/users/tmnd1991/gists{/gist_id}","starred_url":"https://api.github.com/users/tmnd1991/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmnd1991/subscriptions","organizations_url":"https://api.github.com/users/tmnd1991/orgs","repos_url":"https://api.github.com/users/tmnd1991/repos","events_url":"https://api.github.com/users/tmnd1991/events{/privacy}","received_events_url":"https://api.github.com/users/tmnd1991/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-08T23:40:48Z","updated_at":"2023-12-08T23:40:48Z","author_association":"NONE","body":"Finally I got a reproducer inside the codebase, you can find it at `TestSPJWithBucketing`.\r\nSpark 3.4 (same as my app) with the condition on the partitions will actually prune the unaffected partitions, while 3.5 will not.\r\n\r\nAnyway the more I work on this, the more I think the issue should be solved directly on the Scan, not by adding conditions manually. All info should be available to Spark beforehand, am I right?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847976194/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847993683","html_url":"https://github.com/apache/iceberg/issues/7819#issuecomment-1847993683","issue_url":"https://api.github.com/repos/apache/iceberg/issues/7819","id":1847993683,"node_id":"IC_kwDOCW7NX85uJiVT","user":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T00:11:43Z","updated_at":"2023-12-09T00:11:43Z","author_association":"NONE","body":"This issue has been automatically marked as stale because it has been open for 180 days with no activity. It will be closed in next 14 days if no further activity occurs. To permanently prevent this issue from being considered stale, add the label 'not-stale', but commenting on the issue is preferred when possible.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847993683/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":15368,"client_id":"Iv1.05c79e9ad1f6bdfa","slug":"github-actions","node_id":"MDM6QXBwMTUzNjg=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"GitHub Actions","description":"Automate your workflow from idea to production","external_url":"https://help.github.com/en/actions","html_url":"https://github.com/apps/github-actions","created_at":"2018-07-30T09:30:17Z","updated_at":"2024-04-10T20:33:16Z","permissions":{"actions":"write","administration":"read","attestations":"write","checks":"write","contents":"write","deployments":"write","discussions":"write","issues":"write","merge_queues":"write","metadata":"read","packages":"write","pages":"write","pull_requests":"write","repository_hooks":"write","repository_projects":"write","security_events":"write","statuses":"write","vulnerability_alerts":"read"},"events":["branch_protection_rule","check_run","check_suite","create","delete","deployment","deployment_status","discussion","discussion_comment","fork","gollum","issues","issue_comment","label","merge_group","milestone","page_build","project","project_card","project_column","public","pull_request","pull_request_review","pull_request_review_comment","push","registry_package","release","repository","repository_dispatch","status","watch","workflow_dispatch","workflow_run"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847993704","html_url":"https://github.com/apache/iceberg/issues/7793#issuecomment-1847993704","issue_url":"https://api.github.com/repos/apache/iceberg/issues/7793","id":1847993704,"node_id":"IC_kwDOCW7NX85uJiVo","user":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T00:11:45Z","updated_at":"2023-12-09T00:11:45Z","author_association":"NONE","body":"This issue has been automatically marked as stale because it has been open for 180 days with no activity. It will be closed in next 14 days if no further activity occurs. To permanently prevent this issue from being considered stale, add the label 'not-stale', but commenting on the issue is preferred when possible.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847993704/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":15368,"client_id":"Iv1.05c79e9ad1f6bdfa","slug":"github-actions","node_id":"MDM6QXBwMTUzNjg=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"GitHub Actions","description":"Automate your workflow from idea to production","external_url":"https://help.github.com/en/actions","html_url":"https://github.com/apps/github-actions","created_at":"2018-07-30T09:30:17Z","updated_at":"2024-04-10T20:33:16Z","permissions":{"actions":"write","administration":"read","attestations":"write","checks":"write","contents":"write","deployments":"write","discussions":"write","issues":"write","merge_queues":"write","metadata":"read","packages":"write","pages":"write","pull_requests":"write","repository_hooks":"write","repository_projects":"write","security_events":"write","statuses":"write","vulnerability_alerts":"read"},"events":["branch_protection_rule","check_run","check_suite","create","delete","deployment","deployment_status","discussion","discussion_comment","fork","gollum","issues","issue_comment","label","merge_group","milestone","page_build","project","project_card","project_column","public","pull_request","pull_request_review","pull_request_review_comment","push","registry_package","release","repository","repository_dispatch","status","watch","workflow_dispatch","workflow_run"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847993726","html_url":"https://github.com/apache/iceberg/issues/7787#issuecomment-1847993726","issue_url":"https://api.github.com/repos/apache/iceberg/issues/7787","id":1847993726,"node_id":"IC_kwDOCW7NX85uJiV-","user":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T00:11:46Z","updated_at":"2023-12-09T00:11:46Z","author_association":"NONE","body":"This issue has been automatically marked as stale because it has been open for 180 days with no activity. It will be closed in next 14 days if no further activity occurs. To permanently prevent this issue from being considered stale, add the label 'not-stale', but commenting on the issue is preferred when possible.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847993726/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":15368,"client_id":"Iv1.05c79e9ad1f6bdfa","slug":"github-actions","node_id":"MDM6QXBwMTUzNjg=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"GitHub Actions","description":"Automate your workflow from idea to production","external_url":"https://help.github.com/en/actions","html_url":"https://github.com/apps/github-actions","created_at":"2018-07-30T09:30:17Z","updated_at":"2024-04-10T20:33:16Z","permissions":{"actions":"write","administration":"read","attestations":"write","checks":"write","contents":"write","deployments":"write","discussions":"write","issues":"write","merge_queues":"write","metadata":"read","packages":"write","pages":"write","pull_requests":"write","repository_hooks":"write","repository_projects":"write","security_events":"write","statuses":"write","vulnerability_alerts":"read"},"events":["branch_protection_rule","check_run","check_suite","create","delete","deployment","deployment_status","discussion","discussion_comment","fork","gollum","issues","issue_comment","label","merge_group","milestone","page_build","project","project_card","project_column","public","pull_request","pull_request_review","pull_request_review_comment","push","registry_package","release","repository","repository_dispatch","status","watch","workflow_dispatch","workflow_run"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847993762","html_url":"https://github.com/apache/iceberg/issues/7737#issuecomment-1847993762","issue_url":"https://api.github.com/repos/apache/iceberg/issues/7737","id":1847993762,"node_id":"IC_kwDOCW7NX85uJiWi","user":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T00:11:49Z","updated_at":"2023-12-09T00:11:49Z","author_association":"NONE","body":"This issue has been automatically marked as stale because it has been open for 180 days with no activity. It will be closed in next 14 days if no further activity occurs. To permanently prevent this issue from being considered stale, add the label 'not-stale', but commenting on the issue is preferred when possible.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1847993762/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":15368,"client_id":"Iv1.05c79e9ad1f6bdfa","slug":"github-actions","node_id":"MDM6QXBwMTUzNjg=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"GitHub Actions","description":"Automate your workflow from idea to production","external_url":"https://help.github.com/en/actions","html_url":"https://github.com/apps/github-actions","created_at":"2018-07-30T09:30:17Z","updated_at":"2024-04-10T20:33:16Z","permissions":{"actions":"write","administration":"read","attestations":"write","checks":"write","contents":"write","deployments":"write","discussions":"write","issues":"write","merge_queues":"write","metadata":"read","packages":"write","pages":"write","pull_requests":"write","repository_hooks":"write","repository_projects":"write","security_events":"write","statuses":"write","vulnerability_alerts":"read"},"events":["branch_protection_rule","check_run","check_suite","create","delete","deployment","deployment_status","discussion","discussion_comment","fork","gollum","issues","issue_comment","label","merge_group","milestone","page_build","project","project_card","project_column","public","pull_request","pull_request_review","pull_request_review_comment","push","registry_package","release","repository","repository_dispatch","status","watch","workflow_dispatch","workflow_run"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848220358","html_url":"https://github.com/apache/iceberg/issues/9172#issuecomment-1848220358","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9172","id":1848220358,"node_id":"IC_kwDOCW7NX85uKZrG","user":{"login":"amogh-jahagirdar","id":87500546,"node_id":"MDQ6VXNlcjg3NTAwNTQ2","avatar_url":"https://avatars.githubusercontent.com/u/87500546?v=4","gravatar_id":"","url":"https://api.github.com/users/amogh-jahagirdar","html_url":"https://github.com/amogh-jahagirdar","followers_url":"https://api.github.com/users/amogh-jahagirdar/followers","following_url":"https://api.github.com/users/amogh-jahagirdar/following{/other_user}","gists_url":"https://api.github.com/users/amogh-jahagirdar/gists{/gist_id}","starred_url":"https://api.github.com/users/amogh-jahagirdar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amogh-jahagirdar/subscriptions","organizations_url":"https://api.github.com/users/amogh-jahagirdar/orgs","repos_url":"https://api.github.com/users/amogh-jahagirdar/repos","events_url":"https://api.github.com/users/amogh-jahagirdar/events{/privacy}","received_events_url":"https://api.github.com/users/amogh-jahagirdar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T04:13:55Z","updated_at":"2023-12-09T04:15:03Z","author_association":"CONTRIBUTOR","body":"Thanks for the details, one key thing stands out to me:\r\n\r\n\"\r\nI also tested with latest version, iceberg-spark-runtime-3.4_2.12-1.4.2.jar as well, I could see that the second number, part of the file name, is continuously increasing 00001-3200-11773075-523f-4667-936b-88702fe9860c-00001.parquet, however after around 200 execution of stream, the file name got reset 00001-3166-11773075-523f-4667-936b-88702fe9860c-00001.parquet and files were started getting overwritten.\r\n\"\r\n\r\nThis does align with the suspicion in the other issue that task IDs can be reused across epochs (\"after around 200 executions of stream\" I'm reading that as 200 intervals of miccrobatches)\r\n\r\n Which I think makes sense (and anyways that's probably intentional in the DSV2 API which passes in the epochID). I'll put up a draft for adding the epochID to the output path. I'm still trying to get a test in our current unit testing setup to hit this scenario","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848220358/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848292419","html_url":"https://github.com/apache/iceberg/issues/2359#issuecomment-1848292419","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2359","id":1848292419,"node_id":"IC_kwDOCW7NX85uKrRD","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T07:33:24Z","updated_at":"2023-12-09T07:33:24Z","author_association":"CONTRIBUTOR","body":"> 23/12/08 15:18:43 INFO metastore: Trying to connect to metastore with URI http://nessie:19120/api/v1\r\n\r\nThe issue is with the catalog configuration somehow. Sadly I can't help you why wrong metastore configuration is passed to the HMS Client. ðŸ˜¢ ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848292419/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848304506","html_url":"https://github.com/apache/iceberg/issues/2359#issuecomment-1848304506","issue_url":"https://api.github.com/repos/apache/iceberg/issues/2359","id":1848304506,"node_id":"IC_kwDOCW7NX85uKuN6","user":{"login":"ExplorData24","id":149940691,"node_id":"U_kgDOCO_p0w","avatar_url":"https://avatars.githubusercontent.com/u/149940691?v=4","gravatar_id":"","url":"https://api.github.com/users/ExplorData24","html_url":"https://github.com/ExplorData24","followers_url":"https://api.github.com/users/ExplorData24/followers","following_url":"https://api.github.com/users/ExplorData24/following{/other_user}","gists_url":"https://api.github.com/users/ExplorData24/gists{/gist_id}","starred_url":"https://api.github.com/users/ExplorData24/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ExplorData24/subscriptions","organizations_url":"https://api.github.com/users/ExplorData24/orgs","repos_url":"https://api.github.com/users/ExplorData24/repos","events_url":"https://api.github.com/users/ExplorData24/events{/privacy}","received_events_url":"https://api.github.com/users/ExplorData24/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T07:52:24Z","updated_at":"2023-12-09T07:52:24Z","author_association":"NONE","body":"Hi @pvary \r\nFirst of all, thank you very much for your availability.\r\n\r\n- Actually for this error I remove any trace and URI http://nessie:19120/api/v1 specific to the nessie catalog in the .env file for Spark.\r\n\r\n- But I tried again using Hive Catalog to create Iceberg tables with Spark as the execution engine:\r\n\r\n import pyspark\r\nfrom pyspark.sql import SparkSession\r\nimport os\r\n#DEFINE SENSITIVE VARIABLES\r\nHIVE_URI = os.environ.get(\"HIVE_URI\",\"thrift://hive-metastore:9083\")\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY = os.environ.get(\"AWS_ACCESS_KEY\", \"xxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_KEY = os.environ.get(\"AWS_SECRET_KEY\", \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT = os.environ.get(\"AWS_S3_ENDPOINT\", \"http://minioserver:9000/\")\r\n\r\nprint(AWS_S3_ENDPOINT)\r\nprint(HIVE_URI)\r\nprint(WAREHOUSE)\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName('app_name')\r\n#packages\r\n.set('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178')\r\n#SQL Extensions\r\n.set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions')\r\n#Configuring Catalog\r\n.set('spark.sql.catalog.catalog_hive', 'org.apache.iceberg.spark.SparkCatalog')\r\n.set('spark.sql.catalog.catalog_hive.type', 'hive')\r\n.set('spark.sql.catalog.catalog_hive.uri', HIVE_URI)\r\n.set('spark.sql.catalog.catalog_hive.warehouse.dir', WAREHOUSE)\r\n.set('spark.sql.catalog.catalog_hive.endpoint', AWS_S3_ENDPOINT)\r\n.set('spark.sql.catalog.catalog_hive.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\r\n.set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\r\n.set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\r\n.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n)\r\n\r\n#Start Spark Session\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\n#Create a Table\r\nspark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n\r\n#Insert Some Data\r\nspark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('n1'), ('n2'), ('n3')\").show()\r\n\r\n#Query the Data\r\nspark.sql(\"SELECT * FROM catalog_hive.default.my_table;\").show()\r\n\r\nWhen I try to run createTable command it gives me an exception:\r\nSLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\r\nSLF4J: Defaulting to no-operation (NOP) logger implementation\r\nSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\r\n\r\nPy4JJavaError Traceback (most recent call last)\r\nCell In[4], line 38\r\n35 print(\"Spark Running\")\r\n37 #Create a Table\r\n---> 38 spark.sql(\"CREATE TABLE catalog_hive.default.tmy_table (name STRING) USING iceberg;\").show()\r\n40 #Insert Some Data\r\n41 spark.sql(\"INSERT INTO catalog_hive.default.my_table VALUES ('Alex Merced'), ('Dipankar Mazumdar'), ('Jason Hughes')\").show()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:1034, in SparkSession.sql(self, sqlQuery, **kwargs)\r\n1032 sqlQuery = formatter.format(sqlQuery, **kwargs)\r\n1033 try:\r\n-> 1034 return DataFrame(self._jsparkSession.sql(sqlQuery), self)\r\n1035 finally:\r\n1036 if len(kwargs) > 0:\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1321, in JavaMember.call(self, *args)\r\n1315 command = proto.CALL_COMMAND_NAME +\r\n1316 self.command_header +\r\n1317 args_command +\r\n1318 proto.END_COMMAND_PART\r\n1320 answer = self.gateway_client.send_command(command)\r\n-> 1321 return_value = get_return_value(\r\n1322 answer, self.gateway_client, self.target_id, self.name)\r\n1324 for temp_arg in temp_args:\r\n1325 temp_arg._detach()\r\n\r\nFile ~/.local/lib/python3.10/site-packages/pyspark/sql/utils.py:190, in capture_sql_exception..deco(*a, **kw)\r\n188 def deco(*a: Any, **kw: Any) -> Any:\r\n189 try:\r\n--> 190 return f(*a, **kw)\r\n191 except Py4JJavaError as e:\r\n192 converted = convert_exception(e.java_exception)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\r\n324 value = OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\n325 if answer[1] == REFERENCE_TYPE:\r\n--> 326 raise Py4JJavaError(\r\n327 \"An error occurred while calling {0}{1}{2}.\\n\".\r\n328 format(target_id, \".\", name), value)\r\n329 else:\r\n330 raise Py4JError(\r\n331 \"An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n\".\r\n332 format(target_id, \".\", name, value))\r\n\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 2MBCRA6QRAF6SMBQ, Extended Request ID: s41ibIYx6fFDoMXiRK+8TRNkUT/GsiwqEzR5X2Drq9cY213HQkX19/PxSacQwo+SPX8eAqTNy7k=)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleErrorResponse(AwsXmlPredicatedResponseHandler.java:156)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handleResponse(AwsXmlPredicatedResponseHandler.java:108)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:85)\r\nat software.amazon.awssdk.protocols.xml.internal.unmarshall.AwsXmlPredicatedResponseHandler.handle(AwsXmlPredicatedResponseHandler.java:43)\r\nat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler$Crc32ValidationResponseHandler.handle(AwsSyncClientHandler.java:95)\r\nat software.amazon.awssdk.core.internal.handler.BaseClientHandler.lambda$successTransformationResponseHandler$7(BaseClientHandler.java:245)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:30)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:73)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:42)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:78)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:40)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:50)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptMetricCollectionStage.execute(ApiCallAttemptMetricCollectionStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:81)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:56)\r\nat software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:36)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:80)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:48)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallMetricCollectionStage.execute(ApiCallMetricCollectionStage.java:31)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:206)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37)\r\nat software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26)\r\nat software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:193)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:103)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.doExecute(BaseSyncClientHandler.java:167)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.lambda$execute$1(BaseSyncClientHandler.java:82)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.measureApiCallSuccess(BaseSyncClientHandler.java:175)\r\nat software.amazon.awssdk.core.internal.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:76)\r\nat software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:45)\r\nat software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:56)\r\nat software.amazon.awssdk.services.s3.DefaultS3Client.putObject(DefaultS3Client.java:9325)\r\nat org.apache.iceberg.aws.s3.S3OutputStream.completeUploads(S3OutputStream.java:422)\r\nat org.apache.iceberg.aws.s3.S3OutputStream.close(S3OutputStream.java:267)\r\nat java.base/sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:341)\r\nat java.base/sun.nio.cs.StreamEncoder.close(StreamEncoder.java:161)\r\nat java.base/java.io.OutputStreamWriter.close(OutputStreamWriter.java:255)\r\nat org.apache.iceberg.TableMetadataParser.internalWrite(TableMetadataParser.java:127)\r\nat org.apache.iceberg.TableMetadataParser.overwrite(TableMetadataParser.java:110)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadata(BaseMetastoreTableOperations.java:162)\r\nat org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:234)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:133)\r\nat org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:174)\r\nat org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:261)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\r\nat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\r\nat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\r\nat org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:257)\r\nat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:192)\r\nat org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:99)\r\nat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:45)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\r\nat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\nat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\nat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\nat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\nat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\nat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\nat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\nat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\nat org.apache.spark.sql.Dataset.(Dataset.scala:220)\r\nat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\r\nat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\r\nat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\nat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\nat py4j.Gateway.invoke(Gateway.java:282)\r\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\nat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\nat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\nat java.base/java.lang.Thread.run(Thread.java:829)\r\n\r\n- Even if I use a hadoop type iceberg catalog, I always get the same error:\r\n\r\nPy4JJavaError: An error occurred while calling o49.sql.\r\n: software.amazon.awssdk.services.s3.model.S3Exception: null (Service: S3, Status Code: 400, Request ID: 5KRB8NP7R6TJ2JC4, Extended Request ID: aHgN8vSpD8xf5/Mu9u34rRJF7fcKWlupodDS67WAuQJ5+pTiyWqltK51IJADZKYXEcTSqGbgl2Y=)\"\r\n\r\nFor this script:\r\nimport os\r\nimport pyspark\r\nfrom pyspark.sql import SparkSession\r\n\r\nWAREHOUSE = os.environ.get(\"WAREHOUSE\", \"s3a://warehouse/\")\r\nAWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"xxxxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"xxxxxxxxxxxxxxxxxxxxx\")\r\nAWS_S3_ENDPOINT= os.environ.get(\"AWS_S3_ENDPOINT\",\"http://minioserver:9000/\")\r\n\r\nconf = (\r\npyspark.SparkConf()\r\n.setAppName(\"app_name\")\r\n.set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.0.0,org.apache.hadoop:hadoop-aws:3.3.1,software.amazon.awssdk:bundle:2.17.178,software.amazon.awssdk:url-connection-client:2.17.178\")\r\n.set(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\r\n.set(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\r\n.set(\"spark.sql.catalog.iceberg.type\", \"hadoop\")\r\n.set(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\r\n.set(\"spark.sql.catalog.iceberg.warehouse\", WAREHOUSE)\r\n.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\r\n.set(\"spark.hadoop.fs.s3a.endpoint\", AWS_S3_ENDPOINT)\r\n.set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\r\n.set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\r\n.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\r\n##.set(\"spark.hadoop.fs.s3a.requester.pays.enabled\", \"true\")\r\n)\r\n\r\nspark = SparkSession.builder.config(conf=conf).getOrCreate()\r\nprint(\"Spark Running\")\r\n\r\nspark.sql(\"CREATE TABLE iceberg.tab (name string) USING iceberg;\")\r\n\r\n**NB**: (AWS_ACCESS_KEY and AWS_SECRET_KEy, AWS _S3_ENDPOINT and S3A://warehouse / are correct and I have already tested them for read data from minio.\r\n\r\nAny thoughts on what I might be missing. Thank you! \r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848304506/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848319141","html_url":"https://github.com/apache/iceberg/pull/9173#issuecomment-1848319141","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9173","id":1848319141,"node_id":"IC_kwDOCW7NX85uKxyl","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T08:07:14Z","updated_at":"2023-12-09T08:07:14Z","author_association":"CONTRIBUTOR","body":"Merged to main.\r\nThanks for the PR @mas-chen and @stevenzwu for the review!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848319141/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848319352","html_url":"https://github.com/apache/iceberg/pull/9173#issuecomment-1848319352","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9173","id":1848319352,"node_id":"IC_kwDOCW7NX85uKx14","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T08:08:07Z","updated_at":"2023-12-09T08:08:07Z","author_association":"CONTRIBUTOR","body":"@mas-chen: Please do not forget to port the changes to the other Flink versions. \r\nThanks, Peter ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848319352/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848342540","html_url":"https://github.com/apache/iceberg/pull/9241#issuecomment-1848342540","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9241","id":1848342540,"node_id":"IC_kwDOCW7NX85uK3gM","user":{"login":"nastra","id":271029,"node_id":"MDQ6VXNlcjI3MTAyOQ==","avatar_url":"https://avatars.githubusercontent.com/u/271029?v=4","gravatar_id":"","url":"https://api.github.com/users/nastra","html_url":"https://github.com/nastra","followers_url":"https://api.github.com/users/nastra/followers","following_url":"https://api.github.com/users/nastra/following{/other_user}","gists_url":"https://api.github.com/users/nastra/gists{/gist_id}","starred_url":"https://api.github.com/users/nastra/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nastra/subscriptions","organizations_url":"https://api.github.com/users/nastra/orgs","repos_url":"https://api.github.com/users/nastra/repos","events_url":"https://api.github.com/users/nastra/events{/privacy}","received_events_url":"https://api.github.com/users/nastra/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T09:25:38Z","updated_at":"2023-12-09T09:25:38Z","author_association":"CONTRIBUTOR","body":"@lschetanrao this just needs one minor update around the assumption. Can you also please open an issue to address parameterized tests in this module?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848342540/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848403130","html_url":"https://github.com/apache/iceberg/pull/9147#issuecomment-1848403130","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9147","id":1848403130,"node_id":"IC_kwDOCW7NX85uLGS6","user":{"login":"irshadcc","id":24606617,"node_id":"MDQ6VXNlcjI0NjA2NjE3","avatar_url":"https://avatars.githubusercontent.com/u/24606617?v=4","gravatar_id":"","url":"https://api.github.com/users/irshadcc","html_url":"https://github.com/irshadcc","followers_url":"https://api.github.com/users/irshadcc/followers","following_url":"https://api.github.com/users/irshadcc/following{/other_user}","gists_url":"https://api.github.com/users/irshadcc/gists{/gist_id}","starred_url":"https://api.github.com/users/irshadcc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/irshadcc/subscriptions","organizations_url":"https://api.github.com/users/irshadcc/orgs","repos_url":"https://api.github.com/users/irshadcc/repos","events_url":"https://api.github.com/users/irshadcc/events{/privacy}","received_events_url":"https://api.github.com/users/irshadcc/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T13:00:33Z","updated_at":"2023-12-09T13:00:33Z","author_association":"CONTRIBUTOR","body":"@Fokko Can we merge this PR ?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848403130/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848427549","html_url":"https://github.com/apache/iceberg/pull/9233#issuecomment-1848427549","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9233","id":1848427549,"node_id":"IC_kwDOCW7NX85uLMQd","user":{"login":"advancedxy","id":807537,"node_id":"MDQ6VXNlcjgwNzUzNw==","avatar_url":"https://avatars.githubusercontent.com/u/807537?v=4","gravatar_id":"","url":"https://api.github.com/users/advancedxy","html_url":"https://github.com/advancedxy","followers_url":"https://api.github.com/users/advancedxy/followers","following_url":"https://api.github.com/users/advancedxy/following{/other_user}","gists_url":"https://api.github.com/users/advancedxy/gists{/gist_id}","starred_url":"https://api.github.com/users/advancedxy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/advancedxy/subscriptions","organizations_url":"https://api.github.com/users/advancedxy/orgs","repos_url":"https://api.github.com/users/advancedxy/repos","events_url":"https://api.github.com/users/advancedxy/events{/privacy}","received_events_url":"https://api.github.com/users/advancedxy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T14:39:52Z","updated_at":"2023-12-09T14:39:52Z","author_association":"CONTRIBUTOR","body":"> Spark 3.4 (same as my app) with the condition on the partitions will actually prune the unaffected partitions, while 3.5 will not.\r\n\r\nI did some quick debug. The reason why spark 3.4 succeeded is that `org.apache.spark.sql.execution.datasources.v2.RowLevelCommandScanRelationPushDown`  in Iceberg pushes down join conditions into the target source. In spark 3.5, this rule is removed in favor of upstream spark's `GroupBasedRowLevelOperationScanPlanning`, which push down commands' condition instead of rewrite plan's filter(https://github.com/apache/spark/commit/5a92eccd514b7bc0513feaecb041aee2f8cd5a24#diff-635af3d82f2675b4bb3fd07673916477844a2a7b76d65b23b9cda9a63228ec6dR40).\r\n So to make system function push down work for merge statement, you may have to pattern match ReplaceData and MergeRows, etc. Also cc @aokolnychyi .\r\n\r\n> I'm not sure why Filter would be pushed down to the data source for a full outer join\r\n\r\nThis question is answered, it's covered by `RowLevelCommandScanRelationPushDown` or `GroupBasedRowLevelOperationScanPlanning`\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848427549/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848619220","html_url":"https://github.com/apache/iceberg/pull/9254#issuecomment-1848619220","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9254","id":1848619220,"node_id":"IC_kwDOCW7NX85uL7DU","user":{"login":"stevenzwu","id":1545663,"node_id":"MDQ6VXNlcjE1NDU2NjM=","avatar_url":"https://avatars.githubusercontent.com/u/1545663?v=4","gravatar_id":"","url":"https://api.github.com/users/stevenzwu","html_url":"https://github.com/stevenzwu","followers_url":"https://api.github.com/users/stevenzwu/followers","following_url":"https://api.github.com/users/stevenzwu/following{/other_user}","gists_url":"https://api.github.com/users/stevenzwu/gists{/gist_id}","starred_url":"https://api.github.com/users/stevenzwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/stevenzwu/subscriptions","organizations_url":"https://api.github.com/users/stevenzwu/orgs","repos_url":"https://api.github.com/users/stevenzwu/repos","events_url":"https://api.github.com/users/stevenzwu/events{/privacy}","received_events_url":"https://api.github.com/users/stevenzwu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-09T19:12:45Z","updated_at":"2023-12-09T19:12:45Z","author_association":"CONTRIBUTOR","body":"thanks @pvary and @yegangy0718 for the review","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848619220/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848790182","html_url":"https://github.com/apache/iceberg/issues/7794#issuecomment-1848790182","issue_url":"https://api.github.com/repos/apache/iceberg/issues/7794","id":1848790182,"node_id":"IC_kwDOCW7NX85uMkym","user":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T00:12:57Z","updated_at":"2023-12-10T00:12:57Z","author_association":"NONE","body":"This issue has been automatically marked as stale because it has been open for 180 days with no activity. It will be closed in next 14 days if no further activity occurs. To permanently prevent this issue from being considered stale, add the label 'not-stale', but commenting on the issue is preferred when possible.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848790182/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":15368,"client_id":"Iv1.05c79e9ad1f6bdfa","slug":"github-actions","node_id":"MDM6QXBwMTUzNjg=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"GitHub Actions","description":"Automate your workflow from idea to production","external_url":"https://help.github.com/en/actions","html_url":"https://github.com/apps/github-actions","created_at":"2018-07-30T09:30:17Z","updated_at":"2024-04-10T20:33:16Z","permissions":{"actions":"write","administration":"read","attestations":"write","checks":"write","contents":"write","deployments":"write","discussions":"write","issues":"write","merge_queues":"write","metadata":"read","packages":"write","pages":"write","pull_requests":"write","repository_hooks":"write","repository_projects":"write","security_events":"write","statuses":"write","vulnerability_alerts":"read"},"events":["branch_protection_rule","check_run","check_suite","create","delete","deployment","deployment_status","discussion","discussion_comment","fork","gollum","issues","issue_comment","label","merge_group","milestone","page_build","project","project_card","project_column","public","pull_request","pull_request_review","pull_request_review_comment","push","registry_package","release","repository","repository_dispatch","status","watch","workflow_dispatch","workflow_run"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848794660","html_url":"https://github.com/apache/iceberg/pull/9161#issuecomment-1848794660","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9161","id":1848794660,"node_id":"IC_kwDOCW7NX85uMl4k","user":{"login":"GianlucaPrincipini","id":13685556,"node_id":"MDQ6VXNlcjEzNjg1NTU2","avatar_url":"https://avatars.githubusercontent.com/u/13685556?v=4","gravatar_id":"","url":"https://api.github.com/users/GianlucaPrincipini","html_url":"https://github.com/GianlucaPrincipini","followers_url":"https://api.github.com/users/GianlucaPrincipini/followers","following_url":"https://api.github.com/users/GianlucaPrincipini/following{/other_user}","gists_url":"https://api.github.com/users/GianlucaPrincipini/gists{/gist_id}","starred_url":"https://api.github.com/users/GianlucaPrincipini/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/GianlucaPrincipini/subscriptions","organizations_url":"https://api.github.com/users/GianlucaPrincipini/orgs","repos_url":"https://api.github.com/users/GianlucaPrincipini/repos","events_url":"https://api.github.com/users/GianlucaPrincipini/events{/privacy}","received_events_url":"https://api.github.com/users/GianlucaPrincipini/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T00:28:52Z","updated_at":"2023-12-10T00:28:52Z","author_association":"CONTRIBUTOR","body":"Hi @nastra, I migrated `TestIcebergInputFormats` in MR module, which is a slightly more complex test class than `TestDictionaryRowGroupFilter`. I found some other class that could be migrated to ParameterizedTestExtension, but that would imply major impacts on other modules, since some of these are coupled with common test logic and to their class hierarchy. In my opinion this implementation should be abstract enough to satisfy most needs. If you find there's need to migrate some more class let me know. \r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848794660/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848853419","html_url":"https://github.com/apache/iceberg/pull/9198#issuecomment-1848853419","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9198","id":1848853419,"node_id":"IC_kwDOCW7NX85uM0Or","user":{"login":"dependabot[bot]","id":49699333,"node_id":"MDM6Qm90NDk2OTkzMzM=","avatar_url":"https://avatars.githubusercontent.com/in/29110?v=4","gravatar_id":"","url":"https://api.github.com/users/dependabot%5Bbot%5D","html_url":"https://github.com/apps/dependabot","followers_url":"https://api.github.com/users/dependabot%5Bbot%5D/followers","following_url":"https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dependabot%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/dependabot%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/dependabot%5Bbot%5D/repos","events_url":"https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/dependabot%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T04:18:01Z","updated_at":"2023-12-10T04:18:01Z","author_association":"CONTRIBUTOR","body":"Superseded by #9256.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848853419/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":29110,"client_id":"Iv1.4f9a6346434f815e","slug":"dependabot","node_id":"MDM6QXBwMjkxMTA=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"Dependabot","description":"","external_url":"https://dependabot-api.githubapp.com","html_url":"https://github.com/apps/dependabot","created_at":"2019-04-16T22:34:25Z","updated_at":"2024-03-20T21:06:35Z","permissions":{"actions":"read","checks":"write","contents":"write","issues":"write","members":"read","metadata":"read","pull_requests":"write","statuses":"read","vulnerability_alerts":"read","workflows":"write"},"events":["check_suite","issues","issue_comment","label","pull_request","pull_request_review","pull_request_review_comment","repository"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848855492","html_url":"https://github.com/apache/iceberg/pull/9214#issuecomment-1848855492","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9214","id":1848855492,"node_id":"IC_kwDOCW7NX85uM0vE","user":{"login":"dependabot[bot]","id":49699333,"node_id":"MDM6Qm90NDk2OTkzMzM=","avatar_url":"https://avatars.githubusercontent.com/in/29110?v=4","gravatar_id":"","url":"https://api.github.com/users/dependabot%5Bbot%5D","html_url":"https://github.com/apps/dependabot","followers_url":"https://api.github.com/users/dependabot%5Bbot%5D/followers","following_url":"https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dependabot%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/dependabot%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/dependabot%5Bbot%5D/repos","events_url":"https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/dependabot%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T04:30:44Z","updated_at":"2023-12-10T04:30:44Z","author_association":"CONTRIBUTOR","body":"Superseded by #9259.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848855492/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":29110,"client_id":"Iv1.4f9a6346434f815e","slug":"dependabot","node_id":"MDM6QXBwMjkxMTA=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"Dependabot","description":"","external_url":"https://dependabot-api.githubapp.com","html_url":"https://github.com/apps/dependabot","created_at":"2019-04-16T22:34:25Z","updated_at":"2024-03-20T21:06:35Z","permissions":{"actions":"read","checks":"write","contents":"write","issues":"write","members":"read","metadata":"read","pull_requests":"write","statuses":"read","vulnerability_alerts":"read","workflows":"write"},"events":["check_suite","issues","issue_comment","label","pull_request","pull_request_review","pull_request_review_comment","repository"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848855638","html_url":"https://github.com/apache/iceberg/pull/9201#issuecomment-1848855638","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9201","id":1848855638,"node_id":"IC_kwDOCW7NX85uM0xW","user":{"login":"dependabot[bot]","id":49699333,"node_id":"MDM6Qm90NDk2OTkzMzM=","avatar_url":"https://avatars.githubusercontent.com/in/29110?v=4","gravatar_id":"","url":"https://api.github.com/users/dependabot%5Bbot%5D","html_url":"https://github.com/apps/dependabot","followers_url":"https://api.github.com/users/dependabot%5Bbot%5D/followers","following_url":"https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dependabot%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/dependabot%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/dependabot%5Bbot%5D/repos","events_url":"https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/dependabot%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T04:31:47Z","updated_at":"2023-12-10T04:31:47Z","author_association":"CONTRIBUTOR","body":"Superseded by #9262.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848855638/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":29110,"client_id":"Iv1.4f9a6346434f815e","slug":"dependabot","node_id":"MDM6QXBwMjkxMTA=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"Dependabot","description":"","external_url":"https://dependabot-api.githubapp.com","html_url":"https://github.com/apps/dependabot","created_at":"2019-04-16T22:34:25Z","updated_at":"2024-03-20T21:06:35Z","permissions":{"actions":"read","checks":"write","contents":"write","issues":"write","members":"read","metadata":"read","pull_requests":"write","statuses":"read","vulnerability_alerts":"read","workflows":"write"},"events":["check_suite","issues","issue_comment","label","pull_request","pull_request_review","pull_request_review_comment","repository"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848855650","html_url":"https://github.com/apache/iceberg/pull/8986#issuecomment-1848855650","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8986","id":1848855650,"node_id":"IC_kwDOCW7NX85uM0xi","user":{"login":"dependabot[bot]","id":49699333,"node_id":"MDM6Qm90NDk2OTkzMzM=","avatar_url":"https://avatars.githubusercontent.com/in/29110?v=4","gravatar_id":"","url":"https://api.github.com/users/dependabot%5Bbot%5D","html_url":"https://github.com/apps/dependabot","followers_url":"https://api.github.com/users/dependabot%5Bbot%5D/followers","following_url":"https://api.github.com/users/dependabot%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/dependabot%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/dependabot%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dependabot%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/dependabot%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/dependabot%5Bbot%5D/repos","events_url":"https://api.github.com/users/dependabot%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/dependabot%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T04:31:51Z","updated_at":"2023-12-10T04:31:51Z","author_association":"CONTRIBUTOR","body":"Superseded by #9263.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848855650/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":29110,"client_id":"Iv1.4f9a6346434f815e","slug":"dependabot","node_id":"MDM6QXBwMjkxMTA=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"Dependabot","description":"","external_url":"https://dependabot-api.githubapp.com","html_url":"https://github.com/apps/dependabot","created_at":"2019-04-16T22:34:25Z","updated_at":"2024-03-20T21:06:35Z","permissions":{"actions":"read","checks":"write","contents":"write","issues":"write","members":"read","metadata":"read","pull_requests":"write","statuses":"read","vulnerability_alerts":"read","workflows":"write"},"events":["check_suite","issues","issue_comment","label","pull_request","pull_request_review","pull_request_review_comment","repository"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848948812","html_url":"https://github.com/apache/iceberg/issues/9249#issuecomment-1848948812","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9249","id":1848948812,"node_id":"IC_kwDOCW7NX85uNLhM","user":{"login":"Fokko","id":1134248,"node_id":"MDQ6VXNlcjExMzQyNDg=","avatar_url":"https://avatars.githubusercontent.com/u/1134248?v=4","gravatar_id":"","url":"https://api.github.com/users/Fokko","html_url":"https://github.com/Fokko","followers_url":"https://api.github.com/users/Fokko/followers","following_url":"https://api.github.com/users/Fokko/following{/other_user}","gists_url":"https://api.github.com/users/Fokko/gists{/gist_id}","starred_url":"https://api.github.com/users/Fokko/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Fokko/subscriptions","organizations_url":"https://api.github.com/users/Fokko/orgs","repos_url":"https://api.github.com/users/Fokko/repos","events_url":"https://api.github.com/users/Fokko/events{/privacy}","received_events_url":"https://api.github.com/users/Fokko/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T12:18:23Z","updated_at":"2023-12-10T12:18:23Z","author_association":"CONTRIBUTOR","body":"Hi @coolderli thanks for creating a PR here. Have you tried referencing it as `file:///azure_account_name/`?\r\n\r\nThe catalog is an essential concept of the Iceberg, so it is best to reference it through the table name, instead of the path. The path is only supported for [Hadoop based tables](https://iceberg.apache.org/docs/latest/spark-configuration/#catalog) and should not be used against an object store (that doesn't provide atomic renames).","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848948812/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848951846","html_url":"https://github.com/apache/iceberg/issues/9086#issuecomment-1848951846","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9086","id":1848951846,"node_id":"IC_kwDOCW7NX85uNMQm","user":{"login":"chinmay-bhat","id":12948588,"node_id":"MDQ6VXNlcjEyOTQ4NTg4","avatar_url":"https://avatars.githubusercontent.com/u/12948588?v=4","gravatar_id":"","url":"https://api.github.com/users/chinmay-bhat","html_url":"https://github.com/chinmay-bhat","followers_url":"https://api.github.com/users/chinmay-bhat/followers","following_url":"https://api.github.com/users/chinmay-bhat/following{/other_user}","gists_url":"https://api.github.com/users/chinmay-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/chinmay-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chinmay-bhat/subscriptions","organizations_url":"https://api.github.com/users/chinmay-bhat/orgs","repos_url":"https://api.github.com/users/chinmay-bhat/repos","events_url":"https://api.github.com/users/chinmay-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/chinmay-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T12:31:08Z","updated_at":"2023-12-10T12:31:08Z","author_association":"CONTRIBUTOR","body":"Hi @nastra, can I be assigned to this? Looks like the dependent issues have been resolved/merged. \r\n\r\nAlso I've read through the discussion on #7160. Is there any other dependency / resource I need to read before beginning?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1848951846/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849097775","html_url":"https://github.com/apache/iceberg/pull/9012#issuecomment-1849097775","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9012","id":1849097775,"node_id":"IC_kwDOCW7NX85uNv4v","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T21:52:16Z","updated_at":"2023-12-10T21:52:16Z","author_association":"CONTRIBUTOR","body":"Thanks for this update!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849097775/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849131934","html_url":"https://github.com/apache/iceberg/pull/6884#issuecomment-1849131934","issue_url":"https://api.github.com/repos/apache/iceberg/issues/6884","id":1849131934,"node_id":"IC_kwDOCW7NX85uN4Oe","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-10T23:52:36Z","updated_at":"2023-12-10T23:52:36Z","author_association":"CONTRIBUTOR","body":"@ggershinsky, I opened https://github.com/ggershinsky/iceberg/pull/9/files against your branch with the last changes I think are needed to get this in. Please review and let me know so we can get to the next one. Thanks!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849131934/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849140039","html_url":"https://github.com/apache/iceberg/issues/7735#issuecomment-1849140039","issue_url":"https://api.github.com/repos/apache/iceberg/issues/7735","id":1849140039,"node_id":"IC_kwDOCW7NX85uN6NH","user":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T00:12:11Z","updated_at":"2023-12-11T00:12:11Z","author_association":"NONE","body":"This issue has been closed because it has not received any activity in the last 14 days since being marked as 'stale'","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849140039/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":15368,"client_id":"Iv1.05c79e9ad1f6bdfa","slug":"github-actions","node_id":"MDM6QXBwMTUzNjg=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"GitHub Actions","description":"Automate your workflow from idea to production","external_url":"https://help.github.com/en/actions","html_url":"https://github.com/apps/github-actions","created_at":"2018-07-30T09:30:17Z","updated_at":"2024-04-10T20:33:16Z","permissions":{"actions":"write","administration":"read","attestations":"write","checks":"write","contents":"write","deployments":"write","discussions":"write","issues":"write","merge_queues":"write","metadata":"read","packages":"write","pages":"write","pull_requests":"write","repository_hooks":"write","repository_projects":"write","security_events":"write","statuses":"write","vulnerability_alerts":"read"},"events":["branch_protection_rule","check_run","check_suite","create","delete","deployment","deployment_status","discussion","discussion_comment","fork","gollum","issues","issue_comment","label","merge_group","milestone","page_build","project","project_card","project_column","public","pull_request","pull_request_review","pull_request_review_comment","push","registry_package","release","repository","repository_dispatch","status","watch","workflow_dispatch","workflow_run"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849140055","html_url":"https://github.com/apache/iceberg/issues/7706#issuecomment-1849140055","issue_url":"https://api.github.com/repos/apache/iceberg/issues/7706","id":1849140055,"node_id":"IC_kwDOCW7NX85uN6NX","user":{"login":"github-actions[bot]","id":41898282,"node_id":"MDM6Qm90NDE4OTgyODI=","avatar_url":"https://avatars.githubusercontent.com/in/15368?v=4","gravatar_id":"","url":"https://api.github.com/users/github-actions%5Bbot%5D","html_url":"https://github.com/apps/github-actions","followers_url":"https://api.github.com/users/github-actions%5Bbot%5D/followers","following_url":"https://api.github.com/users/github-actions%5Bbot%5D/following{/other_user}","gists_url":"https://api.github.com/users/github-actions%5Bbot%5D/gists{/gist_id}","starred_url":"https://api.github.com/users/github-actions%5Bbot%5D/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github-actions%5Bbot%5D/subscriptions","organizations_url":"https://api.github.com/users/github-actions%5Bbot%5D/orgs","repos_url":"https://api.github.com/users/github-actions%5Bbot%5D/repos","events_url":"https://api.github.com/users/github-actions%5Bbot%5D/events{/privacy}","received_events_url":"https://api.github.com/users/github-actions%5Bbot%5D/received_events","type":"Bot","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T00:12:13Z","updated_at":"2023-12-11T00:12:13Z","author_association":"NONE","body":"This issue has been closed because it has not received any activity in the last 14 days since being marked as 'stale'","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849140055/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":{"id":15368,"client_id":"Iv1.05c79e9ad1f6bdfa","slug":"github-actions","node_id":"MDM6QXBwMTUzNjg=","owner":{"login":"github","id":9919,"node_id":"MDEyOk9yZ2FuaXphdGlvbjk5MTk=","avatar_url":"https://avatars.githubusercontent.com/u/9919?v=4","gravatar_id":"","url":"https://api.github.com/users/github","html_url":"https://github.com/github","followers_url":"https://api.github.com/users/github/followers","following_url":"https://api.github.com/users/github/following{/other_user}","gists_url":"https://api.github.com/users/github/gists{/gist_id}","starred_url":"https://api.github.com/users/github/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/github/subscriptions","organizations_url":"https://api.github.com/users/github/orgs","repos_url":"https://api.github.com/users/github/repos","events_url":"https://api.github.com/users/github/events{/privacy}","received_events_url":"https://api.github.com/users/github/received_events","type":"Organization","user_view_type":"public","site_admin":false},"name":"GitHub Actions","description":"Automate your workflow from idea to production","external_url":"https://help.github.com/en/actions","html_url":"https://github.com/apps/github-actions","created_at":"2018-07-30T09:30:17Z","updated_at":"2024-04-10T20:33:16Z","permissions":{"actions":"write","administration":"read","attestations":"write","checks":"write","contents":"write","deployments":"write","discussions":"write","issues":"write","merge_queues":"write","metadata":"read","packages":"write","pages":"write","pull_requests":"write","repository_hooks":"write","repository_projects":"write","security_events":"write","statuses":"write","vulnerability_alerts":"read"},"events":["branch_protection_rule","check_run","check_suite","create","delete","deployment","deployment_status","discussion","discussion_comment","fork","gollum","issues","issue_comment","label","merge_group","milestone","page_build","project","project_card","project_column","public","pull_request","pull_request_review","pull_request_review_comment","push","registry_package","release","repository","repository_dispatch","status","watch","workflow_dispatch","workflow_run"]}},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849210376","html_url":"https://github.com/apache/iceberg/issues/9127#issuecomment-1849210376","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9127","id":1849210376,"node_id":"IC_kwDOCW7NX85uOLYI","user":{"login":"ronkorving","id":631240,"node_id":"MDQ6VXNlcjYzMTI0MA==","avatar_url":"https://avatars.githubusercontent.com/u/631240?v=4","gravatar_id":"","url":"https://api.github.com/users/ronkorving","html_url":"https://github.com/ronkorving","followers_url":"https://api.github.com/users/ronkorving/followers","following_url":"https://api.github.com/users/ronkorving/following{/other_user}","gists_url":"https://api.github.com/users/ronkorving/gists{/gist_id}","starred_url":"https://api.github.com/users/ronkorving/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ronkorving/subscriptions","organizations_url":"https://api.github.com/users/ronkorving/orgs","repos_url":"https://api.github.com/users/ronkorving/repos","events_url":"https://api.github.com/users/ronkorving/events{/privacy}","received_events_url":"https://api.github.com/users/ronkorving/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T02:03:23Z","updated_at":"2023-12-11T02:03:23Z","author_association":"NONE","body":"I would be happy to make my first contribution :)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849210376/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849306565","html_url":"https://github.com/apache/iceberg/pull/9240#issuecomment-1849306565","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9240","id":1849306565,"node_id":"IC_kwDOCW7NX85uOi3F","user":{"login":"Fokko","id":1134248,"node_id":"MDQ6VXNlcjExMzQyNDg=","avatar_url":"https://avatars.githubusercontent.com/u/1134248?v=4","gravatar_id":"","url":"https://api.github.com/users/Fokko","html_url":"https://github.com/Fokko","followers_url":"https://api.github.com/users/Fokko/followers","following_url":"https://api.github.com/users/Fokko/following{/other_user}","gists_url":"https://api.github.com/users/Fokko/gists{/gist_id}","starred_url":"https://api.github.com/users/Fokko/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Fokko/subscriptions","organizations_url":"https://api.github.com/users/Fokko/orgs","repos_url":"https://api.github.com/users/Fokko/repos","events_url":"https://api.github.com/users/Fokko/events{/privacy}","received_events_url":"https://api.github.com/users/Fokko/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T04:27:57Z","updated_at":"2023-12-11T04:27:57Z","author_association":"CONTRIBUTOR","body":"Thanks @nastra for spotting! I think something went wrong with the rebase. I've updated the PR","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849306565/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849353985","html_url":"https://github.com/apache/iceberg/pull/9241#issuecomment-1849353985","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9241","id":1849353985,"node_id":"IC_kwDOCW7NX85uOucB","user":{"login":"lschetanrao","id":48798234,"node_id":"MDQ6VXNlcjQ4Nzk4MjM0","avatar_url":"https://avatars.githubusercontent.com/u/48798234?v=4","gravatar_id":"","url":"https://api.github.com/users/lschetanrao","html_url":"https://github.com/lschetanrao","followers_url":"https://api.github.com/users/lschetanrao/followers","following_url":"https://api.github.com/users/lschetanrao/following{/other_user}","gists_url":"https://api.github.com/users/lschetanrao/gists{/gist_id}","starred_url":"https://api.github.com/users/lschetanrao/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lschetanrao/subscriptions","organizations_url":"https://api.github.com/users/lschetanrao/orgs","repos_url":"https://api.github.com/users/lschetanrao/repos","events_url":"https://api.github.com/users/lschetanrao/events{/privacy}","received_events_url":"https://api.github.com/users/lschetanrao/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T05:30:27Z","updated_at":"2023-12-11T05:30:27Z","author_association":"CONTRIBUTOR","body":"> @lschetanrao this just needs one minor update around the assumption. Can you also please open an issue to address parameterized tests in this module?\r\n\r\nYes sure. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849353985/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849458811","html_url":"https://github.com/apache/iceberg/pull/8909#issuecomment-1849458811","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8909","id":1849458811,"node_id":"IC_kwDOCW7NX85uPIB7","user":{"login":"ajantha-bhat","id":5889404,"node_id":"MDQ6VXNlcjU4ODk0MDQ=","avatar_url":"https://avatars.githubusercontent.com/u/5889404?v=4","gravatar_id":"","url":"https://api.github.com/users/ajantha-bhat","html_url":"https://github.com/ajantha-bhat","followers_url":"https://api.github.com/users/ajantha-bhat/followers","following_url":"https://api.github.com/users/ajantha-bhat/following{/other_user}","gists_url":"https://api.github.com/users/ajantha-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/ajantha-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ajantha-bhat/subscriptions","organizations_url":"https://api.github.com/users/ajantha-bhat/orgs","repos_url":"https://api.github.com/users/ajantha-bhat/repos","events_url":"https://api.github.com/users/ajantha-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/ajantha-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T07:21:40Z","updated_at":"2023-12-11T07:21:40Z","author_association":"MEMBER","body":"@nastra: Looks like https://github.com/apache/iceberg/pull/9012 is causing test failure for this PR after rebase. I tried fixing at Nessie side. But it opens up another issues. Please take a look.\r\n ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849458811/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849492284","html_url":"https://github.com/apache/iceberg/pull/9271#issuecomment-1849492284","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9271","id":1849492284,"node_id":"IC_kwDOCW7NX85uPQM8","user":{"login":"nastra","id":271029,"node_id":"MDQ6VXNlcjI3MTAyOQ==","avatar_url":"https://avatars.githubusercontent.com/u/271029?v=4","gravatar_id":"","url":"https://api.github.com/users/nastra","html_url":"https://github.com/nastra","followers_url":"https://api.github.com/users/nastra/followers","following_url":"https://api.github.com/users/nastra/following{/other_user}","gists_url":"https://api.github.com/users/nastra/gists{/gist_id}","starred_url":"https://api.github.com/users/nastra/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nastra/subscriptions","organizations_url":"https://api.github.com/users/nastra/orgs","repos_url":"https://api.github.com/users/nastra/repos","events_url":"https://api.github.com/users/nastra/events{/privacy}","received_events_url":"https://api.github.com/users/nastra/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T07:50:09Z","updated_at":"2023-12-11T07:50:09Z","author_association":"CONTRIBUTOR","body":"LGTM","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849492284/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849563358","html_url":"https://github.com/apache/iceberg/issues/9086#issuecomment-1849563358","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9086","id":1849563358,"node_id":"IC_kwDOCW7NX85uPhje","user":{"login":"nastra","id":271029,"node_id":"MDQ6VXNlcjI3MTAyOQ==","avatar_url":"https://avatars.githubusercontent.com/u/271029?v=4","gravatar_id":"","url":"https://api.github.com/users/nastra","html_url":"https://github.com/nastra","followers_url":"https://api.github.com/users/nastra/followers","following_url":"https://api.github.com/users/nastra/following{/other_user}","gists_url":"https://api.github.com/users/nastra/gists{/gist_id}","starred_url":"https://api.github.com/users/nastra/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nastra/subscriptions","organizations_url":"https://api.github.com/users/nastra/orgs","repos_url":"https://api.github.com/users/nastra/repos","events_url":"https://api.github.com/users/nastra/events{/privacy}","received_events_url":"https://api.github.com/users/nastra/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T08:42:06Z","updated_at":"2023-12-11T08:42:06Z","author_association":"CONTRIBUTOR","body":"For parameterized testing we would need to have https://github.com/apache/iceberg/issues/9210 implemented first. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849563358/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849677224","html_url":"https://github.com/apache/iceberg/issues/9127#issuecomment-1849677224","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9127","id":1849677224,"node_id":"IC_kwDOCW7NX85uP9Wo","user":{"login":"ronkorving","id":631240,"node_id":"MDQ6VXNlcjYzMTI0MA==","avatar_url":"https://avatars.githubusercontent.com/u/631240?v=4","gravatar_id":"","url":"https://api.github.com/users/ronkorving","html_url":"https://github.com/ronkorving","followers_url":"https://api.github.com/users/ronkorving/followers","following_url":"https://api.github.com/users/ronkorving/following{/other_user}","gists_url":"https://api.github.com/users/ronkorving/gists{/gist_id}","starred_url":"https://api.github.com/users/ronkorving/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ronkorving/subscriptions","organizations_url":"https://api.github.com/users/ronkorving/orgs","repos_url":"https://api.github.com/users/ronkorving/repos","events_url":"https://api.github.com/users/ronkorving/events{/privacy}","received_events_url":"https://api.github.com/users/ronkorving/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T09:44:40Z","updated_at":"2023-12-11T09:44:40Z","author_association":"NONE","body":"PR is ready for review ^","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849677224/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849745072","html_url":"https://github.com/apache/iceberg/pull/9267#issuecomment-1849745072","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9267","id":1849745072,"node_id":"IC_kwDOCW7NX85uQN6w","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T10:17:47Z","updated_at":"2023-12-11T10:17:47Z","author_association":"CONTRIBUTOR","body":"@nastra @Fokko @flyrain @amogh-jahagirdar, could you check this one?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849745072/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849747058","html_url":"https://github.com/apache/iceberg/issues/9249#issuecomment-1849747058","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9249","id":1849747058,"node_id":"IC_kwDOCW7NX85uQOZy","user":{"login":"coolderli","id":38486782,"node_id":"MDQ6VXNlcjM4NDg2Nzgy","avatar_url":"https://avatars.githubusercontent.com/u/38486782?v=4","gravatar_id":"","url":"https://api.github.com/users/coolderli","html_url":"https://github.com/coolderli","followers_url":"https://api.github.com/users/coolderli/followers","following_url":"https://api.github.com/users/coolderli/following{/other_user}","gists_url":"https://api.github.com/users/coolderli/gists{/gist_id}","starred_url":"https://api.github.com/users/coolderli/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/coolderli/subscriptions","organizations_url":"https://api.github.com/users/coolderli/orgs","repos_url":"https://api.github.com/users/coolderli/repos","events_url":"https://api.github.com/users/coolderli/events{/privacy}","received_events_url":"https://api.github.com/users/coolderli/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T10:18:55Z","updated_at":"2023-12-11T10:18:55Z","author_association":"CONTRIBUTOR","body":"@Fokko Thanks for your reply. I think we can use a table for the files that have schema. \r\n\r\nIf a file does not have a schema, tables cannot be used. Of course, files can be directly used on object storage, but directories are difficult to manage\r\n\r\nI am wondering if it is possible to provide external references to files stored on object storage in a unified directory structure, such as using a fixed prefix : /volume/catalog_ Name/database_ Name/volume_ Name\r\n\r\nWe can record the actual object storage address of the file in the iceberg manifest file, so that we can also provide a snapshot without relying on atomic renaming of object storage\r\n\r\nThis does indeed reduce the read and write performance of file semantics. If submitted too frequently, it may cause small file problems, but it is possible if we use Spark for batch commit. In addition, I found that Spark does not have a catalog of the files, so maybe it is possible to implement a FileCatalog.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849747058/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849868289","html_url":"https://github.com/apache/iceberg/issues/9086#issuecomment-1849868289","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9086","id":1849868289,"node_id":"IC_kwDOCW7NX85uQsAB","user":{"login":"chinmay-bhat","id":12948588,"node_id":"MDQ6VXNlcjEyOTQ4NTg4","avatar_url":"https://avatars.githubusercontent.com/u/12948588?v=4","gravatar_id":"","url":"https://api.github.com/users/chinmay-bhat","html_url":"https://github.com/chinmay-bhat","followers_url":"https://api.github.com/users/chinmay-bhat/followers","following_url":"https://api.github.com/users/chinmay-bhat/following{/other_user}","gists_url":"https://api.github.com/users/chinmay-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/chinmay-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chinmay-bhat/subscriptions","organizations_url":"https://api.github.com/users/chinmay-bhat/orgs","repos_url":"https://api.github.com/users/chinmay-bhat/repos","events_url":"https://api.github.com/users/chinmay-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/chinmay-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T11:18:51Z","updated_at":"2023-12-11T11:19:13Z","author_association":"CONTRIBUTOR","body":"Ok, so can I pick this up once the parameterized tests PR is merged? \r\n\r\nAlso, `iceberg-spark` has folders for each version (v3.2, 3.3, 3.4, 3.5). Do you recommend creating separate PRs for each?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849868289/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849884128","html_url":"https://github.com/apache/iceberg/issues/9086#issuecomment-1849884128","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9086","id":1849884128,"node_id":"IC_kwDOCW7NX85uQv3g","user":{"login":"nastra","id":271029,"node_id":"MDQ6VXNlcjI3MTAyOQ==","avatar_url":"https://avatars.githubusercontent.com/u/271029?v=4","gravatar_id":"","url":"https://api.github.com/users/nastra","html_url":"https://github.com/nastra","followers_url":"https://api.github.com/users/nastra/followers","following_url":"https://api.github.com/users/nastra/following{/other_user}","gists_url":"https://api.github.com/users/nastra/gists{/gist_id}","starred_url":"https://api.github.com/users/nastra/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nastra/subscriptions","organizations_url":"https://api.github.com/users/nastra/orgs","repos_url":"https://api.github.com/users/nastra/repos","events_url":"https://api.github.com/users/nastra/events{/privacy}","received_events_url":"https://api.github.com/users/nastra/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T11:29:37Z","updated_at":"2023-12-11T11:29:37Z","author_association":"CONTRIBUTOR","body":"We should be mostly focusing on migrating Spark 3.5, which is already a big task","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849884128/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849920644","html_url":"https://github.com/apache/iceberg/pull/6887#issuecomment-1849920644","issue_url":"https://api.github.com/repos/apache/iceberg/issues/6887","id":1849920644,"node_id":"IC_kwDOCW7NX85uQ4yE","user":{"login":"ajantha-bhat","id":5889404,"node_id":"MDQ6VXNlcjU4ODk0MDQ=","avatar_url":"https://avatars.githubusercontent.com/u/5889404?v=4","gravatar_id":"","url":"https://api.github.com/users/ajantha-bhat","html_url":"https://github.com/ajantha-bhat","followers_url":"https://api.github.com/users/ajantha-bhat/followers","following_url":"https://api.github.com/users/ajantha-bhat/following{/other_user}","gists_url":"https://api.github.com/users/ajantha-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/ajantha-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ajantha-bhat/subscriptions","organizations_url":"https://api.github.com/users/ajantha-bhat/orgs","repos_url":"https://api.github.com/users/ajantha-bhat/repos","events_url":"https://api.github.com/users/ajantha-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/ajantha-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T11:53:01Z","updated_at":"2023-12-11T11:53:01Z","author_association":"MEMBER","body":"PR is ready @danielcweeks, @nastra, @Fokko   ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1849920644/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850148369","html_url":"https://github.com/apache/iceberg/issues/9086#issuecomment-1850148369","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9086","id":1850148369,"node_id":"IC_kwDOCW7NX85uRwYR","user":{"login":"chinmay-bhat","id":12948588,"node_id":"MDQ6VXNlcjEyOTQ4NTg4","avatar_url":"https://avatars.githubusercontent.com/u/12948588?v=4","gravatar_id":"","url":"https://api.github.com/users/chinmay-bhat","html_url":"https://github.com/chinmay-bhat","followers_url":"https://api.github.com/users/chinmay-bhat/followers","following_url":"https://api.github.com/users/chinmay-bhat/following{/other_user}","gists_url":"https://api.github.com/users/chinmay-bhat/gists{/gist_id}","starred_url":"https://api.github.com/users/chinmay-bhat/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chinmay-bhat/subscriptions","organizations_url":"https://api.github.com/users/chinmay-bhat/orgs","repos_url":"https://api.github.com/users/chinmay-bhat/repos","events_url":"https://api.github.com/users/chinmay-bhat/events{/privacy}","received_events_url":"https://api.github.com/users/chinmay-bhat/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T14:05:29Z","updated_at":"2023-12-11T14:05:29Z","author_association":"CONTRIBUTOR","body":"As it's a big task, I'll get started migrating tests in `iceberg-spark` v3.5 that are not parameterized, and later open a new PR for the parameterized ones :)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850148369/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850354505","html_url":"https://github.com/apache/iceberg/issues/9275#issuecomment-1850354505","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9275","id":1850354505,"node_id":"IC_kwDOCW7NX85uSitJ","user":{"login":"mahendrachandrasekhar","id":86947073,"node_id":"MDQ6VXNlcjg2OTQ3MDcz","avatar_url":"https://avatars.githubusercontent.com/u/86947073?v=4","gravatar_id":"","url":"https://api.github.com/users/mahendrachandrasekhar","html_url":"https://github.com/mahendrachandrasekhar","followers_url":"https://api.github.com/users/mahendrachandrasekhar/followers","following_url":"https://api.github.com/users/mahendrachandrasekhar/following{/other_user}","gists_url":"https://api.github.com/users/mahendrachandrasekhar/gists{/gist_id}","starred_url":"https://api.github.com/users/mahendrachandrasekhar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mahendrachandrasekhar/subscriptions","organizations_url":"https://api.github.com/users/mahendrachandrasekhar/orgs","repos_url":"https://api.github.com/users/mahendrachandrasekhar/repos","events_url":"https://api.github.com/users/mahendrachandrasekhar/events{/privacy}","received_events_url":"https://api.github.com/users/mahendrachandrasekhar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T15:51:10Z","updated_at":"2023-12-11T15:51:10Z","author_association":"NONE","body":"We have this included in our docker image (which is based of bitnami/spark:3.2.4\r\n\r\nRUN curl -L -o /home/airflow/spark/jars/hadoop-aws.jar https://repo.maven.apache.org/maven2/org/apache/hadoop/hadoop-aws/2.10.2/hadoop-aws-2.10.2.jar && \\\r\n    curl -L -o /home/airflow/spark/jars/caffeine.jar https://repo.maven.apache.org/maven2/com/github/ben-manes/caffeine/caffeine/3.1.3/caffeine-3.1.3.jar && \\\r\n    curl -L -o /home/airflow/spark/jars/iceberg-core.jar https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-core/1.4.2/iceberg-core-1.4.2.jar && \\\r\n    curl -L -o /home/airflow/spark/jars/iceberg-spark.jar https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-spark-3.5_2.13/1.4.2/iceberg-spark-3.5_2.13-1.4.2.jar && \\\r\n    curl -L -o /home/airflow/spark/jars/iceberg-spark-runtime.jar https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.13/1.4.2/iceberg-spark-runtime-3.5_2.13-1.4.2.jar && \\\r\n    curl -L -o /home/airflow/spark/jars/iceberg-parquet.jar https://repo.maven.apache.org/maven2/org/apache/iceberg/iceberg-parquet/1.4.2/iceberg-parquet-1.4.2.jar\r\n\r\nENV SPARK_DIST_CLASSPATH $SPARK_DIST_CLASSPATH:/home/airflow/spark/jars/hadoop-aws.jar:/home/airflow/spark/jars/iceberg-core.jar:/home/airflow/spark/jars/caffeine.jar:/home/airflow/spark/jars/iceberg-spark-runtime.jar:/home/airflow/spark/jars/iceberg-parquet.jar:/home/airflow/spark/jars/iceberg-spark.jar\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850354505/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850509409","html_url":"https://github.com/apache/iceberg/pull/9253#issuecomment-1850509409","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9253","id":1850509409,"node_id":"IC_kwDOCW7NX85uTIhh","user":{"login":"puchengy","id":8072956,"node_id":"MDQ6VXNlcjgwNzI5NTY=","avatar_url":"https://avatars.githubusercontent.com/u/8072956?v=4","gravatar_id":"","url":"https://api.github.com/users/puchengy","html_url":"https://github.com/puchengy","followers_url":"https://api.github.com/users/puchengy/followers","following_url":"https://api.github.com/users/puchengy/following{/other_user}","gists_url":"https://api.github.com/users/puchengy/gists{/gist_id}","starred_url":"https://api.github.com/users/puchengy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/puchengy/subscriptions","organizations_url":"https://api.github.com/users/puchengy/orgs","repos_url":"https://api.github.com/users/puchengy/repos","events_url":"https://api.github.com/users/puchengy/events{/privacy}","received_events_url":"https://api.github.com/users/puchengy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T17:12:53Z","updated_at":"2023-12-11T17:12:53Z","author_association":"CONTRIBUTOR","body":"@aokolnychyi Addressed your comment, PTAL, thanks!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850509409/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850720036","html_url":"https://github.com/apache/iceberg/issues/9268#issuecomment-1850720036","issue_url":"https://api.github.com/repos/apache/iceberg/issues/9268","id":1850720036,"node_id":"IC_kwDOCW7NX85uT78k","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T19:13:06Z","updated_at":"2023-12-11T19:13:06Z","author_association":"CONTRIBUTOR","body":"I don't think I'm following the logic here. Is there a case where you're not seeing columns being properly pruned?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850720036/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850721026","html_url":"https://github.com/apache/iceberg/pull/8340#issuecomment-1850721026","issue_url":"https://api.github.com/repos/apache/iceberg/issues/8340","id":1850721026,"node_id":"IC_kwDOCW7NX85uT8MC","user":{"login":"dramaticlly","id":5961173,"node_id":"MDQ6VXNlcjU5NjExNzM=","avatar_url":"https://avatars.githubusercontent.com/u/5961173?v=4","gravatar_id":"","url":"https://api.github.com/users/dramaticlly","html_url":"https://github.com/dramaticlly","followers_url":"https://api.github.com/users/dramaticlly/followers","following_url":"https://api.github.com/users/dramaticlly/following{/other_user}","gists_url":"https://api.github.com/users/dramaticlly/gists{/gist_id}","starred_url":"https://api.github.com/users/dramaticlly/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dramaticlly/subscriptions","organizations_url":"https://api.github.com/users/dramaticlly/orgs","repos_url":"https://api.github.com/users/dramaticlly/repos","events_url":"https://api.github.com/users/dramaticlly/events{/privacy}","received_events_url":"https://api.github.com/users/dramaticlly/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2023-12-11T19:13:46Z","updated_at":"2023-12-11T19:13:46Z","author_association":"CONTRIBUTOR","body":"@rdblue @amogh-jahagirdar can you take another look? I think it's awesome if we can merge this fix.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/1850721026/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]