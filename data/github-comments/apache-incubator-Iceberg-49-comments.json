[{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693579704","html_url":"https://github.com/apache/iceberg/pull/1466#issuecomment-693579704","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1466","id":693579704,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzU3OTcwNA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T18:23:19Z","updated_at":"2020-09-16T18:23:19Z","author_association":"CONTRIBUTOR","body":"Thanks for adding this! I merged it and will redeploy the docs.\r\n\r\nWhat do you think the solution for this is? Should we have a callback to rewrite file paths to match?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693579704/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693585206","html_url":"https://github.com/apache/iceberg/pull/1466#issuecomment-693585206","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1466","id":693585206,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzU4NTIwNg==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T18:33:51Z","updated_at":"2020-09-16T18:33:51Z","author_association":"MEMBER","body":"I guess that may be the correct answer, if you change authorities you should probably re-write all old path information? \r\n\r\nWe were tossing around other ideas of just comparing scheme + path, but I keep imagining HDFS-like implementations which put metadata in other fields. Like one could imagine file://path#someOtherMetadataThatIsUserSpecific screwing everything up again ...","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693585206/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693587829","html_url":"https://github.com/apache/iceberg/issues/1463#issuecomment-693587829","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1463","id":693587829,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzU4NzgyOQ==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T18:38:51Z","updated_at":"2020-09-16T18:38:51Z","author_association":"MEMBER","body":"Spark 3.1 :) Although honestly I could see the case for back porting that. Discarding user input should at the very least warn, but probably should have been an exception.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693587829/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693594535","html_url":"https://github.com/apache/iceberg/issues/1463#issuecomment-693594535","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1463","id":693594535,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzU5NDUzNQ==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T18:51:38Z","updated_at":"2020-09-16T18:51:38Z","author_association":"MEMBER","body":"Doc PR Up","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693594535/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693601812","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-693601812","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":693601812,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzYwMTgxMg==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T19:05:09Z","updated_at":"2020-09-16T19:05:09Z","author_association":"MEMBER","body":"You can use it with S3 with Hadoop client libraries only, you don't actually need a Hadoop cluster or HDFS.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693601812/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693638294","html_url":"https://github.com/apache/iceberg/pull/1404#issuecomment-693638294","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1404","id":693638294,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzYzODI5NA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T20:09:03Z","updated_at":"2020-09-16T20:09:03Z","author_association":"CONTRIBUTOR","body":"This looks good to me so I merged it. \r\n\r\nI agree with @stevenzwu that the serialized state is a compatibility concern. If we eventually plan to move to writing files from a checkpoint as a manifest and that would be a breaking change, then I think it makes sense to get that change done as soon as possible. That would solve the serialization problem, save a later breaking change, and make checkpoints faster. I'd prefer to do this before the 0.10.0 release, if possible.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693638294/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693649726","html_url":"https://github.com/apache/iceberg/pull/1467#issuecomment-693649726","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1467","id":693649726,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzY0OTcyNg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T20:33:06Z","updated_at":"2020-09-16T20:33:06Z","author_association":"CONTRIBUTOR","body":"Thanks @RussellSpitzer!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693649726/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693650772","html_url":"https://github.com/apache/iceberg/pull/1467#issuecomment-693650772","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1467","id":693650772,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzY1MDc3Mg==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T20:35:24Z","updated_at":"2020-09-16T20:35:24Z","author_association":"MEMBER","body":"Thanks for the merge!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693650772/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693653398","html_url":"https://github.com/apache/iceberg/issues/1438#issuecomment-693653398","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1438","id":693653398,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzY1MzM5OA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T20:40:47Z","updated_at":"2020-09-16T20:40:47Z","author_association":"CONTRIBUTOR","body":"While we are considering the state problem, why not go ahead and update the sink to store all the `DataFile` objects in a `ManifestFile` and use @stevenzwu's code to serialize the manifests? Then we have more confidence because we've been running this in prod for a long time now. That avoids breaking state later to move to storing manifest files, fixes the current problem, and should make storing checkpoint state a bit faster because it would be smaller. What do you think?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693653398/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693654158","html_url":"https://github.com/apache/iceberg/issues/1463#issuecomment-693654158","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1463","id":693654158,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzY1NDE1OA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T20:42:30Z","updated_at":"2020-09-16T20:42:30Z","author_association":"CONTRIBUTOR","body":"I merged the PR and deployed docs. Thanks, @RussellSpitzer!\r\n\r\n@lordk911, it looks like the problem is in Spark. Can you try with the new examples? If those work, then we can close this issue.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693654158/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693698994","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-693698994","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":693698994,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzY5ODk5NA==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T22:28:06Z","updated_at":"2020-09-16T22:28:06Z","author_association":"CONTRIBUTOR","body":"Supporting S3 requires Hive, because of S3's characteristic, eventual consistency. I see OSP version of Delta Lake solved it in different way, but pretty much limited. (It assumes concurrent writes for S3 only happen in \"a\" Spark driver. https://github.com/delta-io/delta/blob/master/src/main/scala/org/apache/spark/sql/delta/storage/S3SingleDriverLogStore.scala)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693698994/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693702787","html_url":"https://github.com/apache/iceberg/pull/1465#issuecomment-693702787","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1465","id":693702787,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcwMjc4Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T22:38:34Z","updated_at":"2020-09-16T22:38:34Z","author_association":"CONTRIBUTOR","body":"This fails locally for me in `TestHadoopCatalog` line 508 (not 522). The problem is that `version` is set as a side-effect of `readHint`. As a result, `version` is equal to the new version passed to `updateVersionAndMetadata` so that method assumes that there has been no change and returns the current metadata (`null`) without reading the metadata file.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693702787/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693705618","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-693705618","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":693705618,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcwNTYxOA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T22:47:10Z","updated_at":"2020-09-16T22:47:10Z","author_association":"CONTRIBUTOR","body":"Iceberg works reliably with s3 even if the same table is accessed via multiple clusters and query engines. Using Iceberg requires a catalog that can swap a pointer to the metadata file atomically. This can be done using a compare and swap or lock/unlock API. Iceberg contains a built-in implementation that uses Hive metastore to work with s3 reliably (lock/unlock). Anyone could [easily build](http://iceberg.incubator.apache.org/custom-catalog/) an integration for any catalog. For example, one may have a Cassandra-based catalog and use compare and swap to commit new table versions. That will be enough to work with s3 reliably.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693705618/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693706825","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-693706825","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":693706825,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcwNjgyNQ==","user":{"login":"jacques-n","id":183350,"node_id":"MDQ6VXNlcjE4MzM1MA==","avatar_url":"https://avatars.githubusercontent.com/u/183350?v=4","gravatar_id":"","url":"https://api.github.com/users/jacques-n","html_url":"https://github.com/jacques-n","followers_url":"https://api.github.com/users/jacques-n/followers","following_url":"https://api.github.com/users/jacques-n/following{/other_user}","gists_url":"https://api.github.com/users/jacques-n/gists{/gist_id}","starred_url":"https://api.github.com/users/jacques-n/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jacques-n/subscriptions","organizations_url":"https://api.github.com/users/jacques-n/orgs","repos_url":"https://api.github.com/users/jacques-n/repos","events_url":"https://api.github.com/users/jacques-n/events{/privacy}","received_events_url":"https://api.github.com/users/jacques-n/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T22:51:01Z","updated_at":"2020-09-16T22:51:01Z","author_association":"NONE","body":"We've been working on a non-Hive way to provide this functionality and plan on contributing it to the project within the next two weeks.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693706825/reactions","total_count":8,"+1":0,"-1":0,"laugh":0,"hooray":8,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693706990","html_url":"https://github.com/apache/iceberg/pull/1466#issuecomment-693706990","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1466","id":693706990,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcwNjk5MA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T22:51:33Z","updated_at":"2020-09-16T22:51:33Z","author_association":"CONTRIBUTOR","body":"@rdblue @RussellSpitzer @manishmalhotrawork It sounds reasonable to me to just look at scheme and path parts of URI. That way, we should be safe w.r.t. changing authorities or query params (is it even possible?). I think we definitely need a fix for this.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693706990/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693707636","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-693707636","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":693707636,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcwNzYzNg==","user":{"login":"Lindayangyy","id":3104122,"node_id":"MDQ6VXNlcjMxMDQxMjI=","avatar_url":"https://avatars.githubusercontent.com/u/3104122?v=4","gravatar_id":"","url":"https://api.github.com/users/Lindayangyy","html_url":"https://github.com/Lindayangyy","followers_url":"https://api.github.com/users/Lindayangyy/followers","following_url":"https://api.github.com/users/Lindayangyy/following{/other_user}","gists_url":"https://api.github.com/users/Lindayangyy/gists{/gist_id}","starred_url":"https://api.github.com/users/Lindayangyy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Lindayangyy/subscriptions","organizations_url":"https://api.github.com/users/Lindayangyy/orgs","repos_url":"https://api.github.com/users/Lindayangyy/repos","events_url":"https://api.github.com/users/Lindayangyy/events{/privacy}","received_events_url":"https://api.github.com/users/Lindayangyy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T22:53:47Z","updated_at":"2020-09-16T22:53:47Z","author_association":"NONE","body":"That will be awesome, can't wait to see it. Thank you - jacques-n!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693707636/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693707965","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-693707965","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":693707965,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcwNzk2NQ==","user":{"login":"Lindayangyy","id":3104122,"node_id":"MDQ6VXNlcjMxMDQxMjI=","avatar_url":"https://avatars.githubusercontent.com/u/3104122?v=4","gravatar_id":"","url":"https://api.github.com/users/Lindayangyy","html_url":"https://github.com/Lindayangyy","followers_url":"https://api.github.com/users/Lindayangyy/followers","following_url":"https://api.github.com/users/Lindayangyy/following{/other_user}","gists_url":"https://api.github.com/users/Lindayangyy/gists{/gist_id}","starred_url":"https://api.github.com/users/Lindayangyy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Lindayangyy/subscriptions","organizations_url":"https://api.github.com/users/Lindayangyy/orgs","repos_url":"https://api.github.com/users/Lindayangyy/repos","events_url":"https://api.github.com/users/Lindayangyy/events{/privacy}","received_events_url":"https://api.github.com/users/Lindayangyy/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T22:54:47Z","updated_at":"2020-09-16T22:54:47Z","author_association":"NONE","body":"Thanks for all the responses as alternatives. All answers are great!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693707965/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693709009","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-693709009","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":693709009,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcwOTAwOQ==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T22:58:12Z","updated_at":"2020-09-16T22:58:12Z","author_association":"CONTRIBUTOR","body":"That sounds great! Assuming it still needs to do CAS with external storage (I'd be really curious if it doesn't rely on the external storage) which is that? Is it one of AWS services? If then even better, as there's no external dependency outside of AWS. Given we assume to use S3, which is already locked-in.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693709009/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693717726","html_url":"https://github.com/apache/iceberg/pull/1466#issuecomment-693717726","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1466","id":693717726,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcxNzcyNg==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-16T23:22:25Z","updated_at":"2020-09-16T23:22:25Z","author_association":"CONTRIBUTOR","body":"@rdblue @RussellSpitzer @manishmalhotrawork Right now, we have a UDF to produce file names that we use in the join condition. What about having a UDF that would take 2 strings (one actual location and one location referenced in the metadata) and produce a boolean to indicate whether they match or not. Inside the UDF, we can construct `Path` from `String`, call `toUri` on it, and then call `getScheme` and `getPath`. Path parts must match and the scheme in the metadata may be either null or must match the actual one.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693717726/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693728593","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-693728593","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":693728593,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzcyODU5Mw==","user":{"login":"jacques-n","id":183350,"node_id":"MDQ6VXNlcjE4MzM1MA==","avatar_url":"https://avatars.githubusercontent.com/u/183350?v=4","gravatar_id":"","url":"https://api.github.com/users/jacques-n","html_url":"https://github.com/jacques-n","followers_url":"https://api.github.com/users/jacques-n/followers","following_url":"https://api.github.com/users/jacques-n/following{/other_user}","gists_url":"https://api.github.com/users/jacques-n/gists{/gist_id}","starred_url":"https://api.github.com/users/jacques-n/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jacques-n/subscriptions","organizations_url":"https://api.github.com/users/jacques-n/orgs","repos_url":"https://api.github.com/users/jacques-n/repos","events_url":"https://api.github.com/users/jacques-n/events{/privacy}","received_events_url":"https://api.github.com/users/jacques-n/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T00:02:11Z","updated_at":"2020-09-17T00:02:11Z","author_association":"NONE","body":"We're doing something pluggable but the default implementation is on top of DynamoDB.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693728593/reactions","total_count":4,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693738355","html_url":"https://github.com/apache/iceberg/pull/1466#issuecomment-693738355","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1466","id":693738355,"node_id":"MDEyOklzc3VlQ29tbWVudDY5MzczODM1NQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T00:36:21Z","updated_at":"2020-09-17T00:36:21Z","author_association":"CONTRIBUTOR","body":"Having a UDF that accepts columns from two relations does not eliminate the cross join.\r\n\r\nI guess we have two options:\r\n- keep the join by file name and replace `contains` condition with another UDF that would ignore authority\r\n- replace the existing UDF that produces file names with another UDF that would produce a scheme and a relative path and then use DataFrame operations. That way, we will have only one UDF.\r\n\r\n```\r\nColumn pathCond = actualFileDF.col(\"relative_path\").equalTo(validDataFileDF.col(\"relative_path\"));\r\nColumn schemeEquality = actualFileDF.col(\"scheme\").equalTo(validDataFileDF.col(\"scheme\")); \r\nColumn schemeCond = validDataFileDF.col(\"scheme\").isNull().or(schemeEquality);\r\nColumn joinCond = pathCond.and(schemeCond);\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693738355/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693745460","html_url":"https://github.com/apache/iceberg/pull/1469#issuecomment-693745460","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1469","id":693745460,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc0NTQ2MA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T01:02:51Z","updated_at":"2020-09-17T01:02:51Z","author_association":"CONTRIBUTOR","body":"FYI @rymurr and @prodeezy.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693745460/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693747998","html_url":"https://github.com/apache/iceberg/issues/1463#issuecomment-693747998","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1463","id":693747998,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc0Nzk5OA==","user":{"login":"lordk911","id":19989300,"node_id":"MDQ6VXNlcjE5OTg5MzAw","avatar_url":"https://avatars.githubusercontent.com/u/19989300?v=4","gravatar_id":"","url":"https://api.github.com/users/lordk911","html_url":"https://github.com/lordk911","followers_url":"https://api.github.com/users/lordk911/followers","following_url":"https://api.github.com/users/lordk911/following{/other_user}","gists_url":"https://api.github.com/users/lordk911/gists{/gist_id}","starred_url":"https://api.github.com/users/lordk911/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lordk911/subscriptions","organizations_url":"https://api.github.com/users/lordk911/orgs","repos_url":"https://api.github.com/users/lordk911/repos","events_url":"https://api.github.com/users/lordk911/events{/privacy}","received_events_url":"https://api.github.com/users/lordk911/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T01:11:21Z","updated_at":"2020-09-17T01:11:21Z","author_association":"NONE","body":"**ok , test passed by use load method**:\r\n\r\n```\r\nscala> val df = spark.read.format(\"iceberg\").option(\"snapshot-id\", \"7975731372139528551\").load(\"/user/hive/warehouse/ice/icetest\")\r\ndf: org.apache.spark.sql.DataFrame = [id: bigint, data: string]\r\n\r\nscala> df.show(false)\r\n+---+----+                                                                      \r\n|id |data|\r\n+---+----+\r\n|1  |a   |\r\n|2  |b   |\r\n|3  |c   |\r\n+---+----+\r\n\r\n\r\nscala> val df = spark.read.format(\"iceberg\").option(\"snapshot-id\", \"4063381308553626534\").load(\"/user/hive/warehouse/ice/icetest\")\r\ndf: org.apache.spark.sql.DataFrame = [id: bigint, data: string]\r\n\r\nscala> df.show(false)\r\n+---+----+                                                                      \r\n|id |data|\r\n+---+----+\r\n|2  |b   |\r\n|3  |c   |\r\n+---+----+\r\n```\r\n\r\nThank you !","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693747998/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693752556","html_url":"https://github.com/apache/iceberg/pull/1466#issuecomment-693752556","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1466","id":693752556,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc1MjU1Ng==","user":{"login":"manishmalhotrawork","id":22434010,"node_id":"MDQ6VXNlcjIyNDM0MDEw","avatar_url":"https://avatars.githubusercontent.com/u/22434010?v=4","gravatar_id":"","url":"https://api.github.com/users/manishmalhotrawork","html_url":"https://github.com/manishmalhotrawork","followers_url":"https://api.github.com/users/manishmalhotrawork/followers","following_url":"https://api.github.com/users/manishmalhotrawork/following{/other_user}","gists_url":"https://api.github.com/users/manishmalhotrawork/gists{/gist_id}","starred_url":"https://api.github.com/users/manishmalhotrawork/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/manishmalhotrawork/subscriptions","organizations_url":"https://api.github.com/users/manishmalhotrawork/orgs","repos_url":"https://api.github.com/users/manishmalhotrawork/repos","events_url":"https://api.github.com/users/manishmalhotrawork/events{/privacy}","received_events_url":"https://api.github.com/users/manishmalhotrawork/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T01:26:29Z","updated_at":"2020-09-17T02:56:51Z","author_association":"CONTRIBUTOR","body":"> Having a UDF that accepts columns from two relations does not eliminate the cross join.\r\n> \r\n> I guess we have two options:\r\n> \r\n> * keep the join by file name and replace `contains` condition with another UDF that would ignore authority\r\n> * replace the existing UDF that produces file names with another UDF that would produce a scheme and a relative path and then use DataFrame operations. That way, we will have only one UDF.\r\n> \r\n> ```\r\n> Column pathCond = actualFileDF.col(\"relative_path\").equalTo(validDataFileDF.col(\"relative_path\"));\r\n> Column schemeEquality = actualFileDF.col(\"scheme\").equalTo(validDataFileDF.col(\"scheme\")); \r\n> Column schemeCond = validDataFileDF.col(\"scheme\").isNull().or(schemeEquality);\r\n> Column joinCond = pathCond.and(schemeCond);\r\n> ```\r\n\r\n@aokolnychyi thanks for adding dtails.\r\n\r\nyeah as our internal discussion and internal PR I have is similar to this.\r\nThough while testing, realized checking name of the `scheme` can also be troublesome as name of the scheme is configurable like instead of `hdfs` we can `myhdfs` and same for S3.\r\n\r\nI believe not considering `scheme` should also be ok, as there should not be case where table location is in HDFS but data its pointing to S3 which needs to be removed or vice versa? \r\n\r\nSo, either can avoid checking scheme as well, or have a flag to consider that or not.\r\n\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693752556/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693752779","html_url":"https://github.com/apache/iceberg/pull/1466#issuecomment-693752779","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1466","id":693752779,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc1Mjc3OQ==","user":{"login":"manishmalhotrawork","id":22434010,"node_id":"MDQ6VXNlcjIyNDM0MDEw","avatar_url":"https://avatars.githubusercontent.com/u/22434010?v=4","gravatar_id":"","url":"https://api.github.com/users/manishmalhotrawork","html_url":"https://github.com/manishmalhotrawork","followers_url":"https://api.github.com/users/manishmalhotrawork/followers","following_url":"https://api.github.com/users/manishmalhotrawork/following{/other_user}","gists_url":"https://api.github.com/users/manishmalhotrawork/gists{/gist_id}","starred_url":"https://api.github.com/users/manishmalhotrawork/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/manishmalhotrawork/subscriptions","organizations_url":"https://api.github.com/users/manishmalhotrawork/orgs","repos_url":"https://api.github.com/users/manishmalhotrawork/repos","events_url":"https://api.github.com/users/manishmalhotrawork/events{/privacy}","received_events_url":"https://api.github.com/users/manishmalhotrawork/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T01:27:27Z","updated_at":"2020-09-17T01:27:27Z","author_association":"CONTRIBUTOR","body":"> @rdblue @RussellSpitzer @manishmalhotrawork Right now, we have a UDF to produce file names that we use in the join condition. What about having a UDF that would take 2 strings (one actual location and one location referenced in the metadata) and produce a boolean to indicate whether they match or not. Inside the UDF, we can construct `Path` from `String`, call `toUri` on it, and then call `getScheme` and `getPath`. Path parts must match and the scheme in the metadata may be either null or must match the actual one.\r\n\r\n@aokolnychyi yeah almost similar the change I did, will be raising PR shortly.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693752779/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693757280","html_url":"https://github.com/apache/iceberg/pull/1404#issuecomment-693757280","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1404","id":693757280,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc1NzI4MA==","user":{"login":"openinx","id":5028729,"node_id":"MDQ6VXNlcjUwMjg3Mjk=","avatar_url":"https://avatars.githubusercontent.com/u/5028729?v=4","gravatar_id":"","url":"https://api.github.com/users/openinx","html_url":"https://github.com/openinx","followers_url":"https://api.github.com/users/openinx/followers","following_url":"https://api.github.com/users/openinx/following{/other_user}","gists_url":"https://api.github.com/users/openinx/gists{/gist_id}","starred_url":"https://api.github.com/users/openinx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/openinx/subscriptions","organizations_url":"https://api.github.com/users/openinx/orgs","repos_url":"https://api.github.com/users/openinx/repos","events_url":"https://api.github.com/users/openinx/events{/privacy}","received_events_url":"https://api.github.com/users/openinx/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T01:43:51Z","updated_at":"2020-09-17T01:43:51Z","author_association":"MEMBER","body":"Agreed, I would prepare patch for that today.   Thanks for the merging. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693757280/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693777926","html_url":"https://github.com/apache/iceberg/issues/1438#issuecomment-693777926","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1438","id":693777926,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc3NzkyNg==","user":{"login":"JingsongLi","id":9601882,"node_id":"MDQ6VXNlcjk2MDE4ODI=","avatar_url":"https://avatars.githubusercontent.com/u/9601882?v=4","gravatar_id":"","url":"https://api.github.com/users/JingsongLi","html_url":"https://github.com/JingsongLi","followers_url":"https://api.github.com/users/JingsongLi/followers","following_url":"https://api.github.com/users/JingsongLi/following{/other_user}","gists_url":"https://api.github.com/users/JingsongLi/gists{/gist_id}","starred_url":"https://api.github.com/users/JingsongLi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JingsongLi/subscriptions","organizations_url":"https://api.github.com/users/JingsongLi/orgs","repos_url":"https://api.github.com/users/JingsongLi/repos","events_url":"https://api.github.com/users/JingsongLi/events{/privacy}","received_events_url":"https://api.github.com/users/JingsongLi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T02:57:09Z","updated_at":"2020-09-17T02:57:09Z","author_association":"CONTRIBUTOR","body":"> we also need to implement the custom Kryo Serializers for the network transfer btw writer and committer\r\n\r\nWe can use Flink `DataFileTypeSerializer` to network too.\r\n\r\n+1 to store all the DataFile objects in a `ManifestFile`. \r\n\r\nBut I think it is good to introduce `SimpleVersionedSerializer`s[1] for `DataFile` and `ManifestFile` too, we may need `FlinkDataFile` and `FlinkManifestFile`. The `SimpleVersionedSerializer` is a simpler design than `state-serializers-and-schema-evolution`, it is completely internal implementation in iceberg serializers.\r\n\r\n[1]https://ci.apache.org/projects/flink/flink-docs-master/api/java/org/apache/flink/core/io/SimpleVersionedSerialization.html","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693777926/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693786848","html_url":"https://github.com/apache/iceberg/pull/1471#issuecomment-693786848","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1471","id":693786848,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzc4Njg0OA==","user":{"login":"manishmalhotrawork","id":22434010,"node_id":"MDQ6VXNlcjIyNDM0MDEw","avatar_url":"https://avatars.githubusercontent.com/u/22434010?v=4","gravatar_id":"","url":"https://api.github.com/users/manishmalhotrawork","html_url":"https://github.com/manishmalhotrawork","followers_url":"https://api.github.com/users/manishmalhotrawork/followers","following_url":"https://api.github.com/users/manishmalhotrawork/following{/other_user}","gists_url":"https://api.github.com/users/manishmalhotrawork/gists{/gist_id}","starred_url":"https://api.github.com/users/manishmalhotrawork/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/manishmalhotrawork/subscriptions","organizations_url":"https://api.github.com/users/manishmalhotrawork/orgs","repos_url":"https://api.github.com/users/manishmalhotrawork/repos","events_url":"https://api.github.com/users/manishmalhotrawork/events{/privacy}","received_events_url":"https://api.github.com/users/manishmalhotrawork/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T03:30:36Z","updated_at":"2020-09-17T03:30:36Z","author_association":"CONTRIBUTOR","body":"@rdblue @aokolnychyi can you please review it !!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693786848/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693848246","html_url":"https://github.com/apache/iceberg/pull/796#issuecomment-693848246","issue_url":"https://api.github.com/repos/apache/iceberg/issues/796","id":693848246,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Mzg0ODI0Ng==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T04:56:11Z","updated_at":"2020-09-17T04:56:11Z","author_association":"CONTRIBUTOR","body":"Btw I'm assuming MicroBatch guarantees the orderness of the manifest files & input files in manifest file. Looks like it follows the order of content in the file (manifest list & manifest) so the orderness is guaranteed as of now. If things could be changed, either we need to have a way to provide consistent order across Iceberg versions (e.g. sort by some stable condition), or retrieve the list of files in snapshot and dump along with the offset.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/693848246/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694059129","html_url":"https://github.com/apache/iceberg/issues/1438#issuecomment-694059129","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1438","id":694059129,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDA1OTEyOQ==","user":{"login":"chenjunjiedada","id":3960228,"node_id":"MDQ6VXNlcjM5NjAyMjg=","avatar_url":"https://avatars.githubusercontent.com/u/3960228?v=4","gravatar_id":"","url":"https://api.github.com/users/chenjunjiedada","html_url":"https://github.com/chenjunjiedada","followers_url":"https://api.github.com/users/chenjunjiedada/followers","following_url":"https://api.github.com/users/chenjunjiedada/following{/other_user}","gists_url":"https://api.github.com/users/chenjunjiedada/gists{/gist_id}","starred_url":"https://api.github.com/users/chenjunjiedada/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chenjunjiedada/subscriptions","organizations_url":"https://api.github.com/users/chenjunjiedada/orgs","repos_url":"https://api.github.com/users/chenjunjiedada/repos","events_url":"https://api.github.com/users/chenjunjiedada/events{/privacy}","received_events_url":"https://api.github.com/users/chenjunjiedada/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T07:50:59Z","updated_at":"2020-09-17T07:50:59Z","author_association":"COLLABORATOR","body":"@stevenzwu  The issue didn't happen when doing the rolling upgrade. As I mentioned at the beginning, the issue happens when I submit a new application with new Iceberg version to a Flink cluster which deployed an old version iceberg. After digging deeper, we found it is because the job manager and task manager still deserialize the old version iceberg classes from Flink dependency but Flink client uses the new iceberg class from the user application. This could be resolved by setting parameter `yarn.per-job-cluster.include-user-jar=FIRST` so that the job manager and task manager could load new version iceberg classes at first. \r\n\r\nFor upgraded Flink cluster with existing application, @JingsongLi, Do we expect user upload application along with the iceberg-flink-runtime jar? Or the Flink cluster provides the iceberg-flink-runtime jar? In our case, the Flink cluster has the iceberg-flink-runtime jar. When the Flink cluster upgraded, the user job possibly needs to restart so it would not subject the compatibility issue. When it is a rolling upgrade, it probably still doesn't needs to transfer iceberg class between job manager and task managers during job running.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694059129/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694063925","html_url":"https://github.com/apache/iceberg/issues/1470#issuecomment-694063925","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1470","id":694063925,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDA2MzkyNQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T07:55:04Z","updated_at":"2020-09-17T07:55:04Z","author_association":"CONTRIBUTOR","body":"> * Change `HiveMetaStoreClient` to `IMetaStoreClient`. (Actually depends on `IMetaStoreClient` is enough in Iceberg?)\r\n\r\nIMetaStoreClient definitely should be enough for Iceberg.\r\nWe still have to keep the HMS for testing though (or need a replacement implementation)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694063925/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694070682","html_url":"https://github.com/apache/iceberg/issues/1438#issuecomment-694070682","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1438","id":694070682,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDA3MDY4Mg==","user":{"login":"JingsongLi","id":9601882,"node_id":"MDQ6VXNlcjk2MDE4ODI=","avatar_url":"https://avatars.githubusercontent.com/u/9601882?v=4","gravatar_id":"","url":"https://api.github.com/users/JingsongLi","html_url":"https://github.com/JingsongLi","followers_url":"https://api.github.com/users/JingsongLi/followers","following_url":"https://api.github.com/users/JingsongLi/following{/other_user}","gists_url":"https://api.github.com/users/JingsongLi/gists{/gist_id}","starred_url":"https://api.github.com/users/JingsongLi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JingsongLi/subscriptions","organizations_url":"https://api.github.com/users/JingsongLi/orgs","repos_url":"https://api.github.com/users/JingsongLi/repos","events_url":"https://api.github.com/users/JingsongLi/events{/privacy}","received_events_url":"https://api.github.com/users/JingsongLi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T08:08:09Z","updated_at":"2020-09-17T08:08:09Z","author_association":"CONTRIBUTOR","body":"> @stevenzwu  The issue didn't happen when doing the rolling upgrade. As I mentioned at the beginning, the issue happens when I submit a new application with new Iceberg version to a Flink cluster which deployed an old version iceberg. After digging deeper, we found it is because the job manager and task manager still deserialize the old version iceberg classes from Flink dependency but Flink client uses the new iceberg class from the user application. This could be resolved by setting parameter `yarn.per-job-cluster.include-user-jar=FIRST` so that the job manager and task manager could load new version iceberg classes at first.\r\n> \r\n> For upgraded Flink cluster with existing application, @JingsongLi, Do we expect user upload application along with the iceberg-flink-runtime jar? Or the Flink cluster provides the iceberg-flink-runtime jar? In our case, the Flink cluster has the iceberg-flink-runtime jar. When the Flink cluster upgraded, the user job possibly needs to restart so it would not subject the compatibility issue. When it is a rolling upgrade, it probably still doesn't needs to transfer iceberg class between job manager and task managers during job running.\r\n\r\nHi @chenjunjiedada , got it, I think this should be a problem of cluster deployment.\r\n\r\nThis is because there are two versions of iceberg dependency in your cluster (`flink/lib/*` and user jar), and the class resolution order mechanism of Flink is inconsistent between client and runtime (of course, this can be solved by `yarn.per-job-cluster.include-user-jar`).\r\n- If your cluster is for DataStream users, I think Iceberg dependencies can be include in user jar. Because user program is strongly related to the Iceberg API.\r\n- If your cluster is for SQL users, I think Iceberg dependencies can be include in `flink/lib/*`.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694070682/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694099803","html_url":"https://github.com/apache/iceberg/issues/1472#issuecomment-694099803","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1472","id":694099803,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDA5OTgwMw==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T09:02:34Z","updated_at":"2020-09-17T09:02:34Z","author_association":"CONTRIBUTOR","body":"You'll need to use `REFRESH TABLE` instead as you're referencing Iceberg table as table identifier.\r\n\r\nhttp://spark.apache.org/docs/latest/sql-ref-syntax-aux-cache-refresh-table.html\r\n\r\nI guess `REFRESH resource` doesn't trigger Iceberg catalog to invalidate cache.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694099803/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694112072","html_url":"https://github.com/apache/iceberg/issues/1472#issuecomment-694112072","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1472","id":694112072,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDExMjA3Mg==","user":{"login":"lordk911","id":19989300,"node_id":"MDQ6VXNlcjE5OTg5MzAw","avatar_url":"https://avatars.githubusercontent.com/u/19989300?v=4","gravatar_id":"","url":"https://api.github.com/users/lordk911","html_url":"https://github.com/lordk911","followers_url":"https://api.github.com/users/lordk911/followers","following_url":"https://api.github.com/users/lordk911/following{/other_user}","gists_url":"https://api.github.com/users/lordk911/gists{/gist_id}","starred_url":"https://api.github.com/users/lordk911/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lordk911/subscriptions","organizations_url":"https://api.github.com/users/lordk911/orgs","repos_url":"https://api.github.com/users/lordk911/repos","events_url":"https://api.github.com/users/lordk911/events{/privacy}","received_events_url":"https://api.github.com/users/lordk911/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T09:25:17Z","updated_at":"2020-09-17T09:25:17Z","author_association":"NONE","body":"@HeartSaVioR  Thank you ,your are right.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694112072/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694141549","html_url":"https://github.com/apache/iceberg/pull/1407#issuecomment-694141549","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1407","id":694141549,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDE0MTU0OQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T10:23:47Z","updated_at":"2020-09-17T10:23:47Z","author_association":"CONTRIBUTOR","body":"Can we push this change @massdosage, @rdblue?\r\n\r\nI very much would like to see it in the next release so our testing would be much easier.\r\nAlso I have a patch at hand which would enable Hive users to create previously non-existent tables over Iceberg which is not that big and depending on this. We might even be able to get into the release if we push this soon. With these 2 patches we could write whole end-to-end Hive tests (CREATE/WRITE/READ/DROP) with the next release.\r\n\r\nAll that said, we can work on our branch so I do not want to push too hard  ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694141549/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694186791","html_url":"https://github.com/apache/iceberg/pull/1407#issuecomment-694186791","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1407","id":694186791,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDE4Njc5MQ==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T12:06:35Z","updated_at":"2020-09-17T12:06:35Z","author_association":"CONTRIBUTOR","body":"Hey @pvary, apologies for delays in looking at this. Today I tried this out \"for real\" on a Hive cluster we have running on EMR where I've been doing my InputFormat tests. I built a hive-runtime jar from this PR and added to the classpath of the Hive shell. I then ran:\r\n\r\n`CREATE EXTERNAL TABLE bdp.iceberg_write_table STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' LOCATION 'hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/table_a';`\r\n\r\nHowever this fails with the following exception that I see in the Hive logs:\r\n\r\n```\r\n2020-09-17T11:59:02,006 INFO  [main([])]: conf.HiveConf (HiveConf.java:getLogIdVar(3957)) - Using the default value passed in for log id: 0e8d5f3d-e100-4e3b-bae1-f272b504b04a\r\n2020-09-17T11:59:02,007 INFO  [main([])]: session.SessionState (SessionState.java:updateThreadName(421)) - Updating thread name to 0e8d5f3d-e100-4e3b-bae1-f272b504b04a main\r\n2020-09-17T11:59:02,007 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (Driver.java:compile(429)) - Compiling command(queryId=hadoop_20200917115902_126001a6-6193-4e7e-8def-345036e43056): CREATE TABLE bdp.iceberg_write_table STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' LOCATION 'hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table' TBLPROPERTIES ('iceberg.mr.write.file.format'='parquet')\r\n2020-09-17T11:59:02,010 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: parse.CalcitePlanner (SemanticAnalyzer.java:analyzeInternal(11150)) - Starting Semantic Analysis\r\n2020-09-17T11:59:02,011 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: parse.CalcitePlanner (SemanticAnalyzer.java:analyzeCreateTable(11896)) - Creating table bdp.iceberg_write_table position=13\r\n2020-09-17T11:59:02,043 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (Driver.java:compile(518)) - Semantic Analysis Completed\r\n2020-09-17T11:59:02,044 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (Driver.java:getSchema(290)) - Returning Hive schema: Schema(fieldSchemas:null, properties:null)\r\n2020-09-17T11:59:02,044 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (Driver.java:compile(642)) - Completed compiling command(queryId=hadoop_20200917115902_126001a6-6193-4e7e-8def-345036e43056); Time taken: 0.037 seconds\r\n2020-09-17T11:59:02,044 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (Driver.java:checkConcurrency(210)) - Concurrency mode is disabled, not creating a lock manager\r\n2020-09-17T11:59:02,044 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (Driver.java:execute(1735)) - Executing command(queryId=hadoop_20200917115902_126001a6-6193-4e7e-8def-345036e43056): CREATE TABLE bdp.iceberg_write_table STORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' LOCATION 'hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table' TBLPROPERTIES ('iceberg.mr.write.file.format'='parquet')\r\n2020-09-17T11:59:02,091 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (Driver.java:launchTask(2181)) - Starting task [Stage-0:DDL] in serial mode\r\n2020-09-17T11:59:02,091 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: plan.CreateTableDesc (CreateTableDesc.java:toTable(717)) - Use StorageHandler-supplied org.apache.iceberg.mr.hive.HiveIcebergSerDe for table iceberg_write_table\r\n2020-09-17T11:59:02,097 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: exec.DDLTask (DDLTask.java:createTable(4324)) - creating table bdp.iceberg_write_table on hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table\r\n2020-09-17T11:59:02,116 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: mr.Catalogs (Catalogs.java:loadCatalog(114)) - Catalog is not configured\r\n2020-09-17T11:59:02,124 WARN  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: hadoop.HadoopTableOperations (HadoopTableOperations.java:readVersionHint(292)) - Error reading version hint file hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table/metadata/version-hint.text\r\njava.io.FileNotFoundException: File does not exist: /hiveberg/iceberg_write_table/metadata/version-hint.text\r\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72)\r\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1827)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\r\n\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_212]\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_212]\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_212]\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_212]\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:849) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:836) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:825) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:325) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:286) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:270) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1064) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:329) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:325) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:337) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:790) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.iceberg.hadoop.HadoopTableOperations.readVersionHint(HadoopTableOperations.java:287) ~[?:?]\r\n        at org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:94) ~[?:?]\r\n        at org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:75) ~[?:?]\r\n        at org.apache.iceberg.hadoop.HadoopTables.load(HadoopTables.java:79) ~[?:?]\r\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:77) ~[?:?]\r\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:65) ~[?:?]\r\n        at org.apache.iceberg.mr.hive.HiveIcebergSerDe.initialize(HiveIcebergSerDe.java:66) ~[?:?]\r\n        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:449) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:436) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:641) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:624) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:836) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:872) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_212]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_212]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_212]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_212]\r\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:239) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:153) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\nCaused by: org.apache.hadoop.ipc.RemoteException: File does not exist: /hiveberg/iceberg_write_table/metadata/version-hint.text\r\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72)\r\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1827)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\r\n\r\n        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1435) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1345) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at com.sun.proxy.$Proxy29.getBlockLocations(Unknown Source) ~[?:?]\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:259) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_212]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_212]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_212]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_212]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at com.sun.proxy.$Proxy30.getBlockLocations(Unknown Source) ~[?:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:847) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        ... 49 more\r\n2020-09-17T11:59:02,144 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: conf.HiveConf (HiveConf.java:getLogIdVar(3957)) - Using the default value passed in for log id: 0e8d5f3d-e100-4e3b-bae1-f272b504b04a\r\n2020-09-17T11:59:02,144 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: session.SessionState (SessionState.java:resetThreadName(432)) - Resetting thread name to  main\r\n2020-09-17T11:59:02,128 ERROR [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: metadata.Table (Table.java:getColsInternal(644)) - Unable to get field from serde: org.apache.iceberg.mr.hive.HiveIcebergSerDe\r\norg.apache.iceberg.exceptions.NoSuchTableException: Table does not exist at location: hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table\r\n        at org.apache.iceberg.hadoop.HadoopTables.load(HadoopTables.java:82) ~[?:?]\r\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:77) ~[?:?]\r\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:65) ~[?:?]\r\n        at org.apache.iceberg.mr.hive.HiveIcebergSerDe.initialize(HiveIcebergSerDe.java:66) ~[?:?]\r\n        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:449) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:436) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getColsInternal(Table.java:641) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getCols(Table.java:624) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:836) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:872) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_212]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_212]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_212]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_212]\r\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:239) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:153) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n2020-09-17T11:59:02,128 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: mr.Catalogs (Catalogs.java:loadCatalog(114)) - Catalog is not configured\r\n2020-09-17T11:59:02,129 WARN  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: hadoop.HadoopTableOperations (HadoopTableOperations.java:readVersionHint(292)) - Error reading version hint file hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table/metadata/version-hint.text\r\njava.io.FileNotFoundException: File does not exist: /hiveberg/iceberg_write_table/metadata/version-hint.text\r\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72)\r\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1827)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\r\n\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_212]\r\n        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_212]\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_212]\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_212]\r\n        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:849) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:836) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:825) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:325) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:286) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:270) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1064) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:329) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:325) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:337) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:790) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.iceberg.hadoop.HadoopTableOperations.readVersionHint(HadoopTableOperations.java:287) ~[?:?]\r\n        at org.apache.iceberg.hadoop.HadoopTableOperations.refresh(HadoopTableOperations.java:94) ~[?:?]\r\n        at org.apache.iceberg.hadoop.HadoopTableOperations.current(HadoopTableOperations.java:75) ~[?:?]\r\n        at org.apache.iceberg.hadoop.HadoopTables.load(HadoopTables.java:79) ~[?:?]\r\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:77) ~[?:?]\r\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:65) ~[?:?]\r\n        at org.apache.iceberg.mr.hive.HiveIcebergSerDe.initialize(HiveIcebergSerDe.java:66) ~[?:?]\r\n        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:449) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:436) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:838) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:872) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227) ~[hive-exec-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686) ~[hive-cli-2.3.5-amzn-0.jar:2.3.5-amzn-0]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_212]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_212]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_212]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_212]\r\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:239) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:153) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\nCaused by: org.apache.hadoop.ipc.RemoteException: File does not exist: /hiveberg/iceberg_write_table/metadata/version-hint.text\r\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:72)\r\n        at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:62)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)\r\n        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1827)\r\n        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:692)\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:381)\r\n        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)\r\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:850)\r\n        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:793)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2489)\r\n\r\n        at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1435) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.Client.call(Client.java:1345) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at com.sun.proxy.$Proxy29.getBlockLocations(Unknown Source) ~[?:?]\r\n        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:259) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_212]\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_212]\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_212]\r\n        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_212]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346) ~[hadoop-common-2.8.5-amzn-4.jar:?]\r\n        at com.sun.proxy.$Proxy30.getBlockLocations(Unknown Source) ~[?:?]\r\n        at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:847) ~[hadoop-hdfs-client-2.8.5-amzn-4.jar:?]\r\n        ... 47 more\r\n2020-09-17T11:59:02,143 ERROR [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: exec.DDLTask (DDLTask.java:failed(639)) - org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.iceberg.exceptions.NoSuchTableException: Table does not exist at location: hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:867)\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:872)\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4356)\r\n        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:354)\r\n        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:199)\r\n        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\r\n        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2183)\r\n        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1839)\r\n        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1526)\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\r\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\r\n        at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)\r\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184)\r\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403)\r\n        at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821)\r\n        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759)\r\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:239)\r\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:153)\r\nCaused by: org.apache.iceberg.exceptions.NoSuchTableException: Table does not exist at location: hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table\r\n        at org.apache.iceberg.hadoop.HadoopTables.load(HadoopTables.java:82)\r\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:77)\r\n        at org.apache.iceberg.mr.Catalogs.loadTable(Catalogs.java:65)\r\n        at org.apache.iceberg.mr.hive.HiveIcebergSerDe.initialize(HiveIcebergSerDe.java:66)\r\n        at org.apache.hadoop.hive.serde2.AbstractSerDe.initialize(AbstractSerDe.java:54)\r\n        at org.apache.hadoop.hive.serde2.SerDeUtils.initializeSerDe(SerDeUtils.java:533)\r\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:449)\r\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:436)\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:281)\r\n        at org.apache.hadoop.hive.ql.metadata.Table.getDeserializer(Table.java:263)\r\n        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:838)\r\n        ... 22 more\r\n\r\n2020-09-17T11:59:02,143 ERROR [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (SessionState.java:printError(1126)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.iceberg.exceptions.NoSuchTableException: Table does not exist at location: hdfs://ip-redacted.us-west-2.compute.internal:8020/hiveberg/iceberg_write_table\r\n2020-09-17T11:59:02,143 INFO  [0e8d5f3d-e100-4e3b-bae1-f272b504b04a main([])]: ql.Driver (Driver.java:execute(2050)) - Completed executing command(queryId=hadoop_20200917115902_126001a6-6193-4e7e-8def-345036e43056); Time taken: 0.099 seconds\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694186791/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694187556","html_url":"https://github.com/apache/iceberg/pull/1407#issuecomment-694187556","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1407","id":694187556,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDE4NzU1Ng==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T12:08:22Z","updated_at":"2020-09-17T12:08:22Z","author_association":"CONTRIBUTOR","body":"My guess is that the above is being caused because we're creating the table on HDFS, not the local file system, and this is going down a code path that isn't currently supported by the output format?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694187556/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694239063","html_url":"https://github.com/apache/iceberg/pull/1407#issuecomment-694239063","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1407","id":694239063,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDIzOTA2Mw==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T13:35:33Z","updated_at":"2020-09-17T13:35:33Z","author_association":"CONTRIBUTOR","body":"We had a chat with @massdosage, and found the following.\r\n\r\nThe CREATE TABLE command expects that the **table is already there**. It will not create the actual Iceberg table metadata. That is in the upcoming patch where we implement the HiveMetaHook.\r\n\r\nAlso for writes the **execution engine is set to MR**. Tez/LLAP needs some more work on their side to call the appropriate callbacks on the OutputCommitter in time.\r\n\r\n@massdosage: Run another test with the correct settings on an arbitrary table (not sure about the schema/create script etc.), and it failed with the following exception:\r\n```\r\nError: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"tmp_values_col1\":\"f4pyx-ngNrU4YcHcdZco2CBPBa7!Kz7Zg4UNt.nmB1ximyTK\",\"tmp_values_col2\":\"7604075334510954017\"}\r\n       at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:169)\r\n       at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\r\n       at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:455)\r\n       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:344)\r\n       at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175)\r\n       at java.security.AccessController.doPrivileged(Native Method)\r\n       at javax.security.auth.Subject.doAs(Subject.java:422)\r\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)\r\n       at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:169)\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"tmp_values_col1\":\"f4pyx-ngNrU4YcHcdZco2CBPBa7!Kz7Zg4UNt.nmB1ximyTK\",\"tmp_values_col2\":\"7604075334510954017\"}\r\n       at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:565)\r\n       at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:160)\r\n       ... 8 more\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\r\n       at org.apache.hadoop.hive.ql.exec.FileSinkOperator.createBucketFiles(FileSinkOperator.java:574)\r\n       at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:674)\r\n       at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)\r\n       at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95)\r\n       at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897)\r\n       at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130)\r\n       at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148)\r\n       at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:550)\r\n       ... 9 more\r\n```\r\n\r\nWhich is very strange that we are creating buckets in the FSOperator when we are supposed to use our very own SerDe.\r\n\r\nI will share my development branch which includes the table creation patch, so it would be easier to create arbitrary test tables, in the meantime @massdosage will try to dig up a little history about the table which caused us headaches  ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694239063/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694250398","html_url":"https://github.com/apache/iceberg/pull/1407#issuecomment-694250398","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1407","id":694250398,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDI1MDM5OA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T13:52:38Z","updated_at":"2020-09-17T13:52:38Z","author_association":"CONTRIBUTOR","body":"Here is the \"create table\" branch: https://github.com/pvary/iceberg/commits/create\r\nHow to use: compile, and add the jar to the Hive classpath\r\nI needed one more patch on Hive side, but 2.2.0 and above Hive should have it\r\nCommands I have tested with:\r\n```\r\nCREATE EXTERNAL TABLE test_table \r\nSTORED BY 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler' \r\nLOCATION 'hdfs://ns1/user/hive/warehouse/test_table'\r\nTBLPROPERTIES ('iceberg.mr.table.schema'='{\"type\":\"struct\",\"fields\":[{\"id\":1,\"name\":\"data\",\"required\":true,\"type\":\"string\"},{\"id\":2,\"name\":\"id\",\"required\":true,\"type\":\"long\"}]}', 'iceberg.mr.write.file.format'='PARQUET')\r\n\r\ninsert into test_table values(3,\"3\"),(4,\"4\");\r\ninsert into test_table select * from test_table;\r\ninsert into test_table select * from test_table order by id;\r\n```\r\n\r\n@massdosage: Thanks for spending some time testing this out","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694250398/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694270970","html_url":"https://github.com/apache/iceberg/pull/1407#issuecomment-694270970","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1407","id":694270970,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDI3MDk3MA==","user":{"login":"massdosage","id":29457,"node_id":"MDQ6VXNlcjI5NDU3","avatar_url":"https://avatars.githubusercontent.com/u/29457?v=4","gravatar_id":"","url":"https://api.github.com/users/massdosage","html_url":"https://github.com/massdosage","followers_url":"https://api.github.com/users/massdosage/followers","following_url":"https://api.github.com/users/massdosage/following{/other_user}","gists_url":"https://api.github.com/users/massdosage/gists{/gist_id}","starred_url":"https://api.github.com/users/massdosage/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/massdosage/subscriptions","organizations_url":"https://api.github.com/users/massdosage/orgs","repos_url":"https://api.github.com/users/massdosage/repos","events_url":"https://api.github.com/users/massdosage/events{/privacy}","received_events_url":"https://api.github.com/users/massdosage/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T14:22:55Z","updated_at":"2020-09-17T14:22:55Z","author_association":"CONTRIBUTOR","body":"OK, I've managed to get the write path working using Hive in distributed mode with what is currently in this PR. I pointed the CREATE TABLE at an existing Iceberg table that was created using Spark, added the table property \"'iceberg.mr.write.file.format'='parquet'\", set the execution engine to \"mr\" and I could then execute a successful \"INSERT INTO\". So there are some rough edges when things go wrong but the basic \"happy\" path for a write appears to be working!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694270970/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694337701","html_url":"https://github.com/apache/iceberg/pull/1444#issuecomment-694337701","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1444","id":694337701,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDMzNzcwMQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T16:10:39Z","updated_at":"2020-09-17T16:10:39Z","author_association":"CONTRIBUTOR","body":"Let me do a pass right now.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694337701/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694373135","html_url":"https://github.com/apache/iceberg/pull/1444#issuecomment-694373135","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1444","id":694373135,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDM3MzEzNQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T17:10:42Z","updated_at":"2020-09-17T17:10:42Z","author_association":"CONTRIBUTOR","body":"Yes, I think once we have delete and data file metadata tracking the sort order of a file, we should add the merge optimization for equality deletes.\r\n\r\nAs for fallback, I'm reluctant to provide tools in Iceberg itself for cases where deletes are larger than available memory _and_ must be sorted. Implementing a sort that will spill to disk is an area where we expect processing engines to be much better than Iceberg, so I would hope that we can defer that implementation to the processing engines. This is also an area where a table should be maintained. If sort orders don't match then data or deletes can be rewritten to compact, convert to position deletes, or resort to make the application of deletes mergeable.\r\n\r\nIn the short term, I suspect this will not be a problem in practice because tables will be maintained. We can revisit the fallback case if we need it.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694373135/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694376036","html_url":"https://github.com/apache/iceberg/pull/1444#issuecomment-694376036","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1444","id":694376036,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDM3NjAzNg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T17:15:41Z","updated_at":"2020-09-17T17:15:41Z","author_association":"CONTRIBUTOR","body":"Thanks for the reviews, everyone! I've merged this. Now we should be unblocked to add support to MR and Flink using the `DeleteFilter`.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694376036/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694391075","html_url":"https://github.com/apache/iceberg/issues/1463#issuecomment-694391075","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1463","id":694391075,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDM5MTA3NQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T17:39:31Z","updated_at":"2020-09-17T17:39:31Z","author_association":"CONTRIBUTOR","body":"Thank you for confirming that this works!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694391075/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694423822","html_url":"https://github.com/apache/iceberg/issues/1410#issuecomment-694423822","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1410","id":694423822,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQyMzgyMg==","user":{"login":"skambha","id":16563220,"node_id":"MDQ6VXNlcjE2NTYzMjIw","avatar_url":"https://avatars.githubusercontent.com/u/16563220?v=4","gravatar_id":"","url":"https://api.github.com/users/skambha","html_url":"https://github.com/skambha","followers_url":"https://api.github.com/users/skambha/followers","following_url":"https://api.github.com/users/skambha/following{/other_user}","gists_url":"https://api.github.com/users/skambha/gists{/gist_id}","starred_url":"https://api.github.com/users/skambha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/skambha/subscriptions","organizations_url":"https://api.github.com/users/skambha/orgs","repos_url":"https://api.github.com/users/skambha/repos","events_url":"https://api.github.com/users/skambha/events{/privacy}","received_events_url":"https://api.github.com/users/skambha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T18:38:36Z","updated_at":"2020-09-17T18:38:36Z","author_association":"CONTRIBUTOR","body":"@aokolnychyi , @rdblue,  can you please share your thoughts. For replace -  why do we merge the table properties instead of doing a replace.  Thanks. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694423822/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694436287","html_url":"https://github.com/apache/iceberg/issues/1410#issuecomment-694436287","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1410","id":694436287,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQzNjI4Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T19:01:28Z","updated_at":"2020-09-17T19:01:28Z","author_association":"CONTRIBUTOR","body":"@skambha, the reason is that some of the table properties control behavior that should be preserved. I think that this makes sense with the `REPLACE ... AS SELECT` use case. The purpose is to atomically overwrite an entire table, possibly with a different schema.\r\n\r\nUsers asked for a way to maintain a report table that is replaced daily. Because all of the data is replaced, it made little sense to validate the schema of a query against the table schema and force users to run `ALTER TABLE` commands in addition to changing the `SELECT` that produces the table data. Replacing, rather than overwriting, the table makes the most sense for this use case.\r\n\r\nBut, there are concerns that are orthogonal to replacing the table contents and schema. For example, the target split size for the table would change much more rarely than the data. Likewise, how many old metadata versions to keep, when to age off snapshots, and other settings make more sense outside of the daily replacement lifecycle. Otherwise, each `REPLACE` query would need to carry _all_ of the table settings in a `TBLPROPERTIES` clause.\r\n\r\nThat's why we want some metadata to persist across `REPLACE` operations. Exactly _what_ metadata should persist is a good thing to discuss further -- should partitioning be preserved? And we may want to have a property or flag to discard old properties and completely replace a table if you prefer a different behavior. (Would that be a Spark property or a table property? If it's a table property, do we keep it when we run the next replace?)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694436287/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694485109","html_url":"https://github.com/apache/iceberg/pull/1368#issuecomment-694485109","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1368","id":694485109,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQ4NTEwOQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T20:34:33Z","updated_at":"2020-09-17T20:34:33Z","author_association":"CONTRIBUTOR","body":"> Would it also be abnormal for users in two different time zones to write data to the same iceberg table(created by day partition)?\r\n\r\nAll timestamps are passed to Iceberg using a fixed representation. For timestamp with time zone, values are converted to UTC and passed to Iceberg as micros from epoch. All values, regardless of the SQL session time zone are passed this way. So it doesn't matter to Iceberg what the SQL session time zone was. It just has to handle UTC timestamp values.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694485109/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694488447","html_url":"https://github.com/apache/iceberg/pull/1471#issuecomment-694488447","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1471","id":694488447,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDQ4ODQ0Nw==","user":{"login":"manishmalhotrawork","id":22434010,"node_id":"MDQ6VXNlcjIyNDM0MDEw","avatar_url":"https://avatars.githubusercontent.com/u/22434010?v=4","gravatar_id":"","url":"https://api.github.com/users/manishmalhotrawork","html_url":"https://github.com/manishmalhotrawork","followers_url":"https://api.github.com/users/manishmalhotrawork/followers","following_url":"https://api.github.com/users/manishmalhotrawork/following{/other_user}","gists_url":"https://api.github.com/users/manishmalhotrawork/gists{/gist_id}","starred_url":"https://api.github.com/users/manishmalhotrawork/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/manishmalhotrawork/subscriptions","organizations_url":"https://api.github.com/users/manishmalhotrawork/orgs","repos_url":"https://api.github.com/users/manishmalhotrawork/repos","events_url":"https://api.github.com/users/manishmalhotrawork/events{/privacy}","received_events_url":"https://api.github.com/users/manishmalhotrawork/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T20:40:57Z","updated_at":"2020-09-17T20:40:57Z","author_association":"CONTRIBUTOR","body":"@RussellSpitzer taken care with most of the comments, can you please check.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694488447/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694533632","html_url":"https://github.com/apache/iceberg/pull/1473#issuecomment-694533632","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1473","id":694533632,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDUzMzYzMg==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T22:29:10Z","updated_at":"2020-09-17T22:29:10Z","author_association":"MEMBER","body":"I think this the right direction for adding catalog function support to Spark as well. Hopefully we can make this a de-facto standard for procedural calls in a catalog upstream.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694533632/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694549647","html_url":"https://github.com/apache/iceberg/pull/1474#issuecomment-694549647","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1474","id":694549647,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDU0OTY0Nw==","user":{"login":"sudssf","id":10800873,"node_id":"MDQ6VXNlcjEwODAwODcz","avatar_url":"https://avatars.githubusercontent.com/u/10800873?v=4","gravatar_id":"","url":"https://api.github.com/users/sudssf","html_url":"https://github.com/sudssf","followers_url":"https://api.github.com/users/sudssf/followers","following_url":"https://api.github.com/users/sudssf/following{/other_user}","gists_url":"https://api.github.com/users/sudssf/gists{/gist_id}","starred_url":"https://api.github.com/users/sudssf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sudssf/subscriptions","organizations_url":"https://api.github.com/users/sudssf/orgs","repos_url":"https://api.github.com/users/sudssf/repos","events_url":"https://api.github.com/users/sudssf/events{/privacy}","received_events_url":"https://api.github.com/users/sudssf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T23:17:09Z","updated_at":"2020-09-17T23:17:09Z","author_association":"CONTRIBUTOR","body":"Looks like there is no need to create another reader. I can just pass in existing reader  @rdblue do you know if this is better fix ? `generateOffsetToStartPos(List<BlockMetaData> rowGroups)`","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694549647/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694555388","html_url":"https://github.com/apache/iceberg/pull/1474#issuecomment-694555388","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1474","id":694555388,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDU1NTM4OA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-17T23:33:49Z","updated_at":"2020-09-17T23:33:49Z","author_association":"CONTRIBUTOR","body":"Thanks, @sudssf! I think we should just close the file for now, and improve the way this works in a follow-up.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694555388/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694594525","html_url":"https://github.com/apache/iceberg/pull/1356#issuecomment-694594525","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1356","id":694594525,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDU5NDUyNQ==","user":{"login":"chenjunjiedada","id":3960228,"node_id":"MDQ6VXNlcjM5NjAyMjg=","avatar_url":"https://avatars.githubusercontent.com/u/3960228?v=4","gravatar_id":"","url":"https://api.github.com/users/chenjunjiedada","html_url":"https://github.com/chenjunjiedada","followers_url":"https://api.github.com/users/chenjunjiedada/followers","following_url":"https://api.github.com/users/chenjunjiedada/following{/other_user}","gists_url":"https://api.github.com/users/chenjunjiedada/gists{/gist_id}","starred_url":"https://api.github.com/users/chenjunjiedada/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chenjunjiedada/subscriptions","organizations_url":"https://api.github.com/users/chenjunjiedada/orgs","repos_url":"https://api.github.com/users/chenjunjiedada/repos","events_url":"https://api.github.com/users/chenjunjiedada/events{/privacy}","received_events_url":"https://api.github.com/users/chenjunjiedada/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T01:26:38Z","updated_at":"2020-09-18T01:26:38Z","author_association":"COLLABORATOR","body":"@rdblue , Could you please help to take a look on this?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694594525/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694595353","html_url":"https://github.com/apache/iceberg/pull/1368#issuecomment-694595353","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1368","id":694595353,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDU5NTM1Mw==","user":{"login":"zhangdove","id":25578853,"node_id":"MDQ6VXNlcjI1NTc4ODUz","avatar_url":"https://avatars.githubusercontent.com/u/25578853?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangdove","html_url":"https://github.com/zhangdove","followers_url":"https://api.github.com/users/zhangdove/followers","following_url":"https://api.github.com/users/zhangdove/following{/other_user}","gists_url":"https://api.github.com/users/zhangdove/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangdove/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangdove/subscriptions","organizations_url":"https://api.github.com/users/zhangdove/orgs","repos_url":"https://api.github.com/users/zhangdove/repos","events_url":"https://api.github.com/users/zhangdove/events{/privacy}","received_events_url":"https://api.github.com/users/zhangdove/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T01:29:07Z","updated_at":"2020-09-18T01:29:07Z","author_association":"CONTRIBUTOR","body":"I close this PR. And look forward to new features soon about the third solution.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694595353/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694600590","html_url":"https://github.com/apache/iceberg/pull/1471#issuecomment-694600590","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1471","id":694600590,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDYwMDU5MA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T01:49:12Z","updated_at":"2020-09-18T01:49:12Z","author_association":"CONTRIBUTOR","body":"I've been thinking about this for a while but could not come up with a good alternative. The only other option is to ignore or fail if we could not resolve files against the root location we list. That means some files can be orphan forever.\r\n\r\nIt seems that checking paths should be ok as it will contain both the table location + partition path + file name (that most likely will include some UUID). In the worst case, we will not remove some orphan files and that should be extremely unlikely.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694600590/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694612728","html_url":"https://github.com/apache/iceberg/pull/1471#issuecomment-694612728","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1471","id":694612728,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDYxMjcyOA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T02:34:12Z","updated_at":"2020-09-18T02:36:36Z","author_association":"CONTRIBUTOR","body":"I think this logic can be simplified a bit. My first thought was to leverage the following code:\r\n\r\n```\r\n  private static final UserDefinedFunction relativePathUDF = functions.udf((String location) -> {\r\n    Path fullyQualifiedPath = new Path(location);\r\n    return fullyQualifiedPath.toUri().getPath();\r\n  }, DataTypes.StringType);\r\n\r\n  ...\r\n\r\n  Dataset<Row> validFileDF = withRelativePathColumn(validDataFileDF.union(validMetadataFileDF));\r\n  Dataset<Row> actualFileDF = withRelativePathColumn(buildActualFileDF());\r\n\r\n  ...\r\n\r\n  Column joinCond = actualFileDF.col(\"relative_path\").equalTo(validFileDF.col(\"relative_path\"));\r\n  return actualFileDF.join(validFileDF, joinCond, \"leftanti\").select(\"file_path\")\r\n      .as(Encoders.STRING())\r\n      .collectAsList();\r\n```\r\n\r\nWe wouldn't need the filename UDF and it would be very straightforward. Unfortunately, that does not seem to work for certain locations. For example, `path.toUri().getPath()` for `hdfs://user/location/sublocation/filename.parquet` will return `location/sublocation/filename.parquet` and `user` is considered as authority.\r\n\r\nEven if we have to keep `contains` and equality of file names, I think we can still leverage a single UDF:\r\n\r\n```\r\n  private static final UserDefinedFunction fileDetailUDF = functions.udf((String location) -> {\r\n    Path fullyQualifiedPath = new Path(location);\r\n    String fileName = fullyQualifiedPath.getName();\r\n    String relativePath = fullyQualifiedPath.toUri().getPath();\r\n    return RowFactory.create(fileName, relativePath);\r\n  }, FILE_DETAIL_STRUCT);\r\n```\r\n\r\nThen our join condition will be == on file names and contains on relative locations.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694612728/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694613468","html_url":"https://github.com/apache/iceberg/pull/1471#issuecomment-694613468","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1471","id":694613468,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDYxMzQ2OA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T02:36:51Z","updated_at":"2020-09-18T02:36:51Z","author_association":"CONTRIBUTOR","body":"@rdblue @RussellSpitzer @manishmalhotrawork what do you think?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694613468/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694643878","html_url":"https://github.com/apache/iceberg/issues/1476#issuecomment-694643878","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1476","id":694643878,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDY0Mzg3OA==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T04:31:44Z","updated_at":"2020-09-18T04:31:44Z","author_association":"CONTRIBUTOR","body":"While it seems a bit odd expired snapshots are still shown (Have you run \"refresh table hadoop_prod.ice.recmd_feedback_tb.snapshots\" and \"refresh table hadoop_prod.ice.recmd_feedback_tb.history\"?), it's expected behavior for all data files to be on HDFS. \r\n\r\nYou looks to be confused the behavior of expiring snapshots - it doesn't mean the operations done with these snapshots will be reverted. Data remains the same. You just no longer be able to do time-travel for expired snapshots or roll back to the specific snapshot among them.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694643878/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694649326","html_url":"https://github.com/apache/iceberg/issues/1476#issuecomment-694649326","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1476","id":694649326,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDY0OTMyNg==","user":{"login":"lordk911","id":19989300,"node_id":"MDQ6VXNlcjE5OTg5MzAw","avatar_url":"https://avatars.githubusercontent.com/u/19989300?v=4","gravatar_id":"","url":"https://api.github.com/users/lordk911","html_url":"https://github.com/lordk911","followers_url":"https://api.github.com/users/lordk911/followers","following_url":"https://api.github.com/users/lordk911/following{/other_user}","gists_url":"https://api.github.com/users/lordk911/gists{/gist_id}","starred_url":"https://api.github.com/users/lordk911/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lordk911/subscriptions","organizations_url":"https://api.github.com/users/lordk911/orgs","repos_url":"https://api.github.com/users/lordk911/repos","events_url":"https://api.github.com/users/lordk911/events{/privacy}","received_events_url":"https://api.github.com/users/lordk911/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T04:53:02Z","updated_at":"2020-09-18T04:53:02Z","author_association":"NONE","body":"sorry , In another spark session ,when I query the snapshots, It shows the latest one. But I the data file still on hdfs , it's that mean I shoud delete them which not used by any snapshot manually?\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694649326/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694650536","html_url":"https://github.com/apache/iceberg/issues/1476#issuecomment-694650536","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1476","id":694650536,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDY1MDUzNg==","user":{"login":"simonsssu","id":12323514,"node_id":"MDQ6VXNlcjEyMzIzNTE0","avatar_url":"https://avatars.githubusercontent.com/u/12323514?v=4","gravatar_id":"","url":"https://api.github.com/users/simonsssu","html_url":"https://github.com/simonsssu","followers_url":"https://api.github.com/users/simonsssu/followers","following_url":"https://api.github.com/users/simonsssu/following{/other_user}","gists_url":"https://api.github.com/users/simonsssu/gists{/gist_id}","starred_url":"https://api.github.com/users/simonsssu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/simonsssu/subscriptions","organizations_url":"https://api.github.com/users/simonsssu/orgs","repos_url":"https://api.github.com/users/simonsssu/repos","events_url":"https://api.github.com/users/simonsssu/events{/privacy}","received_events_url":"https://api.github.com/users/simonsssu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T04:57:30Z","updated_at":"2020-09-18T04:57:30Z","author_association":"CONTRIBUTOR","body":"The expired snapshots will have a deleteFile list and addFileList, can you make sure that the file you see is the delete File in current snapshots ? you can find the manifest file referenced by current snapshot and see the file status is 1 (added) or 2 (deleted) referenced by these manifests.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694650536/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694655329","html_url":"https://github.com/apache/iceberg/pull/971#issuecomment-694655329","issue_url":"https://api.github.com/repos/apache/iceberg/issues/971","id":694655329,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDY1NTMyOQ==","user":{"login":"chenjunjiedada","id":3960228,"node_id":"MDQ6VXNlcjM5NjAyMjg=","avatar_url":"https://avatars.githubusercontent.com/u/3960228?v=4","gravatar_id":"","url":"https://api.github.com/users/chenjunjiedada","html_url":"https://github.com/chenjunjiedada","followers_url":"https://api.github.com/users/chenjunjiedada/followers","following_url":"https://api.github.com/users/chenjunjiedada/following{/other_user}","gists_url":"https://api.github.com/users/chenjunjiedada/gists{/gist_id}","starred_url":"https://api.github.com/users/chenjunjiedada/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chenjunjiedada/subscriptions","organizations_url":"https://api.github.com/users/chenjunjiedada/orgs","repos_url":"https://api.github.com/users/chenjunjiedada/repos","events_url":"https://api.github.com/users/chenjunjiedada/events{/privacy}","received_events_url":"https://api.github.com/users/chenjunjiedada/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T05:13:37Z","updated_at":"2020-09-18T05:13:37Z","author_association":"COLLABORATOR","body":"This should be already done by recent row-level delete PRs from Ryan.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694655329/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694660247","html_url":"https://github.com/apache/iceberg/issues/1476#issuecomment-694660247","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1476","id":694660247,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDY2MDI0Nw==","user":{"login":"lordk911","id":19989300,"node_id":"MDQ6VXNlcjE5OTg5MzAw","avatar_url":"https://avatars.githubusercontent.com/u/19989300?v=4","gravatar_id":"","url":"https://api.github.com/users/lordk911","html_url":"https://github.com/lordk911","followers_url":"https://api.github.com/users/lordk911/followers","following_url":"https://api.github.com/users/lordk911/following{/other_user}","gists_url":"https://api.github.com/users/lordk911/gists{/gist_id}","starred_url":"https://api.github.com/users/lordk911/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lordk911/subscriptions","organizations_url":"https://api.github.com/users/lordk911/orgs","repos_url":"https://api.github.com/users/lordk911/repos","events_url":"https://api.github.com/users/lordk911/events{/privacy}","received_events_url":"https://api.github.com/users/lordk911/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T05:28:23Z","updated_at":"2020-09-18T05:28:23Z","author_association":"NONE","body":"some thing I missed to mention, befor I do snapshot expired, I do the rewriteDataFiles operation to rewrite all the small data file to one big file , it generate the latest snapshot , then I try to expired all other snapshot , I think the all small data files should be deleted.\r\n\r\n```\r\nscala> spark.sql(\"select manifest_list from hadoop_prod.ice.recmd_feedback_tb.snapshots\").show(false)\r\n+-------------------------------------------------------------------------------------------------------------------------------------+\r\n|manifest_list                                                                                                                        |\r\n+-------------------------------------------------------------------------------------------------------------------------------------+\r\n|hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/snap-6248485188751590692-1-7d3b47bc-a247-4701-9e05-780b1f53ba58.avro|\r\n+-------------------------------------------------------------------------------------------------------------------------------------+\r\n\r\nhdfs dfs -text /tmp/warehouse/ice/recmd_feedback_tb/metadata/snap-6248485188751590692-1-7d3b47bc-a247-4701-9e05-780b1f53ba58.avro\r\n20/09/18 13:18:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m9.avro\",\"manifest_length\":5976,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":1},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":0},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":19859},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":0}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m4.avro\",\"manifest_length\":5858,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":1},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":1}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m6.avro\",\"manifest_length\":5923,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":1},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":2}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m7.avro\",\"manifest_length\":5888,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":1},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":2}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m3.avro\",\"manifest_length\":6151,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":3},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":4}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m1.avro\",\"manifest_length\":6094,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":2},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":3}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m2.avro\",\"manifest_length\":5833,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":1},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":1}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m5.avro\",\"manifest_length\":6454,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":3},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":3373}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m0.avro\",\"manifest_length\":6439,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":3},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":8042}}\r\n{\"manifest_path\":\"hdfs://nameservice1/tmp/warehouse/ice/recmd_feedback_tb/metadata/7d3b47bc-a247-4701-9e05-780b1f53ba58-m8.avro\",\"manifest_length\":6473,\"partition_spec_id\":0,\"added_snapshot_id\":{\"long\":6248485188751590692},\"added_data_files_count\":{\"int\":0},\"existing_data_files_count\":{\"int\":0},\"deleted_data_files_count\":{\"int\":3},\"partitions\":{\"array\":[]},\"added_rows_count\":{\"long\":0},\"existing_rows_count\":{\"long\":0},\"deleted_rows_count\":{\"long\":8431}}\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694660247/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694679340","html_url":"https://github.com/apache/iceberg/pull/1471#issuecomment-694679340","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1471","id":694679340,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDY3OTM0MA==","user":{"login":"manishmalhotrawork","id":22434010,"node_id":"MDQ6VXNlcjIyNDM0MDEw","avatar_url":"https://avatars.githubusercontent.com/u/22434010?v=4","gravatar_id":"","url":"https://api.github.com/users/manishmalhotrawork","html_url":"https://github.com/manishmalhotrawork","followers_url":"https://api.github.com/users/manishmalhotrawork/followers","following_url":"https://api.github.com/users/manishmalhotrawork/following{/other_user}","gists_url":"https://api.github.com/users/manishmalhotrawork/gists{/gist_id}","starred_url":"https://api.github.com/users/manishmalhotrawork/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/manishmalhotrawork/subscriptions","organizations_url":"https://api.github.com/users/manishmalhotrawork/orgs","repos_url":"https://api.github.com/users/manishmalhotrawork/repos","events_url":"https://api.github.com/users/manishmalhotrawork/events{/privacy}","received_events_url":"https://api.github.com/users/manishmalhotrawork/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T06:20:31Z","updated_at":"2020-09-18T06:20:31Z","author_association":"CONTRIBUTOR","body":"> I think this logic can be simplified a bit. My first thought was to leverage the following code:\r\n> \r\n> ```\r\n>   private static final UserDefinedFunction relativePathUDF = functions.udf((String location) -> {\r\n>     Path fullyQualifiedPath = new Path(location);\r\n>     return fullyQualifiedPath.toUri().getPath();\r\n>   }, DataTypes.StringType);\r\n> \r\n>   ...\r\n> \r\n>   Dataset<Row> validFileDF = withRelativePathColumn(validDataFileDF.union(validMetadataFileDF));\r\n>   Dataset<Row> actualFileDF = withRelativePathColumn(buildActualFileDF());\r\n> \r\n>   ...\r\n> \r\n>   Column joinCond = actualFileDF.col(\"relative_path\").equalTo(validFileDF.col(\"relative_path\"));\r\n>   return actualFileDF.join(validFileDF, joinCond, \"leftanti\").select(\"file_path\")\r\n>       .as(Encoders.STRING())\r\n>       .collectAsList();\r\n> ```\r\n> \r\n> We wouldn't need the filename UDF and it would be very straightforward. Unfortunately, that does not seem to work for certain locations. For example, `path.toUri().getPath()` for `hdfs://user/location/sublocation/filename.parquet` will return `location/sublocation/filename.parquet` and `user` is considered as authority.\r\n> \r\n> Even if we have to keep `contains` and equality of file names, I think we can still leverage a single UDF:\r\n> \r\n> ```\r\n>   private static final UserDefinedFunction fileDetailUDF = functions.udf((String location) -> {\r\n>     Path fullyQualifiedPath = new Path(location);\r\n>     String fileName = fullyQualifiedPath.getName();\r\n>     String relativePath = fullyQualifiedPath.toUri().getPath();\r\n>     return RowFactory.create(fileName, relativePath);\r\n>   }, FILE_DETAIL_STRUCT);\r\n> ```\r\n> \r\n> Then our join condition will be == on file names and contains on relative locations.\r\n\r\nthanks @aokolnychyi !\r\n\r\nWe can keep one UDF for both the fileName and paths.\r\nThough I think, we would need to keep `fullyQualifiedPath` as well, because it's required as orphan files path, to be deleted.\r\nFor join conditions, these 2 columns should be good.\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694679340/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694696567","html_url":"https://github.com/apache/iceberg/issues/1476#issuecomment-694696567","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1476","id":694696567,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDY5NjU2Nw==","user":{"login":"simonsssu","id":12323514,"node_id":"MDQ6VXNlcjEyMzIzNTE0","avatar_url":"https://avatars.githubusercontent.com/u/12323514?v=4","gravatar_id":"","url":"https://api.github.com/users/simonsssu","html_url":"https://github.com/simonsssu","followers_url":"https://api.github.com/users/simonsssu/followers","following_url":"https://api.github.com/users/simonsssu/following{/other_user}","gists_url":"https://api.github.com/users/simonsssu/gists{/gist_id}","starred_url":"https://api.github.com/users/simonsssu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/simonsssu/subscriptions","organizations_url":"https://api.github.com/users/simonsssu/orgs","repos_url":"https://api.github.com/users/simonsssu/repos","events_url":"https://api.github.com/users/simonsssu/events{/privacy}","received_events_url":"https://api.github.com/users/simonsssu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T07:05:13Z","updated_at":"2020-09-18T07:05:13Z","author_association":"CONTRIBUTOR","body":"Actually I think the old files will not be deleted at this point of time. because it still reference by manifests in your current snapshots. you can see the old files list are still reference in one of the manifest file in your current snapshot with status 2 (deleted) , you can try to generate a new snapshot and make current to be the old one, then expire it. ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694696567/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694715531","html_url":"https://github.com/apache/iceberg/issues/1476#issuecomment-694715531","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1476","id":694715531,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDcxNTUzMQ==","user":{"login":"lordk911","id":19989300,"node_id":"MDQ6VXNlcjE5OTg5MzAw","avatar_url":"https://avatars.githubusercontent.com/u/19989300?v=4","gravatar_id":"","url":"https://api.github.com/users/lordk911","html_url":"https://github.com/lordk911","followers_url":"https://api.github.com/users/lordk911/followers","following_url":"https://api.github.com/users/lordk911/following{/other_user}","gists_url":"https://api.github.com/users/lordk911/gists{/gist_id}","starred_url":"https://api.github.com/users/lordk911/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lordk911/subscriptions","organizations_url":"https://api.github.com/users/lordk911/orgs","repos_url":"https://api.github.com/users/lordk911/repos","events_url":"https://api.github.com/users/lordk911/events{/privacy}","received_events_url":"https://api.github.com/users/lordk911/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T07:50:32Z","updated_at":"2020-09-18T07:50:32Z","author_association":"NONE","body":"\" you can try to generate a new snapshot and make current to be the old one, then expire it.\"\r\n@Simon0806 you are right , I misunderstand iceberg's behavior.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694715531/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694939739","html_url":"https://github.com/apache/iceberg/pull/1455#issuecomment-694939739","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1455","id":694939739,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDkzOTczOQ==","user":{"login":"marton-bod","id":19599214,"node_id":"MDQ6VXNlcjE5NTk5MjE0","avatar_url":"https://avatars.githubusercontent.com/u/19599214?v=4","gravatar_id":"","url":"https://api.github.com/users/marton-bod","html_url":"https://github.com/marton-bod","followers_url":"https://api.github.com/users/marton-bod/followers","following_url":"https://api.github.com/users/marton-bod/following{/other_user}","gists_url":"https://api.github.com/users/marton-bod/gists{/gist_id}","starred_url":"https://api.github.com/users/marton-bod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/marton-bod/subscriptions","organizations_url":"https://api.github.com/users/marton-bod/orgs","repos_url":"https://api.github.com/users/marton-bod/repos","events_url":"https://api.github.com/users/marton-bod/events{/privacy}","received_events_url":"https://api.github.com/users/marton-bod/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T15:38:33Z","updated_at":"2020-09-18T15:38:33Z","author_association":"COLLABORATOR","body":"Thanks a lot for your reviews! @rdblue @kbendick @pvary\r\nI have opened a new PR addressing your comments and changing the hive3 build approach:\r\nhttps://github.com/apache/iceberg/pull/1478\r\nI would very much appreciate it if you could review this as well. Thank you!\r\n(in the new PR, I will link back to this one so we have a complete history)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694939739/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694977225","html_url":"https://github.com/apache/iceberg/issues/1438#issuecomment-694977225","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1438","id":694977225,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDk3NzIyNQ==","user":{"login":"stevenzwu","id":1545663,"node_id":"MDQ6VXNlcjE1NDU2NjM=","avatar_url":"https://avatars.githubusercontent.com/u/1545663?v=4","gravatar_id":"","url":"https://api.github.com/users/stevenzwu","html_url":"https://github.com/stevenzwu","followers_url":"https://api.github.com/users/stevenzwu/followers","following_url":"https://api.github.com/users/stevenzwu/following{/other_user}","gists_url":"https://api.github.com/users/stevenzwu/gists{/gist_id}","starred_url":"https://api.github.com/users/stevenzwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/stevenzwu/subscriptions","organizations_url":"https://api.github.com/users/stevenzwu/orgs","repos_url":"https://api.github.com/users/stevenzwu/repos","events_url":"https://api.github.com/users/stevenzwu/events{/privacy}","received_events_url":"https://api.github.com/users/stevenzwu/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T16:57:27Z","updated_at":"2020-09-18T16:57:27Z","author_association":"CONTRIBUTOR","body":"There is a also a broader discussion in Flink community regarding a new unified (stream and batch) sink interface, which can really help the Flink Iceberg sink: (1) simplify committer code (including the ManifestFile discussion here) (2) run committer in jobmanager which can avoid the current limitation of invalidate embarrassingly parallel DAG. If we migrate to the new sink interface, it will likely be an incompatible change (including state). I think the new sink interface is targeted for the upcoming Flink 1.12 (ETA is Nov)\r\n\r\n@rdblue @openinx @JingsongLi  I am wondering if we should release Iceberg with the current V1 product and plus some blocker fixes, like [1]. If we have time for those incompatible changes, that is great. if not, we can come back for the major incompatible changes once Flink released the new sink interface in Flink 1.12. I will also try to help then.\r\n\r\n[1] https://github.com/apache/iceberg/pull/1404 (merged already)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694977225/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694979347","html_url":"https://github.com/apache/iceberg/pull/1474#issuecomment-694979347","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1474","id":694979347,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NDk3OTM0Nw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T17:02:02Z","updated_at":"2020-09-18T17:02:02Z","author_association":"CONTRIBUTOR","body":"Thanks for the quick fix, @sudssf! I've merged this to master.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/694979347/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695025440","html_url":"https://github.com/apache/iceberg/pull/1474#issuecomment-695025440","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1474","id":695025440,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTAyNTQ0MA==","user":{"login":"sudssf","id":10800873,"node_id":"MDQ6VXNlcjEwODAwODcz","avatar_url":"https://avatars.githubusercontent.com/u/10800873?v=4","gravatar_id":"","url":"https://api.github.com/users/sudssf","html_url":"https://github.com/sudssf","followers_url":"https://api.github.com/users/sudssf/followers","following_url":"https://api.github.com/users/sudssf/following{/other_user}","gists_url":"https://api.github.com/users/sudssf/gists{/gist_id}","starred_url":"https://api.github.com/users/sudssf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sudssf/subscriptions","organizations_url":"https://api.github.com/users/sudssf/orgs","repos_url":"https://api.github.com/users/sudssf/repos","events_url":"https://api.github.com/users/sudssf/events{/privacy}","received_events_url":"https://api.github.com/users/sudssf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T18:40:22Z","updated_at":"2020-09-18T18:40:22Z","author_association":"CONTRIBUTOR","body":"> Thanks for the quick fix, @sudssf! I've merged this to master.\r\n\r\nthanks I am happy to add tests to catch this kind of issues in future. we build end to end integration tests using local s3a which help capture some of these bugs. let me know if you think it will add value.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695025440/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695028373","html_url":"https://github.com/apache/iceberg/pull/1465#issuecomment-695028373","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1465","id":695028373,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTAyODM3Mw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T18:46:29Z","updated_at":"2020-09-18T18:46:29Z","author_association":"CONTRIBUTOR","body":"Looks good to me! I merged this.\r\n\r\nI agree that we can follow up with an improvement to avoid corrupt version hints. No need to mix that in here.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695028373/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695029232","html_url":"https://github.com/apache/iceberg/pull/1477#issuecomment-695029232","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1477","id":695029232,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTAyOTIzMg==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T18:48:18Z","updated_at":"2020-09-18T18:48:18Z","author_association":"CONTRIBUTOR","body":"@stevenzwu, could you help review?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695029232/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695030754","html_url":"https://github.com/apache/iceberg/pull/1474#issuecomment-695030754","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1474","id":695030754,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTAzMDc1NA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T18:51:22Z","updated_at":"2020-09-18T18:51:22Z","author_association":"CONTRIBUTOR","body":"Sure, more tests are always welcome. What do you have in mind to catch unclosed files?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695030754/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695049208","html_url":"https://github.com/apache/iceberg/pull/1469#issuecomment-695049208","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1469","id":695049208,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTA0OTIwOA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T19:35:38Z","updated_at":"2020-09-18T19:35:38Z","author_association":"CONTRIBUTOR","body":"> do we want to make the validation mandatory?\r\n\r\nProbably not. There is more than one way to guarantee that a delete is safe and I don't want to force implementations to work around checks imposed by the commit operations.\r\n\r\nFor example, you could have a delete application that applies deletes, commits optimistically, and the goes back to check whether any files were rewritten and delete from those files. That would delete rows more quickly and make incremental progress, at the cost of ensuring all rows were deleted at the same time. Similarly, I could \"lock\" a table with an external service and perform deletes in that window to avoid contention.\r\n\r\nI think the motivation for the suggestion is good -- how do we prevent API callers from making mistakes -- but we do have to trust that they know what they are doing most of the time.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695049208/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695106528","html_url":"https://github.com/apache/iceberg/issues/1479#issuecomment-695106528","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1479","id":695106528,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTEwNjUyOA==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-18T22:03:24Z","updated_at":"2020-09-18T22:04:05Z","author_association":"MEMBER","body":"Should we just be not escaping?\r\n```\r\n      sb.append(field.name()).append(\"=\").append(escape(valueString));\r\n```\r\n\r\nBecause even if this was what we wanted it would be wrong for field names since we are only escaping values","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695106528/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695145610","html_url":"https://github.com/apache/iceberg/pull/1356#issuecomment-695145610","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1356","id":695145610,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTE0NTYxMA==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-19T01:29:25Z","updated_at":"2020-09-19T01:29:25Z","author_association":"CONTRIBUTOR","body":"Thanks @chenjunjiedada, this is definitely on my list to review. I'll take a look as soon as I can.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695145610/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695156033","html_url":"https://github.com/apache/iceberg/issues/1479#issuecomment-695156033","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1479","id":695156033,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTE1NjAzMw==","user":{"login":"manishmalhotrawork","id":22434010,"node_id":"MDQ6VXNlcjIyNDM0MDEw","avatar_url":"https://avatars.githubusercontent.com/u/22434010?v=4","gravatar_id":"","url":"https://api.github.com/users/manishmalhotrawork","html_url":"https://github.com/manishmalhotrawork","followers_url":"https://api.github.com/users/manishmalhotrawork/followers","following_url":"https://api.github.com/users/manishmalhotrawork/following{/other_user}","gists_url":"https://api.github.com/users/manishmalhotrawork/gists{/gist_id}","starred_url":"https://api.github.com/users/manishmalhotrawork/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/manishmalhotrawork/subscriptions","organizations_url":"https://api.github.com/users/manishmalhotrawork/orgs","repos_url":"https://api.github.com/users/manishmalhotrawork/repos","events_url":"https://api.github.com/users/manishmalhotrawork/events{/privacy}","received_events_url":"https://api.github.com/users/manishmalhotrawork/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-19T03:09:32Z","updated_at":"2020-09-19T03:09:32Z","author_association":"CONTRIBUTOR","body":"@RussellSpitzer @aokolnychyi \r\n\r\nyeah, looks like this change can fix the issue.\r\nas `URLEncoder` converts space to `+`\r\n\r\nOr it has to do decoding as encoded?\r\n\r\ntested column_name field with spaces it works ( in case we create table using Iceberg internal API's) with Spark, it has to use back ticks `\r\n\r\nI tested with no `escape` and it seems to be good for this test-case.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695156033/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695480273","html_url":"https://github.com/apache/iceberg/issues/1366#issuecomment-695480273","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1366","id":695480273,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTQ4MDI3Mw==","user":{"login":"rdsr","id":129474,"node_id":"MDQ6VXNlcjEyOTQ3NA==","avatar_url":"https://avatars.githubusercontent.com/u/129474?v=4","gravatar_id":"","url":"https://api.github.com/users/rdsr","html_url":"https://github.com/rdsr","followers_url":"https://api.github.com/users/rdsr/followers","following_url":"https://api.github.com/users/rdsr/following{/other_user}","gists_url":"https://api.github.com/users/rdsr/gists{/gist_id}","starred_url":"https://api.github.com/users/rdsr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdsr/subscriptions","organizations_url":"https://api.github.com/users/rdsr/orgs","repos_url":"https://api.github.com/users/rdsr/repos","events_url":"https://api.github.com/users/rdsr/events{/privacy}","received_events_url":"https://api.github.com/users/rdsr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-20T01:54:36Z","updated_at":"2020-09-20T01:54:36Z","author_association":"CONTRIBUTOR","body":"I believe this is done as part of #1418 ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695480273/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695490407","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-695490407","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":695490407,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTQ5MDQwNw==","user":{"login":"rdsr","id":129474,"node_id":"MDQ6VXNlcjEyOTQ3NA==","avatar_url":"https://avatars.githubusercontent.com/u/129474?v=4","gravatar_id":"","url":"https://api.github.com/users/rdsr","html_url":"https://github.com/rdsr","followers_url":"https://api.github.com/users/rdsr/followers","following_url":"https://api.github.com/users/rdsr/following{/other_user}","gists_url":"https://api.github.com/users/rdsr/gists{/gist_id}","starred_url":"https://api.github.com/users/rdsr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdsr/subscriptions","organizations_url":"https://api.github.com/users/rdsr/orgs","repos_url":"https://api.github.com/users/rdsr/repos","events_url":"https://api.github.com/users/rdsr/events{/privacy}","received_events_url":"https://api.github.com/users/rdsr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-20T02:01:40Z","updated_at":"2020-09-20T02:03:28Z","author_association":"CONTRIBUTOR","body":"Hi @marton-bod thank you for working on this. I have a few questions\r\n\r\n1. I think `hive-metastore` module should be unaffected whether we are running with hive2 or hive3 as `hive-metastore` only uses Metatstoreclient API and I've seen that Metastore upgrades are always backward compatible. I believe Hive metastore 3 should be backward compatible with Hive metastore client 2.  In that respect we could keep using `hive-metastore` module as is. @pvary, @rdblue   thoughts?\r\n2. For the hive code in `mr` module which requires changes because of hive2/hive3 artifacts.  Is it possible to move the hive specific code from `mr` module and have 3 modules [similar to spark2/spark3]  - `hive-exec` for common code, `hive-exec2` and `hive-exec3` for incompatible code. This way our `mr` modules is simplified and it is also aligned with our discussion on the mailing list to have an `hive-exec` module.  This is easier said than done since it will mean we might have to ship different runtime jars. So best to also get agreement from folks like @rdblue @massdosage and @pvary .\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695490407/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695686623","html_url":"https://github.com/apache/iceberg/pull/1469#issuecomment-695686623","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1469","id":695686623,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTY4NjYyMw==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-20T04:19:23Z","updated_at":"2020-09-23T00:49:11Z","author_association":"CONTRIBUTOR","body":"I think this PR raises a very good point that we haven't considered for merge-on-read but already have for copy-on-write.\r\n\r\nThis PR looks good to me but I want us to think through which validation we will eventually need. Let's consider the following use cases: DELETE and UPDATE with positional deletes, DELETE and UPDATE with equality deletes. Each operation may have different isolation levels: serializable and snapshot isolation (can be more but let's skip that for now).\r\n\r\n**DELETE with positional deletes**\r\n\r\n| Isolation | Validation |\r\n| --------- | ------------- |\r\n| serializable  | - no new potentially matching data files since we read<br/> - data files referenced by new deletes must be still present or deleted (not rewritten or overwritten)<br/> - no validation on delete files as it is ok if the row was deleted concurrently\r\n| snapshot  |  - data files referenced by new deletes must be still present or deleted (not rewritten or overwritten) <br/> - no validation on new potentially matching data files since we read<br/>- no validation on delete files as it is ok if the row was deleted concurrently  |\r\n\r\n\r\n**UPDATE with positional deletes**\r\n\r\n| Isolation | Validation |\r\n| --------- | ------------- |\r\n| serializable  | - no new potentially matching data files since we read<br/> - no new potentially matching delete files as it is NOT ok if the row was deleted concurrently <br/> - data files referenced by new deletes must be still present \r\n| snapshot  |  - no new potentially matching delete files as it is NOT ok if the row was deleted concurrently <br/> - data files referenced by new deletes must be still present<br/> - no validation on new potentially matching data files since we read<br/>|\r\n\r\n**DELETE with equality deletes**\r\n\r\n| Isolation | Validation |\r\n| --------- | ------------- |\r\n| serializable  | - no validation on new potentially matching data files since we don't have to read the table<br/> - no validation on delete files as it is ok if the row was deleted concurrently\r\n| snapshot  | - no validation on new potentially matching data files since we don't have to read the table<br/>- no validation on delete files as it is ok if the row was deleted concurrently  |\r\n\r\n\r\n**UPDATE with equality deletes**\r\n\r\n| Isolation | Validation |\r\n| --------- | ------------- |\r\n| serializable  | - no validation on new potentially matching data files since we don't have to read the table <br/> - no validation on new potentially matching delete files as we don't have to read the table\r\n| snapshot  |  - no validation on new potentially matching data files since we don't have to read the table <br/> - no validation on new potentially matching delete files as we don't have to read the table |\r\n\r\nDoes this seem correct?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695686623/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695769420","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-695769420","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":695769420,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTc2OTQyMA==","user":{"login":"marton-bod","id":19599214,"node_id":"MDQ6VXNlcjE5NTk5MjE0","avatar_url":"https://avatars.githubusercontent.com/u/19599214?v=4","gravatar_id":"","url":"https://api.github.com/users/marton-bod","html_url":"https://github.com/marton-bod","followers_url":"https://api.github.com/users/marton-bod/followers","following_url":"https://api.github.com/users/marton-bod/following{/other_user}","gists_url":"https://api.github.com/users/marton-bod/gists{/gist_id}","starred_url":"https://api.github.com/users/marton-bod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/marton-bod/subscriptions","organizations_url":"https://api.github.com/users/marton-bod/orgs","repos_url":"https://api.github.com/users/marton-bod/repos","events_url":"https://api.github.com/users/marton-bod/events{/privacy}","received_events_url":"https://api.github.com/users/marton-bod/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-20T10:13:06Z","updated_at":"2020-09-20T10:13:06Z","author_association":"COLLABORATOR","body":"Thanks @rdsr for sharing your thoughts!\r\n1.) I agree with you, hive-metastore should be left as is. I haven't changed the hive-metastore module other than fixing the teardown problem in the unit tests, which occurs when using hive 3 dependencies (discussed in https://github.com/apache/iceberg/pull/1455).\r\n2.) I think I've done what you just described, only under different module names. Right now, common code resides in `iceberg-mr`, and incompatible codes have been moved out to `iceberg-mr-hive2` and `icebegr-mr-hive3`. I'm open to renaming the modules to `exec`, sounds like a more suitable name. As for shipping different runtime jars, we might want to create separate runtime modules too just like for spark2 and 3.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695769420/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695809197","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-695809197","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":695809197,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTgwOTE5Nw==","user":{"login":"rdsr","id":129474,"node_id":"MDQ6VXNlcjEyOTQ3NA==","avatar_url":"https://avatars.githubusercontent.com/u/129474?v=4","gravatar_id":"","url":"https://api.github.com/users/rdsr","html_url":"https://github.com/rdsr","followers_url":"https://api.github.com/users/rdsr/followers","following_url":"https://api.github.com/users/rdsr/following{/other_user}","gists_url":"https://api.github.com/users/rdsr/gists{/gist_id}","starred_url":"https://api.github.com/users/rdsr/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdsr/subscriptions","organizations_url":"https://api.github.com/users/rdsr/orgs","repos_url":"https://api.github.com/users/rdsr/repos","events_url":"https://api.github.com/users/rdsr/events{/privacy}","received_events_url":"https://api.github.com/users/rdsr/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-20T16:52:19Z","updated_at":"2020-09-20T16:52:19Z","author_association":"CONTRIBUTOR","body":"Thanks @marton-bod for your comment. Is it possible, then, to only have hive3 dependencies under `iceberg-mr-hive3` instead of building other modules too with hive3 and hadoop3?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695809197/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695824861","html_url":"https://github.com/apache/iceberg/pull/1478#issuecomment-695824861","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1478","id":695824861,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTgyNDg2MQ==","user":{"login":"marton-bod","id":19599214,"node_id":"MDQ6VXNlcjE5NTk5MjE0","avatar_url":"https://avatars.githubusercontent.com/u/19599214?v=4","gravatar_id":"","url":"https://api.github.com/users/marton-bod","html_url":"https://github.com/marton-bod","followers_url":"https://api.github.com/users/marton-bod/followers","following_url":"https://api.github.com/users/marton-bod/following{/other_user}","gists_url":"https://api.github.com/users/marton-bod/gists{/gist_id}","starred_url":"https://api.github.com/users/marton-bod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/marton-bod/subscriptions","organizations_url":"https://api.github.com/users/marton-bod/orgs","repos_url":"https://api.github.com/users/marton-bod/repos","events_url":"https://api.github.com/users/marton-bod/events{/privacy}","received_events_url":"https://api.github.com/users/marton-bod/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-20T19:18:12Z","updated_at":"2020-09-20T20:06:58Z","author_association":"COLLABORATOR","body":"In order to build `iceberg-mr`/`iceberg-mr-hive3`, we need to build the other iceberg subprojects with Hive3/Hadoop3 as well which `mr` depends on, such as `hive-metastore`, `core`, `parquet`, etc. For example, there is an HMSHandler API change between Hive2 and Hive3, therefore if we didn't build the `hive-metastore` module with Hive3, `mr` (with Hive3) would not be able to communicate with it.\r\n\r\nHowever, just to reiterate, the idea is that for the 'normal' gradle build, all modules will continue to be built using Hive2/Hadoop2 just as before, with no changes. Only when specifying the `-Dhive3` flag to the build, will Hive3/Hadoop3 dependencies be used (and when this flag's enabled, only those modules are included that are ready for the Hive3-build - i.e. Spark and Flink excluded for now, although they can also be added by the community whenever suitable)","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695824861/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695831364","html_url":"https://github.com/apache/iceberg/issues/1468#issuecomment-695831364","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1468","id":695831364,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NTgzMTM2NA==","user":{"login":"ismailsimsek","id":6005685,"node_id":"MDQ6VXNlcjYwMDU2ODU=","avatar_url":"https://avatars.githubusercontent.com/u/6005685?v=4","gravatar_id":"","url":"https://api.github.com/users/ismailsimsek","html_url":"https://github.com/ismailsimsek","followers_url":"https://api.github.com/users/ismailsimsek/followers","following_url":"https://api.github.com/users/ismailsimsek/following{/other_user}","gists_url":"https://api.github.com/users/ismailsimsek/gists{/gist_id}","starred_url":"https://api.github.com/users/ismailsimsek/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ismailsimsek/subscriptions","organizations_url":"https://api.github.com/users/ismailsimsek/orgs","repos_url":"https://api.github.com/users/ismailsimsek/repos","events_url":"https://api.github.com/users/ismailsimsek/events{/privacy}","received_events_url":"https://api.github.com/users/ismailsimsek/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-20T20:19:18Z","updated_at":"2020-09-20T20:19:18Z","author_association":"CONTRIBUTOR","body":"is it possible to write JDBC based catalog? that could unlock many catalog option","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/695831364/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696046219","html_url":"https://github.com/apache/iceberg/pull/1407#issuecomment-696046219","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1407","id":696046219,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjA0NjIxOQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-21T11:05:36Z","updated_at":"2020-09-21T11:05:36Z","author_association":"CONTRIBUTOR","body":"@rdblue: What are your thoughts on this PR? Can we push it now? Do we need further updates somewhere?\r\n\r\nThanks,\r\nPeter","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696046219/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696155568","html_url":"https://github.com/apache/iceberg/pull/1417#issuecomment-696155568","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1417","id":696155568,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjE1NTU2OA==","user":{"login":"cmathiesen","id":32553472,"node_id":"MDQ6VXNlcjMyNTUzNDcy","avatar_url":"https://avatars.githubusercontent.com/u/32553472?v=4","gravatar_id":"","url":"https://api.github.com/users/cmathiesen","html_url":"https://github.com/cmathiesen","followers_url":"https://api.github.com/users/cmathiesen/followers","following_url":"https://api.github.com/users/cmathiesen/following{/other_user}","gists_url":"https://api.github.com/users/cmathiesen/gists{/gist_id}","starred_url":"https://api.github.com/users/cmathiesen/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/cmathiesen/subscriptions","organizations_url":"https://api.github.com/users/cmathiesen/orgs","repos_url":"https://api.github.com/users/cmathiesen/repos","events_url":"https://api.github.com/users/cmathiesen/events{/privacy}","received_events_url":"https://api.github.com/users/cmathiesen/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-21T14:33:07Z","updated_at":"2020-09-21T14:33:07Z","author_association":"CONTRIBUTOR","body":"Hey @guilload, have you had any luck with solutions for problems with the SerDe? Also pinging @pvary to ask if you have any suggestions for a fix for the problem raised [here](https://github.com/apache/iceberg/pull/1417#issuecomment-686830961)?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696155568/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696289950","html_url":"https://github.com/apache/iceberg/pull/1481#issuecomment-696289950","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1481","id":696289950,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjI4OTk1MA==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-21T18:27:16Z","updated_at":"2020-09-21T18:27:16Z","author_association":"CONTRIBUTOR","body":"CC: @rdblue, @massdosage - another piece of Hive integration","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696289950/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696382951","html_url":"https://github.com/apache/iceberg/pull/1417#issuecomment-696382951","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1417","id":696382951,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjM4Mjk1MQ==","user":{"login":"pvary","id":19705742,"node_id":"MDQ6VXNlcjE5NzA1NzQy","avatar_url":"https://avatars.githubusercontent.com/u/19705742?v=4","gravatar_id":"","url":"https://api.github.com/users/pvary","html_url":"https://github.com/pvary","followers_url":"https://api.github.com/users/pvary/followers","following_url":"https://api.github.com/users/pvary/following{/other_user}","gists_url":"https://api.github.com/users/pvary/gists{/gist_id}","starred_url":"https://api.github.com/users/pvary/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pvary/subscriptions","organizations_url":"https://api.github.com/users/pvary/orgs","repos_url":"https://api.github.com/users/pvary/repos","events_url":"https://api.github.com/users/pvary/events{/privacy}","received_events_url":"https://api.github.com/users/pvary/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-21T21:18:26Z","updated_at":"2020-09-21T21:18:26Z","author_association":"CONTRIBUTOR","body":"> Hey @guilload, have you had any luck with solutions for problems with the SerDe? Also pinging @pvary to ask if you have any suggestions for a fix for the problem raised [here](https://github.com/apache/iceberg/pull/1417#issuecomment-686830961)?\r\n\r\nI have checked the TestInputOutputFormat.testInOutFormat test case in Hive, where we test the ORC column projection:\r\nhttps://github.com/apache/hive/blob/f9d6e84143261c55442ef723197f5b55efb80633/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java#L1890\r\n\r\nBasically ORC returns the full record structure. The fields which are not requested are null:\r\n```\r\nOrctStruct {1, null}\r\n```\r\n\r\nBased on this I would assume that we should wrap the row returned by Iceberg to a Record which has the original full schema and returns the values for the columns we have value, and returns null for non-projected columns.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696382951/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696816484","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696816484","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696816484,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjgxNjQ4NA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T16:02:13Z","updated_at":"2020-09-22T16:02:13Z","author_association":"CONTRIBUTOR","body":"@rdblue @RussellSpitzer @HeartSaVioR what do you think guys?","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696816484/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696817291","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696817291","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696817291,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjgxNzI5MQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T16:03:20Z","updated_at":"2020-09-22T16:03:20Z","author_association":"CONTRIBUTOR","body":"Here is the code I used to refresh the cache in procedures:\r\n\r\n```\r\nprotected void refreshCache(Identifier ident, Table table) {\r\n  SparkSession spark = SparkSession.active();\r\n  CacheManager cacheManager = spark.sharedState().cacheManager();\r\n  DataSourceV2Relation relation = DataSourceV2Relation.create(table, Option.apply(catalog), Option.apply(ident));\r\n  cacheManager.recacheByPlan(spark, relation);\r\n}\r\n```","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696817291/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696837537","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696837537","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696837537,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjgzNzUzNw==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T16:34:56Z","updated_at":"2020-09-22T16:34:56Z","author_association":"MEMBER","body":"I actually would have assumed it was a bug if a \"Cache\" command was invalidated by another table operation, in my mind it should snapshot the table state at that moment. I know because the behavior is lazy in Spark your guarantees on \"when\" are a bit more iffy, but I think the Spark cache shouldn't be automatically invalidated.\r\n\r\n One of my main motivators here is that you could modify this table in a non spark framework and you wouldn't even know that happened inside Spark. For example say I have both Presto and Spark users, why should a Spark user's actions invalidate the cache when a Presto User's would not? Now I have a belief that my actions will always invalidate the cache, but there is a set of changes that would not. I would think it's better to assume \"Cache\" gives you a snapshot which will not change unless specifically asked for.\r\n\r\nThe second case seems more clear to me, we definitely should be refreshing there.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696837537/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696912501","html_url":"https://github.com/apache/iceberg/pull/1469#issuecomment-696912501","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1469","id":696912501,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjkxMjUwMQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T18:50:14Z","updated_at":"2020-09-22T18:50:14Z","author_association":"CONTRIBUTOR","body":"@jacques-n, you may be interested in this discussion.\r\n\r\nFor a DELETE using position delete files, I think that this isn't quite correct: \"data files referenced by new deletes must be still present\". The logic for \"no validation for delete files\" applies to this case: if a data file was deleted, then it's okay to delete the row twice. The validation should be \"data files referenced by new deletes must still be present or must be deleted; i.e., cannot be rewritten or overwritten.\"\r\n\r\nFor a DELETE using equality delete files, I'm not sure that snapshot isolation is distinct. If a data file is added concurrently that has a row that is now deleted, then either that commit is first and the row _is_ deleted or the commit is later and it is appended. Either way, the operations are independent. There is no need to validate \"no new potentially matching data files since we read\" because there is not necessarily a read, and the delete applies to the data automatically.\r\n\r\nUPDATE with position delete files looks correct to me.\r\n\r\nUPDATE with equality delete files also looks correct, but I think it helps to think of that as UPSERT and not as UPDATE. A row that is concurrently written will have values from the last UPSERT operation. This is almost certainly from an external data source because it makes little sense to read a row, update its values, and update it using an equality delete that will delete all copies, including those written since the row was read.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696912501/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696913004","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696913004","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696913004,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjkxMzAwNA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T18:51:11Z","updated_at":"2020-09-22T18:51:11Z","author_association":"CONTRIBUTOR","body":"> I actually would have assumed it was a bug if a \"Cache\" command was invalidated by another table operation, in my mind it should snapshot the table state at that moment. I know because the behavior is lazy in Spark your guarantees on \"when\" are a bit more iffy, but I think the Spark cache shouldn't be automatically invalidated.\r\n\r\nThat depends on how we interpret the cache in Spark. I don't see a description how it is supposed to work so I rely on the current behavior as the correct way of doing things. I interpret cache as something specific to Spark, even more, specific to a Spark application. Internally, `CacheManager` stores a full logical plan and if a table that is part of that plan is refreshed, the cache is invalidated (done by `InsertIntoDataSourceCommand`). I believe users are used to this behavior and would be really surprised if V2 tables behave differently. If we modify a table from the same driver and already know the cache entry is no longer valid, it seems reasonable to update it.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696913004/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696913019","html_url":"https://github.com/apache/iceberg/pull/1469#issuecomment-696913019","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1469","id":696913019,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjkxMzAxOQ==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T18:51:13Z","updated_at":"2020-09-22T18:51:13Z","author_association":"CONTRIBUTOR","body":"One more thing: I think we should expand your table with more operations and add it to the spec as a reference appendix. It is a very useful summary.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696913019/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696916207","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696916207","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696916207,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjkxNjIwNw==","user":{"login":"rdblue","id":87915,"node_id":"MDQ6VXNlcjg3OTE1","avatar_url":"https://avatars.githubusercontent.com/u/87915?v=4","gravatar_id":"","url":"https://api.github.com/users/rdblue","html_url":"https://github.com/rdblue","followers_url":"https://api.github.com/users/rdblue/followers","following_url":"https://api.github.com/users/rdblue/following{/other_user}","gists_url":"https://api.github.com/users/rdblue/gists{/gist_id}","starred_url":"https://api.github.com/users/rdblue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rdblue/subscriptions","organizations_url":"https://api.github.com/users/rdblue/orgs","repos_url":"https://api.github.com/users/rdblue/repos","events_url":"https://api.github.com/users/rdblue/events{/privacy}","received_events_url":"https://api.github.com/users/rdblue/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T18:57:21Z","updated_at":"2020-09-22T18:57:21Z","author_association":"CONTRIBUTOR","body":"@danielcweeks, it would be great to hear your thoughts on what Spark _should_ do in some of these cases.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696916207/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696922254","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696922254","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696922254,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjkyMjI1NA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T19:08:54Z","updated_at":"2020-09-22T19:08:54Z","author_association":"CONTRIBUTOR","body":"cc @holdenk as well","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696922254/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696923006","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696923006","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696923006,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjkyMzAwNg==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T19:10:22Z","updated_at":"2020-09-22T19:10:22Z","author_association":"CONTRIBUTOR","body":"Here is what I see in `CacheManager`:\r\n\r\n```\r\n  /**\r\n   * Tries to re-cache all the cache entries that contain `resourcePath` in one or more\r\n   * `HadoopFsRelation` node(s) as part of its logical plan.\r\n   */\r\n  def recacheByPath(spark: SparkSession, resourcePath: String): Unit = {\r\n    val (fs, qualifiedPath) = {\r\n      val path = new Path(resourcePath)\r\n      val fs = path.getFileSystem(spark.sessionState.newHadoopConf())\r\n      (fs, fs.makeQualified(path))\r\n    }\r\n\r\n    recacheByCondition(spark, _.plan.find(lookupAndRefresh(_, fs, qualifiedPath)).isDefined)\r\n  }\r\n\r\n  /**\r\n   * Traverses a given `plan` and searches for the occurrences of `qualifiedPath` in the\r\n   * [[org.apache.spark.sql.execution.datasources.FileIndex]] of any [[HadoopFsRelation]] nodes\r\n   * in the plan. If found, we refresh the metadata and return true. Otherwise, this method returns\r\n   * false.\r\n   */\r\n  private def lookupAndRefresh(plan: LogicalPlan, fs: FileSystem, qualifiedPath: Path): Boolean = {\r\n    plan match {\r\n      case lr: LogicalRelation => lr.relation match {\r\n        case hr: HadoopFsRelation =>\r\n          refreshFileIndexIfNecessary(hr.location, fs, qualifiedPath)\r\n        case _ => false\r\n      }\r\n\r\n      case DataSourceV2Relation(fileTable: FileTable, _, _, _, _) =>\r\n        refreshFileIndexIfNecessary(fileTable.fileIndex, fs, qualifiedPath)\r\n\r\n      case _ => false\r\n    }\r\n  }\r\n```\r\n\r\nHere, `DataSourceV2Relation` for `FileTable` was already added.  ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696923006/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696931697","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696931697","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696931697,"node_id":"MDEyOklzc3VlQ29tbWVudDY5NjkzMTY5Nw==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T19:27:26Z","updated_at":"2020-09-22T19:27:26Z","author_association":"CONTRIBUTOR","body":"I added one more case to consider.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696931697/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696941015","html_url":"https://github.com/apache/iceberg/pull/1484#issuecomment-696941015","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1484","id":696941015,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Njk0MTAxNQ==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T19:46:09Z","updated_at":"2020-09-22T19:46:09Z","author_association":"CONTRIBUTOR","body":"Thanks, @HeartSaVioR! ","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696941015/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696941278","html_url":"https://github.com/apache/iceberg/pull/1427#issuecomment-696941278","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1427","id":696941278,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Njk0MTI3OA==","user":{"login":"aokolnychyi","id":6235869,"node_id":"MDQ6VXNlcjYyMzU4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/6235869?v=4","gravatar_id":"","url":"https://api.github.com/users/aokolnychyi","html_url":"https://github.com/aokolnychyi","followers_url":"https://api.github.com/users/aokolnychyi/followers","following_url":"https://api.github.com/users/aokolnychyi/following{/other_user}","gists_url":"https://api.github.com/users/aokolnychyi/gists{/gist_id}","starred_url":"https://api.github.com/users/aokolnychyi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/aokolnychyi/subscriptions","organizations_url":"https://api.github.com/users/aokolnychyi/orgs","repos_url":"https://api.github.com/users/aokolnychyi/repos","events_url":"https://api.github.com/users/aokolnychyi/events{/privacy}","received_events_url":"https://api.github.com/users/aokolnychyi/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T19:46:42Z","updated_at":"2020-09-22T19:46:42Z","author_association":"CONTRIBUTOR","body":"Let me take a look at this too.","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696941278/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696958355","html_url":"https://github.com/apache/iceberg/issues/1485#issuecomment-696958355","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1485","id":696958355,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Njk1ODM1NQ==","user":{"login":"RussellSpitzer","id":413025,"node_id":"MDQ6VXNlcjQxMzAyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/413025?v=4","gravatar_id":"","url":"https://api.github.com/users/RussellSpitzer","html_url":"https://github.com/RussellSpitzer","followers_url":"https://api.github.com/users/RussellSpitzer/followers","following_url":"https://api.github.com/users/RussellSpitzer/following{/other_user}","gists_url":"https://api.github.com/users/RussellSpitzer/gists{/gist_id}","starred_url":"https://api.github.com/users/RussellSpitzer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/RussellSpitzer/subscriptions","organizations_url":"https://api.github.com/users/RussellSpitzer/orgs","repos_url":"https://api.github.com/users/RussellSpitzer/repos","events_url":"https://api.github.com/users/RussellSpitzer/events{/privacy}","received_events_url":"https://api.github.com/users/RussellSpitzer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T20:22:10Z","updated_at":"2020-09-22T20:22:10Z","author_association":"MEMBER","body":"I guess my thoughts were related to the programmatic api. I guess in the table/catalog world we can have many more interpretations on what \"cache\" should do\r\n","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696958355/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696973491","html_url":"https://github.com/apache/iceberg/pull/1484#issuecomment-696973491","issue_url":"https://api.github.com/repos/apache/iceberg/issues/1484","id":696973491,"node_id":"MDEyOklzc3VlQ29tbWVudDY5Njk3MzQ5MQ==","user":{"login":"HeartSaVioR","id":1317309,"node_id":"MDQ6VXNlcjEzMTczMDk=","avatar_url":"https://avatars.githubusercontent.com/u/1317309?v=4","gravatar_id":"","url":"https://api.github.com/users/HeartSaVioR","html_url":"https://github.com/HeartSaVioR","followers_url":"https://api.github.com/users/HeartSaVioR/followers","following_url":"https://api.github.com/users/HeartSaVioR/following{/other_user}","gists_url":"https://api.github.com/users/HeartSaVioR/gists{/gist_id}","starred_url":"https://api.github.com/users/HeartSaVioR/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/HeartSaVioR/subscriptions","organizations_url":"https://api.github.com/users/HeartSaVioR/orgs","repos_url":"https://api.github.com/users/HeartSaVioR/repos","events_url":"https://api.github.com/users/HeartSaVioR/events{/privacy}","received_events_url":"https://api.github.com/users/HeartSaVioR/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-09-22T20:51:53Z","updated_at":"2020-09-22T20:51:53Z","author_association":"CONTRIBUTOR","body":"Thanks all for reviewing and merging!","reactions":{"url":"https://api.github.com/repos/apache/iceberg/issues/comments/696973491/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]