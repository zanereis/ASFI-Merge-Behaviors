[{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662818399","html_url":"https://github.com/apache/hudi/issues/1864#issuecomment-662818399","issue_url":"https://api.github.com/repos/apache/hudi/issues/1864","id":662818399,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MjgxODM5OQ==","user":{"login":"garyli1019","id":23007841,"node_id":"MDQ6VXNlcjIzMDA3ODQx","avatar_url":"https://avatars.githubusercontent.com/u/23007841?v=4","gravatar_id":"","url":"https://api.github.com/users/garyli1019","html_url":"https://github.com/garyli1019","followers_url":"https://api.github.com/users/garyli1019/followers","following_url":"https://api.github.com/users/garyli1019/following{/other_user}","gists_url":"https://api.github.com/users/garyli1019/gists{/gist_id}","starred_url":"https://api.github.com/users/garyli1019/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/garyli1019/subscriptions","organizations_url":"https://api.github.com/users/garyli1019/orgs","repos_url":"https://api.github.com/users/garyli1019/repos","events_url":"https://api.github.com/users/garyli1019/events{/privacy}","received_events_url":"https://api.github.com/users/garyli1019/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T04:57:17Z","updated_at":"2020-07-23T04:57:17Z","author_association":"MEMBER","body":"Hi @hrmguilherme2 , there are some users are using Spark 2.2 with Hudi. Please feel free to give it a try :)","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662818399/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662826386","html_url":"https://github.com/apache/hudi/pull/1851#issuecomment-662826386","issue_url":"https://api.github.com/repos/apache/hudi/issues/1851","id":662826386,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MjgyNjM4Ng==","user":{"login":"zherenyu831","id":52404525,"node_id":"MDQ6VXNlcjUyNDA0NTI1","avatar_url":"https://avatars.githubusercontent.com/u/52404525?v=4","gravatar_id":"","url":"https://api.github.com/users/zherenyu831","html_url":"https://github.com/zherenyu831","followers_url":"https://api.github.com/users/zherenyu831/followers","following_url":"https://api.github.com/users/zherenyu831/following{/other_user}","gists_url":"https://api.github.com/users/zherenyu831/gists{/gist_id}","starred_url":"https://api.github.com/users/zherenyu831/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zherenyu831/subscriptions","organizations_url":"https://api.github.com/users/zherenyu831/orgs","repos_url":"https://api.github.com/users/zherenyu831/repos","events_url":"https://api.github.com/users/zherenyu831/events{/privacy}","received_events_url":"https://api.github.com/users/zherenyu831/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T05:33:42Z","updated_at":"2020-07-23T05:33:42Z","author_association":"CONTRIBUTOR","body":"@leesf \r\nsorry for letting you check some many times, I changed the test class name, please review it","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662826386/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662828528","html_url":"https://github.com/apache/hudi/pull/1851#issuecomment-662828528","issue_url":"https://api.github.com/repos/apache/hudi/issues/1851","id":662828528,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MjgyODUyOA==","user":{"login":"leesf","id":10128888,"node_id":"MDQ6VXNlcjEwMTI4ODg4","avatar_url":"https://avatars.githubusercontent.com/u/10128888?v=4","gravatar_id":"","url":"https://api.github.com/users/leesf","html_url":"https://github.com/leesf","followers_url":"https://api.github.com/users/leesf/followers","following_url":"https://api.github.com/users/leesf/following{/other_user}","gists_url":"https://api.github.com/users/leesf/gists{/gist_id}","starred_url":"https://api.github.com/users/leesf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leesf/subscriptions","organizations_url":"https://api.github.com/users/leesf/orgs","repos_url":"https://api.github.com/users/leesf/repos","events_url":"https://api.github.com/users/leesf/events{/privacy}","received_events_url":"https://api.github.com/users/leesf/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T05:43:02Z","updated_at":"2020-07-23T05:43:02Z","author_association":"CONTRIBUTOR","body":"> @leesf\r\n> sorry for letting you check some many times, I changed the test class name, please review it\r\n\r\nno worries..","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662828528/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662829522","html_url":"https://github.com/apache/hudi/pull/1838#issuecomment-662829522","issue_url":"https://api.github.com/repos/apache/hudi/issues/1838","id":662829522,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MjgyOTUyMg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T05:47:12Z","updated_at":"2020-07-23T05:47:12Z","author_association":"MEMBER","body":"@shenh062326 are you able to add this as a test case and update the PR? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662829522/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662830116","html_url":"https://github.com/apache/hudi/pull/1817#issuecomment-662830116","issue_url":"https://api.github.com/repos/apache/hudi/issues/1817","id":662830116,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MjgzMDExNg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T05:49:24Z","updated_at":"2020-07-23T05:49:24Z","author_association":"MEMBER","body":"@garyli1019 are you talking about corner cases not handled in this PR? can you review the PR once for intended functionality? I am trying to see if this can help MOR/Incremental query on spark SQL in some form. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662830116/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662830316","html_url":"https://github.com/apache/hudi/pull/1810#issuecomment-662830316","issue_url":"https://api.github.com/repos/apache/hudi/issues/1810","id":662830316,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MjgzMDMxNg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T05:50:14Z","updated_at":"2020-07-23T05:50:14Z","author_association":"MEMBER","body":"@lw309637554 can you please give me a couple days. I am trying to prioritize all the 0.6.0 blockers for now. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662830316/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662836052","html_url":"https://github.com/apache/hudi/pull/1817#issuecomment-662836052","issue_url":"https://api.github.com/repos/apache/hudi/issues/1817","id":662836052,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MjgzNjA1Mg==","user":{"login":"garyli1019","id":23007841,"node_id":"MDQ6VXNlcjIzMDA3ODQx","avatar_url":"https://avatars.githubusercontent.com/u/23007841?v=4","gravatar_id":"","url":"https://api.github.com/users/garyli1019","html_url":"https://github.com/garyli1019","followers_url":"https://api.github.com/users/garyli1019/followers","following_url":"https://api.github.com/users/garyli1019/following{/other_user}","gists_url":"https://api.github.com/users/garyli1019/gists{/gist_id}","starred_url":"https://api.github.com/users/garyli1019/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/garyli1019/subscriptions","organizations_url":"https://api.github.com/users/garyli1019/orgs","repos_url":"https://api.github.com/users/garyli1019/repos","events_url":"https://api.github.com/users/garyli1019/events{/privacy}","received_events_url":"https://api.github.com/users/garyli1019/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T06:12:36Z","updated_at":"2020-07-23T06:12:36Z","author_association":"MEMBER","body":"> @garyli1019 are you talking about corner cases not handled in this PR? can you review the PR once for intended functionality? I am trying to see if this can help MOR/Incremental query on spark SQL in some form.\r\n\r\n@vinothchandar I think the Spark Datasource will use a different approach. IIUC, this PR is trying to solve when the incremental query started from an uncompacted delta commit, which doesn't have a base file for some file groups and leads to missing the log records. For Spark Datasource, we can create a `HoodieFileSplit` without `baseFile` and read logs only. I am not sure if this could be done in the `HoodieRealtimeFileSplit` and without the extra handle for `HoodieMORIncrementalFileSplit`. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662836052/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662841424","html_url":"https://github.com/apache/hudi/pull/1827#issuecomment-662841424","issue_url":"https://api.github.com/repos/apache/hudi/issues/1827","id":662841424,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjg0MTQyNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T06:33:04Z","updated_at":"2020-07-23T06:33:04Z","author_association":"MEMBER","body":"@Mathieu1124 thanks for being awesome and understanding!. I will help out as much as I can as well. In fact, smoothly landing these large PRs is my sole focus now. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662841424/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662849527","html_url":"https://github.com/apache/hudi/pull/1838#issuecomment-662849527","issue_url":"https://api.github.com/repos/apache/hudi/issues/1838","id":662849527,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjg0OTUyNw==","user":{"login":"shenh062326","id":9527867,"node_id":"MDQ6VXNlcjk1Mjc4Njc=","avatar_url":"https://avatars.githubusercontent.com/u/9527867?v=4","gravatar_id":"","url":"https://api.github.com/users/shenh062326","html_url":"https://github.com/shenh062326","followers_url":"https://api.github.com/users/shenh062326/followers","following_url":"https://api.github.com/users/shenh062326/following{/other_user}","gists_url":"https://api.github.com/users/shenh062326/gists{/gist_id}","starred_url":"https://api.github.com/users/shenh062326/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shenh062326/subscriptions","organizations_url":"https://api.github.com/users/shenh062326/orgs","repos_url":"https://api.github.com/users/shenh062326/repos","events_url":"https://api.github.com/users/shenh062326/events{/privacy}","received_events_url":"https://api.github.com/users/shenh062326/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T06:59:48Z","updated_at":"2020-07-23T06:59:48Z","author_association":"CONTRIBUTOR","body":"> @shenh062326 are you able to add this as a test case and update the PR?\r\n\r\nSure, I will try to add a testcase.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662849527/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662882896","html_url":"https://github.com/apache/hudi/pull/1851#issuecomment-662882896","issue_url":"https://api.github.com/repos/apache/hudi/issues/1851","id":662882896,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjg4Mjg5Ng==","user":{"login":"zherenyu831","id":52404525,"node_id":"MDQ6VXNlcjUyNDA0NTI1","avatar_url":"https://avatars.githubusercontent.com/u/52404525?v=4","gravatar_id":"","url":"https://api.github.com/users/zherenyu831","html_url":"https://github.com/zherenyu831","followers_url":"https://api.github.com/users/zherenyu831/followers","following_url":"https://api.github.com/users/zherenyu831/following{/other_user}","gists_url":"https://api.github.com/users/zherenyu831/gists{/gist_id}","starred_url":"https://api.github.com/users/zherenyu831/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zherenyu831/subscriptions","organizations_url":"https://api.github.com/users/zherenyu831/orgs","repos_url":"https://api.github.com/users/zherenyu831/repos","events_url":"https://api.github.com/users/zherenyu831/events{/privacy}","received_events_url":"https://api.github.com/users/zherenyu831/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T08:27:57Z","updated_at":"2020-07-23T08:27:57Z","author_association":"CONTRIBUTOR","body":"@leesf \r\nGot it, will do thank you for merging","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662882896/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662921730","html_url":"https://github.com/apache/hudi/issues/1845#issuecomment-662921730","issue_url":"https://api.github.com/repos/apache/hudi/issues/1845","id":662921730,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MjkyMTczMA==","user":{"login":"sbernauer","id":29303194,"node_id":"MDQ6VXNlcjI5MzAzMTk0","avatar_url":"https://avatars.githubusercontent.com/u/29303194?v=4","gravatar_id":"","url":"https://api.github.com/users/sbernauer","html_url":"https://github.com/sbernauer","followers_url":"https://api.github.com/users/sbernauer/followers","following_url":"https://api.github.com/users/sbernauer/following{/other_user}","gists_url":"https://api.github.com/users/sbernauer/gists{/gist_id}","starred_url":"https://api.github.com/users/sbernauer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sbernauer/subscriptions","organizations_url":"https://api.github.com/users/sbernauer/orgs","repos_url":"https://api.github.com/users/sbernauer/repos","events_url":"https://api.github.com/users/sbernauer/events{/privacy}","received_events_url":"https://api.github.com/users/sbernauer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T10:01:31Z","updated_at":"2020-07-23T11:22:06Z","author_association":"CONTRIBUTOR","body":"Hi @bvaradar,\r\n\r\nI've created a test using pure avro DataFileWriter to reproduce my DeltaStreamer-test and it kind of works.\r\nYou can find it here https://github.com/sbernauer/avro-schema-evolution-test/blob/master/src/test/java/main/AvroSchemaEvolutionTest.java\r\nThere is no exception, but the evoluted optional field is missing. We can also check the generated avro file.\r\n```\r\n java -jar avro-tools-1.8.2.jar tojson output.avro\r\n{\"timestamp\":0.0,\"rider\":\"myRider0\",\"driver\":\"myDriver0\"}\r\n{\"timestamp\":1.0,\"rider\":\"myRider1\",\"driver\":\"myDriver1\"}\r\n{\"timestamp\":2.0,\"rider\":\"myRider2\",\"driver\":\"myDriver2\"}\r\n```\r\nThe second test\r\n\r\n1.) Write record of B using schema B\r\n2.) Write record of A using schema B\r\nIt fails with ArrayIndexOutOfBoundsException.\r\n\r\nI found another post https://stackoverflow.com/questions/34733604/avro-schema-doesnt-honor-backward-compatibilty, it provides a testcase for schema evolution using a BinaryDecoder and reproduces the EOF exception.\r\nThe mailing list entry http://apache-avro.679487.n3.nabble.com/AVRO-schema-evolution-adding-optional-column-with-default-fails-deserialization-td4043025.html describes the same problem but causes the ArrayIndexOutOfBoundsException.\r\n\r\nMy setup is using avro 1.8.2.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662921730/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662959209","html_url":"https://github.com/apache/hudi/pull/1822#issuecomment-662959209","issue_url":"https://api.github.com/repos/apache/hudi/issues/1822","id":662959209,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjk1OTIwOQ==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T11:41:32Z","updated_at":"2020-07-23T11:41:32Z","author_association":"CONTRIBUTOR","body":"@vinothchandar please take a pass. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662959209/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662962138","html_url":"https://github.com/apache/hudi/pull/1816#issuecomment-662962138","issue_url":"https://api.github.com/repos/apache/hudi/issues/1816","id":662962138,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjk2MjEzOA==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T11:48:59Z","updated_at":"2020-07-23T11:51:25Z","author_association":"CONTRIBUTOR","body":"> @pratyakshsharma Thanks for the updates and sorry for late response. For users not using latest master, they still need use NonpartitionedKeyGenerator, so I think it is valuable to mention it.\r\n\r\nIn that case, we should mention about existing solutions for other key generation cases as well like SimpleKeyGenerator, ComplexKeyGenerator etc. Let me make the changes and update this PR then. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662962138/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662977488","html_url":"https://github.com/apache/hudi/pull/1816#issuecomment-662977488","issue_url":"https://api.github.com/repos/apache/hudi/issues/1816","id":662977488,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mjk3NzQ4OA==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T12:26:26Z","updated_at":"2020-07-23T12:26:26Z","author_association":"CONTRIBUTOR","body":"@leesf  please take a pass. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/662977488/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663000848","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663000848","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663000848,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzAwMDg0OA==","user":{"login":"ssomuah","id":2061955,"node_id":"MDQ6VXNlcjIwNjE5NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2061955?v=4","gravatar_id":"","url":"https://api.github.com/users/ssomuah","html_url":"https://github.com/ssomuah","followers_url":"https://api.github.com/users/ssomuah/followers","following_url":"https://api.github.com/users/ssomuah/following{/other_user}","gists_url":"https://api.github.com/users/ssomuah/gists{/gist_id}","starred_url":"https://api.github.com/users/ssomuah/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ssomuah/subscriptions","organizations_url":"https://api.github.com/users/ssomuah/orgs","repos_url":"https://api.github.com/users/ssomuah/repos","events_url":"https://api.github.com/users/ssomuah/events{/privacy}","received_events_url":"https://api.github.com/users/ssomuah/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T13:16:40Z","updated_at":"2020-07-23T13:16:40Z","author_association":"NONE","body":"I updated to master @ 743ef322b88d90be9775b889f6381925cdda5f35 and then applied the patch you linked above. \r\n\r\nThe first batch that ran had several \"RunCompactionActionExecutor\" \r\n\r\n I'm still consistently seeing long batches \r\n\r\n<img width=\"1592\" alt=\"Screen Shot 2020-07-23 at 8 45 49 AM\" src=\"https://user-images.githubusercontent.com/2061955/88288372-b07da480-ccc1-11ea-9dc6-4e0a0675244a.png\">\r\n\r\n\r\n<img width=\"1151\" alt=\"Screen Shot 2020-07-23 at 8 46 07 AM\" src=\"https://user-images.githubusercontent.com/2061955/88288414-b96e7600-ccc1-11ea-92f8-ba1671466164.png\">\r\n\r\n\r\n<img width=\"1581\" alt=\"Screen Shot 2020-07-23 at 8 46 17 AM\" src=\"https://user-images.githubusercontent.com/2061955/88288429-c0958400-ccc1-11ea-9617-81bffd01a8af.png\">\r\n\r\n\r\n<img width=\"1583\" alt=\"Screen Shot 2020-07-23 at 8 46 33 AM\" src=\"https://user-images.githubusercontent.com/2061955/88288446-c5f2ce80-ccc1-11ea-9032-0c251d2f94d9.png\">\r\n\r\n\r\nThe contents of the  timeline folder are now. \r\n\r\n[dot_hoodie_folder_v2.txt](https://github.com/apache/hudi/files/4966070/dot_hoodie_folder_v2.txt)\r\n\r\n\r\nI think the root of my issue is that I have tons of log files which don't seem to get compacted. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663000848/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663002064","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663002064","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663002064,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzAwMjA2NA==","user":{"login":"ssomuah","id":2061955,"node_id":"MDQ6VXNlcjIwNjE5NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2061955?v=4","gravatar_id":"","url":"https://api.github.com/users/ssomuah","html_url":"https://github.com/ssomuah","followers_url":"https://api.github.com/users/ssomuah/followers","following_url":"https://api.github.com/users/ssomuah/following{/other_user}","gists_url":"https://api.github.com/users/ssomuah/gists{/gist_id}","starred_url":"https://api.github.com/users/ssomuah/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ssomuah/subscriptions","organizations_url":"https://api.github.com/users/ssomuah/orgs","repos_url":"https://api.github.com/users/ssomuah/repos","events_url":"https://api.github.com/users/ssomuah/events{/privacy}","received_events_url":"https://api.github.com/users/ssomuah/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T13:18:54Z","updated_at":"2020-07-23T13:18:54Z","author_association":"NONE","body":"Only 1 compaction.inflight now\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663002064/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663035816","html_url":"https://github.com/apache/hudi/issues/1845#issuecomment-663035816","issue_url":"https://api.github.com/repos/apache/hudi/issues/1845","id":663035816,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzAzNTgxNg==","user":{"login":"prashantwason","id":58448203,"node_id":"MDQ6VXNlcjU4NDQ4MjAz","avatar_url":"https://avatars.githubusercontent.com/u/58448203?v=4","gravatar_id":"","url":"https://api.github.com/users/prashantwason","html_url":"https://github.com/prashantwason","followers_url":"https://api.github.com/users/prashantwason/followers","following_url":"https://api.github.com/users/prashantwason/following{/other_user}","gists_url":"https://api.github.com/users/prashantwason/gists{/gist_id}","starred_url":"https://api.github.com/users/prashantwason/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/prashantwason/subscriptions","organizations_url":"https://api.github.com/users/prashantwason/orgs","repos_url":"https://api.github.com/users/prashantwason/repos","events_url":"https://api.github.com/users/prashantwason/events{/privacy}","received_events_url":"https://api.github.com/users/prashantwason/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T14:21:46Z","updated_at":"2020-07-23T14:21:46Z","author_association":"MEMBER","body":"The  [javadoc for DatumWriter](https://avro.apache.org/docs/1.7.6/api/java/org/apache/avro/file/DataFileWriter.html#append(D)\r\n) says: \r\n\r\n\"Stores in a file a sequence of data conforming to a schema. The schema is stored in the file with the data. Each datum in a file is of the same schema.\"\r\n\r\nYour test case has records with different schema being written to the same file. \r\n\r\nPlease try using the [GenericDatumReader construtor](https://avro.apache.org/docs/1.8.2/api/java/org/apache/avro/generic/GenericDatumReader.html#GenericDatumReader(org.apache.avro.Schema,%20org.apache.avro.Schema)) which specifies both the reader/writer schema. \r\nGenericDatumReader(Schema writer, Schema reader)\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663035816/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663036038","html_url":"https://github.com/apache/hudi/issues/1860#issuecomment-663036038","issue_url":"https://api.github.com/repos/apache/hudi/issues/1860","id":663036038,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzAzNjAzOA==","user":{"login":"bhasudha","id":2179254,"node_id":"MDQ6VXNlcjIxNzkyNTQ=","avatar_url":"https://avatars.githubusercontent.com/u/2179254?v=4","gravatar_id":"","url":"https://api.github.com/users/bhasudha","html_url":"https://github.com/bhasudha","followers_url":"https://api.github.com/users/bhasudha/followers","following_url":"https://api.github.com/users/bhasudha/following{/other_user}","gists_url":"https://api.github.com/users/bhasudha/gists{/gist_id}","starred_url":"https://api.github.com/users/bhasudha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bhasudha/subscriptions","organizations_url":"https://api.github.com/users/bhasudha/orgs","repos_url":"https://api.github.com/users/bhasudha/repos","events_url":"https://api.github.com/users/bhasudha/events{/privacy}","received_events_url":"https://api.github.com/users/bhasudha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T14:22:14Z","updated_at":"2020-07-23T14:22:14Z","author_association":"CONTRIBUTOR","body":"I am not sure if this has to do with Spark caching the table metadata. In any case, could you try adding the  conf `spark.sql.hive.convertMetastoreParquet` to false like here - https://hudi.apache.org/docs/docker_demo.html#step-4-b-run-spark-sql-queries and try again ? \r\n\r\nWere you specifically testing the Spark Datasource API ? Since your table is already registered to Hive, directly querying the hive table table using Spark SQL would also work. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663036038/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663065111","html_url":"https://github.com/apache/hudi/issues/1845#issuecomment-663065111","issue_url":"https://api.github.com/repos/apache/hudi/issues/1845","id":663065111,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzA2NTExMQ==","user":{"login":"sbernauer","id":29303194,"node_id":"MDQ6VXNlcjI5MzAzMTk0","avatar_url":"https://avatars.githubusercontent.com/u/29303194?v=4","gravatar_id":"","url":"https://api.github.com/users/sbernauer","html_url":"https://github.com/sbernauer","followers_url":"https://api.github.com/users/sbernauer/followers","following_url":"https://api.github.com/users/sbernauer/following{/other_user}","gists_url":"https://api.github.com/users/sbernauer/gists{/gist_id}","starred_url":"https://api.github.com/users/sbernauer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sbernauer/subscriptions","organizations_url":"https://api.github.com/users/sbernauer/orgs","repos_url":"https://api.github.com/users/sbernauer/repos","events_url":"https://api.github.com/users/sbernauer/events{/privacy}","received_events_url":"https://api.github.com/users/sbernauer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T15:14:22Z","updated_at":"2020-07-23T15:15:24Z","author_association":"CONTRIBUTOR","body":"Yes, to be correct you have to specify both reader and writer schema to GenericDatumReader. Hudi currenty only passes one schema here https://github.com/apache/hudi/blob/c3279cd5989805946267b046007ea23ba4b615c2/hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java#L107 and here https://github.com/apache/hudi/blob/c3279cd5989805946267b046007ea23ba4b615c2/hudi-common/src/main/java/org/apache/hudi/avro/HoodieAvroUtils.java#L117\r\nI think that's a problem\r\n\r\nI would have used GenericDatumReader with reader and writer schema, but first I wanted to append new evoluted events to an existing file. Looking at \"Each datum in a file is of the same schema\"  this seems expected behavior to be impossible.\r\n\r\n\"Each datum in a file is of the same schema\" - How is the strategy of Hudi when there are Upserts for an evoluted event? The new events can't simply be written to an existing file.\r\nEvery time a file must be updated, read it correctly with the **reader and writer** schema and than create a new file with the new schema?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663065111/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663083174","html_url":"https://github.com/apache/hudi/issues/1854#issuecomment-663083174","issue_url":"https://api.github.com/repos/apache/hudi/issues/1854","id":663083174,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzA4MzE3NA==","user":{"login":"bhasudha","id":2179254,"node_id":"MDQ6VXNlcjIxNzkyNTQ=","avatar_url":"https://avatars.githubusercontent.com/u/2179254?v=4","gravatar_id":"","url":"https://api.github.com/users/bhasudha","html_url":"https://github.com/bhasudha","followers_url":"https://api.github.com/users/bhasudha/followers","following_url":"https://api.github.com/users/bhasudha/following{/other_user}","gists_url":"https://api.github.com/users/bhasudha/gists{/gist_id}","starred_url":"https://api.github.com/users/bhasudha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bhasudha/subscriptions","organizations_url":"https://api.github.com/users/bhasudha/orgs","repos_url":"https://api.github.com/users/bhasudha/repos","events_url":"https://api.github.com/users/bhasudha/events{/privacy}","received_events_url":"https://api.github.com/users/bhasudha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T15:47:45Z","updated_at":"2020-07-23T15:47:45Z","author_association":"CONTRIBUTOR","body":"@qingyuan18  InputFormat and outputFormat for this table `xxxx.xxxx_acidtest2` does not seem to be Hudi related.  The SerDe formats for MOR tables look like :\r\n\r\nfor read optimized MOR table\r\n| # Storage Information         | NULL                                                            | NULL                  |\r\n| SerDe Library:                | org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe     | NULL                  |\r\n| InputFormat:                  | org.apache.hudi.hadoop.HoodieParquetInputFormat                 | NULL                  |\r\n| OutputFormat:                 | org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat  | NULL                  |\r\n\r\nand for _rt suffixed MOR table\r\n\r\n| # Storage Information         | NULL                                                              | NULL                  |\r\n| SerDe Library:                | org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe       | NULL                  |\r\n| InputFormat:                  | org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat  | NULL                  |\r\n| OutputFormat:                 | org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat    | NULL                  |\r\n\r\n\r\nCan you share more context on how this table was loaded initially? Also add your write configs.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663083174/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663112473","html_url":"https://github.com/apache/hudi/issues/1825#issuecomment-663112473","issue_url":"https://api.github.com/repos/apache/hudi/issues/1825","id":663112473,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzExMjQ3Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T16:44:14Z","updated_at":"2020-07-23T16:44:14Z","author_association":"CONTRIBUTOR","body":"@asheeshgarg : Yes, Hudi only supports single writer.This means you need to be running only one ingestion job at a time. Hudi takes care of running asynchronous background jobs like cleaner, archiving and compaction. Note that Hudi currently mandates single writer in-order to provide row level incremental changelogs. \r\nWith the next release - 0.6.0, Hudi will allow concurrent ingestion safely as long as users can guarantee that each concurrent ingestion jobs is writing to different physical partitions of the dataset.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663112473/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663121167","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663121167","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663121167,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzEyMTE2Nw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T17:01:42Z","updated_at":"2020-07-23T17:01:42Z","author_association":"CONTRIBUTOR","body":"@ssomuah : Regarding the patch, it is meant to ensure all pending compactions are completed.  Regarding the slowness, we are working on general and S3 specific performance improvements on the write side  which should be part of next release : 0.6.0","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663121167/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663147934","html_url":"https://github.com/apache/hudi/issues/1782#issuecomment-663147934","issue_url":"https://api.github.com/repos/apache/hudi/issues/1782","id":663147934,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzE0NzkzNA==","user":{"login":"sam-wmt","id":67726885,"node_id":"MDQ6VXNlcjY3NzI2ODg1","avatar_url":"https://avatars.githubusercontent.com/u/67726885?v=4","gravatar_id":"","url":"https://api.github.com/users/sam-wmt","html_url":"https://github.com/sam-wmt","followers_url":"https://api.github.com/users/sam-wmt/followers","following_url":"https://api.github.com/users/sam-wmt/following{/other_user}","gists_url":"https://api.github.com/users/sam-wmt/gists{/gist_id}","starred_url":"https://api.github.com/users/sam-wmt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sam-wmt/subscriptions","organizations_url":"https://api.github.com/users/sam-wmt/orgs","repos_url":"https://api.github.com/users/sam-wmt/repos","events_url":"https://api.github.com/users/sam-wmt/events{/privacy}","received_events_url":"https://api.github.com/users/sam-wmt/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T17:56:09Z","updated_at":"2020-07-23T17:56:09Z","author_association":"NONE","body":"Apologies for the slow response the initial issues appeared to be due to a hanging compaction which was addressed in a newer version which attempts a retry.  We'd done so further work and another devloper has further issues due to a lot of small files which can be seen here.  This Support request can be closed in favor of his: https://github.com/apache/hudi/issues/1852\r\n\r\nKindest Regards,\r\n\r\nSam","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663147934/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663153261","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663153261","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663153261,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzE1MzI2MQ==","user":{"login":"ssomuah","id":2061955,"node_id":"MDQ6VXNlcjIwNjE5NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2061955?v=4","gravatar_id":"","url":"https://api.github.com/users/ssomuah","html_url":"https://github.com/ssomuah","followers_url":"https://api.github.com/users/ssomuah/followers","following_url":"https://api.github.com/users/ssomuah/following{/other_user}","gists_url":"https://api.github.com/users/ssomuah/gists{/gist_id}","starred_url":"https://api.github.com/users/ssomuah/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ssomuah/subscriptions","organizations_url":"https://api.github.com/users/ssomuah/orgs","repos_url":"https://api.github.com/users/ssomuah/repos","events_url":"https://api.github.com/users/ssomuah/events{/privacy}","received_events_url":"https://api.github.com/users/ssomuah/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T18:07:03Z","updated_at":"2020-07-23T18:09:31Z","author_association":"NONE","body":"@bvaradar I think the issue I'm facing is due to configuration, but I can't pinpoint what it is. \r\n\r\nI'm ending up with an extremely large number of files fo a single partition merge on read table. \r\n\r\nI have tens of thousands of log files which I would have thought would get compacted into parquet at some point. \r\n\r\nwhat volume of updates is working well for merge on read tables today?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663153261/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663168345","html_url":"https://github.com/apache/hudi/issues/1845#issuecomment-663168345","issue_url":"https://api.github.com/repos/apache/hudi/issues/1845","id":663168345,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzE2ODM0NQ==","user":{"login":"prashantwason","id":58448203,"node_id":"MDQ6VXNlcjU4NDQ4MjAz","avatar_url":"https://avatars.githubusercontent.com/u/58448203?v=4","gravatar_id":"","url":"https://api.github.com/users/prashantwason","html_url":"https://github.com/prashantwason","followers_url":"https://api.github.com/users/prashantwason/followers","following_url":"https://api.github.com/users/prashantwason/following{/other_user}","gists_url":"https://api.github.com/users/prashantwason/gists{/gist_id}","starred_url":"https://api.github.com/users/prashantwason/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/prashantwason/subscriptions","organizations_url":"https://api.github.com/users/prashantwason/orgs","repos_url":"https://api.github.com/users/prashantwason/repos","events_url":"https://api.github.com/users/prashantwason/events{/privacy}","received_events_url":"https://api.github.com/users/prashantwason/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T18:39:45Z","updated_at":"2020-07-23T18:39:45Z","author_association":"MEMBER","body":"> I would have used GenericDatumReader with reader and writer schema, but first I wanted to append new evoluted events to an existing file\r\nIn HUDI log files, records are added to AvroDataBlock which have a header containing the schema for the writer. Different AvroDataBlock could have different schemas but all records within the same block should have the same schema.\r\n\r\nLog blocks are read using both reader/write schema.\r\n\r\nhttps://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieAvroDataBlock.java#L143\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663168345/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663169546","html_url":"https://github.com/apache/hudi/issues/1845#issuecomment-663169546","issue_url":"https://api.github.com/repos/apache/hudi/issues/1845","id":663169546,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzE2OTU0Ng==","user":{"login":"prashantwason","id":58448203,"node_id":"MDQ6VXNlcjU4NDQ4MjAz","avatar_url":"https://avatars.githubusercontent.com/u/58448203?v=4","gravatar_id":"","url":"https://api.github.com/users/prashantwason","html_url":"https://github.com/prashantwason","followers_url":"https://api.github.com/users/prashantwason/followers","following_url":"https://api.github.com/users/prashantwason/following{/other_user}","gists_url":"https://api.github.com/users/prashantwason/gists{/gist_id}","starred_url":"https://api.github.com/users/prashantwason/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/prashantwason/subscriptions","organizations_url":"https://api.github.com/users/prashantwason/orgs","repos_url":"https://api.github.com/users/prashantwason/repos","events_url":"https://api.github.com/users/prashantwason/events{/privacy}","received_events_url":"https://api.github.com/users/prashantwason/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T18:42:17Z","updated_at":"2020-07-23T18:42:17Z","author_association":"MEMBER","body":"The issue is probably related to using the wrong schema for Upsert. Let me expand your original steps to reproduce and you can correct me:\r\n\r\nSteps to reproduce:\r\n\r\n1. We have some old schema (SCHEMA_V1) and events.\r\n2. We update the schema with a new (SCHEMA_V2), optional union field and restart the DeltaStreamer\r\n3. We ingest new events  (Events with SCHEMA_V2)\r\n4. We ingest old events again (there are some upserts). ?????? **What schema is being used here?**\r\n\r\nIf in step 4 you are using SCHEMA_V1 then it wont be able to read in records ingested at step 3 as that is a newer schema. \r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663169546/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663176816","html_url":"https://github.com/apache/hudi/issues/1860#issuecomment-663176816","issue_url":"https://api.github.com/repos/apache/hudi/issues/1860","id":663176816,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzE3NjgxNg==","user":{"login":"stackfun","id":68627128,"node_id":"MDQ6VXNlcjY4NjI3MTI4","avatar_url":"https://avatars.githubusercontent.com/u/68627128?v=4","gravatar_id":"","url":"https://api.github.com/users/stackfun","html_url":"https://github.com/stackfun","followers_url":"https://api.github.com/users/stackfun/followers","following_url":"https://api.github.com/users/stackfun/following{/other_user}","gists_url":"https://api.github.com/users/stackfun/gists{/gist_id}","starred_url":"https://api.github.com/users/stackfun/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/stackfun/subscriptions","organizations_url":"https://api.github.com/users/stackfun/orgs","repos_url":"https://api.github.com/users/stackfun/repos","events_url":"https://api.github.com/users/stackfun/events{/privacy}","received_events_url":"https://api.github.com/users/stackfun/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T18:57:50Z","updated_at":"2020-07-23T18:57:50Z","author_association":"NONE","body":"I used the setting you recommended, and still get similar results. In this run, I was inserting 200 records in the writer job. \r\n```\r\nHive Query: 600\r\nSpark Query: 777\r\nHive Query: 800\r\nSpark Query: 800\r\nHive Query: 800\r\nSpark Query: 800\r\nHive Query: 800\r\nSpark Query: 800\r\nHive Query: 800\r\nSpark Query: 851\r\nHive Query: 1000\r\nSpark Query: 1000\r\n```\r\n\r\nI'm refreshing the table before each query, so the table metadata in Spark should be cleared. Does this seem like a bug to you, or is there some other setting that I should try?\r\n\r\nI was stress testing Hudi's atomic write feature as our team is determining whether we can use Hudi for an efficient data lake. Directly querying the hive table using Spark SQL seems to work flawlessly, so we're not blocked. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663176816/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663178646","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663178646","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663178646,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzE3ODY0Ng==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T19:01:35Z","updated_at":"2020-07-23T19:01:35Z","author_association":"CONTRIBUTOR","body":"@ssomuah : \r\nSuch a large number of log files indicates your compaction frequency (INLINE_COMPACT_NUM_DELTA_COMMITS_PROP)  is conservative. Many of these log files could also be belonging to older file versions which will be cleaned by Cleaner ( https://cwiki.apache.org/confluence/display/HUDI/FAQ#FAQ-WhatdoestheHudicleanerdo). \r\n\r\nIn addition, note that inline compaction which runs serially with ingestion. We have a working PR which lets compaction run concurrently with ingestion : https://github.com/apache/hudi/pull/1752\r\nNow that pending compactions have finished, you can setup concurrent compaction with the above PR ( config : hoodie.datasource.compaction.async.enable=true). \r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663178646/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663189699","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663189699","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663189699,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzE4OTY5OQ==","user":{"login":"ssomuah","id":2061955,"node_id":"MDQ6VXNlcjIwNjE5NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2061955?v=4","gravatar_id":"","url":"https://api.github.com/users/ssomuah","html_url":"https://github.com/ssomuah","followers_url":"https://api.github.com/users/ssomuah/followers","following_url":"https://api.github.com/users/ssomuah/following{/other_user}","gists_url":"https://api.github.com/users/ssomuah/gists{/gist_id}","starred_url":"https://api.github.com/users/ssomuah/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ssomuah/subscriptions","organizations_url":"https://api.github.com/users/ssomuah/orgs","repos_url":"https://api.github.com/users/ssomuah/repos","events_url":"https://api.github.com/users/ssomuah/events{/privacy}","received_events_url":"https://api.github.com/users/ssomuah/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T19:26:08Z","updated_at":"2020-07-23T19:29:48Z","author_association":"NONE","body":"What do you mean by \"runs serially with ingestion\"? My understanding was that inline compaction happened in the same flow as writing so an inline compaction would simply slow down ingestion. \r\n\r\nDoes INLINE_COMPACT_NUM_DELTA_COMMITS_PROP refer to the number of commits retained in general, or the number of commits for a record? \r\n\r\nI see in the timeline I have several clean.requested and clean.inflight, how can I get these to actually complete?\r\n\r\nWhat determines how many log files are created in each batch for a MOR table?\r\n\r\nEDIT:\r\nIs it possible to force a compaction of the existing log files.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663189699/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663254464","html_url":"https://github.com/apache/hudi/issues/1847#issuecomment-663254464","issue_url":"https://api.github.com/repos/apache/hudi/issues/1847","id":663254464,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI1NDQ2NA==","user":{"login":"bschell","id":8600774,"node_id":"MDQ6VXNlcjg2MDA3NzQ=","avatar_url":"https://avatars.githubusercontent.com/u/8600774?v=4","gravatar_id":"","url":"https://api.github.com/users/bschell","html_url":"https://github.com/bschell","followers_url":"https://api.github.com/users/bschell/followers","following_url":"https://api.github.com/users/bschell/following{/other_user}","gists_url":"https://api.github.com/users/bschell/gists{/gist_id}","starred_url":"https://api.github.com/users/bschell/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bschell/subscriptions","organizations_url":"https://api.github.com/users/bschell/orgs","repos_url":"https://api.github.com/users/bschell/repos","events_url":"https://api.github.com/users/bschell/events{/privacy}","received_events_url":"https://api.github.com/users/bschell/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T22:03:00Z","updated_at":"2020-07-23T22:03:00Z","author_association":"CONTRIBUTOR","body":"@zuyanton S3NativeFileSystem is part of EMRFS from EMR. EMRFS overrides getLen in certain scenarios. Do you happen to have Client Side Encryption (CSE) enabled?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663254464/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663276947","html_url":"https://github.com/apache/hudi/pull/1149#issuecomment-663276947","issue_url":"https://api.github.com/repos/apache/hudi/issues/1149","id":663276947,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI3Njk0Nw==","user":{"login":"yihua","id":2497195,"node_id":"MDQ6VXNlcjI0OTcxOTU=","avatar_url":"https://avatars.githubusercontent.com/u/2497195?v=4","gravatar_id":"","url":"https://api.github.com/users/yihua","html_url":"https://github.com/yihua","followers_url":"https://api.github.com/users/yihua/followers","following_url":"https://api.github.com/users/yihua/following{/other_user}","gists_url":"https://api.github.com/users/yihua/gists{/gist_id}","starred_url":"https://api.github.com/users/yihua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yihua/subscriptions","organizations_url":"https://api.github.com/users/yihua/orgs","repos_url":"https://api.github.com/users/yihua/repos","events_url":"https://api.github.com/users/yihua/events{/privacy}","received_events_url":"https://api.github.com/users/yihua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-23T23:15:48Z","updated_at":"2020-07-23T23:15:48Z","author_association":"CONTRIBUTOR","body":"I also simplified the naming of the sorting modes to: `NONE`, `GLOBAL_SORT`, and `PARTITION_SORT`.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663276947/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663298411","html_url":"https://github.com/apache/hudi/issues/1866#issuecomment-663298411","issue_url":"https://api.github.com/repos/apache/hudi/issues/1866","id":663298411,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzI5ODQxMQ==","user":{"login":"satishkotha","id":2992755,"node_id":"MDQ6VXNlcjI5OTI3NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2992755?v=4","gravatar_id":"","url":"https://api.github.com/users/satishkotha","html_url":"https://github.com/satishkotha","followers_url":"https://api.github.com/users/satishkotha/followers","following_url":"https://api.github.com/users/satishkotha/following{/other_user}","gists_url":"https://api.github.com/users/satishkotha/gists{/gist_id}","starred_url":"https://api.github.com/users/satishkotha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishkotha/subscriptions","organizations_url":"https://api.github.com/users/satishkotha/orgs","repos_url":"https://api.github.com/users/satishkotha/repos","events_url":"https://api.github.com/users/satishkotha/events{/privacy}","received_events_url":"https://api.github.com/users/satishkotha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T00:36:35Z","updated_at":"2020-07-24T00:56:38Z","author_association":"MEMBER","body":"Hi @luffyd  \r\n\r\nBy default, upsert on MOR tables creates 'deltacommits'.  [Compaction](https://cwiki.apache.org/confluence/display/HUDI/Design+And+Architecture#DesignAndArchitecture-Compaction) needs to run to convert deltacommits into commits. Clean works only after compaction runs and commits are created. Clean also does not remove file groups that have pending compaction.  Can you setup inline compaction [using instructions here](https://cwiki.apache.org/confluence/display/HUDI/FAQ#FAQ-HowdoIruncompactionforaMORdataset) for testing and see if that helps?\r\n\r\nIf that doesn't work, can you share screenshot of files in .hoodie folder in 'getHudiPath'","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663298411/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663373241","html_url":"https://github.com/apache/hudi/issues/1845#issuecomment-663373241","issue_url":"https://api.github.com/repos/apache/hudi/issues/1845","id":663373241,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzM3MzI0MQ==","user":{"login":"sbernauer","id":29303194,"node_id":"MDQ6VXNlcjI5MzAzMTk0","avatar_url":"https://avatars.githubusercontent.com/u/29303194?v=4","gravatar_id":"","url":"https://api.github.com/users/sbernauer","html_url":"https://github.com/sbernauer","followers_url":"https://api.github.com/users/sbernauer/followers","following_url":"https://api.github.com/users/sbernauer/following{/other_user}","gists_url":"https://api.github.com/users/sbernauer/gists{/gist_id}","starred_url":"https://api.github.com/users/sbernauer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sbernauer/subscriptions","organizations_url":"https://api.github.com/users/sbernauer/orgs","repos_url":"https://api.github.com/users/sbernauer/repos","events_url":"https://api.github.com/users/sbernauer/events{/privacy}","received_events_url":"https://api.github.com/users/sbernauer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T06:49:32Z","updated_at":"2020-07-24T06:49:32Z","author_association":"CONTRIBUTOR","body":"> 4. We ingest old events again (there are some upserts). ?????? What schema is being used here?\r\n\r\nAt this step I used SCHEMA_V2\r\nWe use Deltastreamer in continues mode and only restart it in step 2, where we provide the new SCHEMA_V2 to the Deltastreamer.\r\nI tried to reproduce everything as good as possible in my [DeltaStreamer-test](https://github.com/apache/hudi/pull/1844/files#diff-2c3763c5782af9c3cbc02e2935211587R476)","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663373241/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663413173","html_url":"https://github.com/apache/hudi/issues/1860#issuecomment-663413173","issue_url":"https://api.github.com/repos/apache/hudi/issues/1860","id":663413173,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzQxMzE3Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T08:39:13Z","updated_at":"2020-07-24T08:39:13Z","author_association":"CONTRIBUTOR","body":"I would expect the data to be same across query engines unless there is some caching or GS is not giving consistent listing view.\r\n\r\nWith Hudi's Spark datasource integration, Hudi reuses spark's parquet Data Source implementation and merely applies file level path filter to pick and choose what files to read. you can do something like select(distinct(\"_hoodie_file_name\")) on both the cases to see if any file is getting missed. You can also run select(max(\"_hoodie_commit_time\") to determine what is the highest committed time and check if they are consistent for checking atomicity. Otherwise, I suggest you can also do similar experiments with Parquet or other datasets. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663413173/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663414676","html_url":"https://github.com/apache/hudi/issues/1867#issuecomment-663414676","issue_url":"https://api.github.com/repos/apache/hudi/issues/1867","id":663414676,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzQxNDY3Ng==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T08:42:23Z","updated_at":"2020-07-24T08:42:23Z","author_association":"CONTRIBUTOR","body":"@umehrot2 : Can you help answer this question. Thanks.\r\nBalaji.V","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663414676/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663427905","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663427905","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663427905,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzQyNzkwNQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T08:59:46Z","updated_at":"2020-07-24T08:59:46Z","author_association":"CONTRIBUTOR","body":"What do you mean by \"runs serially with ingestion\"? My understanding was that inline compaction happened in the same flow as writing so an inline compaction would simply slow down ingestion.\r\n\r\n ===> Yes, that is what I meant. Inline Compaction would run after ingestion but not in parallel. You can use #1752 to have it run concurrently.\r\n\r\nDoes INLINE_COMPACT_NUM_DELTA_COMMITS_PROP refer to the number of commits retained in general, or the number of commits for a record?\r\n\r\n==> INLINE_COMPACT_NUM_DELTA_COMMITS_PROP refers to number of ingestion (deltacommits) between 2 compaction runs. \r\n\r\nI see in the timeline I have several clean.requested and clean.inflight, how can I get these to actually complete?\r\n\r\n==> If it is in inflight state alone, there could be errors when Hudi is trying to cleanup. Please look for exceptions in driver logs. Cleaner run should be run automatically by default. Also, any pending clean operations will automatically get picked up in next ingestion. So, it must have been failing for some reasons. You can turn on logs to see what is happening.\r\n\r\nIs it possible to force a compaction of the existing log files.\r\n\r\n===> Yes, by configuring INLINE_COMPACT_NUM_DELTA_COMMITS_PROP. You can set it to 1 to have aggressive compaction. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663427905/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663442107","html_url":"https://github.com/apache/hudi/pull/1810#issuecomment-663442107","issue_url":"https://api.github.com/repos/apache/hudi/issues/1810","id":663442107,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzQ0MjEwNw==","user":{"login":"lw309637554","id":8501994,"node_id":"MDQ6VXNlcjg1MDE5OTQ=","avatar_url":"https://avatars.githubusercontent.com/u/8501994?v=4","gravatar_id":"","url":"https://api.github.com/users/lw309637554","html_url":"https://github.com/lw309637554","followers_url":"https://api.github.com/users/lw309637554/followers","following_url":"https://api.github.com/users/lw309637554/following{/other_user}","gists_url":"https://api.github.com/users/lw309637554/gists{/gist_id}","starred_url":"https://api.github.com/users/lw309637554/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lw309637554/subscriptions","organizations_url":"https://api.github.com/users/lw309637554/orgs","repos_url":"https://api.github.com/users/lw309637554/repos","events_url":"https://api.github.com/users/lw309637554/events{/privacy}","received_events_url":"https://api.github.com/users/lw309637554/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T09:22:03Z","updated_at":"2020-07-24T09:22:03Z","author_association":"CONTRIBUTOR","body":"> couple\r\n\r\nokay ,thanks ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663442107/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663450319","html_url":"https://github.com/apache/hudi/issues/1847#issuecomment-663450319","issue_url":"https://api.github.com/repos/apache/hudi/issues/1847","id":663450319,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzQ1MDMxOQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T09:35:51Z","updated_at":"2020-07-24T09:35:51Z","author_association":"CONTRIBUTOR","body":"@bschell : Thanks for the information. As getLen() is used extensively both on read and write side, can you let us elaborate more on what cases does it actually result in RPC calls ? Is there an ability to cache within the implementation ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663450319/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663646201","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663646201","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663646201,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY0NjIwMQ==","user":{"login":"ssomuah","id":2061955,"node_id":"MDQ6VXNlcjIwNjE5NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2061955?v=4","gravatar_id":"","url":"https://api.github.com/users/ssomuah","html_url":"https://github.com/ssomuah","followers_url":"https://api.github.com/users/ssomuah/followers","following_url":"https://api.github.com/users/ssomuah/following{/other_user}","gists_url":"https://api.github.com/users/ssomuah/gists{/gist_id}","starred_url":"https://api.github.com/users/ssomuah/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ssomuah/subscriptions","organizations_url":"https://api.github.com/users/ssomuah/orgs","repos_url":"https://api.github.com/users/ssomuah/repos","events_url":"https://api.github.com/users/ssomuah/events{/privacy}","received_events_url":"https://api.github.com/users/ssomuah/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T17:23:06Z","updated_at":"2020-07-24T17:23:06Z","author_association":"NONE","body":"Hi Balaji, I think I've narrowed down my issue somewhat for my MOR table. \r\n\r\nI started again with a fresh table and the initial commits make sense, but after a time I notice It's consistently trying to write 300+ files. \r\n\r\n<img width=\"964\" alt=\"Screen Shot 2020-07-24 at 1 15 17 PM\" src=\"https://user-images.githubusercontent.com/2061955/88417393-da14f980-cdaf-11ea-87ab-63f3aafade83.png\">\r\n\r\n<img width=\"1398\" alt=\"Screen Shot 2020-07-24 at 1 15 36 PM\" src=\"https://user-images.githubusercontent.com/2061955/88417402-de411700-cdaf-11ea-85dd-c10c405851d3.png\">\r\n\r\n<img width=\"1411\" alt=\"Screen Shot 2020-07-24 at 1 15 52 PM\" src=\"https://user-images.githubusercontent.com/2061955/88417424-e5682500-cdaf-11ea-9c4b-534e27d80c45.png\">\r\n\r\n\r\nThe individual tasks don't take that long so I think if I could reduce the number of files it's trying to write it would help. \r\n<img width=\"1409\" alt=\"Screen Shot 2020-07-24 at 1 16 03 PM\" src=\"https://user-images.githubusercontent.com/2061955/88417487-fca71280-cdaf-11ea-9fc0-10a8a074501c.png\">\r\n\r\n\r\nI can also see from the cli that whether it's doing a compaction or a delta commit I still seem to be writing the same number of files for a fraction of the data. \r\n<img width=\"1307\" alt=\"Screen Shot 2020-07-24 at 1 21 36 PM\" src=\"https://user-images.githubusercontent.com/2061955/88417841-aa1a2600-cdb0-11ea-808f-d66595af91ea.png\">\r\n\r\n\r\nIs there something I can tune to reduce the number of files it breaks the data into?\r\n\r\nhoodie.logfile.max.size is 256MB\r\nhoodie.parquet.max.file.size is 256MB\r\nhoodie.parquet.compression.ratio is the default .35","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663646201/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663648589","html_url":"https://github.com/apache/hudi/issues/1866#issuecomment-663648589","issue_url":"https://api.github.com/repos/apache/hudi/issues/1866","id":663648589,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY0ODU4OQ==","user":{"login":"luffyd","id":2287345,"node_id":"MDQ6VXNlcjIyODczNDU=","avatar_url":"https://avatars.githubusercontent.com/u/2287345?v=4","gravatar_id":"","url":"https://api.github.com/users/luffyd","html_url":"https://github.com/luffyd","followers_url":"https://api.github.com/users/luffyd/followers","following_url":"https://api.github.com/users/luffyd/following{/other_user}","gists_url":"https://api.github.com/users/luffyd/gists{/gist_id}","starred_url":"https://api.github.com/users/luffyd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/luffyd/subscriptions","organizations_url":"https://api.github.com/users/luffyd/orgs","repos_url":"https://api.github.com/users/luffyd/repos","events_url":"https://api.github.com/users/luffyd/events{/privacy}","received_events_url":"https://api.github.com/users/luffyd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T17:29:06Z","updated_at":"2020-07-24T17:29:06Z","author_association":"NONE","body":"Thanks saitsh,\r\nI have inline turned on by default, Now I see cleans did happen! Is there a possibility that commits get archived before clean job is resulting in a noop. I will continue to monitor. \r\n\r\nAlso can you confirm If I can run a clean job in a separate spark job concurrently while streaming write is happening, guess it should be fine as compaction runs have that ability\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663648589/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663659622","html_url":"https://github.com/apache/hudi/issues/1867#issuecomment-663659622","issue_url":"https://api.github.com/repos/apache/hudi/issues/1867","id":663659622,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY1OTYyMg==","user":{"login":"luffyd","id":2287345,"node_id":"MDQ6VXNlcjIyODczNDU=","avatar_url":"https://avatars.githubusercontent.com/u/2287345?v=4","gravatar_id":"","url":"https://api.github.com/users/luffyd","html_url":"https://github.com/luffyd","followers_url":"https://api.github.com/users/luffyd/followers","following_url":"https://api.github.com/users/luffyd/following{/other_user}","gists_url":"https://api.github.com/users/luffyd/gists{/gist_id}","starred_url":"https://api.github.com/users/luffyd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/luffyd/subscriptions","organizations_url":"https://api.github.com/users/luffyd/orgs","repos_url":"https://api.github.com/users/luffyd/repos","events_url":"https://api.github.com/users/luffyd/events{/privacy}","received_events_url":"https://api.github.com/users/luffyd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T17:55:10Z","updated_at":"2020-07-24T17:55:10Z","author_association":"NONE","body":"@tsolanki95 Does this happen at the time read? In my tests, I noticed etags are not being in sync for .hoodie folder.\r\nAlso what are your reasons to enable consistent view when using hudi.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663659622/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663677139","html_url":"https://github.com/apache/hudi/issues/1864#issuecomment-663677139","issue_url":"https://api.github.com/repos/apache/hudi/issues/1864","id":663677139,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY3NzEzOQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T18:37:04Z","updated_at":"2020-07-24T18:37:04Z","author_association":"CONTRIBUTOR","body":"Closing this ticket. Please reach out in slack or open a new ticket if you find any issues","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663677139/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663677430","html_url":"https://github.com/apache/hudi/issues/1856#issuecomment-663677430","issue_url":"https://api.github.com/repos/apache/hudi/issues/1856","id":663677430,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY3NzQzMA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T18:37:51Z","updated_at":"2020-07-24T18:37:51Z","author_association":"CONTRIBUTOR","body":"Please reopen if you need further clarifications.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663677430/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663677973","html_url":"https://github.com/apache/hudi/issues/1787#issuecomment-663677973","issue_url":"https://api.github.com/repos/apache/hudi/issues/1787","id":663677973,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY3Nzk3Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T18:39:17Z","updated_at":"2020-07-24T18:39:17Z","author_association":"CONTRIBUTOR","body":"@asheeshgarg : I may have accidentally deleted a comment from. Has the issue been resolved ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663677973/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663683323","html_url":"https://github.com/apache/hudi/issues/1866#issuecomment-663683323","issue_url":"https://api.github.com/repos/apache/hudi/issues/1866","id":663683323,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY4MzMyMw==","user":{"login":"satishkotha","id":2992755,"node_id":"MDQ6VXNlcjI5OTI3NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2992755?v=4","gravatar_id":"","url":"https://api.github.com/users/satishkotha","html_url":"https://github.com/satishkotha","followers_url":"https://api.github.com/users/satishkotha/followers","following_url":"https://api.github.com/users/satishkotha/following{/other_user}","gists_url":"https://api.github.com/users/satishkotha/gists{/gist_id}","starred_url":"https://api.github.com/users/satishkotha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishkotha/subscriptions","organizations_url":"https://api.github.com/users/satishkotha/orgs","repos_url":"https://api.github.com/users/satishkotha/repos","events_url":"https://api.github.com/users/satishkotha/events{/privacy}","received_events_url":"https://api.github.com/users/satishkotha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T18:52:48Z","updated_at":"2020-07-24T21:59:05Z","author_association":"MEMBER","body":"> Is there a possibility that commits get archived before clean job is resulting in a noop. I will continue to monitor.\r\n\r\nclean and archival are somewhat independent today. So this 'noop' should not happen.\r\n\r\n> Also can you confirm If I can run a clean job in a separate spark job concurrently while streaming write is happening, guess it should be fine as compaction runs have that ability\r\n\r\nWhy are you considering separate spark job for clean? Are you seeing clean take a lot of time? You can consider running clean concurrently with write by setting 'hoodie.clean.async' to true. (This runs clean in same job, but concurrently with write). \r\n\r\nI don't know of anyone using separate spark job to run clean. Theoretically, I think it is possible. But you may have to do some testing because it isn't used like this afaik.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663683323/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663688088","html_url":"https://github.com/apache/hudi/issues/1872#issuecomment-663688088","issue_url":"https://api.github.com/repos/apache/hudi/issues/1872","id":663688088,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY4ODA4OA==","user":{"login":"satishkotha","id":2992755,"node_id":"MDQ6VXNlcjI5OTI3NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2992755?v=4","gravatar_id":"","url":"https://api.github.com/users/satishkotha","html_url":"https://github.com/satishkotha","followers_url":"https://api.github.com/users/satishkotha/followers","following_url":"https://api.github.com/users/satishkotha/following{/other_user}","gists_url":"https://api.github.com/users/satishkotha/gists{/gist_id}","starred_url":"https://api.github.com/users/satishkotha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishkotha/subscriptions","organizations_url":"https://api.github.com/users/satishkotha/orgs","repos_url":"https://api.github.com/users/satishkotha/repos","events_url":"https://api.github.com/users/satishkotha/events{/privacy}","received_events_url":"https://api.github.com/users/satishkotha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T19:04:54Z","updated_at":"2020-07-24T19:04:54Z","author_association":"MEMBER","body":"This is likely more of AWS support question. A quick search shows https://aws.amazon.com/premiumsupport/knowledge-center/emr-s3-503-slow-down/ \r\n\r\nCan you see if any of the solutions there work for you? You may have to slow down ingestion. \r\n\r\n(I dont have a lot of experience with AWS EMR. Others in the community, please comment if you have worked around similar problem before)","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663688088/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663688633","html_url":"https://github.com/apache/hudi/issues/1867#issuecomment-663688633","issue_url":"https://api.github.com/repos/apache/hudi/issues/1867","id":663688633,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY4ODYzMw==","user":{"login":"tsolanki95","id":5753542,"node_id":"MDQ6VXNlcjU3NTM1NDI=","avatar_url":"https://avatars.githubusercontent.com/u/5753542?v=4","gravatar_id":"","url":"https://api.github.com/users/tsolanki95","html_url":"https://github.com/tsolanki95","followers_url":"https://api.github.com/users/tsolanki95/followers","following_url":"https://api.github.com/users/tsolanki95/following{/other_user}","gists_url":"https://api.github.com/users/tsolanki95/gists{/gist_id}","starred_url":"https://api.github.com/users/tsolanki95/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tsolanki95/subscriptions","organizations_url":"https://api.github.com/users/tsolanki95/orgs","repos_url":"https://api.github.com/users/tsolanki95/repos","events_url":"https://api.github.com/users/tsolanki95/events{/privacy}","received_events_url":"https://api.github.com/users/tsolanki95/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T19:06:23Z","updated_at":"2020-07-24T19:30:02Z","author_association":"NONE","body":"@luffyd We put in consistent view as a solution earlier, based on AWS support, to solve issues with using spark with S3 eventual consistency model causing duplicates in our data. We are now looking towards changing some of our datasets to utilize hudi but our compute resources still utilize EMRFS consistent view. As part of the transition, when some of our datasets utilize hudi and some do not, it would be good to be able to run spark with hudi on EMRFS consistent view.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663688633/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663688727","html_url":"https://github.com/apache/hudi/issues/1787#issuecomment-663688727","issue_url":"https://api.github.com/repos/apache/hudi/issues/1787","id":663688727,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY4ODcyNw==","user":{"login":"asheeshgarg","id":4348671,"node_id":"MDQ6VXNlcjQzNDg2NzE=","avatar_url":"https://avatars.githubusercontent.com/u/4348671?v=4","gravatar_id":"","url":"https://api.github.com/users/asheeshgarg","html_url":"https://github.com/asheeshgarg","followers_url":"https://api.github.com/users/asheeshgarg/followers","following_url":"https://api.github.com/users/asheeshgarg/following{/other_user}","gists_url":"https://api.github.com/users/asheeshgarg/gists{/gist_id}","starred_url":"https://api.github.com/users/asheeshgarg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/asheeshgarg/subscriptions","organizations_url":"https://api.github.com/users/asheeshgarg/orgs","repos_url":"https://api.github.com/users/asheeshgarg/repos","events_url":"https://api.github.com/users/asheeshgarg/events{/privacy}","received_events_url":"https://api.github.com/users/asheeshgarg/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T19:06:38Z","updated_at":"2020-07-24T19:06:38Z","author_association":"NONE","body":"@bvaradar I am getting the same exception I had added the jars to the --jars option of submit so its available to both driver and executors.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663688727/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663690493","html_url":"https://github.com/apache/hudi/issues/1872#issuecomment-663690493","issue_url":"https://api.github.com/repos/apache/hudi/issues/1872","id":663690493,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY5MDQ5Mw==","user":{"login":"luffyd","id":2287345,"node_id":"MDQ6VXNlcjIyODczNDU=","avatar_url":"https://avatars.githubusercontent.com/u/2287345?v=4","gravatar_id":"","url":"https://api.github.com/users/luffyd","html_url":"https://github.com/luffyd","followers_url":"https://api.github.com/users/luffyd/followers","following_url":"https://api.github.com/users/luffyd/following{/other_user}","gists_url":"https://api.github.com/users/luffyd/gists{/gist_id}","starred_url":"https://api.github.com/users/luffyd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/luffyd/subscriptions","organizations_url":"https://api.github.com/users/luffyd/orgs","repos_url":"https://api.github.com/users/luffyd/repos","events_url":"https://api.github.com/users/luffyd/events{/privacy}","received_events_url":"https://api.github.com/users/luffyd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T19:11:04Z","updated_at":"2020-07-24T19:11:04Z","author_association":"NONE","body":"I have noticed slowing down ingestion worked.\r\nIt seems like number of calls to \"HoodieWriteHandle.createMarkerFile\" is resulting an S3 call.\r\nBut can you give any hints on \r\n1. how number calls to \"HoodieWriteHandle.createMarkerFile\" is related to number of partitions\r\n2. how number calls to \"HoodieWriteHandle.createMarkerFile\" is related to number of files in a partition\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663690493/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663698781","html_url":"https://github.com/apache/hudi/issues/1867#issuecomment-663698781","issue_url":"https://api.github.com/repos/apache/hudi/issues/1867","id":663698781,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzY5ODc4MQ==","user":{"login":"tsolanki95","id":5753542,"node_id":"MDQ6VXNlcjU3NTM1NDI=","avatar_url":"https://avatars.githubusercontent.com/u/5753542?v=4","gravatar_id":"","url":"https://api.github.com/users/tsolanki95","html_url":"https://github.com/tsolanki95","followers_url":"https://api.github.com/users/tsolanki95/followers","following_url":"https://api.github.com/users/tsolanki95/following{/other_user}","gists_url":"https://api.github.com/users/tsolanki95/gists{/gist_id}","starred_url":"https://api.github.com/users/tsolanki95/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tsolanki95/subscriptions","organizations_url":"https://api.github.com/users/tsolanki95/orgs","repos_url":"https://api.github.com/users/tsolanki95/repos","events_url":"https://api.github.com/users/tsolanki95/events{/privacy}","received_events_url":"https://api.github.com/users/tsolanki95/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T19:33:13Z","updated_at":"2020-07-24T19:33:13Z","author_association":"NONE","body":"This is also a field where data quality, precision, and accuracy are important. EMRFS consistent view helps us keep us having issues with s3 consistency, some of the features that hudi provides with rollback capabilities, and auditing and tracking changes made to our table are incredibly powerful for helping find and isolate data quality errors and rollback and rerun data with fixed input data/code.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663698781/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663728553","html_url":"https://github.com/apache/hudi/issues/1847#issuecomment-663728553","issue_url":"https://api.github.com/repos/apache/hudi/issues/1847","id":663728553,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzcyODU1Mw==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T20:50:19Z","updated_at":"2020-07-24T20:50:19Z","author_association":"CONTRIBUTOR","body":"@bvaradar EMR only overrides the `getLen()` if the customer has explicitly enabled `Client Side Encryption` using the EmrFS property `fs.s3.cse.enabled`. In that case I see that EmrFS needs to make a couple of `S3 calls`. But, based on my brief conversation with @zuyanton he mentioned he is most likely not enabling this feature. But I would let him confirm this, and if its true EMR team can further look into the possibility of any optimizations in that code path.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663728553/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663732545","html_url":"https://github.com/apache/hudi/issues/1867#issuecomment-663732545","issue_url":"https://api.github.com/repos/apache/hudi/issues/1867","id":663732545,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzczMjU0NQ==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T21:01:41Z","updated_at":"2020-07-24T21:01:41Z","author_association":"CONTRIBUTOR","body":"@tsolanki95 have you tried using `hoodie.consistency.check.enabled` which is Hudi's in-built mechanism for avoiding `eventual consistency` issues instead ?\r\n\r\nAs for this particular issue with `EmrFS consistent view` are these temporary errors which resolve on retrying or is it causing the job to fail ? Yes disabling `fs.s3.consistent.metadata.etag.verification.enabled` could be a way ahead if this is blocking you while EMR team can try investigating this issue.\r\n\r\ncc @bschell who actually worked on the etag feature in EmrFS. Do you see any obvious cause for this ? Else, we can possibly have them open a ticket to AWS EMR support and investigate from there.\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663732545/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663734319","html_url":"https://github.com/apache/hudi/issues/1867#issuecomment-663734319","issue_url":"https://api.github.com/repos/apache/hudi/issues/1867","id":663734319,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzczNDMxOQ==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T21:06:42Z","updated_at":"2020-07-24T21:06:42Z","author_association":"CONTRIBUTOR","body":"Also on a side note, we always recommend using latest EMR releases as it has latest fixes and version of applications. So you may want to use `emr-5.30.1` instead.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663734319/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663741729","html_url":"https://github.com/apache/hudi/issues/1866#issuecomment-663741729","issue_url":"https://api.github.com/repos/apache/hudi/issues/1866","id":663741729,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzc0MTcyOQ==","user":{"login":"luffyd","id":2287345,"node_id":"MDQ6VXNlcjIyODczNDU=","avatar_url":"https://avatars.githubusercontent.com/u/2287345?v=4","gravatar_id":"","url":"https://api.github.com/users/luffyd","html_url":"https://github.com/luffyd","followers_url":"https://api.github.com/users/luffyd/followers","following_url":"https://api.github.com/users/luffyd/following{/other_user}","gists_url":"https://api.github.com/users/luffyd/gists{/gist_id}","starred_url":"https://api.github.com/users/luffyd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/luffyd/subscriptions","organizations_url":"https://api.github.com/users/luffyd/orgs","repos_url":"https://api.github.com/users/luffyd/repos","events_url":"https://api.github.com/users/luffyd/events{/privacy}","received_events_url":"https://api.github.com/users/luffyd/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T21:27:59Z","updated_at":"2020-07-24T21:29:35Z","author_association":"NONE","body":"Ok thanks\r\nNo I was not thinking to run as separate process continuously, but I wanted to execute \"clean commands\" from cli o that my streaming tests progress faster.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663741729/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663751174","html_url":"https://github.com/apache/hudi/issues/1866#issuecomment-663751174","issue_url":"https://api.github.com/repos/apache/hudi/issues/1866","id":663751174,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzc1MTE3NA==","user":{"login":"satishkotha","id":2992755,"node_id":"MDQ6VXNlcjI5OTI3NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2992755?v=4","gravatar_id":"","url":"https://api.github.com/users/satishkotha","html_url":"https://github.com/satishkotha","followers_url":"https://api.github.com/users/satishkotha/followers","following_url":"https://api.github.com/users/satishkotha/following{/other_user}","gists_url":"https://api.github.com/users/satishkotha/gists{/gist_id}","starred_url":"https://api.github.com/users/satishkotha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishkotha/subscriptions","organizations_url":"https://api.github.com/users/satishkotha/orgs","repos_url":"https://api.github.com/users/satishkotha/repos","events_url":"https://api.github.com/users/satishkotha/events{/privacy}","received_events_url":"https://api.github.com/users/satishkotha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-24T22:00:09Z","updated_at":"2020-07-24T22:00:09Z","author_association":"MEMBER","body":"Sounds good. Please try it and let me know if you see any issues.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663751174/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663787403","html_url":"https://github.com/apache/hudi/pull/1876#issuecomment-663787403","issue_url":"https://api.github.com/repos/apache/hudi/issues/1876","id":663787403,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzc4NzQwMw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T01:02:45Z","updated_at":"2020-07-25T01:02:45Z","author_association":"MEMBER","body":"@bvaradar @umehrot2 after many valiant efforts, finally rebased the original #1678  here. Will be working on getting the code review comments addressed and tests passing over the weekend. \r\nWill be then trying to redo #1702 on top of that. \r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663787403/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663788217","html_url":"https://github.com/apache/hudi/issues/1872#issuecomment-663788217","issue_url":"https://api.github.com/repos/apache/hudi/issues/1872","id":663788217,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzc4ODIxNw==","user":{"login":"satishkotha","id":2992755,"node_id":"MDQ6VXNlcjI5OTI3NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2992755?v=4","gravatar_id":"","url":"https://api.github.com/users/satishkotha","html_url":"https://github.com/satishkotha","followers_url":"https://api.github.com/users/satishkotha/followers","following_url":"https://api.github.com/users/satishkotha/following{/other_user}","gists_url":"https://api.github.com/users/satishkotha/gists{/gist_id}","starred_url":"https://api.github.com/users/satishkotha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishkotha/subscriptions","organizations_url":"https://api.github.com/users/satishkotha/orgs","repos_url":"https://api.github.com/users/satishkotha/repos","events_url":"https://api.github.com/users/satishkotha/events{/privacy}","received_events_url":"https://api.github.com/users/satishkotha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T01:09:46Z","updated_at":"2020-07-25T01:09:46Z","author_association":"MEMBER","body":"Hi,\r\n\r\nNumber of 'createMarkerFile' calls = (number of partitions) + (number of file groups) *touched* by upsert operation. \r\n\r\nWhat is the partition for your workload? What is the 'hoodie.parquet.small.file.limit'?  If you have a lot of small files, then we likely need to create a lot of markers (if upsert workload is distributed across multiple file groups).","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663788217/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663806475","html_url":"https://github.com/apache/hudi/issues/1878#issuecomment-663806475","issue_url":"https://api.github.com/repos/apache/hudi/issues/1878","id":663806475,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzgwNjQ3NQ==","user":{"login":"rubenssoto","id":36298331,"node_id":"MDQ6VXNlcjM2Mjk4MzMx","avatar_url":"https://avatars.githubusercontent.com/u/36298331?v=4","gravatar_id":"","url":"https://api.github.com/users/rubenssoto","html_url":"https://github.com/rubenssoto","followers_url":"https://api.github.com/users/rubenssoto/followers","following_url":"https://api.github.com/users/rubenssoto/following{/other_user}","gists_url":"https://api.github.com/users/rubenssoto/gists{/gist_id}","starred_url":"https://api.github.com/users/rubenssoto/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rubenssoto/subscriptions","organizations_url":"https://api.github.com/users/rubenssoto/orgs","repos_url":"https://api.github.com/users/rubenssoto/repos","events_url":"https://api.github.com/users/rubenssoto/events{/privacy}","received_events_url":"https://api.github.com/users/rubenssoto/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T04:20:29Z","updated_at":"2020-07-25T04:20:29Z","author_association":"NONE","body":"I tried resizing the cluster with 3 more nodes, so in total(4 nodes) after resizing I had 4 cores in each node and 16gb of ram each, and it wasn't any difference, the job keeps very slow and with memory errors.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663806475/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663813751","html_url":"https://github.com/apache/hudi/issues/1878#issuecomment-663813751","issue_url":"https://api.github.com/repos/apache/hudi/issues/1878","id":663813751,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzgxMzc1MQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T05:45:00Z","updated_at":"2020-07-25T05:45:00Z","author_association":"CONTRIBUTOR","body":"This is a spark tuning issue in general. The slowness is due to memory pressure and node failures due to it. Atleast in one of the batches, I see task failures (and retries) during reading from source parquet file itself. \r\n\r\nAs mentioned in the suggestion  \"Consider boosting spark.yarn.executor.memoryOverhead or disabling yarn.nodemanager.vmem-check-enabled because of YARN-4714.\", you need to increase spark.yarn.executor.memoryOverhead. You are running 2 executors per machine with 8GB room for each which may not have lot of room. If you are trying to compare parquet write with hudi, note that hudi adds metadata fields which gives incremental pull, indexing and other benefits. If your original record size is very small and comparable to metadata overhead and your setup is already close to hitting the limit for parquet write, then you would need to give more resources. \r\n\r\nOn a related note, since you are trying to use streaming for bootstrapping from a fixed source, have you considered using bulk insert or insert (for size handling) in batch mode which would sort and write the data once. With this mode of incremental inserting, Hudi would try to increase a small file generated in the previous batch. This means that it has to read the small file and apply new insert and write a newer version of the file (which is bigger). As you can see, more the number of iterations here, the more repeated reads will happen. Hence, you would benefit by throwing more resources for a potentially shorter time to do this migration. \r\n\r\n \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663813751/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663814729","html_url":"https://github.com/apache/hudi/issues/1875#issuecomment-663814729","issue_url":"https://api.github.com/repos/apache/hudi/issues/1875","id":663814729,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzgxNDcyOQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T05:57:30Z","updated_at":"2020-07-25T05:57:30Z","author_association":"CONTRIBUTOR","body":"@umehrot2 @bschell @zhedoubushishi : Can you guys chime in here.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663814729/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663816363","html_url":"https://github.com/apache/hudi/issues/1852#issuecomment-663816363","issue_url":"https://api.github.com/repos/apache/hudi/issues/1852","id":663816363,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzgxNjM2Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T06:16:41Z","updated_at":"2020-07-25T06:16:41Z","author_association":"CONTRIBUTOR","body":"@ssomuah : Looking at the commit metadata, it is the case where your updates are spread across a large number of files. For example, in latest commit, 334 files sees updates whereas only one file is newly created due to inserts. It looks like this is the nature of your workload. \r\n\r\nIf your record key has some sort of ordering, then you can initially bootstrap using \"bulk-insert\" which would sort and write the data in record-key order. This can potentially help reduce the number of files getting updated if each batch of writes have similar ordering.  You can also try recreating the dataset with larger parquet file size and higher small file limit and async compactions (more frequent to keep the number of active log files in check). \r\n\r\nHowever, you are basically trying to reduce the number of files getting appended at the expense of more data getting appended to a single file. This is a general upsert problem due to the nature of your workload.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663816363/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663906344","html_url":"https://github.com/apache/hudi/issues/1878#issuecomment-663906344","issue_url":"https://api.github.com/repos/apache/hudi/issues/1878","id":663906344,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzkwNjM0NA==","user":{"login":"rubenssoto","id":36298331,"node_id":"MDQ6VXNlcjM2Mjk4MzMx","avatar_url":"https://avatars.githubusercontent.com/u/36298331?v=4","gravatar_id":"","url":"https://api.github.com/users/rubenssoto","html_url":"https://github.com/rubenssoto","followers_url":"https://api.github.com/users/rubenssoto/followers","following_url":"https://api.github.com/users/rubenssoto/following{/other_user}","gists_url":"https://api.github.com/users/rubenssoto/gists{/gist_id}","starred_url":"https://api.github.com/users/rubenssoto/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rubenssoto/subscriptions","organizations_url":"https://api.github.com/users/rubenssoto/orgs","repos_url":"https://api.github.com/users/rubenssoto/repos","events_url":"https://api.github.com/users/rubenssoto/events{/privacy}","received_events_url":"https://api.github.com/users/rubenssoto/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T21:08:42Z","updated_at":"2020-07-25T21:08:42Z","author_association":"NONE","body":"Hi bvaradar, thank you for your awnser.\r\n\r\nI tried to increase spark.yarn.executor.memoryOverhead to 2GB with foreachbatch option inside writestream and it worked. 4 nodes with 4 cores and 32gb each, took 52 minutes is a good time for this hardware configuration? I think that could be better but Im very happy.\r\nWhats the difference with spark streaming with or without foreachbatch, Am I lost anything important? I tried, because I saw in delta lake docs, they use foreachbatch for merge in spark streaming.\r\n\r\n<img width=\"1680\" alt=\"Captura de Tela 2020-07-25 às 18 04 27\" src=\"https://user-images.githubusercontent.com/36298331/88466311-6b17cd80-cea1-11ea-9dbd-97753a2e6978.png\">\r\n<img width=\"1680\" alt=\"Captura de Tela 2020-07-25 às 18 04 53\" src=\"https://user-images.githubusercontent.com/36298331/88466313-6eab5480-cea1-11ea-8cb9-0e9a5c30b6c4.png\">\r\n<img width=\"1680\" alt=\"Captura de Tela 2020-07-25 às 18 04 40\" src=\"https://user-images.githubusercontent.com/36298331/88466316-70751800-cea1-11ea-8ec6-23bd69e51b17.png\">\r\n\r\n\r\nSome jobs took more time, do you know why some jobs created a lot of tasks? I think that could be more efficient if they write with fewer tasks. \r\nNow I will try do the same thing with write operation \"upsert\" because my data set could have some duplicated values and I don't know what files are they.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663906344/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663911960","html_url":"https://github.com/apache/hudi/pull/1873#issuecomment-663911960","issue_url":"https://api.github.com/repos/apache/hudi/issues/1873","id":663911960,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzkxMTk2MA==","user":{"login":"xushiyan","id":2701446,"node_id":"MDQ6VXNlcjI3MDE0NDY=","avatar_url":"https://avatars.githubusercontent.com/u/2701446?v=4","gravatar_id":"","url":"https://api.github.com/users/xushiyan","html_url":"https://github.com/xushiyan","followers_url":"https://api.github.com/users/xushiyan/followers","following_url":"https://api.github.com/users/xushiyan/following{/other_user}","gists_url":"https://api.github.com/users/xushiyan/gists{/gist_id}","starred_url":"https://api.github.com/users/xushiyan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xushiyan/subscriptions","organizations_url":"https://api.github.com/users/xushiyan/orgs","repos_url":"https://api.github.com/users/xushiyan/repos","events_url":"https://api.github.com/users/xushiyan/events{/privacy}","received_events_url":"https://api.github.com/users/xushiyan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T22:21:21Z","updated_at":"2020-07-25T22:21:21Z","author_association":"MEMBER","body":"@yanghua would you be able to take a pass on this change please? thanks","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663911960/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663915325","html_url":"https://github.com/apache/hudi/pull/1819#issuecomment-663915325","issue_url":"https://api.github.com/repos/apache/hudi/issues/1819","id":663915325,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzkxNTMyNQ==","user":{"login":"shenh062326","id":9527867,"node_id":"MDQ6VXNlcjk1Mjc4Njc=","avatar_url":"https://avatars.githubusercontent.com/u/9527867?v=4","gravatar_id":"","url":"https://api.github.com/users/shenh062326","html_url":"https://github.com/shenh062326","followers_url":"https://api.github.com/users/shenh062326/followers","following_url":"https://api.github.com/users/shenh062326/following{/other_user}","gists_url":"https://api.github.com/users/shenh062326/gists{/gist_id}","starred_url":"https://api.github.com/users/shenh062326/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shenh062326/subscriptions","organizations_url":"https://api.github.com/users/shenh062326/orgs","repos_url":"https://api.github.com/users/shenh062326/repos","events_url":"https://api.github.com/users/shenh062326/events{/privacy}","received_events_url":"https://api.github.com/users/shenh062326/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T23:12:27Z","updated_at":"2020-07-25T23:12:27Z","author_association":"CONTRIBUTOR","body":"> I don't see any tests being added as part of the patch. Would be nice to have some tests covering the new code that was added at all levels.\r\n> \r\n> * WriteClient\r\n> * Datasource if there is an existing suite of tests for other write operations\r\n> * Deltastreamer\r\n\r\nDone.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663915325/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663917822","html_url":"https://github.com/apache/hudi/pull/1868#issuecomment-663917822","issue_url":"https://api.github.com/repos/apache/hudi/issues/1868","id":663917822,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzkxNzgyMg==","user":{"login":"shenh062326","id":9527867,"node_id":"MDQ6VXNlcjk1Mjc4Njc=","avatar_url":"https://avatars.githubusercontent.com/u/9527867?v=4","gravatar_id":"","url":"https://api.github.com/users/shenh062326","html_url":"https://github.com/shenh062326","followers_url":"https://api.github.com/users/shenh062326/followers","following_url":"https://api.github.com/users/shenh062326/following{/other_user}","gists_url":"https://api.github.com/users/shenh062326/gists{/gist_id}","starred_url":"https://api.github.com/users/shenh062326/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shenh062326/subscriptions","organizations_url":"https://api.github.com/users/shenh062326/orgs","repos_url":"https://api.github.com/users/shenh062326/repos","events_url":"https://api.github.com/users/shenh062326/events{/privacy}","received_events_url":"https://api.github.com/users/shenh062326/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T23:50:45Z","updated_at":"2020-07-25T23:53:49Z","author_association":"CONTRIBUTOR","body":"Add a performance test, which insert 100000 records, 1000 fileGroups, each fileGroup's weight is 0.001.\r\n```\r\n  public void partitionWeightPerformance() throws Exception {\r\n    final String testPartitionPath = \"2016/09/26\";\r\n    int totalInsertNum = 100000;\r\n\r\n    HoodieWriteConfig config = makeHoodieClientConfigBuilder()\r\n        .withCompactionConfig(HoodieCompactionConfig.newBuilder().compactionSmallFileSize(0)\r\n            .insertSplitSize(100).autoTuneInsertSplits(false).build()).build();\r\n\r\n    HoodieClientTestUtils.fakeCommit(basePath, \"001\");\r\n    metaClient = HoodieTableMetaClient.reload(metaClient);\r\n    HoodieCopyOnWriteTable table = (HoodieCopyOnWriteTable) HoodieTable.create(metaClient, config, hadoopConf);\r\n    HoodieTestDataGenerator dataGenerator = new HoodieTestDataGenerator(new String[]{testPartitionPath});\r\n    List<HoodieRecord> insertRecords = dataGenerator.generateInserts(\"001\", totalInsertNum);\r\n    WorkloadProfile profile = new WorkloadProfile(jsc.parallelize(insertRecords));\r\n    UpsertPartitioner partitioner = new UpsertPartitioner(profile, jsc, table, config);\r\n\r\n    for (int i = 0; i < 10; i++) {\r\n      long start = System.currentTimeMillis();\r\n      Map<Integer, Integer> partition2numRecords = new HashMap<Integer, Integer>();\r\n      for (HoodieRecord hoodieRecord : insertRecords) {\r\n        int partition = partitioner.getPartition(new Tuple2<>(\r\n            hoodieRecord.getKey(), Option.ofNullable(hoodieRecord.getCurrentLocation())));\r\n        if (!partition2numRecords.containsKey(partition)) {\r\n          partition2numRecords.put(partition, 0);\r\n        }\r\n        partition2numRecords.put(partition, partition2numRecords.get(partition) + 1);\r\n      }\r\n\r\n      System.out.println(\"cost: \" + (System.currentTimeMillis() - start));\r\n    }\r\n  }\r\n```\r\nTest it ten times, the result before the optimization:\r\n```\r\n  cost: 190\r\n  cost: 122\r\n  cost: 150\r\n  cost: 100\r\n  cost: 104\r\n  cost: 114\r\n  cost: 104\r\n  cost: 110\r\n  cost: 104\r\n  cost: 117\r\n```\r\n\r\nThe result after the optimization:\r\n```\r\n  cost: 154\r\n  cost: 83\r\n  cost: 77\r\n  cost: 84\r\n  cost: 85\r\n  cost: 84\r\n  cost: 87\r\n  cost: 99\r\n  cost: 102\r\n  cost: 85\r\n```\r\n\r\nIt can significantly optimize performance.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663917822/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663918069","html_url":"https://github.com/apache/hudi/pull/1868#issuecomment-663918069","issue_url":"https://api.github.com/repos/apache/hudi/issues/1868","id":663918069,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzkxODA2OQ==","user":{"login":"shenh062326","id":9527867,"node_id":"MDQ6VXNlcjk1Mjc4Njc=","avatar_url":"https://avatars.githubusercontent.com/u/9527867?v=4","gravatar_id":"","url":"https://api.github.com/users/shenh062326","html_url":"https://github.com/shenh062326","followers_url":"https://api.github.com/users/shenh062326/followers","following_url":"https://api.github.com/users/shenh062326/following{/other_user}","gists_url":"https://api.github.com/users/shenh062326/gists{/gist_id}","starred_url":"https://api.github.com/users/shenh062326/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shenh062326/subscriptions","organizations_url":"https://api.github.com/users/shenh062326/orgs","repos_url":"https://api.github.com/users/shenh062326/repos","events_url":"https://api.github.com/users/shenh062326/events{/privacy}","received_events_url":"https://api.github.com/users/shenh062326/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-25T23:54:57Z","updated_at":"2020-07-25T23:54:57Z","author_association":"CONTRIBUTOR","body":"@nsivabalan @vinothchandar Can you take a look at this pull request.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663918069/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663927747","html_url":"https://github.com/apache/hudi/pull/1873#issuecomment-663927747","issue_url":"https://api.github.com/repos/apache/hudi/issues/1873","id":663927747,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzkyNzc0Nw==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-26T02:32:50Z","updated_at":"2020-07-26T02:32:50Z","author_association":"CONTRIBUTOR","body":"> @yanghua would you be able to take a pass on this change please? thanks\r\n\r\nYes, I'd like to review this PR.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663927747/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663927931","html_url":"https://github.com/apache/hudi/issues/1878#issuecomment-663927931","issue_url":"https://api.github.com/repos/apache/hudi/issues/1878","id":663927931,"node_id":"MDEyOklzc3VlQ29tbWVudDY2MzkyNzkzMQ==","user":{"login":"rubenssoto","id":36298331,"node_id":"MDQ6VXNlcjM2Mjk4MzMx","avatar_url":"https://avatars.githubusercontent.com/u/36298331?v=4","gravatar_id":"","url":"https://api.github.com/users/rubenssoto","html_url":"https://github.com/rubenssoto","followers_url":"https://api.github.com/users/rubenssoto/followers","following_url":"https://api.github.com/users/rubenssoto/following{/other_user}","gists_url":"https://api.github.com/users/rubenssoto/gists{/gist_id}","starred_url":"https://api.github.com/users/rubenssoto/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rubenssoto/subscriptions","organizations_url":"https://api.github.com/users/rubenssoto/orgs","repos_url":"https://api.github.com/users/rubenssoto/repos","events_url":"https://api.github.com/users/rubenssoto/events{/privacy}","received_events_url":"https://api.github.com/users/rubenssoto/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-26T02:36:01Z","updated_at":"2020-07-26T02:36:01Z","author_association":"NONE","body":"Hi Again. 👍 \r\n\r\nWhen I changed the insert option to upsert the performance got worse.\r\n1 Master Node m5.xlarge(4 vcpu, 16gb Ram)\r\n1 Core Node r5.xlarge(4 vcpu, 32gb ram)\r\n4 Task Nodes r5.xlarge(4 vcpu, 32 ram)\r\nspark.yarn.executor.memoryOverhead: 2048\r\nIm reading 10 files on each trigger, at the beginning my file size is 1gb each\r\n\r\n**hudi options**\r\nhudi_options = {\r\n  'hoodie.table.name': tableName,\r\n  'hoodie.datasource.write.recordkey.field': 'id',\r\n  'hoodie.datasource.write.partitionpath.field': 'event_date',\r\n  'hoodie.datasource.write.table.name': tableName,\r\n  'hoodie.datasource.write.operation': 'upsert',\r\n  'hoodie.datasource.write.precombine.field': 'LineCreatedTimestamp',\r\n  'hoodie.datasource.write.hive_style_partitioning': 'true',\r\n  'hoodie.parquet.small.file.limit': 500000000,\r\n  'hoodie.parquet.max.file.size': 900000000,\r\n  'hoodie.datasource.hive_sync.enable': 'true',\r\n  'hoodie.datasource.hive_sync.table': tableName,\r\n  'hoodie.datasource.hive_sync.database': 'datalake_raw',\r\n  'hoodie.datasource.hive_sync.partition_fields': 'event_date',\r\n  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\r\n  'hoodie.datasource.hive_sync.jdbcurl':'jdbc:hive2://ip-10-0-53-190.us-west-2.compute.internal:10000'\r\n}\r\n\r\nI totally understand what you said about hudi metadata and ordering operations, but I'm trying to process only 25gb of data and only on tasks nodes I have more than 100gb of ram, I am probably  doing something wrong hehehe\r\nAll process took 1 hour and 40 minutes.\r\n\r\n<img width=\"1680\" alt=\"Captura de Tela 2020-07-25 às 22 03 23\" src=\"https://user-images.githubusercontent.com/36298331/88469957-3c175100-cecd-11ea-9857-a82e9f249fa1.png\">\r\n<img width=\"1680\" alt=\"Captura de Tela 2020-07-25 às 22 04 03\" src=\"https://user-images.githubusercontent.com/36298331/88469959-40436e80-cecd-11ea-98f6-225b9b30f01d.png\">\r\n<img width=\"1680\" alt=\"Captura de Tela 2020-07-25 às 22 03 34\" src=\"https://user-images.githubusercontent.com/36298331/88469961-42a5c880-cecd-11ea-8c28-996afe0e1547.png\">\r\n\r\n\r\nI tried the same operation in batch mode with insert operation it took 46 minutes, the overall performance it seems much better in batch mode like you could see in the follow image\r\n<img width=\"1680\" alt=\"Captura de Tela 2020-07-25 às 23 22 30\" src=\"https://user-images.githubusercontent.com/36298331/88470018-d37ca400-cecd-11ea-817c-ac28f62a2276.png\">\r\n\r\nbut this batch execution created a lot of 50Mb files, is there way to get better?\r\n\r\n------------\r\n\r\nI think to process big workloads in batch mode with insert operation could be much more scalable, what do you think? My situation is, I have some datasets that I need to process all data, my data has to be deduplicated because is CDC data and after that I need to keep updating the data with streaming. These new datasets will be a source to create many others tables in the company.\r\n\r\nCould you advise me wich could be the better solution? I think, that I could batch all data and after that keep running a streaming solution to keep the data updated.\r\n\r\nLast question, when I run in insert mode on streaming job with foreachbatch, hudi will deduplicate only data that exist inside this specific batch? For example, I'm reading 10 files on each trigger, so, if in the next batch trigger has data that exists in the previous batch trigger, data wont be deduplicate, I'm right?\r\n\r\nThank you so much, and I'm sorry for a lot of query, but I need to use Hudi on production ASAP\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663927931/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663954904","html_url":"https://github.com/apache/hudi/pull/1704#issuecomment-663954904","issue_url":"https://api.github.com/repos/apache/hudi/issues/1704","id":663954904,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzk1NDkwNA==","user":{"login":"bhasudha","id":2179254,"node_id":"MDQ6VXNlcjIxNzkyNTQ=","avatar_url":"https://avatars.githubusercontent.com/u/2179254?v=4","gravatar_id":"","url":"https://api.github.com/users/bhasudha","html_url":"https://github.com/bhasudha","followers_url":"https://api.github.com/users/bhasudha/followers","following_url":"https://api.github.com/users/bhasudha/following{/other_user}","gists_url":"https://api.github.com/users/bhasudha/gists{/gist_id}","starred_url":"https://api.github.com/users/bhasudha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bhasudha/subscriptions","organizations_url":"https://api.github.com/users/bhasudha/orgs","repos_url":"https://api.github.com/users/bhasudha/repos","events_url":"https://api.github.com/users/bhasudha/events{/privacy}","received_events_url":"https://api.github.com/users/bhasudha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-26T07:55:24Z","updated_at":"2020-07-26T07:55:24Z","author_association":"CONTRIBUTOR","body":"> @bhasudha The PR looks good to me. Looks like the same ordering field will be honored in all places. One high level question before I accept it -> If `preCombine` & `combineAndGetUpdateValue` are using the same `orderingVal`, I'm guessing it is expected from the user to use the constructor with the `orderingVal` and up to the user to ensure the `orderingVal` used in the constructor is the same as the one passed in `Map<..>`. If this is true, does `HoodieDeltaStreamer` allow for this kind of constructor invocation ?\r\n> Also, please rebase and push the PR, once the build succeeds can merge it.\r\n\r\n@n3nash Thanks! Yes user needs to ensure that the Map uses the same orderingVal as the one configured for precombine. We are kind of abstracting this logic for users by passing in the precombine field ourselves in the HoodieMergeHandle where this method is invoked. In DeltaStreamer this constructor invocation is happening already here - https://github.com/apache/hudi/blob/0cb24e4a2defd8e639437b6cd145a26f038ef1af/hudi-utilities/src/main/java/org/apache/hudi/utilities/deltastreamer/DeltaSync.java#L341 which passes the cfg.sourceOrderingField for ordering val. And in this PR we are also setting the same cfg.sourceOrderingField in the https://github.com/apache/hudi/pull/1704/files#diff-2f9f867c64be16dd87a816fe90f88e87R517  which is what is used in the MergeHandle - https://github.com/apache/hudi/pull/1704/files#diff-78cfbf493ffb0a24c10b52d9602531a1R222 . Hope that answers your question. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663954904/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663955621","html_url":"https://github.com/apache/hudi/pull/1704#issuecomment-663955621","issue_url":"https://api.github.com/repos/apache/hudi/issues/1704","id":663955621,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzk1NTYyMQ==","user":{"login":"bhasudha","id":2179254,"node_id":"MDQ6VXNlcjIxNzkyNTQ=","avatar_url":"https://avatars.githubusercontent.com/u/2179254?v=4","gravatar_id":"","url":"https://api.github.com/users/bhasudha","html_url":"https://github.com/bhasudha","followers_url":"https://api.github.com/users/bhasudha/followers","following_url":"https://api.github.com/users/bhasudha/following{/other_user}","gists_url":"https://api.github.com/users/bhasudha/gists{/gist_id}","starred_url":"https://api.github.com/users/bhasudha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bhasudha/subscriptions","organizations_url":"https://api.github.com/users/bhasudha/orgs","repos_url":"https://api.github.com/users/bhasudha/repos","events_url":"https://api.github.com/users/bhasudha/events{/privacy}","received_events_url":"https://api.github.com/users/bhasudha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-26T08:01:13Z","updated_at":"2020-07-26T08:01:13Z","author_association":"CONTRIBUTOR","body":"@xushiyan  This PR fails CI due to log limits over threshold - https://travis-ci.org/github/apache/hudi/builds/711685577. I see this error message consistently - `The job exceeded the maximum log length, and has been terminated.` . Wondering if you have any idea on this ? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663955621/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663997844","html_url":"https://github.com/apache/hudi/issues/857#issuecomment-663997844","issue_url":"https://api.github.com/repos/apache/hudi/issues/857","id":663997844,"node_id":"MDEyOklzc3VlQ29tbWVudDY2Mzk5Nzg0NA==","user":{"login":"jackwellsxyz","id":18624942,"node_id":"MDQ6VXNlcjE4NjI0OTQy","avatar_url":"https://avatars.githubusercontent.com/u/18624942?v=4","gravatar_id":"","url":"https://api.github.com/users/jackwellsxyz","html_url":"https://github.com/jackwellsxyz","followers_url":"https://api.github.com/users/jackwellsxyz/followers","following_url":"https://api.github.com/users/jackwellsxyz/following{/other_user}","gists_url":"https://api.github.com/users/jackwellsxyz/gists{/gist_id}","starred_url":"https://api.github.com/users/jackwellsxyz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jackwellsxyz/subscriptions","organizations_url":"https://api.github.com/users/jackwellsxyz/orgs","repos_url":"https://api.github.com/users/jackwellsxyz/repos","events_url":"https://api.github.com/users/jackwellsxyz/events{/privacy}","received_events_url":"https://api.github.com/users/jackwellsxyz/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-26T14:51:56Z","updated_at":"2020-07-26T14:51:56Z","author_association":"NONE","body":"From what I can tell, the Delta Lake file format is open source (https://github.com/delta-io/delta), but many of the optimizations like ZORDER are part of Delta Engine which sits atop Delta Lake and is proprietary. Also, there are no plans to support pandas, or for pandas to support Delta Lake.\r\n![image](https://user-images.githubusercontent.com/18624942/88482208-b3bc9e80-cf14-11ea-8295-ddad58cbba1f.png)\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/663997844/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664024837","html_url":"https://github.com/apache/hudi/pull/1704#issuecomment-664024837","issue_url":"https://api.github.com/repos/apache/hudi/issues/1704","id":664024837,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDAyNDgzNw==","user":{"login":"xushiyan","id":2701446,"node_id":"MDQ6VXNlcjI3MDE0NDY=","avatar_url":"https://avatars.githubusercontent.com/u/2701446?v=4","gravatar_id":"","url":"https://api.github.com/users/xushiyan","html_url":"https://github.com/xushiyan","followers_url":"https://api.github.com/users/xushiyan/followers","following_url":"https://api.github.com/users/xushiyan/following{/other_user}","gists_url":"https://api.github.com/users/xushiyan/gists{/gist_id}","starred_url":"https://api.github.com/users/xushiyan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xushiyan/subscriptions","organizations_url":"https://api.github.com/users/xushiyan/orgs","repos_url":"https://api.github.com/users/xushiyan/repos","events_url":"https://api.github.com/users/xushiyan/events{/privacy}","received_events_url":"https://api.github.com/users/xushiyan/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-26T18:36:27Z","updated_at":"2020-07-26T18:36:27Z","author_association":"MEMBER","body":"Hi @bhasudha this means there are too many logs got printed out during the test. In ideal scenario, a test does not output anything unless there is a failure. Could you check if any log or print statement can be removed?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664024837/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664055034","html_url":"https://github.com/apache/hudi/pull/1848#issuecomment-664055034","issue_url":"https://api.github.com/repos/apache/hudi/issues/1848","id":664055034,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDA1NTAzNA==","user":{"login":"garyli1019","id":23007841,"node_id":"MDQ6VXNlcjIzMDA3ODQx","avatar_url":"https://avatars.githubusercontent.com/u/23007841?v=4","gravatar_id":"","url":"https://api.github.com/users/garyli1019","html_url":"https://github.com/garyli1019","followers_url":"https://api.github.com/users/garyli1019/followers","following_url":"https://api.github.com/users/garyli1019/following{/other_user}","gists_url":"https://api.github.com/users/garyli1019/gists{/gist_id}","starred_url":"https://api.github.com/users/garyli1019/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/garyli1019/subscriptions","organizations_url":"https://api.github.com/users/garyli1019/orgs","repos_url":"https://api.github.com/users/garyli1019/repos","events_url":"https://api.github.com/users/garyli1019/events{/privacy}","received_events_url":"https://api.github.com/users/garyli1019/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-26T23:34:18Z","updated_at":"2020-07-26T23:34:18Z","author_association":"MEMBER","body":"Added support for `PruneFilterScan`. Please review this PR again. Thank you!","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664055034/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664060914","html_url":"https://github.com/apache/hudi/pull/1876#issuecomment-664060914","issue_url":"https://api.github.com/repos/apache/hudi/issues/1876","id":664060914,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDA2MDkxNA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T00:22:05Z","updated_at":"2020-07-27T00:22:05Z","author_association":"MEMBER","body":"Behind on getting the tests to pass again. Working on it ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664060914/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664064544","html_url":"https://github.com/apache/hudi/issues/1847#issuecomment-664064544","issue_url":"https://api.github.com/repos/apache/hudi/issues/1847","id":664064544,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDA2NDU0NA==","user":{"login":"zuyanton","id":67354813,"node_id":"MDQ6VXNlcjY3MzU0ODEz","avatar_url":"https://avatars.githubusercontent.com/u/67354813?v=4","gravatar_id":"","url":"https://api.github.com/users/zuyanton","html_url":"https://github.com/zuyanton","followers_url":"https://api.github.com/users/zuyanton/followers","following_url":"https://api.github.com/users/zuyanton/following{/other_user}","gists_url":"https://api.github.com/users/zuyanton/gists{/gist_id}","starred_url":"https://api.github.com/users/zuyanton/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zuyanton/subscriptions","organizations_url":"https://api.github.com/users/zuyanton/orgs","repos_url":"https://api.github.com/users/zuyanton/repos","events_url":"https://api.github.com/users/zuyanton/events{/privacy}","received_events_url":"https://api.github.com/users/zuyanton/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T00:45:11Z","updated_at":"2020-07-27T00:46:55Z","author_association":"NONE","body":"@umehrot2 @bschell Looks like CSE is disabled on the cluster, however I can see that we still specify CSE key id in cluster config. is ```fs.s3.cse.enabled``` is the only flag that triggers EMR to override ```getLen```?\r\n![cluster_capture](https://user-images.githubusercontent.com/67354813/88493752-b72d4580-cf67-11ea-8371-f8e36897ebbd.PNG)\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664064544/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664069552","html_url":"https://github.com/apache/hudi/issues/1830#issuecomment-664069552","issue_url":"https://api.github.com/repos/apache/hudi/issues/1830","id":664069552,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDA2OTU1Mg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T01:13:07Z","updated_at":"2020-07-27T01:13:07Z","author_association":"MEMBER","body":"@bvaradar any updates from trying this on emr? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664069552/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664077582","html_url":"https://github.com/apache/hudi/pull/1869#issuecomment-664077582","issue_url":"https://api.github.com/repos/apache/hudi/issues/1869","id":664077582,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDA3NzU4Mg==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T01:50:34Z","updated_at":"2020-07-27T01:50:34Z","author_association":"MEMBER","body":"cc @umehrot2 can you please take a pass at this. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664077582/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664250509","html_url":"https://github.com/apache/hudi/pull/1567#issuecomment-664250509","issue_url":"https://api.github.com/repos/apache/hudi/issues/1567","id":664250509,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDI1MDUwOQ==","user":{"login":"hddong","id":17537134,"node_id":"MDQ6VXNlcjE3NTM3MTM0","avatar_url":"https://avatars.githubusercontent.com/u/17537134?v=4","gravatar_id":"","url":"https://api.github.com/users/hddong","html_url":"https://github.com/hddong","followers_url":"https://api.github.com/users/hddong/followers","following_url":"https://api.github.com/users/hddong/following{/other_user}","gists_url":"https://api.github.com/users/hddong/gists{/gist_id}","starred_url":"https://api.github.com/users/hddong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hddong/subscriptions","organizations_url":"https://api.github.com/users/hddong/orgs","repos_url":"https://api.github.com/users/hddong/repos","events_url":"https://api.github.com/users/hddong/events{/privacy}","received_events_url":"https://api.github.com/users/hddong/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T09:55:24Z","updated_at":"2020-07-27T09:55:47Z","author_association":"CONTRIBUTOR","body":"@vinothchandar :\r\nhttps://github.com/apache/hudi/blob/0cb24e4a2defd8e639437b6cd145a26f038ef1af/hudi-common/src/main/java/org/apache/hudi/common/table/TableSchemaResolver.java#L382\r\n`readSchemaFromLogFile` may read the blank file and the blank file had been read before in `HoodieLogFileCommand`(modified to avoid reading the blank files)","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664250509/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664416929","html_url":"https://github.com/apache/hudi/pull/1149#issuecomment-664416929","issue_url":"https://api.github.com/repos/apache/hudi/issues/1149","id":664416929,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDQxNjkyOQ==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T14:05:04Z","updated_at":"2020-07-27T14:05:04Z","author_association":"MEMBER","body":"@yihua I see the tests are still failing? ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664416929/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664433714","html_url":"https://github.com/apache/hudi/issues/1830#issuecomment-664433714","issue_url":"https://api.github.com/repos/apache/hudi/issues/1830","id":664433714,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDQzMzcxNA==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T14:34:13Z","updated_at":"2020-07-27T14:34:13Z","author_association":"CONTRIBUTOR","body":"Some updates @srsteinmetz @vinothchandar \r\n\r\nWe ran more tests during the period of last week. We replicated the following setup(s)\r\n\r\n1. Hot partitions (most incoming \"new\" records going to latest partition(s))  with 10-20% of incoming batch are updates going to \"older\" partitions.  UUID used as record keys\r\n2. Higher velocity - Insert only type workload using a timestamp based record-key .\r\n3. Update only type workload on a static dataset on both COW and MOR table.\r\n\r\nFor all these experiments, we let it ran for around a day.\r\n\r\nIn cases (1), there is variance in processing time which are attributed to either parquet writing or index lookup or both. In this case, the number of files was growing slowly (due to small file handling) compared to (2). \r\nIn case (2) as the dataset keeps growing (number of files kept growing), there is gradual increase in index lookup time (no dynamic allocation) which is expected. \r\nIn case (3), the variance is almost negligible and is due to parquet writing \r\n\r\nTLDR : We cannot reproduce case when the increase in processing time cannot be attributed to the expected stages of parquet writing and/or index lookup.  We are continuing to investigate.\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664433714/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664527869","html_url":"https://github.com/apache/hudi/issues/1586#issuecomment-664527869","issue_url":"https://api.github.com/repos/apache/hudi/issues/1586","id":664527869,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDUyNzg2OQ==","user":{"login":"nandurj","id":67474555,"node_id":"MDQ6VXNlcjY3NDc0NTU1","avatar_url":"https://avatars.githubusercontent.com/u/67474555?v=4","gravatar_id":"","url":"https://api.github.com/users/nandurj","html_url":"https://github.com/nandurj","followers_url":"https://api.github.com/users/nandurj/followers","following_url":"https://api.github.com/users/nandurj/following{/other_user}","gists_url":"https://api.github.com/users/nandurj/gists{/gist_id}","starred_url":"https://api.github.com/users/nandurj/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nandurj/subscriptions","organizations_url":"https://api.github.com/users/nandurj/orgs","repos_url":"https://api.github.com/users/nandurj/repos","events_url":"https://api.github.com/users/nandurj/events{/privacy}","received_events_url":"https://api.github.com/users/nandurj/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T17:19:20Z","updated_at":"2020-07-27T17:19:20Z","author_association":"NONE","body":"This issue got resolved by passing the configs through a props file, e.g:\r\n\r\ncat test.properties \r\nhoodie.datasource.write.recordkey.field=f1,f2,f3\r\nhoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.ComplexKeyGenerator\r\nhoodie.datasource.write.partitionpath.field=dt\r\nhoodie.datasource.hive_sync.database=default\r\nhoodie.datasource.hive_sync.table=hudi_table\r\nhoodie.datasource.hive_sync.partition_fields=dt\r\nhoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.MultiPartKeysValueExtractor \r\nhoodie.deltastreamer.source.dfs.root=s3://test/hudi/data\r\n\r\n\r\nspark-submit --class org.apache.hudi.utilities.deltastreamer.HoodieDeltaStreamer  \\\r\n--jars /usr/lib/spark/external/lib/spark-avro_2.11-2.4.5-amzn-0.jar \\\r\n--master yarn --deploy-mode client \\\r\n~/hudi-utilities-bundle* \\\r\n--table-type COPY_ON_WRITE \\\r\n--source-ordering-field tp \\\r\n--continuous \\\r\n--enable-hive-sync \\\r\n--min-sync-interval-seconds 30 \\\r\n--op UPSERT \\\r\n--source-class org.apache.hudi.utilities.sources.ParquetDFSSource \\\r\n--transformer-class org.apache.hudi.utilities.transform.AWSDmsTransformer \\\r\n--target-base-path s3://test-bucket/hudi/tables/hudi_table--target-table hudi_table \\\r\n--props file:////home/hadoop/test.properties","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664527869/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664551791","html_url":"https://github.com/apache/hudi/pull/1859#issuecomment-664551791","issue_url":"https://api.github.com/repos/apache/hudi/issues/1859","id":664551791,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDU1MTc5MQ==","user":{"login":"satishkotha","id":2992755,"node_id":"MDQ6VXNlcjI5OTI3NTU=","avatar_url":"https://avatars.githubusercontent.com/u/2992755?v=4","gravatar_id":"","url":"https://api.github.com/users/satishkotha","html_url":"https://github.com/satishkotha","followers_url":"https://api.github.com/users/satishkotha/followers","following_url":"https://api.github.com/users/satishkotha/following{/other_user}","gists_url":"https://api.github.com/users/satishkotha/gists{/gist_id}","starred_url":"https://api.github.com/users/satishkotha/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/satishkotha/subscriptions","organizations_url":"https://api.github.com/users/satishkotha/orgs","repos_url":"https://api.github.com/users/satishkotha/repos","events_url":"https://api.github.com/users/satishkotha/events{/privacy}","received_events_url":"https://api.github.com/users/satishkotha/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T18:04:29Z","updated_at":"2020-07-27T18:04:29Z","author_association":"MEMBER","body":"> High level approach LGTM. Can do a more thorough review as a follow up.\r\n> \r\n> can you clarify what the state transitions are for REPLACE? would it be like compaction?\r\n> \r\n> t1.replace.requested, t1.replace.inflight, t1.commit?\r\n> or\r\n> t1.replace.requested, t1.replace.inflight, t1.replace\r\n\r\nSo, in the approach I implemented, we will have both t1.replace and t1.commit files. i.e., t1.replace.requested, t1.replace.inflight, t1.replace, t1.commit\r\n\r\nThere are few reasons for doing this:\r\n1) On the query side, we want to find all replace instants fast, so its important to have '.replace' extension to easily identify all file groups replaced.  We want to keep metadata in '.replace' file small to be able to load it efficiently as well.  Corresponding '.commit' will have additional details for debugging, audit etc\r\n2) replace metadata has different structure from 'commit'. So we cannot convert .replace.inflight to .commit extension.\r\n3) There are many assumptions in the code to say commit action type is tied to table type. For example, action type can only be either '.commit' or '.deltacommit'. [here](https://github.com/apache/hudi/blob/master/hudi-common/src/main/java/org/apache/hudi/common/table/HoodieTableMetaClient.java#L474). Changing this to add replace seemed error prone and tedious. The way we identify if a parquet file is valid is by checking if theres a corresponding '.commit' file. If we just create .replace file, we have to change lot of places to make sure new files created by replace are  used.\r\n\r\nIn short, 't1.replace ' and 't1.commit' together define changes done during t1 instant. After consolidated metadata lands, I think this can be simplified quite a bit. \r\n\r\nI discussed this with few others offline and implemented this approach. But, let me know if you think there is a better way to do this. Its still early stages and i'm happy to implement cleaner approach, if theres one.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664551791/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664580093","html_url":"https://github.com/apache/hudi/issues/1878#issuecomment-664580093","issue_url":"https://api.github.com/repos/apache/hudi/issues/1878","id":664580093,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDU4MDA5Mw==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T19:01:30Z","updated_at":"2020-07-27T19:01:30Z","author_association":"CONTRIBUTOR","body":"With inserts, there is no index lookup which essentially means we are appending to a dataset but making sure we are writing to correct partition path and ensuring file sizing is honored. \r\n\r\nWith upserts, we also do index lookup to ensure record level merge happens fine. Its important to know the workload pattern before I can say what is happening. Can you please attach the output of \"commits show --includeExtraMetadata\". \r\nMore related questions : Is the record key : \"id\" a UUID ? Is there a natural ordering to the keys ? If there is natural ordering, you will benefit by initially using bulk Insert (which sorts and writes the data) during the initial bootstrap followed by  upserts. \r\n\r\nNote that there is fundamental bottleneck with taking in updates for Copy-On-Write mode: If your input  batch has updates present in a large number of files, then those files need to be rewritten.  You can throw in more executors to write them all in parallel but parquet writing will be a lower bound. \r\n\r\nMerge On Read mode performs way better in this case. For Structured streaming, you can try using Merge-On-Read mode (Need to have this PR patch : https://github.com/apache/hudi/pull/1752 to automatically manage compactions).  Here, Hudi does not write Parquet file for every single batch but just writes them to delta-log files.  Note that if you are planning to use DeltaStreamer, Merge-On-Read with async compaction is already supported  in 0.5.x releases which will be ideal for your CDC use-case. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664580093/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664581476","html_url":"https://github.com/apache/hudi/issues/1586#issuecomment-664581476","issue_url":"https://api.github.com/repos/apache/hudi/issues/1586","id":664581476,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDU4MTQ3Ng==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-27T19:04:14Z","updated_at":"2020-07-27T19:04:14Z","author_association":"CONTRIBUTOR","body":"The properties should be passable in both ways (file or through command line params). @pratyakshsharma : Can you try and let us know if this is an issue ?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664581476/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664709899","html_url":"https://github.com/apache/hudi/issues/1847#issuecomment-664709899","issue_url":"https://api.github.com/repos/apache/hudi/issues/1847","id":664709899,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDcwOTg5OQ==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T00:40:26Z","updated_at":"2020-07-28T00:40:26Z","author_association":"CONTRIBUTOR","body":"@zuyanton yes `fs.s3.cse.enabled` is required for client side encryption to kick in. I wonder why you still have the `fs.s3.cse..kms.keyId` there. Also you don't use EmrFS consistent view right ?\r\n\r\nAt this point I don't see a reason for `getLen()` taking time, since like @bvaradar mentioned its just cached when the FileStatus is created. However, I would still suggest trying by removing the unnecessary configurations that you have for EmrFS. Another thing that I would like you to do is enable EmrFS debug logs, by going to `/etc/spark/conf/log4j.properties` and add an entry with `DEBUG` log level for `com.amazon.ws.emr.hadoop.fs` namespace. This should give more information if there are any S3 calls being made during that time of 100ms. If it does not reveal anything, I will try to work with you internally to reproduce the issue.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664709899/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664710483","html_url":"https://github.com/apache/hudi/pull/1880#issuecomment-664710483","issue_url":"https://api.github.com/repos/apache/hudi/issues/1880","id":664710483,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDcxMDQ4Mw==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T00:42:28Z","updated_at":"2020-07-28T00:42:37Z","author_association":"CONTRIBUTOR","body":"@linshan-ma Thanks for your contribution. Two suggestions:\r\n\r\n1) It contains some irrelevant commits you should remove;\r\n2) Each PR must be completed and test-able before merging it into the codebase, otherwise, you can only provide a completed implementation.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664710483/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664711400","html_url":"https://github.com/apache/hudi/pull/1870#issuecomment-664711400","issue_url":"https://api.github.com/repos/apache/hudi/issues/1870","id":664711400,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDcxMTQwMA==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T00:46:07Z","updated_at":"2020-07-28T00:46:07Z","author_association":"CONTRIBUTOR","body":"@zhedoubushishi There is a conflict file. Can you please fix it?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664711400/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664741636","html_url":"https://github.com/apache/hudi/pull/1768#issuecomment-664741636","issue_url":"https://api.github.com/repos/apache/hudi/issues/1768","id":664741636,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDc0MTYzNg==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T02:38:35Z","updated_at":"2020-07-28T02:38:35Z","author_association":"CONTRIBUTOR","body":"> @umehrot2 just landed the changes I mentioned. can we rework this PR and try again . We can make things parallel i.e working for s3 for now. and then we can adjust for HDFS later on. So we should be able close the loop faster.\r\n> \r\n> I do want to get this into 0.6.0 so also please let me know if you are unable to take a stab at this\r\n\r\nWorking on it @vinothchandar. There has been quite a refactoring it seems, which is making the re-basing tricky as now these functions are being called from places which do not even have `spark context`.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664741636/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664755684","html_url":"https://github.com/apache/hudi/pull/1881#issuecomment-664755684","issue_url":"https://api.github.com/repos/apache/hudi/issues/1881","id":664755684,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDc1NTY4NA==","user":{"login":"umehrot2","id":8647012,"node_id":"MDQ6VXNlcjg2NDcwMTI=","avatar_url":"https://avatars.githubusercontent.com/u/8647012?v=4","gravatar_id":"","url":"https://api.github.com/users/umehrot2","html_url":"https://github.com/umehrot2","followers_url":"https://api.github.com/users/umehrot2/followers","following_url":"https://api.github.com/users/umehrot2/following{/other_user}","gists_url":"https://api.github.com/users/umehrot2/gists{/gist_id}","starred_url":"https://api.github.com/users/umehrot2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/umehrot2/subscriptions","organizations_url":"https://api.github.com/users/umehrot2/orgs","repos_url":"https://api.github.com/users/umehrot2/repos","events_url":"https://api.github.com/users/umehrot2/events{/privacy}","received_events_url":"https://api.github.com/users/umehrot2/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T03:32:10Z","updated_at":"2020-07-28T03:32:10Z","author_association":"CONTRIBUTOR","body":"@bvaradar @vinothchandar Master is failing with a compilation issue. Minor fix.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664755684/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664762563","html_url":"https://github.com/apache/hudi/pull/1880#issuecomment-664762563","issue_url":"https://api.github.com/repos/apache/hudi/issues/1880","id":664762563,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDc2MjU2Mw==","user":{"login":"linshan-ma","id":23548282,"node_id":"MDQ6VXNlcjIzNTQ4Mjgy","avatar_url":"https://avatars.githubusercontent.com/u/23548282?v=4","gravatar_id":"","url":"https://api.github.com/users/linshan-ma","html_url":"https://github.com/linshan-ma","followers_url":"https://api.github.com/users/linshan-ma/followers","following_url":"https://api.github.com/users/linshan-ma/following{/other_user}","gists_url":"https://api.github.com/users/linshan-ma/gists{/gist_id}","starred_url":"https://api.github.com/users/linshan-ma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/linshan-ma/subscriptions","organizations_url":"https://api.github.com/users/linshan-ma/orgs","repos_url":"https://api.github.com/users/linshan-ma/repos","events_url":"https://api.github.com/users/linshan-ma/events{/privacy}","received_events_url":"https://api.github.com/users/linshan-ma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T03:59:30Z","updated_at":"2020-07-28T03:59:30Z","author_association":"CONTRIBUTOR","body":"> @linshan-ma Thanks for your contribution. Two suggestions:\r\n> \r\n> 1. It contains some irrelevant commits you should remove;\r\n> 2. Each PR must be completed and test-able before merging it into the codebase, otherwise, you can only provide a completed implementation.\r\n\r\n@yanghua Thank you for your advice。1 I checked.i will remove irrelevant code  2 I have tested the code ,Are you asking me to submit a test class?   3,the code is completed  about build framework .The jiar [HUDI-1126] is other sub-task to Implement in detail","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664762563/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664795679","html_url":"https://github.com/apache/hudi/pull/1881#issuecomment-664795679","issue_url":"https://api.github.com/repos/apache/hudi/issues/1881","id":664795679,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDc5NTY3OQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T06:01:46Z","updated_at":"2020-07-28T06:01:46Z","author_association":"CONTRIBUTOR","body":"@umehrot2 : Landing this. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664795679/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664807718","html_url":"https://github.com/apache/hudi/issues/1845#issuecomment-664807718","issue_url":"https://api.github.com/repos/apache/hudi/issues/1845","id":664807718,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDgwNzcxOA==","user":{"login":"sbernauer","id":29303194,"node_id":"MDQ6VXNlcjI5MzAzMTk0","avatar_url":"https://avatars.githubusercontent.com/u/29303194?v=4","gravatar_id":"","url":"https://api.github.com/users/sbernauer","html_url":"https://github.com/sbernauer","followers_url":"https://api.github.com/users/sbernauer/followers","following_url":"https://api.github.com/users/sbernauer/following{/other_user}","gists_url":"https://api.github.com/users/sbernauer/gists{/gist_id}","starred_url":"https://api.github.com/users/sbernauer/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sbernauer/subscriptions","organizations_url":"https://api.github.com/users/sbernauer/orgs","repos_url":"https://api.github.com/users/sbernauer/repos","events_url":"https://api.github.com/users/sbernauer/events{/privacy}","received_events_url":"https://api.github.com/users/sbernauer/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T06:35:42Z","updated_at":"2020-07-28T06:35:42Z","author_association":"CONTRIBUTOR","body":"Is there anything we can do further to resolve this issue?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664807718/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664834387","html_url":"https://github.com/apache/hudi/pull/1149#issuecomment-664834387","issue_url":"https://api.github.com/repos/apache/hudi/issues/1149","id":664834387,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDgzNDM4Nw==","user":{"login":"yihua","id":2497195,"node_id":"MDQ6VXNlcjI0OTcxOTU=","avatar_url":"https://avatars.githubusercontent.com/u/2497195?v=4","gravatar_id":"","url":"https://api.github.com/users/yihua","html_url":"https://github.com/yihua","followers_url":"https://api.github.com/users/yihua/followers","following_url":"https://api.github.com/users/yihua/following{/other_user}","gists_url":"https://api.github.com/users/yihua/gists{/gist_id}","starred_url":"https://api.github.com/users/yihua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yihua/subscriptions","organizations_url":"https://api.github.com/users/yihua/orgs","repos_url":"https://api.github.com/users/yihua/repos","events_url":"https://api.github.com/users/yihua/events{/privacy}","received_events_url":"https://api.github.com/users/yihua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T07:41:25Z","updated_at":"2020-07-28T07:41:25Z","author_association":"CONTRIBUTOR","body":"@vinothchandar @nsivabalan this PR is ready for another review.\r\n\r\nI fixed the failing tests.  I also simplified the bulk insert logic regarding different sort modes.  Besides, I added more javadocs and cleaned up the code style.\r\n\r\nFor the default sort mode for bulk insert, shall we set it to a mode other than GLOBAL_SORT?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664834387/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664851598","html_url":"https://github.com/apache/hudi/pull/1880#issuecomment-664851598","issue_url":"https://api.github.com/repos/apache/hudi/issues/1880","id":664851598,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NDg1MTU5OA==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T08:17:22Z","updated_at":"2020-07-28T08:17:22Z","author_association":"CONTRIBUTOR","body":"> > @linshan-ma Thanks for your contribution. Two suggestions:\r\n> > \r\n> > 1. It contains some irrelevant commits you should remove;\r\n> > 2. Each PR must be completed and test-able before merging it into the codebase, otherwise, you can only provide a completed implementation.\r\n> \r\n> @yanghua Thank you for your advice。1 I checked.i will remove irrelevant code 2 I have tested the code ,Are you asking me to submit a test class? 3,the code is completed about build framework .The jiar [HUDI-1126] is other sub-task to Implement in detail\r\n\r\nHi,\r\n\r\n> 1 I checked.i will remove irrelevant code\r\n\r\nI mean the irrelevant PRs, as the first version of this PR, it would be better to only contain one commit, right?\r\n\r\n> 2 I have tested the code ,Are you asking me to submit a test class?\r\n\r\nYes, it would be better to add test cases for your changes.\r\n\r\n> 3,the code is completed about build framework .The jiar [HUDI-1126] is other sub-task to Implement in detail\r\n\r\nI mean we should provide a completed feature, especially for some newly introduced features so that the reviewer can make sure all the changes are good for merging into the codebase. Just a suggestion, if you make sure this PR is the basis of subsequent PRs. Please ignore it.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/664851598/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665040109","html_url":"https://github.com/apache/hudi/issues/1825#issuecomment-665040109","issue_url":"https://api.github.com/repos/apache/hudi/issues/1825","id":665040109,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NTA0MDEwOQ==","user":{"login":"asheeshgarg","id":4348671,"node_id":"MDQ6VXNlcjQzNDg2NzE=","avatar_url":"https://avatars.githubusercontent.com/u/4348671?v=4","gravatar_id":"","url":"https://api.github.com/users/asheeshgarg","html_url":"https://github.com/asheeshgarg","followers_url":"https://api.github.com/users/asheeshgarg/followers","following_url":"https://api.github.com/users/asheeshgarg/following{/other_user}","gists_url":"https://api.github.com/users/asheeshgarg/gists{/gist_id}","starred_url":"https://api.github.com/users/asheeshgarg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/asheeshgarg/subscriptions","organizations_url":"https://api.github.com/users/asheeshgarg/orgs","repos_url":"https://api.github.com/users/asheeshgarg/repos","events_url":"https://api.github.com/users/asheeshgarg/events{/privacy}","received_events_url":"https://api.github.com/users/asheeshgarg/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T13:28:24Z","updated_at":"2020-07-28T13:28:24Z","author_association":"NONE","body":"@bvaradar so even if I change the partition such that I have a different partition per day for different datasets so that only one write happens in the partition does it still going to be issue in 0.5.3?","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665040109/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665071390","html_url":"https://github.com/apache/hudi/pull/1876#issuecomment-665071390","issue_url":"https://api.github.com/repos/apache/hudi/issues/1876","id":665071390,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NTA3MTM5MA==","user":{"login":"yanghua","id":2283778,"node_id":"MDQ6VXNlcjIyODM3Nzg=","avatar_url":"https://avatars.githubusercontent.com/u/2283778?v=4","gravatar_id":"","url":"https://api.github.com/users/yanghua","html_url":"https://github.com/yanghua","followers_url":"https://api.github.com/users/yanghua/followers","following_url":"https://api.github.com/users/yanghua/following{/other_user}","gists_url":"https://api.github.com/users/yanghua/gists{/gist_id}","starred_url":"https://api.github.com/users/yanghua/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yanghua/subscriptions","organizations_url":"https://api.github.com/users/yanghua/orgs","repos_url":"https://api.github.com/users/yanghua/repos","events_url":"https://api.github.com/users/yanghua/events{/privacy}","received_events_url":"https://api.github.com/users/yanghua/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T14:23:57Z","updated_at":"2020-07-28T14:23:57Z","author_association":"CONTRIBUTOR","body":"@vinothchandar conflicts...","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665071390/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665107888","html_url":"https://github.com/apache/hudi/issues/1586#issuecomment-665107888","issue_url":"https://api.github.com/repos/apache/hudi/issues/1586","id":665107888,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NTEwNzg4OA==","user":{"login":"pratyakshsharma","id":30863489,"node_id":"MDQ6VXNlcjMwODYzNDg5","avatar_url":"https://avatars.githubusercontent.com/u/30863489?v=4","gravatar_id":"","url":"https://api.github.com/users/pratyakshsharma","html_url":"https://github.com/pratyakshsharma","followers_url":"https://api.github.com/users/pratyakshsharma/followers","following_url":"https://api.github.com/users/pratyakshsharma/following{/other_user}","gists_url":"https://api.github.com/users/pratyakshsharma/gists{/gist_id}","starred_url":"https://api.github.com/users/pratyakshsharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pratyakshsharma/subscriptions","organizations_url":"https://api.github.com/users/pratyakshsharma/orgs","repos_url":"https://api.github.com/users/pratyakshsharma/repos","events_url":"https://api.github.com/users/pratyakshsharma/events{/privacy}","received_events_url":"https://api.github.com/users/pratyakshsharma/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T15:27:33Z","updated_at":"2020-07-28T15:27:33Z","author_association":"CONTRIBUTOR","body":"Will take a look at it. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665107888/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665120755","html_url":"https://github.com/apache/hudi/issues/1825#issuecomment-665120755","issue_url":"https://api.github.com/repos/apache/hudi/issues/1825","id":665120755,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NTEyMDc1NQ==","user":{"login":"bvaradar","id":3021376,"node_id":"MDQ6VXNlcjMwMjEzNzY=","avatar_url":"https://avatars.githubusercontent.com/u/3021376?v=4","gravatar_id":"","url":"https://api.github.com/users/bvaradar","html_url":"https://github.com/bvaradar","followers_url":"https://api.github.com/users/bvaradar/followers","following_url":"https://api.github.com/users/bvaradar/following{/other_user}","gists_url":"https://api.github.com/users/bvaradar/gists{/gist_id}","starred_url":"https://api.github.com/users/bvaradar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bvaradar/subscriptions","organizations_url":"https://api.github.com/users/bvaradar/orgs","repos_url":"https://api.github.com/users/bvaradar/repos","events_url":"https://api.github.com/users/bvaradar/events{/privacy}","received_events_url":"https://api.github.com/users/bvaradar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T15:50:27Z","updated_at":"2020-07-28T15:50:27Z","author_association":"CONTRIBUTOR","body":"@asheeshgarg : Yes, Currently, concurrent writing could interfere with one another as part of automatic rollback process. We are revamping this in 0.6 which will allow parallel writing across partitions. ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665120755/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665155023","html_url":"https://github.com/apache/hudi/pull/1876#issuecomment-665155023","issue_url":"https://api.github.com/repos/apache/hudi/issues/1876","id":665155023,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NTE1NTAyMw==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T16:54:17Z","updated_at":"2020-07-28T16:54:17Z","author_association":"MEMBER","body":"@yanghua on it. still trying to make the tests all pass with master ","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665155023/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null},{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665155548","html_url":"https://github.com/apache/hudi/pull/1149#issuecomment-665155548","issue_url":"https://api.github.com/repos/apache/hudi/issues/1149","id":665155548,"node_id":"MDEyOklzc3VlQ29tbWVudDY2NTE1NTU0OA==","user":{"login":"vinothchandar","id":1179324,"node_id":"MDQ6VXNlcjExNzkzMjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1179324?v=4","gravatar_id":"","url":"https://api.github.com/users/vinothchandar","html_url":"https://github.com/vinothchandar","followers_url":"https://api.github.com/users/vinothchandar/followers","following_url":"https://api.github.com/users/vinothchandar/following{/other_user}","gists_url":"https://api.github.com/users/vinothchandar/gists{/gist_id}","starred_url":"https://api.github.com/users/vinothchandar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinothchandar/subscriptions","organizations_url":"https://api.github.com/users/vinothchandar/orgs","repos_url":"https://api.github.com/users/vinothchandar/repos","events_url":"https://api.github.com/users/vinothchandar/events{/privacy}","received_events_url":"https://api.github.com/users/vinothchandar/received_events","type":"User","user_view_type":"public","site_admin":false},"created_at":"2020-07-28T16:55:19Z","updated_at":"2020-07-28T16:55:19Z","author_association":"MEMBER","body":">For the default sort mode for bulk insert, shall we set it to a mode other than GLOBAL_SORT?\r\n\r\nyes. sg. we can retain existing behavior.","reactions":{"url":"https://api.github.com/repos/apache/hudi/issues/comments/665155548/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"performed_via_github_app":null}]